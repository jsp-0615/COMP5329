{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":98164,"databundleVersionId":11699570,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score\nfrom torchvision import transforms\nfrom torchvision.models import efficientnet_b2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset,DataLoader\nfrom PIL import Image\nfrom torchvision import transforms\nfrom transformers import BertTokenizer\nimport os\nfrom torchvision.models import resnet50,resnet18 \nfrom transformers import AutoModel, AutoTokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dir_path=\"/kaggle/input/multi-label-classification-competition-2025/COMP5329S1A2Dataset/\"\ntrain_file_name=\"train.csv\"\ntest_file_name =\"test.csv\"\ntrain_csv = pd.read_csv(os.path.join(dir_path,train_file_name),usecols=[0,1,2])\ntest_csv = pd.read_csv(os.path.join(dir_path,test_file_name),usecols=[0,1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_label_lists = train_csv['Labels'].apply(lambda x: list(map(int, x.split())))\nflat_labels = [label for sublist in all_label_lists for label in sublist]\nnum_classes = max(flat_labels) + 1\ndef multilable_to_onehot(label_str,num_classes):\n    labels=list(map(int,label_str.split()))\n    onehot= np.zeros(num_classes,dtype=np.float32)\n    onehot[labels]=1\n    return onehot\ntrain_csv[\"onehot\"]= train_csv['Labels'].apply(lambda x: multilable_to_onehot(x,num_classes))\ntrain_csv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pre-processing","metadata":{}},{"cell_type":"code","source":"print(\"Number of Training data:\",train_csv.shape[0])\nclass_counts=train_csv['Labels'].value_counts()\nprint(class_counts)\n# classify different labels\nsmall_classes = class_counts[class_counts < 0.01 * class_counts.sum()]\nother_count = small_classes.sum()\nclass_counts = class_counts[class_counts >= 0.01 * class_counts.sum()]\nclass_counts['Classes < 1%'] = other_count\n\n# Plot pie chart\ncolors = sns.color_palette(\"Set2\", n_colors=len(class_counts))\n\n# 画饼图\nplt.figure(figsize=(8, 8))\nplt.pie(\n    class_counts.values,\n    labels=class_counts.index,\n    autopct='%1.1f%%',\n    startangle=140,\n    colors=colors,\n    wedgeprops={'edgecolor': 'white'}\n)\nplt.title(\"Class Distribution\")\nplt.axis('equal')\nplt.tight_layout()\nplt.show()\n\n\n#plot bar chart\nlabel_counts=train_csv['onehot'].sum(axis=0)\nplt.figure(figsize=(12,8))\nbars=plt.bar(range(len(label_counts)),label_counts)\nfor i, count in enumerate(label_counts):\n    plt.text(i,count+0.5, str(int(count)), ha='center', va='bottom')\n\nplt.xlabel(\"Label Index\")\nplt.ylabel(\"Count\")\nplt.title(\"Label Distribution in Multi-label Dataset\")\nplt.xticks(range(len(label_counts))) \nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#we have 30k images, which is big enough, just do some kind of augmentation\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor()\n])\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\nclass Ass2Dataset(Dataset):\n    def __init__(self, df, image_dir, tokenizer, transform=None, max_length=128):\n        self.df = df\n        self.image_dir = image_dir\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = os.path.join(self.image_dir, row['ImageID'])\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"No Such file idx={idx}, error={e}\")\n            return self.__getitem__((idx + 1) % len(self))\n          \n        if self.transform:\n            image = self.transform(image)\n    \n        # 处理文本\n        text = row['Caption']\n        try:\n            encoded = self.tokenizer(\n                text,\n                padding='max_length',\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n        except Exception as e:\n            print(f\"No text idx={idx}, error={e}\")\n            return self.__getitem__((idx + 1) % len(self))\n    \n        input_ids = encoded['input_ids'].squeeze(0)\n        attention_mask = encoded['attention_mask'].squeeze(0)\n    \n        labels = torch.tensor(row['onehot']).float()\n    \n        return {\n            'image': image,\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'label': labels\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\nalpha=1.0/(label_counts+1e-6)\nalpha=alpha/alpha.sum()\nclass BCEFocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0, bce_weight=0.5):\n        super(BCEFocalLoss, self).__init__()\n        self.alpha = alpha  # Tensor of shape (num_classes,) or None\n        self.gamma = gamma\n        self.bce_weight = bce_weight\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')  # no reduction, we reduce manually\n\n    def forward(self, inputs, targets):\n        bce_loss = self.bce(inputs, targets)\n\n        probas = torch.sigmoid(inputs)\n        pt = torch.where(targets == 1, probas, 1 - probas)\n\n        focal_loss = bce_loss * ((1 - pt) ** self.gamma)\n\n        if self.alpha is not None:\n            alpha = self.alpha.to(inputs.device)\n            bce_loss = bce_loss * alpha\n            focal_loss = focal_loss * alpha\n\n        loss = self.bce_weight * bce_loss.mean() + (1 - self.bce_weight) * focal_loss.mean()\n        return loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class MultiModalClassifier(nn.Module):\n    def __init__(self, num_labels=20, resnet_out=256, bert_out=256, dropout=0.3):\n        super(MultiModalClassifier, self).__init__()\n\n        # 使用 ResNet18\n        self.resnet18 = resnet18(pretrained=True)\n        self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, resnet_out)\n\n        # 使用 Google 官方小 BERT\n        self.bert = AutoModel.from_pretrained(\"google/bert_uncased_L-4_H-256_A-4\")\n        self.text_fc = nn.Linear(bert_out, bert_out)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(resnet_out + bert_out, 256),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, num_labels)\n        )\n\n    def forward(self, image, input_ids, attention_mask):\n        image_features = self.resnet18(image)\n        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = bert_output.last_hidden_state[:, 0, :]\n        text_features = self.text_fc(text_features)\n        fused = torch.cat((image_features, text_features), dim=1)\n        return self.classifier(fused)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(dataloader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        outputs = model(batch['image'], batch['input_ids'], batch['attention_mask'])\n        loss = criterion(outputs, batch['label'])\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\n\ndef evaluate(model, dataloader, device, threshold=0.5):\n    model.eval()\n    all_preds, all_targets = [], []\n    with torch.no_grad():\n        for batch in tqdm(dataloader):\n            batch = {k: v.to(device) for k, v in batch.items()}\n\n            outputs = model(batch['image'], batch['input_ids'], batch['attention_mask'])\n            preds = torch.sigmoid(outputs) > threshold\n\n            all_preds.append(preds.cpu().numpy())\n            all_targets.append(batch['label'].cpu().numpy())\n\n    all_preds = np.concatenate(all_preds)\n    all_targets = np.concatenate(all_targets)\n\n    micro_f1 = f1_score(all_targets, all_preds, average='micro', zero_division=0)\n    macro_f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n    return micro_f1, macro_f1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.models import resnet50\ndef get_model_size(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    model_size_mb = total_params * 4 / (1024 ** 2)  # float32 = 4 bytes\n\n    print(f\"✅ 模型总参数量: {total_params:,}\")\n    print(f\"✅ 可训练参数量: {trainable_params:,}\")\n    print(f\"✅ 模型大小（内存占用，float32）: {model_size_mb:.2f} MB\")\n\n    return total_params, model_size_mb\ndef main():\n    tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-4_H-256_A-4\")\n    train_df = train_csv.sample(frac=0.8, random_state=42)\n    val_df = train_csv.drop(train_df.index)\n    \n    train_df = train_df.reset_index(drop=True)\n    val_df = val_df.reset_index(drop=True)\n    train_dataset = Ass2Dataset(train_df, os.path.join(dir_path,\"data\"), tokenizer, train_transform)\n    val_dataset = Ass2Dataset(val_df, os.path.join(dir_path,\"data\"), tokenizer, val_transform)\n    \n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = MultiModalClassifier().to(device)\n    get_model_size(model)\n    # 根据标签频率初始化 alpha\n    label_freq = torch.tensor(train_csv[\"onehot\"].tolist()).sum(dim=0)\n    alpha = 1.0 / (label_freq + 1e-6)\n    alpha = alpha / alpha.sum()\n    \n    criterion = BCEFocalLoss(alpha=alpha, gamma=2, bce_weight=0.5)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n    \n    for epoch in range(10):\n        train_loss = train_one_epoch(model,train_loader,criterion,optimizer,device=device)\n        micro_f1, macro_f1 = evaluate(model,val_loader,device=device)\n    \n        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Micro-F1: {micro_f1:.4f} | Macro-F1: {macro_f1:.4f}\")\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}