{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.197360Z",
     "start_time": "2025-03-17T23:44:57.192410Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import time\n",
    "import seaborn as sns\n",
    "import math\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.284703Z",
     "start_time": "2025-03-17T23:44:57.268578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('Start time: ', time.ctime())\n",
    "        start_time = time.time()  # start time\n",
    "\n",
    "        result = func(*args, **kwargs)  # run\n",
    "\n",
    "        end_time = time.time()  # end time\n",
    "        print('End time: ', time.ctime())\n",
    "        print(f\"{func.__name__} executed in {(end_time - start_time):.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ],
   "id": "98e36a535e7c2b42",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pre-process\n",
    "\n",
    "Min-max normalization:\n",
    "\n",
    "$$x_{min-max} = {{x-min(x)}\\over{max(x)-min(x)}}$$\n",
    "\n",
    "Standardization:\n",
    "\n",
    "$$x_{norm} = {{x-\\mu}\\over{\\sigma}}$$"
   ],
   "id": "1ea172e08505f1e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.289423Z",
     "start_time": "2025-03-17T23:44:57.284703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pre_processing(X, mode=None):\n",
    "    if mode == 'min-max':\n",
    "        print('Pre-process: min-max normalization')\n",
    "        min_each_feature = np.min(X, axis=0)\n",
    "        max_each_feature = np.max(X, axis=0)\n",
    "        scale = max_each_feature - min_each_feature\n",
    "        scale[scale == 0] = 1   # To avoid divided by 0\n",
    "        scaled_train = (X - min_each_feature) / scale\n",
    "        return scaled_train\n",
    "\n",
    "    if mode == 'standardization':\n",
    "        print('Pre-process: standardization')\n",
    "        std_each_feature = np.std(X, axis=0)\n",
    "        mean_each_feature = np.mean(X, axis=0)\n",
    "        std_each_feature[std_each_feature == 0] = 1     # To avoid divided by 0\n",
    "        norm_train = (X - mean_each_feature) / std_each_feature\n",
    "        norm_test = (X - mean_each_feature) / std_each_feature\n",
    "        return norm_train\n",
    "\n",
    "    print('No pre-process')\n",
    "\n",
    "    return X"
   ],
   "id": "2578bdbe5f4e1107",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.303837Z",
     "start_time": "2025-03-17T23:44:57.299932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def accuracy(y_hat,y):\n",
    "    '''\n",
    "    y_hat : predicted value\n",
    "    :param y_hat: [batch_size,num_of_class]\n",
    "    :param y: [batch_size,1\n",
    "    :return: \n",
    "    '''\n",
    "    preds=y_hat.argmax(axis=1,keepdims=True)\n",
    "    return np.mean(preds == y)*100"
   ],
   "id": "5f6ecbfb8386710",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.321274Z",
     "start_time": "2025-03-17T23:44:57.303837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_gain(nonlinearity, param=None):\n",
    "    r\"\"\"Return the recommended gain value for the given nonlinearity function.\n",
    "    The values are as follows:\n",
    "\n",
    "    ================= ====================================================\n",
    "    nonlinearity      gain\n",
    "    ================= ====================================================\n",
    "    Linear / Identity :math:`1`\n",
    "    Conv{1,2,3}D      :math:`1`\n",
    "    Sigmoid           :math:`1`\n",
    "    Tanh              :math:`\\frac{5}{3}`\n",
    "    ReLU              :math:`\\sqrt{2}`\n",
    "    Leaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\n",
    "    SELU              :math:`\\frac{3}{4}`\n",
    "    ================= ====================================================\n",
    "    \"\"\"\n",
    "\n",
    "    if nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    elif nonlinearity == 'leaky_relu':\n",
    "        if param is None:\n",
    "            negative_slope = 0.01\n",
    "        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n",
    "            # True/False are instances of int, hence check above\n",
    "            negative_slope = param\n",
    "        else:\n",
    "            raise ValueError(f\"negative_slope {param} not a valid number\")\n",
    "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
    "    elif nonlinearity == 'selu':\n",
    "        return 3.0 / 4  # Value found empirically (https://github.com/pytorch/pytorch/pull/50664)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported nonlinearity {nonlinearity}\")\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(array):\n",
    "    dimensions = len(array.shape)\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n",
    "\n",
    "    num_input_fmaps = array.shape[1]\n",
    "    num_output_fmaps = array.shape[0]\n",
    "    receptive_field_size = 1\n",
    "    if dimensions > 2:\n",
    "        # math.prod is not always available, accumulate the product manually\n",
    "        # we could use functools.reduce but that is not supported by TorchScript\n",
    "        for s in array.shape[2:]:\n",
    "            receptive_field_size *= s\n",
    "    fan_in = num_input_fmaps * receptive_field_size\n",
    "    fan_out = num_output_fmaps * receptive_field_size\n",
    "\n",
    "    return fan_in, fan_out\n",
    "\n",
    "def _calculate_correct_fan(array, mode):\n",
    "    mode = mode.lower()\n",
    "    valid_modes = ['fan_in', 'fan_out']\n",
    "    if mode not in valid_modes:\n",
    "        raise ValueError(f\"Mode {mode} not supported, please use one of {valid_modes}\")\n",
    "\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(array)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "def kaiming_normal_(array: np.array, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'relu'):\n",
    "    fan = _calculate_correct_fan(array, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    return np.random.normal(0, std, array.shape)"
   ],
   "id": "ac167cc0d89add8",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.332189Z",
     "start_time": "2025-03-17T23:44:57.322783Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "66b30fbd3542f3ca",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.353162Z",
     "start_time": "2025-03-17T23:44:57.347706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Parameter(object):\n",
    "    \"\"\"Parameter class for saving data and gradients\"\"\"\n",
    "    def __init__(self, data, requires_grad, skip_decay=False):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.skip_decay = skip_decay\n",
    "        self.requires_grad = requires_grad"
   ],
   "id": "785071ee87e7f9d7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.377233Z",
     "start_time": "2025-03-17T23:44:57.363665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        self.name = name\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "    def _forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    def _backward(self, *args):\n",
    "        pass"
   ],
   "id": "767328eb1e97167e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.384039Z",
     "start_time": "2025-03-17T23:44:57.379735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _backward(self, delta):\n",
    "        delta[self.x <= 0] = 0\n",
    "        return delta"
   ],
   "id": "8c6d47201896a516",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Forward:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{xW} + \\mathbf{b}$$\n",
    "\n",
    "Backward:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{W}^T$$\n",
    "\n",
    "Gradient of W:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{x}^T$$\n",
    "\n",
    "Gradient of b:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{b}} = \\frac{\\partial L}{\\partial \\mathbf{y}} $$\n",
    "\n",
    "Gradient of x:\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{W}^T$$"
   ],
   "id": "6d64cedcd4ee0dc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.421061Z",
     "start_time": "2025-03-17T23:44:57.411216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FCLayer(Layer):\n",
    "    def  __init__(self,name: str,n_in: int,n_out: int)->None:\n",
    "        '''\n",
    "        Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,)\n",
    "        :param n_in: dimensionality of input\n",
    "        :param n_out: number of hidden units\n",
    "        '''\n",
    "        super().__init__(name, requires_grad=True)\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        W = kaiming_normal_(np.array([0] * n_in * n_out).reshape(n_in, n_out), a=math.sqrt(5))\n",
    "        self.W = Parameter(W, self.requires_grad)\n",
    "        self.b = Parameter(np.zeros(n_out), self.requires_grad)\n",
    "        \n",
    "       \n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "            x: [batch size, n_in]\n",
    "            W: [n_in, n_out]\n",
    "            b: [n_out]\n",
    "        \"\"\"\n",
    "        self.x=x\n",
    "\n",
    "        #[batch_size,n_in] @ [n_in,n_out] + [n_output] => [batch_size,n_out]\n",
    "        return x @ self.W.data + self.b.data\n",
    "\n",
    "    def _backward(self, delta: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        delta: the gradient of the loss function respect to this layer's output 这层损失函数对于这层输出的梯度\n",
    "        :param delta: [batch size, n_out]:\n",
    "        :return: \n",
    "        '''\n",
    "        batch_size = delta.shape[0]\n",
    "        self.W.grad = self.x.T @ delta / batch_size # [batch_size,n_in]^T @ [batch size, n_out] => [n_in,n_out]\n",
    "        self.b.grad = delta.sum(axis=0) / batch_size #divide by batch size to get average of gradient\n",
    "        return delta @ self.W.data.T # return the gradient of input(x) back to last layer \n"
   ],
   "id": "e5b00fb819887fa6",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Softmax \n",
    "Formula:\n",
    "$$softmax(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}$$\n",
    "\n",
    "不需要计算 softmax 的完整 Jacobian 矩阵，因为与交叉熵结合后公式极大简化了。\n",
    "只需用 preds - ground_truth 作为梯度，这个计算在 CrossEntropyLoss 里完成了：\n",
    "self.grad = preds - ground_truth\n",
    "因此，在 softmax.backward(delta) 时，不需要额外计算 softmax 的梯度，而是直接返回 delta\n"
   ],
   "id": "86a2cacdcffb889f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.436048Z",
     "start_time": "2025-03-17T23:44:57.429215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self,name,requires_grad=False):\n",
    "        super().__init__(name,requires_grad)\n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        x_exp =  np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return x_exp/x_exp.sum(axis=1, keepdims=True)\n",
    "    def _backward(self, delta: np.ndarray) -> np.ndarray:\n",
    "        return delta"
   ],
   "id": "2883c1fd2bf68ed0",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loss Function - Cross Entropy\n",
    "Formula:\n",
    "$$CrossEntropy= - \\sum_{i=1}^{n} y_i log(\\hat {y_i})$$\n",
    "\n",
    "Gradient of softmax:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_k} = \\sum_{i}^{c} \\left( \\frac{\\partial L}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_k} \\right)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y}_i} = - \\frac{y_i}{\\hat{y}_i}, \\qquad \\frac{\\partial \\hat{y}_i}{\\partial z_k} = \\begin{cases}\n",
    "\\hat{y}_i(1 - \\hat{y}_i) & \\text{if } i = k \\\\\n",
    "-\\hat{y}_k\\hat{y}_i & \\text{if } i \\neq k\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_k} = - \\left( (y_k(1 - \\hat{y}_k)) - \\sum_{i \\neq k}^{c} y_i \\hat{y}_k \\right) = -(y_k - \\hat{y}_k \\sum_{i}^{c} y_i) = \\hat{y}_k - y_k\n",
    "$$\n",
    "\n",
    "$$=> \\frac{\\partial L}{\\partial z} = \\hat{y} - y$$\n"
   ],
   "id": "8860ec1e183838c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.466835Z",
     "start_time": "2025-03-17T23:44:57.460982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossEntropy(object):\n",
    "    def __init__(self):\n",
    "        self.softmax = Softmax('softmax')\n",
    "\n",
    "    def __call__(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "\n",
    "        :param x:\n",
    "        :param y: [batch_size, 1]\n",
    "        :return:\n",
    "        '''\n",
    "        self.batch_size = x.shape[0]\n",
    "        self.class_num = x.shape[1]\n",
    "\n",
    "        y_hat = self.softmax._forward(x) #[batch_size,num_class]\n",
    "\n",
    "        y=self.one_hot_encoding(y)\n",
    "        self.grad = y_hat - y\n",
    "\n",
    "        loss = -1 * (y * np.log(y_hat + 1e-8)).sum() / self.batch_size  # to avoid divided by 0\n",
    "        return loss\n",
    "\n",
    "    def one_hot_encoding(self, x):\n",
    "        one_hot_encoded = np.zeros((self.batch_size, self.class_num))\n",
    "        one_hot_encoded[np.arange(x.shape[0]), x.flatten()] = 1\n",
    "        return one_hot_encoded"
   ],
   "id": "bdb959962b6db01c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.498710Z",
     "start_time": "2025-03-17T23:44:57.492811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(object):\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.num_layers =0\n",
    "        self.params=[]\n",
    "    def add_layer(self,layer):\n",
    "        self.layers.append(layer)\n",
    "        self.num_layers+=1\n",
    "        if layer.requires_grad:\n",
    "            if hasattr(layer,'W'):\n",
    "                self.params.append(layer.W)\n",
    "            if hasattr(layer,'b'):\n",
    "                self.params.append(layer.b)\n",
    "            # if hasattr(layer,'gamma'):\n",
    "            #     self.params.append(layer.gamma)\n",
    "            # if hasattr(layer,'beta'):\n",
    "            #     self.params.append(layer.beta)\n",
    "    def _forward(self,x: np.ndarray) -> np.ndarray:\n",
    "        for layer in self.layers:\n",
    "            x = layer._forward(x)\n",
    "        return x\n",
    "    def _backward(self,x: np.ndarray) -> np.ndarray:\n",
    "        #backward from the last layer to the first layer\n",
    "        for layer in self.layers[::-1]:\n",
    "            x = layer._backward(x)\n",
    "        return x\n",
    "    def _fit(self,mode='train'):\n",
    "        if mode=='train':\n",
    "            for layer in self.layers:\n",
    "                layer.train=True\n",
    "        elif mode=='eval':\n",
    "            for layer in self.layers:\n",
    "                layer.train=False"
   ],
   "id": "d220debee7e093de",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### SGD with Momentum\n",
    "SGD Formula:\n",
    "$$θ_{t+1}=θ_t−\\eta  \\cdot ∇L(θ_t)$$\n",
    "\n",
    "Momentum 梯度下降的公式如下：\n",
    "$$\n",
    "\\begin{equation}\n",
    "v_t = \\beta v_{t-1} - \\eta \\nabla L(\\theta_t)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t + v_t\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$$\n",
    "    \\( v_t \\) 是当前动量\\\\\n",
    "    \\( \\beta \\) 是动量系数（通常取 0.9）\\\\\n",
    "    \\( \\eta \\) 是学习率\\\\\n",
    "    \\( \\nabla L(\\theta_t) \\) 是损失函数对参数的梯度\n",
    "$$"
   ],
   "id": "d37f07c8cd64eb59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.542170Z",
     "start_time": "2025-03-17T23:44:57.536748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGDMomentum(object):\n",
    "    def __init__(self,parameters,lr=0.01,momentum=0.9,weight_decay=0.0001):\n",
    "        self.parameters = parameters\n",
    "        self.momentum = momentum\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.v = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "    def step(self):\n",
    "        for i,(v,p) in enumerate(zip(self.v,self.parameters)):\n",
    "            if not p.skip_decay:\n",
    "                p.data -= self.weight_decay * p.data\n",
    "            v = self.momentum * v + self.lr * p.grad\n",
    "            self.v[i] = v\n",
    "            p.data -= self.v[i]"
   ],
   "id": "f414f1bd779f820b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Normalization\n",
    "Forward:\n",
    "$$\\mathbf{y}=\\gamma\\frac{\\mathbf{x}-E(\\mathbf{x})}{\\sqrt{\\sigma^2_B+\\epsilon}}+\\beta$$\n",
    "\n",
    "$E(\\mathbf{x})$ is the mean of the current mini-batch, $\\sigma^2_B$ is the variance of the current mini-batch\n",
    "\n",
    "Backward:\n"
   ],
   "id": "2eecd3ec6ec4360c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.573099Z",
     "start_time": "2025-03-17T23:44:57.568411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BatchNormalization(Layer):\n",
    "    def __init__(self,name,epsilon=1e-5):\n",
    "        super().__init__(name,requires_grad=True)\n",
    "        self.epsilon = epsilon\n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "        "
   ],
   "id": "a471a89cc91ed1b",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.599572Z",
     "start_time": "2025-03-17T23:44:57.584118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AverageMeterics(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ],
   "id": "9e82f74a1f63aea1",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.609251Z",
     "start_time": "2025-03-17T23:44:57.599572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adam:\n",
    "    pass\n",
    "\n",
    "\n",
    "class CosineLR:\n",
    "    pass\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self,config,model=None,train_loader=None,valid_loader=None):\n",
    "        self.config=config\n",
    "        self.epochs = self.config['epoch']\n",
    "        self.lr=self.config['lr']\n",
    "        self.model=model\n",
    "        self.train_loader=train_loader\n",
    "        self.valid_loader=valid_loader\n",
    "        self.print_freq=self.config['print_freq']\n",
    "        # self.scheduler= self.config['scheduler']\n",
    "        self.train_precision=[]\n",
    "        self.valid_precision=[]\n",
    "        self.train_loss=[]\n",
    "        self.valid_loss=[]\n",
    "        self.criterion=CrossEntropy()\n",
    "        if self.config['optimizer'] == 'sgd':\n",
    "            self.optimizer = SGDMomentum(self.model.params,self.lr,self.config['momentum'],self.config['weight_decay'])\n",
    "        # elif self.config['optimizer'] == 'adam':\n",
    "        #     self.optimizer = Adam(self.model.params, self.lr, self.config['weight_decay'])\n",
    "        # if self.scheduler == 'cos':\n",
    "        #     self.train_scheduler = CosineLR(self.optimizer, T_max=self.epochs)\n",
    "    @timer\n",
    "    def train(self):\n",
    "        best_accuracy=0\n",
    "        for epoch in range(self.epochs):\n",
    "            print('current lr {:.5e}'.format(self.optimizer.lr))\n",
    "            self.train_per_epoch(epoch)\n",
    "            acc1 = self.validate(epoch)\n",
    "\n",
    "            # remember best prec@1\n",
    "            best_acc1 = max(acc1, best_accuracy)\n",
    "            output_best = 'Best Prec@1: %.3f\\n' % (best_acc1)\n",
    "            print(output_best)\n",
    "    def train_per_epoch(self,epoch):\n",
    "        batch_time=AverageMeterics()\n",
    "        losses=AverageMeterics()\n",
    "        top1=AverageMeterics()        \n",
    "        self.model._fit()\n",
    "        end_time = time.time()\n",
    "        for i,(X,y) in enumerate(self.train_loader):\n",
    "            y_hat=self.model._forward(X)\n",
    "            loss=self.criterion(y_hat,y)\n",
    "            \n",
    "            self.model._backward(self.criterion.grad)\n",
    "            self.optimizer.step()\n",
    "            precision=accuracy(y_hat,y)\n",
    "            losses.update(loss,X.shape[0])\n",
    "            top1.update(precision,X.shape[0])\n",
    "            \n",
    "            batch_time.update(time.time() - end_time)\n",
    "            end_time = time.time()\n",
    "            if (i%self.print_freq ==0) or (i==len(self.train_loader)-1):\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        epoch + 1, i, len(self.train_loader) - 1, batch_time=batch_time,\n",
    "                        loss=losses, top1=top1))\n",
    "        print('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} Loss: {losses.avg:.4f}'.format(epoch=epoch + 1 , flag='train', top1=top1, losses=losses))\n",
    "        self.train_loss.append(losses.avg)\n",
    "        self.train_precision.append(top1.avg)\n",
    "    def validate(self, epoch):\n",
    "        batch_time = AverageMeterics()\n",
    "        losses = AverageMeterics()\n",
    "        top1 = AverageMeterics()\n",
    "\n",
    "        self.model._fit(mode='test')\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (X, y) in enumerate(self.valid_loader):\n",
    "            # compute output\n",
    "            y_hat = self.model._forward(X)\n",
    "            loss = self.criterion(y_hat, y)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            precision = accuracy(y_hat, y)\n",
    "            losses.update(loss, X.shape[0])\n",
    "            top1.update(precision, X.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if (i % self.print_freq == 0) or (i == len(self.valid_loader) - 1):\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Accuracy {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                    i, len(self.valid_loader) - 1, batch_time=batch_time, loss=losses,\n",
    "                    top1=top1))\n",
    "\n",
    "        print('EPOCH: {epoch} {flag} Results: Accuracy {top1.avg:.3f} Loss: {losses.avg:.4f}'.format(epoch=epoch + 1,\n",
    "                                                                                                   flag='val',\n",
    "                                                                                                   top1=top1,\n",
    "                                                                                                   losses=losses))\n",
    "        self.valid_loss.append(losses.avg)\n",
    "        self.valid_precision.append(top1.avg)\n",
    "\n",
    "        return top1.avg"
   ],
   "id": "bf29c48b65013fe3",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.627900Z",
     "start_time": "2025-03-17T23:44:57.616008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, name, drop_rate, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.fix_value = 1 / (1 - self.drop_rate)   # to keep average fixed\n",
    "\n",
    "    def _forward(self, x):\n",
    "        if self.train:\n",
    "            self.mask = np.random.uniform(0, 1, x.shape) > self.drop_rate\n",
    "            return x * self.mask * self.fix_value\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def _backward(self, grad_output):\n",
    "        if self.train:\n",
    "            return grad_output * self.mask\n",
    "        else:\n",
    "            return grad_output"
   ],
   "id": "66ffb78d9831ae51",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.636731Z",
     "start_time": "2025-03-17T23:44:57.630902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dataloader(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=True, seed=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.index = np.arange(X.shape[0])\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            if self.seed is not None:\n",
    "                np.random.seed(self.seed)\n",
    "            np.random.shuffle(self.index)\n",
    "        self.n = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.n >= len(self.index):\n",
    "            raise StopIteration\n",
    "\n",
    "        index = self.index[self.n:self.n + self.batch_size]\n",
    "        batch_X = self.X[index]\n",
    "        batch_y = self.y[index]\n",
    "        self.n += self.batch_size\n",
    "\n",
    "        return batch_X, batch_y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            num of batch\n",
    "        \"\"\"\n",
    "        return (len(self.index) + self.batch_size - 1) // self.batch_size  # ceiling"
   ],
   "id": "66f87db9952bdaef",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.650712Z",
     "start_time": "2025-03-17T23:44:57.636731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {'linear': FCLayer,'relu': ReLU,'dropout': Dropout}\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    return model\n",
    "\n",
    "\n",
    "layers=[\n",
    "    {'type': 'linear','params':{'name':'fc1','n_in':128,'n_out':256}},\n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.3}},\n",
    "    {'type':'relu', 'params': {'name': 'relu1'}},\n",
    "    {'type':'linear', 'params': {'name': 'fc2', 'n_in':256,'n_out':128}},\n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.3}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}},\n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'n_in': 128, 'n_out': 10}},\n",
    "]\n",
    "batch_size=1024\n",
    "config={'layers': layers,'lr': 0.01,'batch_size': batch_size,'momentum': 0.9,'weight_decay': 5e-4,'seed': 0,'epoch': 200,\n",
    "    'optimizer': 'sgd',     # adam, sgd\n",
    "    'scheduler': None,      # cos, None\n",
    "    'pre-process': 'standardization',      # min-max, standardization, None\n",
    "    'print_freq': 50000 // batch_size // 5\n",
    "}\n",
    "np.random.seed(config['seed'])"
   ],
   "id": "d76807819d7f07c2",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.705728Z",
     "start_time": "2025-03-17T23:44:57.650712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir_path='E:\\\\Postgraduate\\\\25S1\\\\COMP5329\\\\Assignment\\\\Assignment1\\\\Assignment1-Dataset\\\\'\n",
    "train_file='train_data.npy'\n",
    "train_label_file='train_label.npy'\n",
    "train_data=np.load(dir_path+train_file)\n",
    "train_label=np.load(dir_path+train_label_file)\n",
    "test_file='test_data.npy'\n",
    "test_label_file='test_label.npy'"
   ],
   "id": "649ce56cf5d9e69a",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.713315Z",
     "start_time": "2025-03-17T23:44:57.706992Z"
    }
   },
   "cell_type": "code",
   "source": "model = build_model(config['layers'])",
   "id": "b000da5de410dfaa",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:45:05.489042Z",
     "start_time": "2025-03-17T23:44:57.713315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_X=pre_processing(train_data,config['pre-process'])\n",
    "train_dataloader=Dataloader(train_X, train_label, config['batch_size'], shuffle=True, seed=config['seed'])\n",
    "test_X=np.load(dir_path+test_file)\n",
    "test_label=np.load(dir_path+test_label_file)\n",
    "test_X=pre_processing(test_X,config['pre-process'])\n",
    "test_dataloader=Dataloader(test_X, test_label, config['batch_size'], shuffle=False, seed=config['seed'])\n",
    "\n",
    "model = build_model(config['layers'])\n",
    "trainer=Trainer(config,model,train_dataloader,test_dataloader)\n",
    "trainer.train()\n"
   ],
   "id": "d376e649804a297d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process: standardization\n",
      "Pre-process: standardization\n",
      "Start time:  Tue Mar 18 10:44:57 2025\n",
      "current lr 1.00000e-02\n",
      "Epoch: [1][0/48]\tTime 0.120 (0.120)\tLoss 8.8418 (8.8418)\tPrec@1 11.133 (11.133)\n",
      "Epoch: [1][9/48]\tTime 0.000 (0.025)\tLoss 7.5353 (8.2363)\tPrec@1 8.887 (10.508)\n",
      "Epoch: [1][18/48]\tTime 0.016 (0.020)\tLoss 4.9636 (7.1147)\tPrec@1 13.379 (11.143)\n",
      "Epoch: [1][27/48]\tTime 0.000 (0.017)\tLoss 3.6640 (6.1759)\tPrec@1 13.965 (11.652)\n",
      "Epoch: [1][36/48]\tTime 0.016 (0.016)\tLoss 3.0724 (5.4722)\tPrec@1 13.184 (12.049)\n",
      "Epoch: [1][45/48]\tTime 0.016 (0.016)\tLoss 2.7522 (4.9499)\tPrec@1 13.965 (12.555)\n",
      "Epoch: [1][48/48]\tTime 0.016 (0.016)\tLoss 2.6720 (4.8183)\tPrec@1 12.618 (12.634)\n",
      "EPOCH: 1 train Results: Prec@1 12.634 Loss: 4.8183\n",
      "Test: [0/9]\tTime 0.000 (0.000)\tLoss 2.6104 (2.6104)\tAccuracy 13.867 (13.867)\n",
      "Test: [9/9]\tTime 0.016 (0.008)\tLoss 2.6771 (2.6323)\tAccuracy 13.776 (13.990)\n",
      "EPOCH: 1 val Results: Accuracy 13.990 Loss: 2.6323\n",
      "Best Prec@1: 13.990\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [2][0/48]\tTime 0.016 (0.016)\tLoss 2.6223 (2.6223)\tPrec@1 13.965 (13.965)\n",
      "Epoch: [2][9/48]\tTime 0.016 (0.014)\tLoss 2.4905 (2.5414)\tPrec@1 13.574 (14.824)\n",
      "Epoch: [2][18/48]\tTime 0.004 (0.013)\tLoss 2.4734 (2.4938)\tPrec@1 15.723 (14.844)\n",
      "Epoch: [2][27/48]\tTime 0.016 (0.014)\tLoss 2.3733 (2.4651)\tPrec@1 15.527 (14.844)\n",
      "Epoch: [2][36/48]\tTime 0.016 (0.014)\tLoss 2.3792 (2.4467)\tPrec@1 14.551 (14.675)\n",
      "Epoch: [2][45/48]\tTime 0.016 (0.013)\tLoss 2.3467 (2.4275)\tPrec@1 15.137 (14.750)\n",
      "Epoch: [2][48/48]\tTime 0.016 (0.013)\tLoss 2.3216 (2.4217)\tPrec@1 15.448 (14.766)\n",
      "EPOCH: 2 train Results: Prec@1 14.766 Loss: 2.4217\n",
      "Test: [0/9]\tTime 0.000 (0.000)\tLoss 2.3355 (2.3355)\tAccuracy 15.137 (15.137)\n",
      "Test: [9/9]\tTime 0.000 (0.006)\tLoss 2.3838 (2.3446)\tAccuracy 13.265 (14.890)\n",
      "EPOCH: 2 val Results: Accuracy 14.890 Loss: 2.3446\n",
      "Best Prec@1: 14.890\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [3][0/48]\tTime 0.016 (0.016)\tLoss 2.3819 (2.3819)\tPrec@1 13.770 (13.770)\n",
      "Epoch: [3][9/48]\tTime 0.012 (0.013)\tLoss 2.3373 (2.3400)\tPrec@1 13.770 (14.824)\n",
      "Epoch: [3][18/48]\tTime 0.016 (0.013)\tLoss 2.3308 (2.3268)\tPrec@1 16.504 (15.363)\n",
      "Epoch: [3][27/48]\tTime 0.013 (0.014)\tLoss 2.3319 (2.3167)\tPrec@1 13.672 (15.555)\n",
      "Epoch: [3][36/48]\tTime 0.016 (0.013)\tLoss 2.3074 (2.3079)\tPrec@1 14.844 (15.625)\n",
      "Epoch: [3][45/48]\tTime 0.016 (0.014)\tLoss 2.2687 (2.3029)\tPrec@1 16.406 (15.721)\n",
      "Epoch: [3][48/48]\tTime 0.000 (0.013)\tLoss 2.2674 (2.3006)\tPrec@1 17.689 (15.780)\n",
      "EPOCH: 3 train Results: Prec@1 15.780 Loss: 2.3006\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 2.2768 (2.2768)\tAccuracy 15.918 (15.918)\n",
      "Test: [9/9]\tTime 0.016 (0.008)\tLoss 2.3144 (2.2826)\tAccuracy 15.051 (15.970)\n",
      "EPOCH: 3 val Results: Accuracy 15.970 Loss: 2.2826\n",
      "Best Prec@1: 15.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [4][0/48]\tTime 0.016 (0.016)\tLoss 2.2918 (2.2918)\tPrec@1 14.355 (14.355)\n",
      "Epoch: [4][9/48]\tTime 0.016 (0.012)\tLoss 2.2715 (2.2719)\tPrec@1 15.723 (16.338)\n",
      "Epoch: [4][18/48]\tTime 0.016 (0.012)\tLoss 2.2638 (2.2611)\tPrec@1 15.234 (16.941)\n",
      "Epoch: [4][27/48]\tTime 0.016 (0.012)\tLoss 2.2286 (2.2575)\tPrec@1 19.727 (17.055)\n",
      "Epoch: [4][36/48]\tTime 0.004 (0.012)\tLoss 2.2474 (2.2542)\tPrec@1 18.457 (17.011)\n",
      "Epoch: [4][45/48]\tTime 0.016 (0.012)\tLoss 2.2231 (2.2493)\tPrec@1 18.848 (17.230)\n",
      "Epoch: [4][48/48]\tTime 0.016 (0.012)\tLoss 2.2098 (2.2483)\tPrec@1 17.689 (17.288)\n",
      "EPOCH: 4 train Results: Prec@1 17.288 Loss: 2.2483\n",
      "Test: [0/9]\tTime 0.000 (0.000)\tLoss 2.2360 (2.2360)\tAccuracy 17.676 (17.676)\n",
      "Test: [9/9]\tTime 0.011 (0.007)\tLoss 2.2636 (2.2393)\tAccuracy 16.709 (17.710)\n",
      "EPOCH: 4 val Results: Accuracy 17.710 Loss: 2.2393\n",
      "Best Prec@1: 17.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [5][0/48]\tTime 0.005 (0.005)\tLoss 2.2129 (2.2129)\tPrec@1 19.629 (19.629)\n",
      "Epoch: [5][9/48]\tTime 0.016 (0.013)\tLoss 2.2147 (2.2153)\tPrec@1 18.359 (19.004)\n",
      "Epoch: [5][18/48]\tTime 0.016 (0.013)\tLoss 2.2099 (2.2168)\tPrec@1 19.629 (18.478)\n",
      "Epoch: [5][27/48]\tTime 0.016 (0.013)\tLoss 2.1978 (2.2114)\tPrec@1 18.652 (18.907)\n",
      "Epoch: [5][36/48]\tTime 0.016 (0.013)\tLoss 2.1795 (2.2070)\tPrec@1 21.387 (19.159)\n",
      "Epoch: [5][45/48]\tTime 0.016 (0.012)\tLoss 2.2109 (2.2042)\tPrec@1 17.480 (19.149)\n",
      "Epoch: [5][48/48]\tTime 0.016 (0.013)\tLoss 2.2122 (2.2043)\tPrec@1 20.047 (19.148)\n",
      "EPOCH: 5 train Results: Prec@1 19.148 Loss: 2.2043\n",
      "Test: [0/9]\tTime 0.000 (0.000)\tLoss 2.1907 (2.1907)\tAccuracy 18.945 (18.945)\n",
      "Test: [9/9]\tTime 0.016 (0.007)\tLoss 2.2088 (2.1940)\tAccuracy 17.219 (19.550)\n",
      "EPOCH: 5 val Results: Accuracy 19.550 Loss: 2.1940\n",
      "Best Prec@1: 19.550\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [6][0/48]\tTime 0.016 (0.016)\tLoss 2.1823 (2.1823)\tPrec@1 20.605 (20.605)\n",
      "Epoch: [6][9/48]\tTime 0.016 (0.014)\tLoss 2.1555 (2.1672)\tPrec@1 21.875 (20.664)\n",
      "Epoch: [6][18/48]\tTime 0.016 (0.013)\tLoss 2.1844 (2.1682)\tPrec@1 22.168 (20.862)\n",
      "Epoch: [6][27/48]\tTime 0.016 (0.014)\tLoss 2.1480 (2.1640)\tPrec@1 24.316 (21.080)\n",
      "Epoch: [6][36/48]\tTime 0.010 (0.013)\tLoss 2.1423 (2.1580)\tPrec@1 20.996 (21.094)\n",
      "Epoch: [6][45/48]\tTime 0.016 (0.014)\tLoss 2.1279 (2.1529)\tPrec@1 20.996 (21.213)\n",
      "Epoch: [6][48/48]\tTime 0.016 (0.013)\tLoss 2.1266 (2.1509)\tPrec@1 22.759 (21.290)\n",
      "EPOCH: 6 train Results: Prec@1 21.290 Loss: 2.1509\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 2.1447 (2.1447)\tAccuracy 21.484 (21.484)\n",
      "Test: [9/9]\tTime 0.000 (0.006)\tLoss 2.1580 (2.1464)\tAccuracy 19.260 (21.470)\n",
      "EPOCH: 6 val Results: Accuracy 21.470 Loss: 2.1464\n",
      "Best Prec@1: 21.470\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [7][0/48]\tTime 0.016 (0.016)\tLoss 2.1266 (2.1266)\tPrec@1 23.047 (23.047)\n",
      "Epoch: [7][9/48]\tTime 0.016 (0.014)\tLoss 2.1314 (2.1267)\tPrec@1 21.289 (22.842)\n",
      "Epoch: [7][18/48]\tTime 0.016 (0.014)\tLoss 2.0855 (2.1147)\tPrec@1 25.195 (23.011)\n",
      "Epoch: [7][27/48]\tTime 0.016 (0.014)\tLoss 2.0779 (2.1118)\tPrec@1 24.023 (22.914)\n",
      "Epoch: [7][36/48]\tTime 0.016 (0.014)\tLoss 2.1149 (2.1080)\tPrec@1 21.973 (23.076)\n",
      "Epoch: [7][45/48]\tTime 0.016 (0.013)\tLoss 2.0718 (2.1042)\tPrec@1 24.219 (23.176)\n",
      "Epoch: [7][48/48]\tTime 0.003 (0.013)\tLoss 2.0825 (2.1034)\tPrec@1 26.061 (23.266)\n",
      "EPOCH: 7 train Results: Prec@1 23.266 Loss: 2.1034\n",
      "Test: [0/9]\tTime 0.000 (0.000)\tLoss 2.1027 (2.1027)\tAccuracy 23.438 (23.438)\n",
      "Test: [9/9]\tTime 0.016 (0.008)\tLoss 2.1092 (2.1009)\tAccuracy 21.556 (23.070)\n",
      "EPOCH: 7 val Results: Accuracy 23.070 Loss: 2.1009\n",
      "Best Prec@1: 23.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [8][0/48]\tTime 0.016 (0.016)\tLoss 2.0750 (2.0750)\tPrec@1 23.535 (23.535)\n",
      "Epoch: [8][9/48]\tTime 0.016 (0.012)\tLoss 2.0811 (2.0843)\tPrec@1 24.414 (23.428)\n",
      "Epoch: [8][18/48]\tTime 0.016 (0.013)\tLoss 2.0399 (2.0823)\tPrec@1 25.586 (23.679)\n",
      "Epoch: [8][27/48]\tTime 0.016 (0.013)\tLoss 2.0756 (2.0757)\tPrec@1 23.340 (23.873)\n",
      "Epoch: [8][36/48]\tTime 0.016 (0.013)\tLoss 1.9915 (2.0677)\tPrec@1 27.734 (24.264)\n",
      "Epoch: [8][45/48]\tTime 0.009 (0.013)\tLoss 2.0600 (2.0662)\tPrec@1 23.633 (24.265)\n",
      "Epoch: [8][48/48]\tTime 0.000 (0.013)\tLoss 2.0335 (2.0650)\tPrec@1 26.415 (24.330)\n",
      "EPOCH: 8 train Results: Prec@1 24.330 Loss: 2.0650\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 2.0631 (2.0631)\tAccuracy 24.902 (24.902)\n",
      "Test: [9/9]\tTime 0.002 (0.008)\tLoss 2.0641 (2.0600)\tAccuracy 23.087 (24.530)\n",
      "EPOCH: 8 val Results: Accuracy 24.530 Loss: 2.0600\n",
      "Best Prec@1: 24.530\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [9][0/48]\tTime 0.016 (0.016)\tLoss 2.0123 (2.0123)\tPrec@1 26.074 (26.074)\n",
      "Epoch: [9][9/48]\tTime 0.016 (0.013)\tLoss 2.0511 (2.0423)\tPrec@1 25.391 (24.736)\n",
      "Epoch: [9][18/48]\tTime 0.016 (0.013)\tLoss 2.0183 (2.0328)\tPrec@1 23.828 (25.149)\n",
      "Epoch: [9][27/48]\tTime 0.016 (0.013)\tLoss 2.0250 (2.0321)\tPrec@1 25.391 (25.422)\n",
      "Epoch: [9][36/48]\tTime 0.016 (0.013)\tLoss 2.0102 (2.0295)\tPrec@1 28.223 (25.483)\n",
      "Epoch: [9][45/48]\tTime 0.016 (0.013)\tLoss 2.0113 (2.0264)\tPrec@1 24.902 (25.611)\n",
      "Epoch: [9][48/48]\tTime 0.000 (0.013)\tLoss 2.0112 (2.0244)\tPrec@1 28.184 (25.720)\n",
      "EPOCH: 9 train Results: Prec@1 25.720 Loss: 2.0244\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 2.0274 (2.0274)\tAccuracy 26.367 (26.367)\n",
      "Test: [9/9]\tTime 0.000 (0.006)\tLoss 2.0292 (2.0237)\tAccuracy 24.235 (25.790)\n",
      "EPOCH: 9 val Results: Accuracy 25.790 Loss: 2.0237\n",
      "Best Prec@1: 25.790\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [10][0/48]\tTime 0.031 (0.031)\tLoss 2.0071 (2.0071)\tPrec@1 24.707 (24.707)\n",
      "Epoch: [10][9/48]\tTime 0.016 (0.014)\tLoss 1.9909 (1.9908)\tPrec@1 28.027 (26.748)\n",
      "Epoch: [10][18/48]\tTime 0.000 (0.014)\tLoss 1.9746 (1.9946)\tPrec@1 27.148 (26.943)\n",
      "Epoch: [10][27/48]\tTime 0.013 (0.014)\tLoss 1.9913 (1.9941)\tPrec@1 28.125 (26.929)\n",
      "Epoch: [10][36/48]\tTime 0.016 (0.014)\tLoss 1.9515 (1.9890)\tPrec@1 29.395 (27.133)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m model \u001B[38;5;241m=\u001B[39m build_model(config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlayers\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      9\u001B[0m trainer\u001B[38;5;241m=\u001B[39mTrainer(config,model,train_dataloader,test_dataloader)\n\u001B[1;32m---> 10\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "Cell \u001B[1;32mIn[16], line 6\u001B[0m, in \u001B[0;36mtimer.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStart time: \u001B[39m\u001B[38;5;124m'\u001B[39m, time\u001B[38;5;241m.\u001B[39mctime())\n\u001B[0;32m      4\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()  \u001B[38;5;66;03m# start time\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m result \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# run\u001B[39;00m\n\u001B[0;32m      8\u001B[0m end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()  \u001B[38;5;66;03m# end time\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEnd time: \u001B[39m\u001B[38;5;124m'\u001B[39m, time\u001B[38;5;241m.\u001B[39mctime())\n",
      "Cell \u001B[1;32mIn[30], line 35\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepochs):\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcurrent lr \u001B[39m\u001B[38;5;132;01m{:.5e}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mlr))\n\u001B[1;32m---> 35\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_per_epoch(epoch)\n\u001B[0;32m     36\u001B[0m     acc1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidate(epoch)\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;66;03m# remember best prec@1\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[30], line 52\u001B[0m, in \u001B[0;36mTrainer.train_per_epoch\u001B[1;34m(self, epoch)\u001B[0m\n\u001B[0;32m     49\u001B[0m y_hat\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39m_forward(X)\n\u001B[0;32m     50\u001B[0m loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(y_hat,y)\n\u001B[1;32m---> 52\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39m_backward(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion\u001B[38;5;241m.\u001B[39mgrad)\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     54\u001B[0m precision\u001B[38;5;241m=\u001B[39maccuracy(y_hat,y)\n",
      "Cell \u001B[1;32mIn[26], line 25\u001B[0m, in \u001B[0;36mMLP._backward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_backward\u001B[39m(\u001B[38;5;28mself\u001B[39m,x: np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;66;03m#backward from the last layer to the first layer\u001B[39;00m\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m---> 25\u001B[0m         x \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39m_backward(x)\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fa5126bd65ef1423",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
