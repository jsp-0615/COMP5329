{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.197360Z",
     "start_time": "2025-03-17T23:44:57.192410Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import time\n",
    "import seaborn as sns\n",
    "import math\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.284703Z",
     "start_time": "2025-03-17T23:44:57.268578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('Start time: ', time.ctime())\n",
    "        start_time = time.time()  # start time\n",
    "\n",
    "        result = func(*args, **kwargs)  # run\n",
    "\n",
    "        end_time = time.time()  # end time\n",
    "        print('End time: ', time.ctime())\n",
    "        print(f\"{func.__name__} executed in {(end_time - start_time):.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ],
   "id": "98e36a535e7c2b42",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pre-process\n",
    "\n",
    "Min-max normalization:\n",
    "\n",
    "$$x_{min-max} = {{x-min(x)}\\over{max(x)-min(x)}}$$\n",
    "\n",
    "Standardization:\n",
    "\n",
    "$$x_{norm} = {{x-\\mu}\\over{\\sigma}}$$"
   ],
   "id": "1ea172e08505f1e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.289423Z",
     "start_time": "2025-03-17T23:44:57.284703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pre_processing(X, mode=None):\n",
    "    if mode == 'min-max':\n",
    "        print('Pre-process: min-max normalization')\n",
    "        min_each_feature = np.min(X, axis=0)\n",
    "        max_each_feature = np.max(X, axis=0)\n",
    "        scale = max_each_feature - min_each_feature\n",
    "        scale[scale == 0] = 1   # To avoid divided by 0\n",
    "        scaled_train = (X - min_each_feature) / scale\n",
    "        return scaled_train\n",
    "\n",
    "    if mode == 'standardization':\n",
    "        print('Pre-process: standardization')\n",
    "        std_each_feature = np.std(X, axis=0)\n",
    "        mean_each_feature = np.mean(X, axis=0)\n",
    "        std_each_feature[std_each_feature == 0] = 1     # To avoid divided by 0\n",
    "        norm_train = (X - mean_each_feature) / std_each_feature\n",
    "        norm_test = (X - mean_each_feature) / std_each_feature\n",
    "        return norm_train\n",
    "\n",
    "    print('No pre-process')\n",
    "\n",
    "    return X"
   ],
   "id": "2578bdbe5f4e1107",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.303837Z",
     "start_time": "2025-03-17T23:44:57.299932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def accuracy(y_hat,y):\n",
    "    '''\n",
    "    y_hat : predicted value\n",
    "    :param y_hat: [batch_size,num_of_class]\n",
    "    :param y: [batch_size,1\n",
    "    :return: \n",
    "    '''\n",
    "    preds=y_hat.argmax(axis=1,keepdims=True)\n",
    "    return np.mean(preds == y)*100"
   ],
   "id": "5f6ecbfb8386710",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.321274Z",
     "start_time": "2025-03-17T23:44:57.303837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_gain(nonlinearity, param=None):\n",
    "    r\"\"\"Return the recommended gain value for the given nonlinearity function.\n",
    "    The values are as follows:\n",
    "\n",
    "    ================= ====================================================\n",
    "    nonlinearity      gain\n",
    "    ================= ====================================================\n",
    "    Linear / Identity :math:`1`\n",
    "    Conv{1,2,3}D      :math:`1`\n",
    "    Sigmoid           :math:`1`\n",
    "    Tanh              :math:`\\frac{5}{3}`\n",
    "    ReLU              :math:`\\sqrt{2}`\n",
    "    Leaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\n",
    "    SELU              :math:`\\frac{3}{4}`\n",
    "    ================= ====================================================\n",
    "    \"\"\"\n",
    "\n",
    "    if nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    elif nonlinearity == 'leaky_relu':\n",
    "        if param is None:\n",
    "            negative_slope = 0.01\n",
    "        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n",
    "            # True/False are instances of int, hence check above\n",
    "            negative_slope = param\n",
    "        else:\n",
    "            raise ValueError(f\"negative_slope {param} not a valid number\")\n",
    "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
    "    elif nonlinearity == 'selu':\n",
    "        return 3.0 / 4  # Value found empirically (https://github.com/pytorch/pytorch/pull/50664)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported nonlinearity {nonlinearity}\")\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(array):\n",
    "    dimensions = len(array.shape)\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n",
    "\n",
    "    num_input_fmaps = array.shape[1]\n",
    "    num_output_fmaps = array.shape[0]\n",
    "    receptive_field_size = 1\n",
    "    if dimensions > 2:\n",
    "        # math.prod is not always available, accumulate the product manually\n",
    "        # we could use functools.reduce but that is not supported by TorchScript\n",
    "        for s in array.shape[2:]:\n",
    "            receptive_field_size *= s\n",
    "    fan_in = num_input_fmaps * receptive_field_size\n",
    "    fan_out = num_output_fmaps * receptive_field_size\n",
    "\n",
    "    return fan_in, fan_out\n",
    "\n",
    "def _calculate_correct_fan(array, mode):\n",
    "    mode = mode.lower()\n",
    "    valid_modes = ['fan_in', 'fan_out']\n",
    "    if mode not in valid_modes:\n",
    "        raise ValueError(f\"Mode {mode} not supported, please use one of {valid_modes}\")\n",
    "\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(array)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "def kaiming_normal_(array: np.array, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'relu'):\n",
    "    fan = _calculate_correct_fan(array, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    return np.random.normal(0, std, array.shape)"
   ],
   "id": "ac167cc0d89add8",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.332189Z",
     "start_time": "2025-03-17T23:44:57.322783Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "66b30fbd3542f3ca",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.353162Z",
     "start_time": "2025-03-17T23:44:57.347706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Parameter(object):\n",
    "    \"\"\"Parameter class for saving data and gradients\"\"\"\n",
    "    def __init__(self, data, requires_grad, skip_decay=False):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.skip_decay = skip_decay\n",
    "        self.requires_grad = requires_grad"
   ],
   "id": "785071ee87e7f9d7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.377233Z",
     "start_time": "2025-03-17T23:44:57.363665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        self.name = name\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "    def _forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    def _backward(self, *args):\n",
    "        pass"
   ],
   "id": "767328eb1e97167e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.384039Z",
     "start_time": "2025-03-17T23:44:57.379735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _backward(self, delta):\n",
    "        delta[self.x <= 0] = 0\n",
    "        return delta"
   ],
   "id": "8c6d47201896a516",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Forward:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{xW} + \\mathbf{b}$$\n",
    "\n",
    "Backward:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{W}^T$$\n",
    "\n",
    "Gradient of W:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{x}^T$$\n",
    "\n",
    "Gradient of b:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{b}} = \\frac{\\partial L}{\\partial \\mathbf{y}} $$\n",
    "\n",
    "Gradient of x:\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{W}^T$$"
   ],
   "id": "6d64cedcd4ee0dc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.421061Z",
     "start_time": "2025-03-17T23:44:57.411216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FCLayer(Layer):\n",
    "    def  __init__(self,name: str,n_in: int,n_out: int)->None:\n",
    "        '''\n",
    "        Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,)\n",
    "        :param n_in: dimensionality of input\n",
    "        :param n_out: number of hidden units\n",
    "        '''\n",
    "        super().__init__(name, requires_grad=True)\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        W = kaiming_normal_(np.array([0] * n_in * n_out).reshape(n_in, n_out), a=math.sqrt(5))\n",
    "        self.W = Parameter(W, self.requires_grad)\n",
    "        self.b = Parameter(np.zeros(n_out), self.requires_grad)\n",
    "        \n",
    "       \n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "            x: [batch size, n_in]\n",
    "            W: [n_in, n_out]\n",
    "            b: [n_out]\n",
    "        \"\"\"\n",
    "        self.x=x\n",
    "\n",
    "        #[batch_size,n_in] @ [n_in,n_out] + [n_output] => [batch_size,n_out]\n",
    "        return x @ self.W.data + self.b.data\n",
    "\n",
    "    def _backward(self, delta: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        delta: the gradient of the loss function respect to this layer's output 这层损失函数对于这层输出的梯度\n",
    "        :param delta: [batch size, n_out]:\n",
    "        :return: \n",
    "        '''\n",
    "        batch_size = delta.shape[0]\n",
    "        self.W.grad = self.x.T @ delta / batch_size # [batch_size,n_in]^T @ [batch size, n_out] => [n_in,n_out]\n",
    "        self.b.grad = delta.sum(axis=0) / batch_size #divide by batch size to get average of gradient\n",
    "        return delta @ self.W.data.T # return the gradient of input(x) back to last layer \n"
   ],
   "id": "e5b00fb819887fa6",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Softmax \n",
    "Formula:\n",
    "$$softmax(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}$$\n",
    "\n",
    "不需要计算 softmax 的完整 Jacobian 矩阵，因为与交叉熵结合后公式极大简化了。\n",
    "只需用 preds - ground_truth 作为梯度，这个计算在 CrossEntropyLoss 里完成了：\n",
    "self.grad = preds - ground_truth\n",
    "因此，在 softmax.backward(delta) 时，不需要额外计算 softmax 的梯度，而是直接返回 delta\n"
   ],
   "id": "86a2cacdcffb889f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.436048Z",
     "start_time": "2025-03-17T23:44:57.429215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self,name,requires_grad=False):\n",
    "        super().__init__(name,requires_grad)\n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        x_exp =  np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return x_exp/x_exp.sum(axis=1, keepdims=True)\n",
    "    def _backward(self, delta: np.ndarray) -> np.ndarray:\n",
    "        return delta"
   ],
   "id": "2883c1fd2bf68ed0",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loss Function - Cross Entropy\n",
    "Formula:\n",
    "$$CrossEntropy= - \\sum_{i=1}^{n} y_i log(\\hat {y_i})$$\n",
    "\n",
    "Gradient of softmax:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_k} = \\sum_{i}^{c} \\left( \\frac{\\partial L}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_k} \\right)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y}_i} = - \\frac{y_i}{\\hat{y}_i}, \\qquad \\frac{\\partial \\hat{y}_i}{\\partial z_k} = \\begin{cases}\n",
    "\\hat{y}_i(1 - \\hat{y}_i) & \\text{if } i = k \\\\\n",
    "-\\hat{y}_k\\hat{y}_i & \\text{if } i \\neq k\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_k} = - \\left( (y_k(1 - \\hat{y}_k)) - \\sum_{i \\neq k}^{c} y_i \\hat{y}_k \\right) = -(y_k - \\hat{y}_k \\sum_{i}^{c} y_i) = \\hat{y}_k - y_k\n",
    "$$\n",
    "\n",
    "$$=> \\frac{\\partial L}{\\partial z} = \\hat{y} - y$$\n"
   ],
   "id": "8860ec1e183838c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.466835Z",
     "start_time": "2025-03-17T23:44:57.460982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossEntropy(object):\n",
    "    def __init__(self):\n",
    "        self.softmax = Softmax('softmax')\n",
    "\n",
    "    def __call__(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "\n",
    "        :param x:\n",
    "        :param y: [batch_size, 1]\n",
    "        :return:\n",
    "        '''\n",
    "        self.batch_size = x.shape[0]\n",
    "        self.class_num = x.shape[1]\n",
    "\n",
    "        y_hat = self.softmax._forward(x) #[batch_size,num_class]\n",
    "\n",
    "        y=self.one_hot_encoding(y)\n",
    "        self.grad = y_hat - y\n",
    "\n",
    "        loss = -1 * (y * np.log(y_hat + 1e-8)).sum() / self.batch_size  # to avoid divided by 0\n",
    "        return loss\n",
    "\n",
    "    def one_hot_encoding(self, x):\n",
    "        one_hot_encoded = np.zeros((self.batch_size, self.class_num))\n",
    "        one_hot_encoded[np.arange(x.shape[0]), x.flatten()] = 1\n",
    "        return one_hot_encoded"
   ],
   "id": "bdb959962b6db01c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.498710Z",
     "start_time": "2025-03-17T23:44:57.492811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(object):\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.num_layers =0\n",
    "        self.params=[]\n",
    "    def add_layer(self,layer):\n",
    "        self.layers.append(layer)\n",
    "        self.num_layers+=1\n",
    "        if layer.requires_grad:\n",
    "            if hasattr(layer,'W'):#检查是否有属性W\n",
    "                self.params.append(layer.W)\n",
    "            if hasattr(layer,'b'):\n",
    "                self.params.append(layer.b)\n",
    "            # if hasattr(layer,'gamma'):\n",
    "            #     self.params.append(layer.gamma)\n",
    "            # if hasattr(layer,'beta'):\n",
    "            #     self.params.append(layer.beta)\n",
    "    def _forward(self,x: np.ndarray) -> np.ndarray:\n",
    "        for layer in self.layers:\n",
    "            x = layer._forward(x)\n",
    "        return x\n",
    "    def _backward(self,x: np.ndarray) -> np.ndarray:\n",
    "        #backward from the last layer to the first layer\n",
    "        for layer in self.layers[::-1]:\n",
    "            x = layer._backward(x)\n",
    "        return x\n",
    "    def _fit(self,mode='train'):\n",
    "        if mode=='train':\n",
    "            for layer in self.layers:\n",
    "                layer.train=True\n",
    "        elif mode=='eval':\n",
    "            for layer in self.layers:\n",
    "                layer.train=False"
   ],
   "id": "d220debee7e093de",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T00:42:41.595167Z",
     "start_time": "2025-03-18T00:42:41.584126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "    {'type': 'linear','params':{'name':'fc1','n_in':128,'n_out':256}},\n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.3}},\n",
    "    {'type':'relu', 'params': {'name': 'relu1'}},\n",
    "    {'type':'linear', 'params': {'name': 'fc2', 'n_in':256,'n_out':128}},\n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.3}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}},\n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'n_in': 128, 'n_out': 10}},\n",
    "'''\n",
    "class MLP_V2():\n",
    "    def __init__(self):\n",
    "        self.fc1 = FCLayer('fc1', n_in=128, n_out=256)\n",
    "        self.dropout1=Dropout('dropout1',0.3)\n",
    "        self.relu1=ReLU('relu1')\n",
    "        self.fc2 = FCLayer('fc2', n_in=256, n_out=128)\n",
    "        self.dropout2=Dropout('dropout2',0.3)\n",
    "        self.relu2=ReLU('relu2')\n",
    "        self.fc3 = FCLayer('fc3', n_in=128, n_out=10)\n",
    "        # self.softmax=Softmax('softmax')\n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        x=self.fc1._forward(x)\n",
    "        x=self.dropout1._forward(x)\n",
    "        x=self.relu1._forward(x)\n",
    "        x=self.fc2._forward(x)\n",
    "        x=self.dropout2._forward(x)\n",
    "        x=self.relu2._forward(x)\n",
    "        x=self.fc3._forward(x)\n",
    "        return x\n",
    "    def _backward(self, delta: np.ndarray) -> np.ndarray:\n",
    "        delta=self.fc3._backward(delta)\n",
    "        delta=self.relu2._backward(delta)\n",
    "        delta=self.dropout2._backward(delta)\n",
    "        delta=self.fc2._backward(delta)\n",
    "        delta=self.dropout1._backward(delta)\n",
    "        delta=self.relu1._backward(delta)\n",
    "        delta=self.fc1._backward(delta)\n",
    "        return delta\n",
    "        "
   ],
   "id": "6dc7ad6e3067e7bd",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### SGD with Momentum\n",
    "SGD Formula:\n",
    "$$θ_{t+1}=θ_t−\\eta  \\cdot ∇L(θ_t)$$\n",
    "\n",
    "Momentum 梯度下降的公式如下：\n",
    "$$\n",
    "\\begin{equation}\n",
    "v_t = \\beta v_{t-1} - \\eta \\nabla L(\\theta_t)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t + v_t\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$$\n",
    "    \\( v_t \\) 是当前动量\\\\\n",
    "    \\( \\beta \\) 是动量系数（通常取 0.9）\\\\\n",
    "    \\( \\eta \\) 是学习率\\\\\n",
    "    \\( \\nabla L(\\theta_t) \\) 是损失函数对参数的梯度\n",
    "$$"
   ],
   "id": "d37f07c8cd64eb59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.542170Z",
     "start_time": "2025-03-17T23:44:57.536748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGDMomentum(object):\n",
    "    def __init__(self,parameters,lr=0.01,momentum=0.9,weight_decay=0.0001):\n",
    "        self.parameters = parameters\n",
    "        self.momentum = momentum\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.v = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "    def step(self):\n",
    "        for i,(v,p) in enumerate(zip(self.v,self.parameters)):\n",
    "            if not p.skip_decay:\n",
    "                p.data -= self.weight_decay * p.data\n",
    "            v = self.momentum * v + self.lr * p.grad\n",
    "            self.v[i] = v\n",
    "            p.data -= self.v[i]"
   ],
   "id": "f414f1bd779f820b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Normalization\n",
    "Forward:\n",
    "$$\\mathbf{y}=\\gamma\\frac{\\mathbf{x}-E(\\mathbf{x})}{\\sqrt{\\sigma^2_B+\\epsilon}}+\\beta$$\n",
    "\n",
    "$E(\\mathbf{x})$ is the mean of the current mini-batch, $\\sigma^2_B$ is the variance of the current mini-batch\n",
    "\n",
    "Backward:\n"
   ],
   "id": "2eecd3ec6ec4360c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.573099Z",
     "start_time": "2025-03-17T23:44:57.568411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BatchNormalization(Layer):\n",
    "    def __init__(self,name,epsilon=1e-5):\n",
    "        super().__init__(name,requires_grad=True)\n",
    "        self.epsilon = epsilon\n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "        "
   ],
   "id": "a471a89cc91ed1b",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.599572Z",
     "start_time": "2025-03-17T23:44:57.584118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AverageMeterics(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ],
   "id": "9e82f74a1f63aea1",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.609251Z",
     "start_time": "2025-03-17T23:44:57.599572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adam:\n",
    "    pass\n",
    "\n",
    "\n",
    "class CosineLR:\n",
    "    pass\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self,config,model=None,train_loader=None,valid_loader=None):\n",
    "        self.config=config\n",
    "        self.epochs = self.config['epoch']\n",
    "        self.lr=self.config['lr']\n",
    "        self.model=model\n",
    "        self.train_loader=train_loader\n",
    "        self.valid_loader=valid_loader\n",
    "        self.print_freq=self.config['print_freq']\n",
    "        # self.scheduler= self.config['scheduler']\n",
    "        self.train_precision=[]\n",
    "        self.valid_precision=[]\n",
    "        self.train_loss=[]\n",
    "        self.valid_loss=[]\n",
    "        self.criterion=CrossEntropy()\n",
    "        if self.config['optimizer'] == 'sgd':\n",
    "            self.optimizer = SGDMomentum(self.model.params,self.lr,self.config['momentum'],self.config['weight_decay'])\n",
    "        # elif self.config['optimizer'] == 'adam':\n",
    "        #     self.optimizer = Adam(self.model.params, self.lr, self.config['weight_decay'])\n",
    "        # if self.scheduler == 'cos':\n",
    "        #     self.train_scheduler = CosineLR(self.optimizer, T_max=self.epochs)\n",
    "    @timer\n",
    "    def train(self):\n",
    "        best_accuracy=0\n",
    "        for epoch in range(self.epochs):\n",
    "            print('current lr {:.5e}'.format(self.optimizer.lr))\n",
    "            self.train_per_epoch(epoch)\n",
    "            acc1 = self.validate(epoch)\n",
    "\n",
    "            # remember best prec@1\n",
    "            best_acc1 = max(acc1, best_accuracy)\n",
    "            output_best = 'Best Prec@1: %.3f\\n' % (best_acc1)\n",
    "            print(output_best)\n",
    "    def train_per_epoch(self,epoch):\n",
    "        batch_time=AverageMeterics()\n",
    "        losses=AverageMeterics()\n",
    "        top1=AverageMeterics()        \n",
    "        self.model._fit()\n",
    "        end_time = time.time()\n",
    "        for i,(X,y) in enumerate(self.train_loader):\n",
    "            y_hat=self.model._forward(X)\n",
    "            loss=self.criterion(y_hat,y)\n",
    "            \n",
    "            self.model._backward(self.criterion.grad)\n",
    "            self.optimizer.step()\n",
    "            precision=accuracy(y_hat,y)\n",
    "            losses.update(loss,X.shape[0])\n",
    "            top1.update(precision,X.shape[0])\n",
    "            \n",
    "            batch_time.update(time.time() - end_time)\n",
    "            end_time = time.time()\n",
    "            if (i%self.print_freq ==0) or (i==len(self.train_loader)-1):\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        epoch + 1, i, len(self.train_loader) - 1, batch_time=batch_time,\n",
    "                        loss=losses, top1=top1))\n",
    "        print('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} Loss: {losses.avg:.4f}'.format(epoch=epoch + 1 , flag='train', top1=top1, losses=losses))\n",
    "        self.train_loss.append(losses.avg)\n",
    "        self.train_precision.append(top1.avg)\n",
    "    def validate(self, epoch):\n",
    "        batch_time = AverageMeterics()\n",
    "        losses = AverageMeterics()\n",
    "        top1 = AverageMeterics()\n",
    "\n",
    "        self.model._fit(mode='test')\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (X, y) in enumerate(self.valid_loader):\n",
    "            # compute output\n",
    "            y_hat = self.model._forward(X)\n",
    "            loss = self.criterion(y_hat, y)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            precision = accuracy(y_hat, y)\n",
    "            losses.update(loss, X.shape[0])\n",
    "            top1.update(precision, X.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if (i % self.print_freq == 0) or (i == len(self.valid_loader) - 1):\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Accuracy {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                    i, len(self.valid_loader) - 1, batch_time=batch_time, loss=losses,\n",
    "                    top1=top1))\n",
    "\n",
    "        print('EPOCH: {epoch} {flag} Results: Accuracy {top1.avg:.3f} Loss: {losses.avg:.4f}'.format(epoch=epoch + 1,\n",
    "                                                                                                   flag='val',\n",
    "                                                                                                   top1=top1,\n",
    "                                                                                                   losses=losses))\n",
    "        self.valid_loss.append(losses.avg)\n",
    "        self.valid_precision.append(top1.avg)\n",
    "\n",
    "        return top1.avg"
   ],
   "id": "bf29c48b65013fe3",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.627900Z",
     "start_time": "2025-03-17T23:44:57.616008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, name, drop_rate, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.fix_value = 1 / (1 - self.drop_rate)   # to keep average fixed\n",
    "\n",
    "    def _forward(self, x):\n",
    "        if self.train:\n",
    "            self.mask = np.random.uniform(0, 1, x.shape) > self.drop_rate\n",
    "            return x * self.mask * self.fix_value\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def _backward(self, grad_output):\n",
    "        if self.train:\n",
    "            return grad_output * self.mask\n",
    "        else:\n",
    "            return grad_output"
   ],
   "id": "66ffb78d9831ae51",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.636731Z",
     "start_time": "2025-03-17T23:44:57.630902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dataloader(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=True, seed=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.index = np.arange(X.shape[0])\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            if self.seed is not None:\n",
    "                np.random.seed(self.seed)\n",
    "            np.random.shuffle(self.index)\n",
    "        self.n = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.n >= len(self.index):\n",
    "            raise StopIteration\n",
    "\n",
    "        index = self.index[self.n:self.n + self.batch_size]\n",
    "        batch_X = self.X[index]\n",
    "        batch_y = self.y[index]\n",
    "        self.n += self.batch_size\n",
    "\n",
    "        return batch_X, batch_y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            num of batch\n",
    "        \"\"\"\n",
    "        return (len(self.index) + self.batch_size - 1) // self.batch_size  # ceiling"
   ],
   "id": "66f87db9952bdaef",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.650712Z",
     "start_time": "2025-03-17T23:44:57.636731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {'linear': FCLayer,'relu': ReLU,'dropout': Dropout}\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    return model\n",
    "\n",
    "\n",
    "layers=[\n",
    "    {'type': 'linear','params':{'name':'fc1','n_in':128,'n_out':256}},\n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.3}},\n",
    "    {'type':'relu', 'params': {'name': 'relu1'}},\n",
    "    {'type':'linear', 'params': {'name': 'fc2', 'n_in':256,'n_out':128}},\n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.3}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}},\n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'n_in': 128, 'n_out': 10}},\n",
    "]\n",
    "batch_size=1024\n",
    "config={'layers': layers,'lr': 0.01,'batch_size': batch_size,'momentum': 0.9,'weight_decay': 5e-4,'seed': 0,'epoch': 200,\n",
    "    'optimizer': 'sgd',     # adam, sgd\n",
    "    'scheduler': None,      # cos, None\n",
    "    'pre-process': 'standardization',      # min-max, standardization, None\n",
    "    'print_freq': 50000 // batch_size // 5\n",
    "}\n",
    "np.random.seed(config['seed'])"
   ],
   "id": "d76807819d7f07c2",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.705728Z",
     "start_time": "2025-03-17T23:44:57.650712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir_path='E:\\\\Postgraduate\\\\25S1\\\\COMP5329\\\\Assignment\\\\Assignment1\\\\Assignment1-Dataset\\\\'\n",
    "train_file='train_data.npy'\n",
    "train_label_file='train_label.npy'\n",
    "train_data=np.load(dir_path+train_file)\n",
    "train_label=np.load(dir_path+train_label_file)\n",
    "test_file='test_data.npy'\n",
    "test_label_file='test_label.npy'"
   ],
   "id": "649ce56cf5d9e69a",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T23:44:57.713315Z",
     "start_time": "2025-03-17T23:44:57.706992Z"
    }
   },
   "cell_type": "code",
   "source": "model = build_model(config['layers'])",
   "id": "b000da5de410dfaa",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T00:42:45.184306Z",
     "start_time": "2025-03-18T00:42:45.069967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_X=pre_processing(train_data,config['pre-process'])\n",
    "train_dataloader=Dataloader(train_X, train_label, config['batch_size'], shuffle=True, seed=config['seed'])\n",
    "test_X=np.load(dir_path+test_file)\n",
    "test_label=np.load(dir_path+test_label_file)\n",
    "test_X=pre_processing(test_X,config['pre-process'])\n",
    "test_dataloader=Dataloader(test_X, test_label, config['batch_size'], shuffle=False, seed=config['seed'])\n",
    "\n",
    "# model = build_model(config['layers'])\n",
    "model = MLP_V2()\n",
    "trainer=Trainer(config,model,train_dataloader,test_dataloader)\n",
    "trainer.train()\n"
   ],
   "id": "d376e649804a297d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process: standardization\n",
      "Pre-process: standardization\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MLP_V2' object has no attribute 'params'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# model = build_model(config['layers'])\u001B[39;00m\n\u001B[0;32m      9\u001B[0m model \u001B[38;5;241m=\u001B[39m MLP_V2()\n\u001B[1;32m---> 10\u001B[0m trainer\u001B[38;5;241m=\u001B[39mTrainer(config,model,train_dataloader,test_dataloader)\n\u001B[0;32m     11\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "Cell \u001B[1;32mIn[30], line 25\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, config, model, train_loader, valid_loader)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion\u001B[38;5;241m=\u001B[39mCrossEntropy()\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msgd\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m---> 25\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer \u001B[38;5;241m=\u001B[39m SGDMomentum(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mparams,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmomentum\u001B[39m\u001B[38;5;124m'\u001B[39m],\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'MLP_V2' object has no attribute 'params'"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fa5126bd65ef1423",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
