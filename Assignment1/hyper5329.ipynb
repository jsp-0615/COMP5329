{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2LlXicIB-XA",
        "outputId": "edff9e03-636b-436a-ad08-53caa9030d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: [61][19/78]\tTime 0.047 (Avg-Time 0.081)\t Loss 1.1043 (Avg-Loss 1.0325)\tAcc 61.7188 (Avg-Acc 63.1836)\n",
            "Epoch: [61][38/78]\tTime 0.047 (Avg-Time 0.066)\t Loss 0.9973 (Avg-Loss 1.0279)\tAcc 65.0391 (Avg-Acc 63.5517)\n",
            "Epoch: [61][57/78]\tTime 0.052 (Avg-Time 0.060)\t Loss 1.0339 (Avg-Loss 1.0319)\tAcc 65.6250 (Avg-Acc 63.5237)\n",
            "Epoch: [61][76/78]\tTime 0.048 (Avg-Time 0.058)\t Loss 1.0527 (Avg-Loss 1.0382)\tAcc 61.9141 (Avg-Acc 63.2787)\n",
            "Epoch: [61][78/78]\tTime 0.009 (Avg-Time 0.057)\t Loss 1.1588 (Avg-Loss 1.0382)\tAcc 54.6875 (Avg-Acc 63.2875)\n",
            "EPOCH: 61 train Results: Acc 63.288 Loss: 1.0382\n",
            "Epoch: [61][0/19]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1804 (Avg-Loss 1.1804)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [61][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2617 (Avg-Loss 1.2250)\tAcc 51.8382 (Avg-Acc 56.7500)\n",
            "EPOCH: 61 Validation Results: Acc 56.750 Loss: 1.2250\n",
            "Epoch: [62][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9639 (Avg-Loss 0.9639)\tAcc 65.4297 (Avg-Acc 65.4297)\n",
            "Epoch: [62][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0171 (Avg-Loss 0.9962)\tAcc 63.8672 (Avg-Acc 65.3711)\n",
            "Epoch: [62][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0507 (Avg-Loss 1.0135)\tAcc 62.8906 (Avg-Acc 64.4932)\n",
            "Epoch: [62][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0934 (Avg-Loss 1.0175)\tAcc 62.1094 (Avg-Acc 64.5508)\n",
            "Epoch: [62][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.1525 (Avg-Loss 1.0278)\tAcc 59.1797 (Avg-Acc 64.1208)\n",
            "Epoch: [62][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0797 (Avg-Loss 1.0282)\tAcc 57.8125 (Avg-Acc 64.1200)\n",
            "EPOCH: 62 train Results: Acc 64.120 Loss: 1.0282\n",
            "Epoch: [62][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2011 (Avg-Loss 1.2011)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [62][19/19]\tTime 0.010 (Avg-Time 0.013)\t Loss 1.2514 (Avg-Loss 1.2294)\tAcc 54.0441 (Avg-Acc 56.4500)\n",
            "EPOCH: 62 Validation Results: Acc 56.450 Loss: 1.2294\n",
            "Epoch: [63][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0131 (Avg-Loss 1.0131)\tAcc 64.8438 (Avg-Acc 64.8438)\n",
            "Epoch: [63][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9527 (Avg-Loss 0.9984)\tAcc 65.8203 (Avg-Acc 65.2539)\n",
            "Epoch: [63][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0717 (Avg-Loss 1.0104)\tAcc 61.1328 (Avg-Acc 64.6685)\n",
            "Epoch: [63][57/78]\tTime 0.166 (Avg-Time 0.068)\t Loss 0.9940 (Avg-Loss 1.0228)\tAcc 64.8438 (Avg-Acc 64.2915)\n",
            "Epoch: [63][76/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 1.0270 (Avg-Loss 1.0280)\tAcc 63.2812 (Avg-Acc 64.0600)\n",
            "Epoch: [63][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.0197 (Avg-Loss 1.0284)\tAcc 64.0625 (Avg-Acc 64.0450)\n",
            "EPOCH: 63 train Results: Acc 64.045 Loss: 1.0284\n",
            "Epoch: [63][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1905 (Avg-Loss 1.1905)\tAcc 60.1562 (Avg-Acc 60.1562)\n",
            "Epoch: [63][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2508 (Avg-Loss 1.2201)\tAcc 54.7794 (Avg-Acc 56.7700)\n",
            "EPOCH: 63 Validation Results: Acc 56.770 Loss: 1.2201\n",
            "Epoch: [64][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9543 (Avg-Loss 0.9543)\tAcc 65.8203 (Avg-Acc 65.8203)\n",
            "Epoch: [64][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9785 (Avg-Loss 0.9954)\tAcc 66.7969 (Avg-Acc 65.3711)\n",
            "Epoch: [64][38/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 0.9841 (Avg-Loss 1.0063)\tAcc 65.6250 (Avg-Acc 64.6885)\n",
            "Epoch: [64][57/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.0548 (Avg-Loss 1.0111)\tAcc 64.4531 (Avg-Acc 64.6249)\n",
            "Epoch: [64][76/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.0252 (Avg-Loss 1.0199)\tAcc 63.6719 (Avg-Acc 64.1031)\n",
            "Epoch: [64][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.1935 (Avg-Loss 1.0208)\tAcc 57.8125 (Avg-Acc 64.0775)\n",
            "EPOCH: 64 train Results: Acc 64.078 Loss: 1.0208\n",
            "Epoch: [64][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1964 (Avg-Loss 1.1964)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [64][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2406 (Avg-Loss 1.2223)\tAcc 56.2500 (Avg-Acc 56.5200)\n",
            "EPOCH: 64 Validation Results: Acc 56.520 Loss: 1.2223\n",
            "Epoch: [65][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9076 (Avg-Loss 0.9076)\tAcc 72.6562 (Avg-Acc 72.6562)\n",
            "Epoch: [65][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9886 (Avg-Loss 0.9976)\tAcc 65.0391 (Avg-Acc 65.0391)\n",
            "Epoch: [65][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9780 (Avg-Loss 1.0092)\tAcc 65.6250 (Avg-Acc 64.6985)\n",
            "Epoch: [65][57/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0566 (Avg-Loss 1.0178)\tAcc 66.4062 (Avg-Acc 64.3588)\n",
            "Epoch: [65][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0321 (Avg-Loss 1.0216)\tAcc 64.2578 (Avg-Acc 64.2375)\n",
            "Epoch: [65][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.3135 (Avg-Loss 1.0227)\tAcc 57.8125 (Avg-Acc 64.2000)\n",
            "EPOCH: 65 train Results: Acc 64.200 Loss: 1.0227\n",
            "Epoch: [65][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1828 (Avg-Loss 1.1828)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [65][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2552 (Avg-Loss 1.2229)\tAcc 55.5147 (Avg-Acc 56.7600)\n",
            "EPOCH: 65 Validation Results: Acc 56.760 Loss: 1.2229\n",
            "Epoch: [66][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 1.0389 (Avg-Loss 1.0389)\tAcc 61.5234 (Avg-Acc 61.5234)\n",
            "Epoch: [66][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0177 (Avg-Loss 1.0051)\tAcc 64.6484 (Avg-Acc 64.6777)\n",
            "Epoch: [66][38/78]\tTime 0.104 (Avg-Time 0.097)\t Loss 1.0806 (Avg-Loss 1.0055)\tAcc 59.9609 (Avg-Acc 64.7236)\n",
            "Epoch: [66][57/78]\tTime 0.048 (Avg-Time 0.085)\t Loss 0.9431 (Avg-Loss 1.0073)\tAcc 68.1641 (Avg-Acc 64.6855)\n",
            "Epoch: [66][76/78]\tTime 0.047 (Avg-Time 0.077)\t Loss 1.0646 (Avg-Loss 1.0185)\tAcc 62.3047 (Avg-Acc 64.3085)\n",
            "Epoch: [66][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.2727 (Avg-Loss 1.0179)\tAcc 56.2500 (Avg-Acc 64.3500)\n",
            "EPOCH: 66 train Results: Acc 64.350 Loss: 1.0179\n",
            "Epoch: [66][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1907 (Avg-Loss 1.1907)\tAcc 60.1562 (Avg-Acc 60.1562)\n",
            "Epoch: [66][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2777 (Avg-Loss 1.2176)\tAcc 52.5735 (Avg-Acc 56.8900)\n",
            "EPOCH: 66 Validation Results: Acc 56.890 Loss: 1.2176\n",
            "Epoch: [67][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0442 (Avg-Loss 1.0442)\tAcc 64.4531 (Avg-Acc 64.4531)\n",
            "Epoch: [67][19/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 0.9488 (Avg-Loss 0.9872)\tAcc 66.2109 (Avg-Acc 65.4785)\n",
            "Epoch: [67][38/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.0963 (Avg-Loss 0.9922)\tAcc 60.3516 (Avg-Acc 65.3095)\n",
            "Epoch: [67][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1081 (Avg-Loss 1.0014)\tAcc 61.9141 (Avg-Acc 64.9953)\n",
            "Epoch: [67][76/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0752 (Avg-Loss 1.0145)\tAcc 61.5234 (Avg-Acc 64.5318)\n",
            "Epoch: [67][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 0.7224 (Avg-Loss 1.0146)\tAcc 78.1250 (Avg-Acc 64.5300)\n",
            "EPOCH: 67 train Results: Acc 64.530 Loss: 1.0146\n",
            "Epoch: [67][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1444 (Avg-Loss 1.1444)\tAcc 60.3516 (Avg-Acc 60.3516)\n",
            "Epoch: [67][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2345 (Avg-Loss 1.2155)\tAcc 58.0882 (Avg-Acc 57.1000)\n",
            "EPOCH: 67 Validation Results: Acc 57.100 Loss: 1.2155\n",
            "Epoch: [68][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.9999 (Avg-Loss 0.9999)\tAcc 65.2344 (Avg-Acc 65.2344)\n",
            "Epoch: [68][19/78]\tTime 0.046 (Avg-Time 0.051)\t Loss 1.0603 (Avg-Loss 0.9739)\tAcc 61.7188 (Avg-Acc 66.1914)\n",
            "Epoch: [68][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0250 (Avg-Loss 0.9886)\tAcc 63.0859 (Avg-Acc 65.6751)\n",
            "Epoch: [68][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0168 (Avg-Loss 0.9946)\tAcc 64.0625 (Avg-Acc 65.3994)\n",
            "Epoch: [68][76/78]\tTime 0.089 (Avg-Time 0.051)\t Loss 0.9987 (Avg-Loss 1.0086)\tAcc 65.8203 (Avg-Acc 64.8082)\n",
            "Epoch: [68][78/78]\tTime 0.023 (Avg-Time 0.051)\t Loss 0.8757 (Avg-Loss 1.0085)\tAcc 70.3125 (Avg-Acc 64.8125)\n",
            "EPOCH: 68 train Results: Acc 64.812 Loss: 1.0085\n",
            "Epoch: [68][0/19]\tTime 0.021 (Avg-Time 0.021)\t Loss 1.1701 (Avg-Loss 1.1701)\tAcc 60.7422 (Avg-Acc 60.7422)\n",
            "Epoch: [68][19/19]\tTime 0.059 (Avg-Time 0.033)\t Loss 1.2604 (Avg-Loss 1.2179)\tAcc 55.8824 (Avg-Acc 57.2600)\n",
            "EPOCH: 68 Validation Results: Acc 57.260 Loss: 1.2179\n",
            "Epoch: [69][0/78]\tTime 0.281 (Avg-Time 0.281)\t Loss 0.9797 (Avg-Loss 0.9797)\tAcc 66.6016 (Avg-Acc 66.6016)\n",
            "Epoch: [69][19/78]\tTime 0.048 (Avg-Time 0.128)\t Loss 0.9899 (Avg-Loss 0.9662)\tAcc 63.2812 (Avg-Acc 66.5625)\n",
            "Epoch: [69][38/78]\tTime 0.048 (Avg-Time 0.090)\t Loss 0.9228 (Avg-Loss 0.9782)\tAcc 68.5547 (Avg-Acc 65.7402)\n",
            "Epoch: [69][57/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 1.0403 (Avg-Loss 0.9917)\tAcc 65.4297 (Avg-Acc 65.3017)\n",
            "Epoch: [69][76/78]\tTime 0.048 (Avg-Time 0.070)\t Loss 1.0080 (Avg-Loss 1.0037)\tAcc 64.8438 (Avg-Acc 64.7829)\n",
            "Epoch: [69][78/78]\tTime 0.009 (Avg-Time 0.069)\t Loss 1.2263 (Avg-Loss 1.0052)\tAcc 56.2500 (Avg-Acc 64.7000)\n",
            "EPOCH: 69 train Results: Acc 64.700 Loss: 1.0052\n",
            "Epoch: [69][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.1846 (Avg-Loss 1.1846)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [69][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2324 (Avg-Loss 1.2082)\tAcc 54.7794 (Avg-Acc 57.5300)\n",
            "EPOCH: 69 Validation Results: Acc 57.530 Loss: 1.2082\n",
            "Epoch: [70][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9403 (Avg-Loss 0.9403)\tAcc 69.3359 (Avg-Acc 69.3359)\n",
            "Epoch: [70][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9900 (Avg-Loss 0.9772)\tAcc 66.2109 (Avg-Acc 66.4062)\n",
            "Epoch: [70][38/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9024 (Avg-Loss 0.9856)\tAcc 69.3359 (Avg-Acc 65.7352)\n",
            "Epoch: [70][57/78]\tTime 0.050 (Avg-Time 0.051)\t Loss 0.9991 (Avg-Loss 0.9974)\tAcc 63.6719 (Avg-Acc 65.0593)\n",
            "Epoch: [70][76/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 0.9817 (Avg-Loss 1.0056)\tAcc 66.4062 (Avg-Acc 64.7753)\n",
            "Epoch: [70][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.3097 (Avg-Loss 1.0068)\tAcc 56.2500 (Avg-Acc 64.7225)\n",
            "EPOCH: 70 train Results: Acc 64.722 Loss: 1.0068\n",
            "Epoch: [70][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1583 (Avg-Loss 1.1583)\tAcc 61.3281 (Avg-Acc 61.3281)\n",
            "Epoch: [70][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2298 (Avg-Loss 1.2153)\tAcc 55.5147 (Avg-Acc 57.5100)\n",
            "EPOCH: 70 Validation Results: Acc 57.510 Loss: 1.2153\n",
            "Epoch: [71][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9047 (Avg-Loss 0.9047)\tAcc 69.5312 (Avg-Acc 69.5312)\n",
            "Epoch: [71][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9924 (Avg-Loss 0.9682)\tAcc 66.7969 (Avg-Acc 66.3965)\n",
            "Epoch: [71][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9525 (Avg-Loss 0.9878)\tAcc 66.7969 (Avg-Acc 65.6100)\n",
            "Epoch: [71][57/78]\tTime 0.176 (Avg-Time 0.066)\t Loss 0.9818 (Avg-Loss 0.9941)\tAcc 62.5000 (Avg-Acc 65.1064)\n",
            "Epoch: [71][76/78]\tTime 0.046 (Avg-Time 0.076)\t Loss 1.0185 (Avg-Loss 1.0002)\tAcc 63.8672 (Avg-Acc 64.9959)\n",
            "Epoch: [71][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.1966 (Avg-Loss 1.0014)\tAcc 54.6875 (Avg-Acc 64.9075)\n",
            "EPOCH: 71 train Results: Acc 64.907 Loss: 1.0014\n",
            "Epoch: [71][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1754 (Avg-Loss 1.1754)\tAcc 60.5469 (Avg-Acc 60.5469)\n",
            "Epoch: [71][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2442 (Avg-Loss 1.2119)\tAcc 56.6176 (Avg-Acc 57.4300)\n",
            "EPOCH: 71 Validation Results: Acc 57.430 Loss: 1.2119\n",
            "Epoch: [72][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8878 (Avg-Loss 0.8878)\tAcc 69.9219 (Avg-Acc 69.9219)\n",
            "Epoch: [72][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9369 (Avg-Loss 0.9722)\tAcc 69.7266 (Avg-Acc 66.2207)\n",
            "Epoch: [72][38/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 1.0300 (Avg-Loss 0.9883)\tAcc 63.6719 (Avg-Acc 65.5599)\n",
            "Epoch: [72][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0259 (Avg-Loss 0.9905)\tAcc 67.3828 (Avg-Acc 65.5374)\n",
            "Epoch: [72][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0585 (Avg-Loss 0.9970)\tAcc 63.8672 (Avg-Acc 65.2496)\n",
            "Epoch: [72][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.2122 (Avg-Loss 0.9991)\tAcc 54.6875 (Avg-Acc 65.1750)\n",
            "EPOCH: 72 train Results: Acc 65.175 Loss: 0.9991\n",
            "Epoch: [72][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2019 (Avg-Loss 1.2019)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [72][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3032 (Avg-Loss 1.2192)\tAcc 54.4118 (Avg-Acc 57.0800)\n",
            "EPOCH: 72 Validation Results: Acc 57.080 Loss: 1.2192\n",
            "Epoch: [73][0/78]\tTime 0.057 (Avg-Time 0.057)\t Loss 0.8878 (Avg-Loss 0.8878)\tAcc 70.1172 (Avg-Acc 70.1172)\n",
            "Epoch: [73][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9749 (Avg-Loss 0.9515)\tAcc 66.7969 (Avg-Acc 66.8262)\n",
            "Epoch: [73][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0049 (Avg-Loss 0.9705)\tAcc 66.2109 (Avg-Acc 66.0357)\n",
            "Epoch: [73][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9445 (Avg-Loss 0.9841)\tAcc 68.5547 (Avg-Acc 65.7664)\n",
            "Epoch: [73][76/78]\tTime 0.068 (Avg-Time 0.050)\t Loss 1.0222 (Avg-Loss 0.9998)\tAcc 63.4766 (Avg-Acc 65.2699)\n",
            "Epoch: [73][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.2118 (Avg-Loss 1.0006)\tAcc 51.5625 (Avg-Acc 65.2450)\n",
            "EPOCH: 73 train Results: Acc 65.245 Loss: 1.0006\n",
            "Epoch: [73][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1863 (Avg-Loss 1.1863)\tAcc 60.1562 (Avg-Acc 60.1562)\n",
            "Epoch: [73][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2775 (Avg-Loss 1.2144)\tAcc 54.4118 (Avg-Acc 57.7400)\n",
            "EPOCH: 73 Validation Results: Acc 57.740 Loss: 1.2144\n",
            "Epoch: [74][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9323 (Avg-Loss 0.9323)\tAcc 67.5781 (Avg-Acc 67.5781)\n",
            "Epoch: [74][19/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 0.9504 (Avg-Loss 0.9672)\tAcc 65.4297 (Avg-Acc 65.5859)\n",
            "Epoch: [74][38/78]\tTime 0.080 (Avg-Time 0.095)\t Loss 0.9761 (Avg-Loss 0.9856)\tAcc 68.9453 (Avg-Acc 65.3145)\n",
            "Epoch: [74][57/78]\tTime 0.070 (Avg-Time 0.085)\t Loss 0.9999 (Avg-Loss 0.9926)\tAcc 65.2344 (Avg-Acc 65.0020)\n",
            "Epoch: [74][76/78]\tTime 0.046 (Avg-Time 0.076)\t Loss 1.0068 (Avg-Loss 1.0018)\tAcc 64.4531 (Avg-Acc 64.6966)\n",
            "Epoch: [74][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.0647 (Avg-Loss 1.0015)\tAcc 59.3750 (Avg-Acc 64.7275)\n",
            "EPOCH: 74 train Results: Acc 64.728 Loss: 1.0015\n",
            "Epoch: [74][0/19]\tTime 0.032 (Avg-Time 0.032)\t Loss 1.1844 (Avg-Loss 1.1844)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [74][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2808 (Avg-Loss 1.2180)\tAcc 54.7794 (Avg-Acc 57.6000)\n",
            "EPOCH: 74 Validation Results: Acc 57.600 Loss: 1.2180\n",
            "Epoch: [75][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9250 (Avg-Loss 0.9250)\tAcc 67.5781 (Avg-Acc 67.5781)\n",
            "Epoch: [75][19/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9117 (Avg-Loss 0.9448)\tAcc 70.8984 (Avg-Acc 67.5586)\n",
            "Epoch: [75][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9187 (Avg-Loss 0.9697)\tAcc 68.7500 (Avg-Acc 66.3762)\n",
            "Epoch: [75][57/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 1.0429 (Avg-Loss 0.9842)\tAcc 64.4531 (Avg-Acc 65.8473)\n",
            "Epoch: [75][76/78]\tTime 0.050 (Avg-Time 0.049)\t Loss 1.0640 (Avg-Loss 0.9903)\tAcc 62.6953 (Avg-Acc 65.4094)\n",
            "Epoch: [75][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0742 (Avg-Loss 0.9915)\tAcc 56.2500 (Avg-Acc 65.3350)\n",
            "EPOCH: 75 train Results: Acc 65.335 Loss: 0.9915\n",
            "Epoch: [75][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1970 (Avg-Loss 1.1970)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [75][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3113 (Avg-Loss 1.2151)\tAcc 54.4118 (Avg-Acc 57.5600)\n",
            "EPOCH: 75 Validation Results: Acc 57.560 Loss: 1.2151\n",
            "Epoch: [76][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9277 (Avg-Loss 0.9277)\tAcc 68.9453 (Avg-Acc 68.9453)\n",
            "Epoch: [76][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9689 (Avg-Loss 0.9571)\tAcc 65.2344 (Avg-Acc 66.7578)\n",
            "Epoch: [76][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0051 (Avg-Loss 0.9725)\tAcc 64.2578 (Avg-Acc 66.0507)\n",
            "Epoch: [76][57/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9975 (Avg-Loss 0.9783)\tAcc 62.8906 (Avg-Acc 65.8708)\n",
            "Epoch: [76][76/78]\tTime 0.095 (Avg-Time 0.051)\t Loss 1.0268 (Avg-Loss 0.9881)\tAcc 63.4766 (Avg-Acc 65.5920)\n",
            "Epoch: [76][78/78]\tTime 0.016 (Avg-Time 0.051)\t Loss 1.0199 (Avg-Loss 0.9882)\tAcc 65.6250 (Avg-Acc 65.6100)\n",
            "EPOCH: 76 train Results: Acc 65.610 Loss: 0.9882\n",
            "Epoch: [76][0/19]\tTime 0.032 (Avg-Time 0.032)\t Loss 1.1829 (Avg-Loss 1.1829)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [76][19/19]\tTime 0.025 (Avg-Time 0.028)\t Loss 1.2837 (Avg-Loss 1.2173)\tAcc 55.1471 (Avg-Acc 57.0300)\n",
            "EPOCH: 76 Validation Results: Acc 57.030 Loss: 1.2173\n",
            "Epoch: [77][0/78]\tTime 0.230 (Avg-Time 0.230)\t Loss 0.9124 (Avg-Loss 0.9124)\tAcc 65.0391 (Avg-Acc 65.0391)\n",
            "Epoch: [77][19/78]\tTime 0.047 (Avg-Time 0.134)\t Loss 1.0614 (Avg-Loss 0.9587)\tAcc 62.3047 (Avg-Acc 66.2598)\n",
            "Epoch: [77][38/78]\tTime 0.065 (Avg-Time 0.093)\t Loss 0.9362 (Avg-Loss 0.9676)\tAcc 68.7500 (Avg-Acc 66.1709)\n",
            "Epoch: [77][57/78]\tTime 0.048 (Avg-Time 0.079)\t Loss 0.9398 (Avg-Loss 0.9830)\tAcc 67.3828 (Avg-Acc 65.5610)\n",
            "Epoch: [77][76/78]\tTime 0.048 (Avg-Time 0.071)\t Loss 1.0546 (Avg-Loss 0.9907)\tAcc 61.9141 (Avg-Acc 65.2800)\n",
            "Epoch: [77][78/78]\tTime 0.009 (Avg-Time 0.070)\t Loss 0.9070 (Avg-Loss 0.9917)\tAcc 67.1875 (Avg-Acc 65.2575)\n",
            "EPOCH: 77 train Results: Acc 65.257 Loss: 0.9917\n",
            "Epoch: [77][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2024 (Avg-Loss 1.2024)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [77][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2788 (Avg-Loss 1.2241)\tAcc 54.7794 (Avg-Acc 57.2100)\n",
            "EPOCH: 77 Validation Results: Acc 57.210 Loss: 1.2241\n",
            "Epoch: [78][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9518 (Avg-Loss 0.9518)\tAcc 65.8203 (Avg-Acc 65.8203)\n",
            "Epoch: [78][19/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0066 (Avg-Loss 0.9327)\tAcc 64.4531 (Avg-Acc 67.1680)\n",
            "Epoch: [78][38/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9846 (Avg-Loss 0.9580)\tAcc 64.0625 (Avg-Acc 66.2210)\n",
            "Epoch: [78][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9575 (Avg-Loss 0.9705)\tAcc 67.9688 (Avg-Acc 65.8574)\n",
            "Epoch: [78][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9593 (Avg-Loss 0.9825)\tAcc 68.1641 (Avg-Acc 65.5616)\n",
            "Epoch: [78][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0223 (Avg-Loss 0.9831)\tAcc 67.1875 (Avg-Acc 65.5275)\n",
            "EPOCH: 78 train Results: Acc 65.528 Loss: 0.9831\n",
            "Epoch: [78][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.1752 (Avg-Loss 1.1752)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [78][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2790 (Avg-Loss 1.2159)\tAcc 55.5147 (Avg-Acc 56.9800)\n",
            "EPOCH: 78 Validation Results: Acc 56.980 Loss: 1.2159\n",
            "Epoch: [79][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9231 (Avg-Loss 0.9231)\tAcc 67.7734 (Avg-Acc 67.7734)\n",
            "Epoch: [79][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 1.0190 (Avg-Loss 0.9515)\tAcc 66.6016 (Avg-Acc 66.8262)\n",
            "Epoch: [79][38/78]\tTime 0.084 (Avg-Time 0.050)\t Loss 0.9852 (Avg-Loss 0.9588)\tAcc 66.2109 (Avg-Acc 66.5715)\n",
            "Epoch: [79][57/78]\tTime 0.183 (Avg-Time 0.059)\t Loss 0.9591 (Avg-Loss 0.9746)\tAcc 65.0391 (Avg-Acc 66.0224)\n",
            "Epoch: [79][76/78]\tTime 0.050 (Avg-Time 0.075)\t Loss 1.0450 (Avg-Loss 0.9861)\tAcc 63.2812 (Avg-Acc 65.4424)\n",
            "Epoch: [79][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 1.0021 (Avg-Loss 0.9873)\tAcc 70.3125 (Avg-Acc 65.4075)\n",
            "EPOCH: 79 train Results: Acc 65.407 Loss: 0.9873\n",
            "Epoch: [79][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1850 (Avg-Loss 1.1850)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [79][19/19]\tTime 0.017 (Avg-Time 0.014)\t Loss 1.2959 (Avg-Loss 1.2092)\tAcc 54.0441 (Avg-Acc 57.2000)\n",
            "EPOCH: 79 Validation Results: Acc 57.200 Loss: 1.2092\n",
            "Epoch: [80][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.8954 (Avg-Loss 0.8954)\tAcc 68.1641 (Avg-Acc 68.1641)\n",
            "Epoch: [80][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9177 (Avg-Loss 0.9367)\tAcc 66.4062 (Avg-Acc 67.2461)\n",
            "Epoch: [80][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9724 (Avg-Loss 0.9540)\tAcc 66.6016 (Avg-Acc 66.7568)\n",
            "Epoch: [80][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0175 (Avg-Loss 0.9655)\tAcc 62.1094 (Avg-Acc 66.3153)\n",
            "Epoch: [80][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0078 (Avg-Loss 0.9790)\tAcc 65.6250 (Avg-Acc 65.6935)\n",
            "Epoch: [80][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0546 (Avg-Loss 0.9790)\tAcc 54.6875 (Avg-Acc 65.6775)\n",
            "EPOCH: 80 train Results: Acc 65.677 Loss: 0.9790\n",
            "Epoch: [80][0/19]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2117 (Avg-Loss 1.2117)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [80][19/19]\tTime 0.011 (Avg-Time 0.013)\t Loss 1.3045 (Avg-Loss 1.2138)\tAcc 53.6765 (Avg-Acc 57.0900)\n",
            "EPOCH: 80 Validation Results: Acc 57.090 Loss: 1.2138\n",
            "Epoch: [81][0/78]\tTime 0.066 (Avg-Time 0.066)\t Loss 0.8924 (Avg-Loss 0.8924)\tAcc 67.5781 (Avg-Acc 67.5781)\n",
            "Epoch: [81][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9349 (Avg-Loss 0.9482)\tAcc 66.7969 (Avg-Acc 67.1680)\n",
            "Epoch: [81][38/78]\tTime 0.053 (Avg-Time 0.050)\t Loss 0.9099 (Avg-Loss 0.9677)\tAcc 69.3359 (Avg-Acc 66.1909)\n",
            "Epoch: [81][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9619 (Avg-Loss 0.9710)\tAcc 68.3594 (Avg-Acc 66.0493)\n",
            "Epoch: [81][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0195 (Avg-Loss 0.9801)\tAcc 60.9375 (Avg-Acc 65.6275)\n",
            "Epoch: [81][78/78]\tTime 0.010 (Avg-Time 0.050)\t Loss 0.9615 (Avg-Loss 0.9809)\tAcc 68.7500 (Avg-Acc 65.6000)\n",
            "EPOCH: 81 train Results: Acc 65.600 Loss: 0.9809\n",
            "Epoch: [81][0/19]\tTime 0.017 (Avg-Time 0.017)\t Loss 1.2239 (Avg-Loss 1.2239)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [81][19/19]\tTime 0.011 (Avg-Time 0.013)\t Loss 1.2813 (Avg-Loss 1.2212)\tAcc 52.2059 (Avg-Acc 57.1500)\n",
            "EPOCH: 81 Validation Results: Acc 57.150 Loss: 1.2212\n",
            "Epoch: [82][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.9415 (Avg-Loss 0.9415)\tAcc 67.5781 (Avg-Acc 67.5781)\n",
            "Epoch: [82][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 1.0089 (Avg-Loss 0.9688)\tAcc 63.6719 (Avg-Acc 66.4062)\n",
            "Epoch: [82][38/78]\tTime 0.110 (Avg-Time 0.089)\t Loss 0.9821 (Avg-Loss 0.9689)\tAcc 66.4062 (Avg-Acc 66.4413)\n",
            "Epoch: [82][57/78]\tTime 0.048 (Avg-Time 0.086)\t Loss 0.9345 (Avg-Loss 0.9682)\tAcc 65.6250 (Avg-Acc 66.4399)\n",
            "Epoch: [82][76/78]\tTime 0.048 (Avg-Time 0.077)\t Loss 0.9441 (Avg-Loss 0.9775)\tAcc 66.9922 (Avg-Acc 65.8989)\n",
            "Epoch: [82][78/78]\tTime 0.009 (Avg-Time 0.076)\t Loss 1.1677 (Avg-Loss 0.9780)\tAcc 57.8125 (Avg-Acc 65.8700)\n",
            "EPOCH: 82 train Results: Acc 65.870 Loss: 0.9780\n",
            "Epoch: [82][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1750 (Avg-Loss 1.1750)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [82][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2834 (Avg-Loss 1.2093)\tAcc 52.2059 (Avg-Acc 57.1600)\n",
            "EPOCH: 82 Validation Results: Acc 57.160 Loss: 1.2093\n",
            "Epoch: [83][0/78]\tTime 0.055 (Avg-Time 0.055)\t Loss 0.9120 (Avg-Loss 0.9120)\tAcc 69.7266 (Avg-Acc 69.7266)\n",
            "Epoch: [83][19/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.9479 (Avg-Loss 0.9398)\tAcc 67.5781 (Avg-Acc 67.6562)\n",
            "Epoch: [83][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0370 (Avg-Loss 0.9576)\tAcc 65.4297 (Avg-Acc 66.6967)\n",
            "Epoch: [83][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9534 (Avg-Loss 0.9696)\tAcc 65.0391 (Avg-Acc 66.0897)\n",
            "Epoch: [83][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0119 (Avg-Loss 0.9777)\tAcc 63.6719 (Avg-Acc 65.7797)\n",
            "Epoch: [83][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 0.9124 (Avg-Loss 0.9784)\tAcc 67.1875 (Avg-Acc 65.7800)\n",
            "EPOCH: 83 train Results: Acc 65.780 Loss: 0.9784\n",
            "Epoch: [83][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1734 (Avg-Loss 1.1734)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [83][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3267 (Avg-Loss 1.2202)\tAcc 51.8382 (Avg-Acc 56.7400)\n",
            "EPOCH: 83 Validation Results: Acc 56.740 Loss: 1.2202\n",
            "Epoch: [84][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9515 (Avg-Loss 0.9515)\tAcc 66.9922 (Avg-Acc 66.9922)\n",
            "Epoch: [84][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9079 (Avg-Loss 0.9423)\tAcc 68.7500 (Avg-Acc 67.1582)\n",
            "Epoch: [84][38/78]\tTime 0.053 (Avg-Time 0.050)\t Loss 0.9225 (Avg-Loss 0.9607)\tAcc 66.0156 (Avg-Acc 66.5865)\n",
            "Epoch: [84][57/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9819 (Avg-Loss 0.9708)\tAcc 66.2109 (Avg-Acc 66.0830)\n",
            "Epoch: [84][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0246 (Avg-Loss 0.9789)\tAcc 64.2578 (Avg-Acc 65.6884)\n",
            "Epoch: [84][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.9156 (Avg-Loss 0.9801)\tAcc 70.3125 (Avg-Acc 65.6075)\n",
            "EPOCH: 84 train Results: Acc 65.608 Loss: 0.9801\n",
            "Epoch: [84][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1617 (Avg-Loss 1.1617)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [84][19/19]\tTime 0.017 (Avg-Time 0.018)\t Loss 1.2940 (Avg-Loss 1.2112)\tAcc 53.3088 (Avg-Acc 57.3000)\n",
            "EPOCH: 84 Validation Results: Acc 57.300 Loss: 1.2112\n",
            "Epoch: [85][0/78]\tTime 0.104 (Avg-Time 0.104)\t Loss 0.8655 (Avg-Loss 0.8655)\tAcc 69.3359 (Avg-Acc 69.3359)\n",
            "Epoch: [85][19/78]\tTime 0.101 (Avg-Time 0.136)\t Loss 0.9533 (Avg-Loss 0.9371)\tAcc 68.1641 (Avg-Acc 67.5391)\n",
            "Epoch: [85][38/78]\tTime 0.047 (Avg-Time 0.099)\t Loss 0.9099 (Avg-Loss 0.9447)\tAcc 68.1641 (Avg-Acc 67.2075)\n",
            "Epoch: [85][57/78]\tTime 0.047 (Avg-Time 0.083)\t Loss 1.0914 (Avg-Loss 0.9617)\tAcc 61.5234 (Avg-Acc 66.5982)\n",
            "Epoch: [85][76/78]\tTime 0.047 (Avg-Time 0.075)\t Loss 0.9492 (Avg-Loss 0.9710)\tAcc 66.4062 (Avg-Acc 66.1653)\n",
            "Epoch: [85][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 1.2774 (Avg-Loss 0.9716)\tAcc 50.0000 (Avg-Acc 66.1400)\n",
            "EPOCH: 85 train Results: Acc 66.140 Loss: 0.9716\n",
            "Epoch: [85][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1993 (Avg-Loss 1.1993)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [85][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2972 (Avg-Loss 1.2125)\tAcc 49.6324 (Avg-Acc 57.0800)\n",
            "EPOCH: 85 Validation Results: Acc 57.080 Loss: 1.2125\n",
            "Epoch: [86][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.8768 (Avg-Loss 0.8768)\tAcc 67.5781 (Avg-Acc 67.5781)\n",
            "Epoch: [86][19/78]\tTime 0.052 (Avg-Time 0.050)\t Loss 0.9478 (Avg-Loss 0.9383)\tAcc 68.5547 (Avg-Acc 67.3828)\n",
            "Epoch: [86][38/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 0.9713 (Avg-Loss 0.9519)\tAcc 67.3828 (Avg-Acc 66.9621)\n",
            "Epoch: [86][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0303 (Avg-Loss 0.9575)\tAcc 61.9141 (Avg-Acc 66.8070)\n",
            "Epoch: [86][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9893 (Avg-Loss 0.9685)\tAcc 66.9922 (Avg-Acc 66.3428)\n",
            "Epoch: [86][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 0.9538 (Avg-Loss 0.9697)\tAcc 62.5000 (Avg-Acc 66.2975)\n",
            "EPOCH: 86 train Results: Acc 66.297 Loss: 0.9697\n",
            "Epoch: [86][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1829 (Avg-Loss 1.1829)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [86][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2784 (Avg-Loss 1.2131)\tAcc 55.8824 (Avg-Acc 57.6800)\n",
            "EPOCH: 86 Validation Results: Acc 57.680 Loss: 1.2131\n",
            "Epoch: [87][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9257 (Avg-Loss 0.9257)\tAcc 68.7500 (Avg-Acc 68.7500)\n",
            "Epoch: [87][19/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9560 (Avg-Loss 0.9377)\tAcc 66.9922 (Avg-Acc 67.3047)\n",
            "Epoch: [87][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9332 (Avg-Loss 0.9462)\tAcc 68.9453 (Avg-Acc 66.8670)\n",
            "Epoch: [87][57/78]\tTime 0.066 (Avg-Time 0.050)\t Loss 0.9600 (Avg-Loss 0.9552)\tAcc 66.6016 (Avg-Acc 66.4938)\n",
            "Epoch: [87][76/78]\tTime 0.092 (Avg-Time 0.074)\t Loss 0.9890 (Avg-Loss 0.9656)\tAcc 64.2578 (Avg-Acc 66.1551)\n",
            "Epoch: [87][78/78]\tTime 0.137 (Avg-Time 0.076)\t Loss 0.9844 (Avg-Loss 0.9661)\tAcc 67.1875 (Avg-Acc 66.1275)\n",
            "EPOCH: 87 train Results: Acc 66.127 Loss: 0.9661\n",
            "Epoch: [87][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1816 (Avg-Loss 1.1816)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [87][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2836 (Avg-Loss 1.2105)\tAcc 56.9853 (Avg-Acc 57.6900)\n",
            "EPOCH: 87 Validation Results: Acc 57.690 Loss: 1.2105\n",
            "Epoch: [88][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.8961 (Avg-Loss 0.8961)\tAcc 68.1641 (Avg-Acc 68.1641)\n",
            "Epoch: [88][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9287 (Avg-Loss 0.9308)\tAcc 66.9922 (Avg-Acc 67.1973)\n",
            "Epoch: [88][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9131 (Avg-Loss 0.9437)\tAcc 68.3594 (Avg-Acc 67.2426)\n",
            "Epoch: [88][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0097 (Avg-Loss 0.9549)\tAcc 63.0859 (Avg-Acc 66.6790)\n",
            "Epoch: [88][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0854 (Avg-Loss 0.9657)\tAcc 61.1328 (Avg-Acc 66.1374)\n",
            "Epoch: [88][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.2531 (Avg-Loss 0.9665)\tAcc 60.9375 (Avg-Acc 66.1100)\n",
            "EPOCH: 88 train Results: Acc 66.110 Loss: 0.9665\n",
            "Epoch: [88][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1900 (Avg-Loss 1.1900)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [88][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2924 (Avg-Loss 1.2094)\tAcc 54.0441 (Avg-Acc 57.5000)\n",
            "EPOCH: 88 Validation Results: Acc 57.500 Loss: 1.2094\n",
            "Epoch: [89][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8500 (Avg-Loss 0.8500)\tAcc 72.0703 (Avg-Acc 72.0703)\n",
            "Epoch: [89][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9344 (Avg-Loss 0.9365)\tAcc 66.6016 (Avg-Acc 67.0605)\n",
            "Epoch: [89][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0197 (Avg-Loss 0.9484)\tAcc 65.4297 (Avg-Acc 66.5915)\n",
            "Epoch: [89][57/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9789 (Avg-Loss 0.9521)\tAcc 66.2109 (Avg-Acc 66.4231)\n",
            "Epoch: [89][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9899 (Avg-Loss 0.9671)\tAcc 66.2109 (Avg-Acc 65.8660)\n",
            "Epoch: [89][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.2088 (Avg-Loss 0.9673)\tAcc 59.3750 (Avg-Acc 65.8375)\n",
            "EPOCH: 89 train Results: Acc 65.838 Loss: 0.9673\n",
            "Epoch: [89][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1710 (Avg-Loss 1.1710)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [89][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2534 (Avg-Loss 1.2168)\tAcc 56.2500 (Avg-Acc 57.4400)\n",
            "EPOCH: 89 Validation Results: Acc 57.440 Loss: 1.2168\n",
            "Epoch: [90][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.8534 (Avg-Loss 0.8534)\tAcc 69.9219 (Avg-Acc 69.9219)\n",
            "Epoch: [90][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9438 (Avg-Loss 0.9398)\tAcc 67.3828 (Avg-Acc 67.8418)\n",
            "Epoch: [90][38/78]\tTime 0.125 (Avg-Time 0.073)\t Loss 1.0071 (Avg-Loss 0.9506)\tAcc 63.6719 (Avg-Acc 67.2075)\n",
            "Epoch: [90][57/78]\tTime 0.047 (Avg-Time 0.085)\t Loss 0.9700 (Avg-Loss 0.9588)\tAcc 65.6250 (Avg-Acc 66.6016)\n",
            "Epoch: [90][76/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 1.0188 (Avg-Loss 0.9652)\tAcc 63.2812 (Avg-Acc 66.3327)\n",
            "Epoch: [90][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.0054 (Avg-Loss 0.9654)\tAcc 59.3750 (Avg-Acc 66.3000)\n",
            "EPOCH: 90 train Results: Acc 66.300 Loss: 0.9654\n",
            "Epoch: [90][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1592 (Avg-Loss 1.1592)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [90][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2700 (Avg-Loss 1.2122)\tAcc 54.7794 (Avg-Acc 57.3800)\n",
            "EPOCH: 90 Validation Results: Acc 57.380 Loss: 1.2122\n",
            "Epoch: [91][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8482 (Avg-Loss 0.8482)\tAcc 69.7266 (Avg-Acc 69.7266)\n",
            "Epoch: [91][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9819 (Avg-Loss 0.9220)\tAcc 66.0156 (Avg-Acc 67.7734)\n",
            "Epoch: [91][38/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 0.9942 (Avg-Loss 0.9452)\tAcc 66.6016 (Avg-Acc 67.0673)\n",
            "Epoch: [91][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9412 (Avg-Loss 0.9572)\tAcc 68.7500 (Avg-Acc 66.4972)\n",
            "Epoch: [91][76/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0299 (Avg-Loss 0.9700)\tAcc 64.2578 (Avg-Acc 66.1501)\n",
            "Epoch: [91][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 0.9561 (Avg-Loss 0.9710)\tAcc 62.5000 (Avg-Acc 66.1025)\n",
            "EPOCH: 91 train Results: Acc 66.103 Loss: 0.9710\n",
            "Epoch: [91][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1883 (Avg-Loss 1.1883)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [91][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2437 (Avg-Loss 1.2128)\tAcc 54.4118 (Avg-Acc 57.3600)\n",
            "EPOCH: 91 Validation Results: Acc 57.360 Loss: 1.2128\n",
            "Epoch: [92][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8874 (Avg-Loss 0.8874)\tAcc 69.9219 (Avg-Acc 69.9219)\n",
            "Epoch: [92][19/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.9479 (Avg-Loss 0.9244)\tAcc 67.3828 (Avg-Acc 67.9980)\n",
            "Epoch: [92][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0260 (Avg-Loss 0.9419)\tAcc 62.1094 (Avg-Acc 67.1775)\n",
            "Epoch: [92][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0429 (Avg-Loss 0.9530)\tAcc 63.8672 (Avg-Acc 66.7060)\n",
            "Epoch: [92][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0239 (Avg-Loss 0.9609)\tAcc 64.2578 (Avg-Acc 66.3682)\n",
            "Epoch: [92][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0793 (Avg-Loss 0.9627)\tAcc 62.5000 (Avg-Acc 66.2975)\n",
            "EPOCH: 92 train Results: Acc 66.297 Loss: 0.9627\n",
            "Epoch: [92][0/19]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.1828 (Avg-Loss 1.1828)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [92][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2704 (Avg-Loss 1.2071)\tAcc 55.1471 (Avg-Acc 57.7400)\n",
            "EPOCH: 92 Validation Results: Acc 57.740 Loss: 1.2071\n",
            "Epoch: [93][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9329 (Avg-Loss 0.9329)\tAcc 67.5781 (Avg-Acc 67.5781)\n",
            "Epoch: [93][19/78]\tTime 0.172 (Avg-Time 0.129)\t Loss 0.8704 (Avg-Loss 0.9329)\tAcc 72.2656 (Avg-Acc 67.3438)\n",
            "Epoch: [93][38/78]\tTime 0.048 (Avg-Time 0.102)\t Loss 0.9296 (Avg-Loss 0.9417)\tAcc 67.7734 (Avg-Acc 67.0373)\n",
            "Epoch: [93][57/78]\tTime 0.047 (Avg-Time 0.085)\t Loss 1.0935 (Avg-Loss 0.9507)\tAcc 63.2812 (Avg-Acc 66.6083)\n",
            "Epoch: [93][76/78]\tTime 0.065 (Avg-Time 0.076)\t Loss 0.9896 (Avg-Loss 0.9594)\tAcc 66.2109 (Avg-Acc 66.3860)\n",
            "Epoch: [93][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 0.9231 (Avg-Loss 0.9602)\tAcc 68.7500 (Avg-Acc 66.3375)\n",
            "EPOCH: 93 train Results: Acc 66.338 Loss: 0.9602\n",
            "Epoch: [93][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1782 (Avg-Loss 1.1782)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [93][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2733 (Avg-Loss 1.2099)\tAcc 54.4118 (Avg-Acc 57.7200)\n",
            "EPOCH: 93 Validation Results: Acc 57.720 Loss: 1.2099\n",
            "Epoch: [94][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.9947 (Avg-Loss 0.9947)\tAcc 65.4297 (Avg-Acc 65.4297)\n",
            "Epoch: [94][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9218 (Avg-Loss 0.9350)\tAcc 67.5781 (Avg-Acc 67.0020)\n",
            "Epoch: [94][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9189 (Avg-Loss 0.9524)\tAcc 70.1172 (Avg-Acc 66.3962)\n",
            "Epoch: [94][57/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0093 (Avg-Loss 0.9574)\tAcc 65.0391 (Avg-Acc 66.1975)\n",
            "Epoch: [94][76/78]\tTime 0.068 (Avg-Time 0.050)\t Loss 1.0047 (Avg-Loss 0.9627)\tAcc 63.4766 (Avg-Acc 65.9573)\n",
            "Epoch: [94][78/78]\tTime 0.010 (Avg-Time 0.049)\t Loss 0.9829 (Avg-Loss 0.9632)\tAcc 68.7500 (Avg-Acc 65.9350)\n",
            "EPOCH: 94 train Results: Acc 65.935 Loss: 0.9632\n",
            "Epoch: [94][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1921 (Avg-Loss 1.1921)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [94][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2711 (Avg-Loss 1.2170)\tAcc 55.1471 (Avg-Acc 57.0200)\n",
            "EPOCH: 94 Validation Results: Acc 57.020 Loss: 1.2170\n",
            "Epoch: [95][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9245 (Avg-Loss 0.9245)\tAcc 68.1641 (Avg-Acc 68.1641)\n",
            "Epoch: [95][19/78]\tTime 0.049 (Avg-Time 0.053)\t Loss 0.8785 (Avg-Loss 0.9259)\tAcc 69.3359 (Avg-Acc 68.1055)\n",
            "Epoch: [95][38/78]\tTime 0.046 (Avg-Time 0.051)\t Loss 0.9231 (Avg-Loss 0.9373)\tAcc 67.1875 (Avg-Acc 67.6532)\n",
            "Epoch: [95][57/78]\tTime 0.052 (Avg-Time 0.051)\t Loss 1.0205 (Avg-Loss 0.9446)\tAcc 64.8438 (Avg-Acc 67.1841)\n",
            "Epoch: [95][76/78]\tTime 0.112 (Avg-Time 0.075)\t Loss 1.0347 (Avg-Loss 0.9570)\tAcc 63.8672 (Avg-Acc 66.7335)\n",
            "Epoch: [95][78/78]\tTime 0.018 (Avg-Time 0.075)\t Loss 1.1096 (Avg-Loss 0.9582)\tAcc 59.3750 (Avg-Acc 66.6675)\n",
            "EPOCH: 95 train Results: Acc 66.668 Loss: 0.9582\n",
            "Epoch: [95][0/19]\tTime 0.027 (Avg-Time 0.027)\t Loss 1.1859 (Avg-Loss 1.1859)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [95][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2553 (Avg-Loss 1.2058)\tAcc 55.5147 (Avg-Acc 58.0700)\n",
            "EPOCH: 95 Validation Results: Acc 58.070 Loss: 1.2058\n",
            "Epoch: [96][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.8820 (Avg-Loss 0.8820)\tAcc 68.9453 (Avg-Acc 68.9453)\n",
            "Epoch: [96][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0357 (Avg-Loss 0.9364)\tAcc 66.0156 (Avg-Acc 67.8418)\n",
            "Epoch: [96][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9741 (Avg-Loss 0.9459)\tAcc 66.6016 (Avg-Acc 67.0873)\n",
            "Epoch: [96][57/78]\tTime 0.056 (Avg-Time 0.050)\t Loss 0.9913 (Avg-Loss 0.9520)\tAcc 62.6953 (Avg-Acc 66.9585)\n",
            "Epoch: [96][76/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0736 (Avg-Loss 0.9594)\tAcc 63.4766 (Avg-Acc 66.6523)\n",
            "Epoch: [96][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1117 (Avg-Loss 0.9597)\tAcc 59.3750 (Avg-Acc 66.6250)\n",
            "EPOCH: 96 train Results: Acc 66.625 Loss: 0.9597\n",
            "Epoch: [96][0/19]\tTime 0.016 (Avg-Time 0.016)\t Loss 1.2183 (Avg-Loss 1.2183)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [96][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2910 (Avg-Loss 1.2198)\tAcc 57.7206 (Avg-Acc 57.4100)\n",
            "EPOCH: 96 Validation Results: Acc 57.410 Loss: 1.2198\n",
            "Epoch: [97][0/78]\tTime 0.053 (Avg-Time 0.053)\t Loss 0.9486 (Avg-Loss 0.9486)\tAcc 66.9922 (Avg-Acc 66.9922)\n",
            "Epoch: [97][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9573 (Avg-Loss 0.9290)\tAcc 65.2344 (Avg-Acc 67.6172)\n",
            "Epoch: [97][38/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9526 (Avg-Loss 0.9385)\tAcc 67.7734 (Avg-Acc 67.0974)\n",
            "Epoch: [97][57/78]\tTime 0.063 (Avg-Time 0.050)\t Loss 0.9635 (Avg-Loss 0.9460)\tAcc 66.0156 (Avg-Acc 66.8844)\n",
            "Epoch: [97][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0637 (Avg-Loss 0.9553)\tAcc 61.1328 (Avg-Acc 66.6117)\n",
            "Epoch: [97][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.0956 (Avg-Loss 0.9564)\tAcc 60.9375 (Avg-Acc 66.5525)\n",
            "EPOCH: 97 train Results: Acc 66.552 Loss: 0.9564\n",
            "Epoch: [97][0/19]\tTime 0.017 (Avg-Time 0.017)\t Loss 1.2111 (Avg-Loss 1.2111)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [97][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2589 (Avg-Loss 1.2180)\tAcc 54.0441 (Avg-Acc 57.1900)\n",
            "EPOCH: 97 Validation Results: Acc 57.190 Loss: 1.2180\n",
            "Epoch: [98][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8772 (Avg-Loss 0.8772)\tAcc 68.5547 (Avg-Acc 68.5547)\n",
            "Epoch: [98][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9400 (Avg-Loss 0.9257)\tAcc 68.3594 (Avg-Acc 67.5879)\n",
            "Epoch: [98][38/78]\tTime 0.097 (Avg-Time 0.076)\t Loss 0.8683 (Avg-Loss 0.9368)\tAcc 69.3359 (Avg-Acc 67.1725)\n",
            "Epoch: [98][57/78]\tTime 0.047 (Avg-Time 0.085)\t Loss 0.9462 (Avg-Loss 0.9508)\tAcc 64.2578 (Avg-Acc 66.5409)\n",
            "Epoch: [98][76/78]\tTime 0.049 (Avg-Time 0.076)\t Loss 0.9361 (Avg-Loss 0.9582)\tAcc 66.7969 (Avg-Acc 66.1881)\n",
            "Epoch: [98][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.0770 (Avg-Loss 0.9590)\tAcc 60.9375 (Avg-Acc 66.1725)\n",
            "EPOCH: 98 train Results: Acc 66.172 Loss: 0.9590\n",
            "Epoch: [98][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1725 (Avg-Loss 1.1725)\tAcc 61.3281 (Avg-Acc 61.3281)\n",
            "Epoch: [98][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2830 (Avg-Loss 1.2110)\tAcc 54.4118 (Avg-Acc 57.9500)\n",
            "EPOCH: 98 Validation Results: Acc 57.950 Loss: 1.2110\n",
            "Epoch: [99][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8922 (Avg-Loss 0.8922)\tAcc 71.2891 (Avg-Acc 71.2891)\n",
            "Epoch: [99][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8983 (Avg-Loss 0.8993)\tAcc 68.3594 (Avg-Acc 69.0820)\n",
            "Epoch: [99][38/78]\tTime 0.070 (Avg-Time 0.050)\t Loss 0.9211 (Avg-Loss 0.9253)\tAcc 68.5547 (Avg-Acc 67.7584)\n",
            "Epoch: [99][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0158 (Avg-Loss 0.9403)\tAcc 62.6953 (Avg-Acc 67.2313)\n",
            "Epoch: [99][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9653 (Avg-Loss 0.9501)\tAcc 64.6484 (Avg-Acc 66.7918)\n",
            "Epoch: [99][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1263 (Avg-Loss 0.9496)\tAcc 59.3750 (Avg-Acc 66.8150)\n",
            "EPOCH: 99 train Results: Acc 66.815 Loss: 0.9496\n",
            "Epoch: [99][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1866 (Avg-Loss 1.1866)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [99][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2846 (Avg-Loss 1.2087)\tAcc 52.9412 (Avg-Acc 57.5600)\n",
            "EPOCH: 99 Validation Results: Acc 57.560 Loss: 1.2087\n",
            "Epoch: [100][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8878 (Avg-Loss 0.8878)\tAcc 69.9219 (Avg-Acc 69.9219)\n",
            "Epoch: [100][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9302 (Avg-Loss 0.9205)\tAcc 66.4062 (Avg-Acc 67.9395)\n",
            "Epoch: [100][38/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9361 (Avg-Loss 0.9340)\tAcc 68.3594 (Avg-Acc 67.5881)\n",
            "Epoch: [100][57/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 1.0016 (Avg-Loss 0.9442)\tAcc 65.8203 (Avg-Acc 66.9147)\n",
            "Epoch: [100][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9867 (Avg-Loss 0.9540)\tAcc 65.4297 (Avg-Acc 66.6168)\n",
            "Epoch: [100][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.9691 (Avg-Loss 0.9540)\tAcc 67.1875 (Avg-Acc 66.5850)\n",
            "EPOCH: 100 train Results: Acc 66.585 Loss: 0.9540\n",
            "Epoch: [100][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1760 (Avg-Loss 1.1760)\tAcc 60.7422 (Avg-Acc 60.7422)\n",
            "Epoch: [100][19/19]\tTime 0.007 (Avg-Time 0.015)\t Loss 1.2866 (Avg-Loss 1.2124)\tAcc 55.1471 (Avg-Acc 57.5800)\n",
            "EPOCH: 100 Validation Results: Acc 57.580 Loss: 1.2124\n",
            "Epoch: [101][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9156 (Avg-Loss 0.9156)\tAcc 66.6016 (Avg-Acc 66.6016)\n",
            "Epoch: [101][19/78]\tTime 0.083 (Avg-Time 0.131)\t Loss 1.0112 (Avg-Loss 0.9264)\tAcc 63.6719 (Avg-Acc 67.8320)\n",
            "Epoch: [101][38/78]\tTime 0.048 (Avg-Time 0.102)\t Loss 0.9066 (Avg-Loss 0.9357)\tAcc 68.7500 (Avg-Acc 67.3427)\n",
            "Epoch: [101][57/78]\tTime 0.047 (Avg-Time 0.085)\t Loss 0.9265 (Avg-Loss 0.9408)\tAcc 69.7266 (Avg-Acc 67.1437)\n",
            "Epoch: [101][76/78]\tTime 0.049 (Avg-Time 0.076)\t Loss 1.0190 (Avg-Loss 0.9548)\tAcc 64.0625 (Avg-Acc 66.5889)\n",
            "Epoch: [101][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.1155 (Avg-Loss 0.9556)\tAcc 60.9375 (Avg-Acc 66.5475)\n",
            "EPOCH: 101 train Results: Acc 66.547 Loss: 0.9556\n",
            "Epoch: [101][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1771 (Avg-Loss 1.1771)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [101][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3060 (Avg-Loss 1.2092)\tAcc 54.0441 (Avg-Acc 57.6200)\n",
            "EPOCH: 101 Validation Results: Acc 57.620 Loss: 1.2092\n",
            "Epoch: [102][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.9174 (Avg-Loss 0.9174)\tAcc 69.7266 (Avg-Acc 69.7266)\n",
            "Epoch: [102][19/78]\tTime 0.065 (Avg-Time 0.049)\t Loss 0.9927 (Avg-Loss 0.9270)\tAcc 65.8203 (Avg-Acc 67.5488)\n",
            "Epoch: [102][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9747 (Avg-Loss 0.9325)\tAcc 67.5781 (Avg-Acc 67.5881)\n",
            "Epoch: [102][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0155 (Avg-Loss 0.9467)\tAcc 62.3047 (Avg-Acc 67.0831)\n",
            "Epoch: [102][76/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0270 (Avg-Loss 0.9531)\tAcc 63.6719 (Avg-Acc 66.8730)\n",
            "Epoch: [102][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0579 (Avg-Loss 0.9533)\tAcc 65.6250 (Avg-Acc 66.8775)\n",
            "EPOCH: 102 train Results: Acc 66.877 Loss: 0.9533\n",
            "Epoch: [102][0/19]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1978 (Avg-Loss 1.1978)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [102][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2636 (Avg-Loss 1.2091)\tAcc 55.1471 (Avg-Acc 57.9000)\n",
            "EPOCH: 102 Validation Results: Acc 57.900 Loss: 1.2091\n",
            "Epoch: [103][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8396 (Avg-Loss 0.8396)\tAcc 69.7266 (Avg-Acc 69.7266)\n",
            "Epoch: [103][19/78]\tTime 0.073 (Avg-Time 0.050)\t Loss 0.9523 (Avg-Loss 0.9148)\tAcc 66.4062 (Avg-Acc 68.1250)\n",
            "Epoch: [103][38/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9484 (Avg-Loss 0.9399)\tAcc 65.8203 (Avg-Acc 67.1474)\n",
            "Epoch: [103][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0129 (Avg-Loss 0.9517)\tAcc 65.2344 (Avg-Acc 66.6285)\n",
            "Epoch: [103][76/78]\tTime 0.240 (Avg-Time 0.064)\t Loss 1.0166 (Avg-Loss 0.9578)\tAcc 67.1875 (Avg-Acc 66.4316)\n",
            "Epoch: [103][78/78]\tTime 0.038 (Avg-Time 0.067)\t Loss 1.0629 (Avg-Loss 0.9580)\tAcc 65.6250 (Avg-Acc 66.4200)\n",
            "EPOCH: 103 train Results: Acc 66.420 Loss: 0.9580\n",
            "Epoch: [103][0/19]\tTime 0.026 (Avg-Time 0.026)\t Loss 1.1792 (Avg-Loss 1.1792)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [103][19/19]\tTime 0.020 (Avg-Time 0.030)\t Loss 1.2551 (Avg-Loss 1.2070)\tAcc 56.2500 (Avg-Acc 57.8500)\n",
            "EPOCH: 103 Validation Results: Acc 57.850 Loss: 1.2070\n",
            "Epoch: [104][0/78]\tTime 0.103 (Avg-Time 0.103)\t Loss 0.8446 (Avg-Loss 0.8446)\tAcc 71.0938 (Avg-Acc 71.0938)\n",
            "Epoch: [104][19/78]\tTime 0.047 (Avg-Time 0.065)\t Loss 0.9061 (Avg-Loss 0.9153)\tAcc 68.7500 (Avg-Acc 68.4863)\n",
            "Epoch: [104][38/78]\tTime 0.047 (Avg-Time 0.058)\t Loss 0.9162 (Avg-Loss 0.9234)\tAcc 66.6016 (Avg-Acc 68.0689)\n",
            "Epoch: [104][57/78]\tTime 0.047 (Avg-Time 0.055)\t Loss 0.9441 (Avg-Loss 0.9335)\tAcc 66.0156 (Avg-Acc 67.6488)\n",
            "Epoch: [104][76/78]\tTime 0.047 (Avg-Time 0.054)\t Loss 1.0195 (Avg-Loss 0.9433)\tAcc 64.0625 (Avg-Acc 67.1292)\n",
            "Epoch: [104][78/78]\tTime 0.009 (Avg-Time 0.053)\t Loss 0.9267 (Avg-Loss 0.9438)\tAcc 59.3750 (Avg-Acc 67.0550)\n",
            "EPOCH: 104 train Results: Acc 67.055 Loss: 0.9438\n",
            "Epoch: [104][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1855 (Avg-Loss 1.1855)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [104][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2429 (Avg-Loss 1.2079)\tAcc 55.8824 (Avg-Acc 57.7800)\n",
            "EPOCH: 104 Validation Results: Acc 57.780 Loss: 1.2079\n",
            "Epoch: [105][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9322 (Avg-Loss 0.9322)\tAcc 69.3359 (Avg-Acc 69.3359)\n",
            "Epoch: [105][19/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.9504 (Avg-Loss 0.9099)\tAcc 67.7734 (Avg-Acc 68.3105)\n",
            "Epoch: [105][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9614 (Avg-Loss 0.9167)\tAcc 66.6016 (Avg-Acc 68.3794)\n",
            "Epoch: [105][57/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.9414 (Avg-Loss 0.9306)\tAcc 65.4297 (Avg-Acc 67.5242)\n",
            "Epoch: [105][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0236 (Avg-Loss 0.9414)\tAcc 65.4297 (Avg-Acc 67.1114)\n",
            "Epoch: [105][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0431 (Avg-Loss 0.9416)\tAcc 70.3125 (Avg-Acc 67.1225)\n",
            "EPOCH: 105 train Results: Acc 67.123 Loss: 0.9416\n",
            "Epoch: [105][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1837 (Avg-Loss 1.1837)\tAcc 59.9609 (Avg-Acc 59.9609)\n",
            "Epoch: [105][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2440 (Avg-Loss 1.2155)\tAcc 57.7206 (Avg-Acc 57.2200)\n",
            "EPOCH: 105 Validation Results: Acc 57.220 Loss: 1.2155\n",
            "Epoch: [106][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.9447 (Avg-Loss 0.9447)\tAcc 67.1875 (Avg-Acc 67.1875)\n",
            "Epoch: [106][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8536 (Avg-Loss 0.8907)\tAcc 72.0703 (Avg-Acc 69.2285)\n",
            "Epoch: [106][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8982 (Avg-Loss 0.9150)\tAcc 69.3359 (Avg-Acc 68.3393)\n",
            "Epoch: [106][57/78]\tTime 0.078 (Avg-Time 0.077)\t Loss 0.9598 (Avg-Loss 0.9291)\tAcc 67.7734 (Avg-Acc 67.7128)\n",
            "Epoch: [106][76/78]\tTime 0.048 (Avg-Time 0.075)\t Loss 0.9649 (Avg-Loss 0.9393)\tAcc 65.2344 (Avg-Acc 67.1418)\n",
            "Epoch: [106][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 0.9948 (Avg-Loss 0.9395)\tAcc 73.4375 (Avg-Acc 67.1675)\n",
            "EPOCH: 106 train Results: Acc 67.168 Loss: 0.9395\n",
            "Epoch: [106][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1862 (Avg-Loss 1.1862)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [106][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2639 (Avg-Loss 1.2171)\tAcc 54.4118 (Avg-Acc 57.1900)\n",
            "EPOCH: 106 Validation Results: Acc 57.190 Loss: 1.2171\n",
            "Epoch: [107][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9553 (Avg-Loss 0.9553)\tAcc 65.6250 (Avg-Acc 65.6250)\n",
            "Epoch: [107][19/78]\tTime 0.047 (Avg-Time 0.052)\t Loss 0.9333 (Avg-Loss 0.9113)\tAcc 68.3594 (Avg-Acc 68.4473)\n",
            "Epoch: [107][38/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 0.9462 (Avg-Loss 0.9222)\tAcc 66.2109 (Avg-Acc 67.8035)\n",
            "Epoch: [107][57/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 1.0337 (Avg-Loss 0.9310)\tAcc 63.8672 (Avg-Acc 67.6084)\n",
            "Epoch: [107][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8657 (Avg-Loss 0.9430)\tAcc 71.4844 (Avg-Acc 67.1139)\n",
            "Epoch: [107][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.2813 (Avg-Loss 0.9443)\tAcc 51.5625 (Avg-Acc 67.0650)\n",
            "EPOCH: 107 train Results: Acc 67.065 Loss: 0.9443\n",
            "Epoch: [107][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1803 (Avg-Loss 1.1803)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [107][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2697 (Avg-Loss 1.2093)\tAcc 53.6765 (Avg-Acc 57.7900)\n",
            "EPOCH: 107 Validation Results: Acc 57.790 Loss: 1.2093\n",
            "Epoch: [108][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.9105 (Avg-Loss 0.9105)\tAcc 68.1641 (Avg-Acc 68.1641)\n",
            "Epoch: [108][19/78]\tTime 0.050 (Avg-Time 0.049)\t Loss 0.9244 (Avg-Loss 0.9199)\tAcc 66.2109 (Avg-Acc 67.7734)\n",
            "Epoch: [108][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8910 (Avg-Loss 0.9263)\tAcc 69.5312 (Avg-Acc 67.6082)\n",
            "Epoch: [108][57/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9952 (Avg-Loss 0.9358)\tAcc 64.4531 (Avg-Acc 67.2313)\n",
            "Epoch: [108][76/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9759 (Avg-Loss 0.9452)\tAcc 64.6484 (Avg-Acc 66.8172)\n",
            "Epoch: [108][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.8741 (Avg-Loss 0.9461)\tAcc 71.8750 (Avg-Acc 66.7900)\n",
            "EPOCH: 108 train Results: Acc 66.790 Loss: 0.9461\n",
            "Epoch: [108][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2052 (Avg-Loss 1.2052)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [108][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2888 (Avg-Loss 1.2192)\tAcc 57.3529 (Avg-Acc 57.0800)\n",
            "EPOCH: 108 Validation Results: Acc 57.080 Loss: 1.2192\n",
            "Epoch: [109][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.9636 (Avg-Loss 0.9636)\tAcc 67.3828 (Avg-Acc 67.3828)\n",
            "Epoch: [109][19/78]\tTime 0.100 (Avg-Time 0.064)\t Loss 0.9343 (Avg-Loss 0.9154)\tAcc 68.3594 (Avg-Acc 68.2129)\n",
            "Epoch: [109][38/78]\tTime 0.140 (Avg-Time 0.103)\t Loss 0.9729 (Avg-Loss 0.9250)\tAcc 64.8438 (Avg-Acc 67.8486)\n",
            "Epoch: [109][57/78]\tTime 0.050 (Avg-Time 0.086)\t Loss 0.9183 (Avg-Loss 0.9324)\tAcc 67.9688 (Avg-Acc 67.4603)\n",
            "Epoch: [109][76/78]\tTime 0.048 (Avg-Time 0.077)\t Loss 0.9891 (Avg-Loss 0.9438)\tAcc 67.5781 (Avg-Acc 67.0835)\n",
            "Epoch: [109][78/78]\tTime 0.009 (Avg-Time 0.076)\t Loss 1.0033 (Avg-Loss 0.9431)\tAcc 65.6250 (Avg-Acc 67.1425)\n",
            "EPOCH: 109 train Results: Acc 67.142 Loss: 0.9431\n",
            "Epoch: [109][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1566 (Avg-Loss 1.1566)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [109][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2530 (Avg-Loss 1.2021)\tAcc 55.1471 (Avg-Acc 57.6600)\n",
            "EPOCH: 109 Validation Results: Acc 57.660 Loss: 1.2021\n",
            "Epoch: [110][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.8526 (Avg-Loss 0.8526)\tAcc 69.3359 (Avg-Acc 69.3359)\n",
            "Epoch: [110][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8739 (Avg-Loss 0.8955)\tAcc 68.9453 (Avg-Acc 69.1016)\n",
            "Epoch: [110][38/78]\tTime 0.050 (Avg-Time 0.049)\t Loss 0.9840 (Avg-Loss 0.9129)\tAcc 64.6484 (Avg-Acc 68.1741)\n",
            "Epoch: [110][57/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9277 (Avg-Loss 0.9243)\tAcc 67.3828 (Avg-Acc 67.7768)\n",
            "Epoch: [110][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0363 (Avg-Loss 0.9341)\tAcc 66.4062 (Avg-Acc 67.4970)\n",
            "Epoch: [110][78/78]\tTime 0.012 (Avg-Time 0.049)\t Loss 0.7546 (Avg-Loss 0.9338)\tAcc 73.4375 (Avg-Acc 67.4900)\n",
            "EPOCH: 110 train Results: Acc 67.490 Loss: 0.9338\n",
            "Epoch: [110][0/19]\tTime 0.016 (Avg-Time 0.016)\t Loss 1.1576 (Avg-Loss 1.1576)\tAcc 60.5469 (Avg-Acc 60.5469)\n",
            "Epoch: [110][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2861 (Avg-Loss 1.2001)\tAcc 53.6765 (Avg-Acc 57.9700)\n",
            "EPOCH: 110 Validation Results: Acc 57.970 Loss: 1.2001\n",
            "Epoch: [111][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8649 (Avg-Loss 0.8649)\tAcc 68.7500 (Avg-Acc 68.7500)\n",
            "Epoch: [111][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9049 (Avg-Loss 0.8988)\tAcc 68.3594 (Avg-Acc 68.4473)\n",
            "Epoch: [111][38/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.8801 (Avg-Loss 0.8972)\tAcc 71.4844 (Avg-Acc 68.3694)\n",
            "Epoch: [111][57/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.9034 (Avg-Loss 0.9173)\tAcc 70.1172 (Avg-Acc 67.7532)\n",
            "Epoch: [111][76/78]\tTime 0.068 (Avg-Time 0.050)\t Loss 0.9764 (Avg-Loss 0.9320)\tAcc 68.9453 (Avg-Acc 67.4082)\n",
            "Epoch: [111][78/78]\tTime 0.018 (Avg-Time 0.050)\t Loss 1.0973 (Avg-Loss 0.9338)\tAcc 64.0625 (Avg-Acc 67.3425)\n",
            "EPOCH: 111 train Results: Acc 67.343 Loss: 0.9338\n",
            "Epoch: [111][0/19]\tTime 0.030 (Avg-Time 0.030)\t Loss 1.1752 (Avg-Loss 1.1752)\tAcc 60.3516 (Avg-Acc 60.3516)\n",
            "Epoch: [111][19/19]\tTime 0.023 (Avg-Time 0.040)\t Loss 1.2581 (Avg-Loss 1.2056)\tAcc 55.8824 (Avg-Acc 57.2800)\n",
            "EPOCH: 111 Validation Results: Acc 57.280 Loss: 1.2056\n",
            "Epoch: [112][0/78]\tTime 0.093 (Avg-Time 0.093)\t Loss 0.8468 (Avg-Loss 0.8468)\tAcc 71.8750 (Avg-Acc 71.8750)\n",
            "Epoch: [112][19/78]\tTime 0.047 (Avg-Time 0.117)\t Loss 0.9390 (Avg-Loss 0.9025)\tAcc 68.1641 (Avg-Acc 69.0137)\n",
            "Epoch: [112][38/78]\tTime 0.047 (Avg-Time 0.084)\t Loss 0.9604 (Avg-Loss 0.9149)\tAcc 66.7969 (Avg-Acc 68.0990)\n",
            "Epoch: [112][57/78]\tTime 0.047 (Avg-Time 0.072)\t Loss 0.8905 (Avg-Loss 0.9288)\tAcc 68.1641 (Avg-Acc 67.6253)\n",
            "Epoch: [112][76/78]\tTime 0.047 (Avg-Time 0.067)\t Loss 1.0196 (Avg-Loss 0.9392)\tAcc 65.2344 (Avg-Acc 67.3118)\n",
            "Epoch: [112][78/78]\tTime 0.009 (Avg-Time 0.066)\t Loss 1.2491 (Avg-Loss 0.9395)\tAcc 67.1875 (Avg-Acc 67.3500)\n",
            "EPOCH: 112 train Results: Acc 67.350 Loss: 0.9395\n",
            "Epoch: [112][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1705 (Avg-Loss 1.1705)\tAcc 60.5469 (Avg-Acc 60.5469)\n",
            "Epoch: [112][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2260 (Avg-Loss 1.2129)\tAcc 57.7206 (Avg-Acc 57.3000)\n",
            "EPOCH: 112 Validation Results: Acc 57.300 Loss: 1.2129\n",
            "Epoch: [113][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8702 (Avg-Loss 0.8702)\tAcc 71.6797 (Avg-Acc 71.6797)\n",
            "Epoch: [113][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9578 (Avg-Loss 0.9062)\tAcc 66.4062 (Avg-Acc 68.4668)\n",
            "Epoch: [113][38/78]\tTime 0.052 (Avg-Time 0.051)\t Loss 0.9553 (Avg-Loss 0.9233)\tAcc 69.7266 (Avg-Acc 67.9637)\n",
            "Epoch: [113][57/78]\tTime 0.059 (Avg-Time 0.050)\t Loss 0.9151 (Avg-Loss 0.9284)\tAcc 67.5781 (Avg-Acc 67.6724)\n",
            "Epoch: [113][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8869 (Avg-Loss 0.9450)\tAcc 69.7266 (Avg-Acc 67.0176)\n",
            "Epoch: [113][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.1633 (Avg-Loss 0.9460)\tAcc 60.9375 (Avg-Acc 66.9650)\n",
            "EPOCH: 113 train Results: Acc 66.965 Loss: 0.9460\n",
            "Epoch: [113][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1732 (Avg-Loss 1.1732)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [113][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2497 (Avg-Loss 1.2113)\tAcc 55.5147 (Avg-Acc 57.2800)\n",
            "EPOCH: 113 Validation Results: Acc 57.280 Loss: 1.2113\n",
            "Epoch: [114][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8884 (Avg-Loss 0.8884)\tAcc 70.7031 (Avg-Acc 70.7031)\n",
            "Epoch: [114][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9769 (Avg-Loss 0.9128)\tAcc 63.8672 (Avg-Acc 68.4180)\n",
            "Epoch: [114][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9411 (Avg-Loss 0.9182)\tAcc 66.7969 (Avg-Acc 67.9137)\n",
            "Epoch: [114][57/78]\tTime 0.113 (Avg-Time 0.057)\t Loss 0.9459 (Avg-Loss 0.9248)\tAcc 68.5547 (Avg-Acc 67.8374)\n",
            "Epoch: [114][76/78]\tTime 0.102 (Avg-Time 0.075)\t Loss 0.9668 (Avg-Loss 0.9344)\tAcc 66.6016 (Avg-Acc 67.5198)\n",
            "Epoch: [114][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 0.6792 (Avg-Loss 0.9343)\tAcc 75.0000 (Avg-Acc 67.5275)\n",
            "EPOCH: 114 train Results: Acc 67.528 Loss: 0.9343\n",
            "Epoch: [114][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1875 (Avg-Loss 1.1875)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [114][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2780 (Avg-Loss 1.2146)\tAcc 54.7794 (Avg-Acc 57.4100)\n",
            "EPOCH: 114 Validation Results: Acc 57.410 Loss: 1.2146\n",
            "Epoch: [115][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9745 (Avg-Loss 0.9745)\tAcc 63.8672 (Avg-Acc 63.8672)\n",
            "Epoch: [115][19/78]\tTime 0.068 (Avg-Time 0.049)\t Loss 0.8594 (Avg-Loss 0.9099)\tAcc 69.7266 (Avg-Acc 68.0957)\n",
            "Epoch: [115][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9515 (Avg-Loss 0.9075)\tAcc 69.7266 (Avg-Acc 68.1490)\n",
            "Epoch: [115][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9994 (Avg-Loss 0.9201)\tAcc 64.8438 (Avg-Acc 67.7768)\n",
            "Epoch: [115][76/78]\tTime 0.050 (Avg-Time 0.049)\t Loss 0.9806 (Avg-Loss 0.9323)\tAcc 64.2578 (Avg-Acc 67.2154)\n",
            "Epoch: [115][78/78]\tTime 0.020 (Avg-Time 0.049)\t Loss 1.2538 (Avg-Loss 0.9331)\tAcc 50.0000 (Avg-Acc 67.1775)\n",
            "EPOCH: 115 train Results: Acc 67.177 Loss: 0.9331\n",
            "Epoch: [115][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1712 (Avg-Loss 1.1712)\tAcc 60.5469 (Avg-Acc 60.5469)\n",
            "Epoch: [115][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2949 (Avg-Loss 1.2070)\tAcc 54.7794 (Avg-Acc 57.5700)\n",
            "EPOCH: 115 Validation Results: Acc 57.570 Loss: 1.2070\n",
            "Epoch: [116][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.9124 (Avg-Loss 0.9124)\tAcc 69.3359 (Avg-Acc 69.3359)\n",
            "Epoch: [116][19/78]\tTime 0.060 (Avg-Time 0.049)\t Loss 0.8729 (Avg-Loss 0.9219)\tAcc 70.1172 (Avg-Acc 68.0371)\n",
            "Epoch: [116][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8932 (Avg-Loss 0.9248)\tAcc 67.9688 (Avg-Acc 67.7384)\n",
            "Epoch: [116][57/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9150 (Avg-Loss 0.9282)\tAcc 68.3594 (Avg-Acc 67.5445)\n",
            "Epoch: [116][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9382 (Avg-Loss 0.9345)\tAcc 67.3828 (Avg-Acc 67.3727)\n",
            "Epoch: [116][78/78]\tTime 0.012 (Avg-Time 0.049)\t Loss 1.1503 (Avg-Loss 0.9359)\tAcc 65.6250 (Avg-Acc 67.3275)\n",
            "EPOCH: 116 train Results: Acc 67.328 Loss: 0.9359\n",
            "Epoch: [116][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1873 (Avg-Loss 1.1873)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [116][19/19]\tTime 0.012 (Avg-Time 0.013)\t Loss 1.3107 (Avg-Loss 1.2018)\tAcc 54.7794 (Avg-Acc 57.6900)\n",
            "EPOCH: 116 Validation Results: Acc 57.690 Loss: 1.2018\n",
            "Epoch: [117][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.9421 (Avg-Loss 0.9421)\tAcc 65.8203 (Avg-Acc 65.8203)\n",
            "Epoch: [117][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9233 (Avg-Loss 0.9038)\tAcc 68.5547 (Avg-Acc 68.1738)\n",
            "Epoch: [117][38/78]\tTime 0.195 (Avg-Time 0.086)\t Loss 0.9979 (Avg-Loss 0.9143)\tAcc 63.6719 (Avg-Acc 67.9587)\n",
            "Epoch: [117][57/78]\tTime 0.047 (Avg-Time 0.084)\t Loss 0.9478 (Avg-Loss 0.9178)\tAcc 66.9922 (Avg-Acc 67.9654)\n",
            "Epoch: [117][76/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 0.9489 (Avg-Loss 0.9327)\tAcc 67.3828 (Avg-Acc 67.4589)\n",
            "Epoch: [117][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.1649 (Avg-Loss 0.9329)\tAcc 60.9375 (Avg-Acc 67.4700)\n",
            "EPOCH: 117 train Results: Acc 67.470 Loss: 0.9329\n",
            "Epoch: [117][0/19]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1923 (Avg-Loss 1.1923)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [117][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2834 (Avg-Loss 1.2044)\tAcc 53.3088 (Avg-Acc 57.4200)\n",
            "EPOCH: 117 Validation Results: Acc 57.420 Loss: 1.2044\n",
            "Epoch: [118][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.9169 (Avg-Loss 0.9169)\tAcc 68.5547 (Avg-Acc 68.5547)\n",
            "Epoch: [118][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9049 (Avg-Loss 0.9049)\tAcc 66.7969 (Avg-Acc 68.8184)\n",
            "Epoch: [118][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8888 (Avg-Loss 0.9144)\tAcc 70.1172 (Avg-Acc 68.3243)\n",
            "Epoch: [118][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0671 (Avg-Loss 0.9267)\tAcc 63.6719 (Avg-Acc 67.7095)\n",
            "Epoch: [118][76/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9230 (Avg-Loss 0.9366)\tAcc 69.7266 (Avg-Acc 67.3422)\n",
            "Epoch: [118][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1645 (Avg-Loss 0.9369)\tAcc 57.8125 (Avg-Acc 67.3325)\n",
            "EPOCH: 118 train Results: Acc 67.332 Loss: 0.9369\n",
            "Epoch: [118][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1644 (Avg-Loss 1.1644)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [118][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2349 (Avg-Loss 1.2103)\tAcc 57.7206 (Avg-Acc 57.2700)\n",
            "EPOCH: 118 Validation Results: Acc 57.270 Loss: 1.2103\n",
            "Epoch: [119][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8869 (Avg-Loss 0.8869)\tAcc 68.5547 (Avg-Acc 68.5547)\n",
            "Epoch: [119][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9215 (Avg-Loss 0.9062)\tAcc 67.5781 (Avg-Acc 68.4277)\n",
            "Epoch: [119][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9174 (Avg-Loss 0.9125)\tAcc 68.7500 (Avg-Acc 68.3944)\n",
            "Epoch: [119][57/78]\tTime 0.073 (Avg-Time 0.050)\t Loss 0.9343 (Avg-Loss 0.9197)\tAcc 67.3828 (Avg-Acc 68.1068)\n",
            "Epoch: [119][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9449 (Avg-Loss 0.9316)\tAcc 66.4062 (Avg-Acc 67.4056)\n",
            "Epoch: [119][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 0.8496 (Avg-Loss 0.9325)\tAcc 68.7500 (Avg-Acc 67.3425)\n",
            "EPOCH: 119 train Results: Acc 67.343 Loss: 0.9325\n",
            "Epoch: [119][0/19]\tTime 0.016 (Avg-Time 0.016)\t Loss 1.1599 (Avg-Loss 1.1599)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [119][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2700 (Avg-Loss 1.2089)\tAcc 56.6176 (Avg-Acc 57.6300)\n",
            "EPOCH: 119 Validation Results: Acc 57.630 Loss: 1.2089\n",
            "Epoch: [120][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8287 (Avg-Loss 0.8287)\tAcc 72.0703 (Avg-Acc 72.0703)\n",
            "Epoch: [120][19/78]\tTime 0.124 (Avg-Time 0.130)\t Loss 0.9001 (Avg-Loss 0.8917)\tAcc 66.6016 (Avg-Acc 68.9355)\n",
            "Epoch: [120][38/78]\tTime 0.048 (Avg-Time 0.101)\t Loss 0.9814 (Avg-Loss 0.9029)\tAcc 63.6719 (Avg-Acc 68.4645)\n",
            "Epoch: [120][57/78]\tTime 0.047 (Avg-Time 0.084)\t Loss 0.9182 (Avg-Loss 0.9132)\tAcc 65.6250 (Avg-Acc 67.9586)\n",
            "Epoch: [120][76/78]\tTime 0.051 (Avg-Time 0.076)\t Loss 0.9798 (Avg-Loss 0.9224)\tAcc 65.8203 (Avg-Acc 67.7506)\n",
            "Epoch: [120][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.1839 (Avg-Loss 0.9238)\tAcc 59.3750 (Avg-Acc 67.6950)\n",
            "EPOCH: 120 train Results: Acc 67.695 Loss: 0.9238\n",
            "Epoch: [120][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1748 (Avg-Loss 1.1748)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [120][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2747 (Avg-Loss 1.2054)\tAcc 54.4118 (Avg-Acc 57.7000)\n",
            "EPOCH: 120 Validation Results: Acc 57.700 Loss: 1.2054\n",
            "Epoch: [121][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8137 (Avg-Loss 0.8137)\tAcc 70.5078 (Avg-Acc 70.5078)\n",
            "Epoch: [121][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9548 (Avg-Loss 0.8955)\tAcc 65.4297 (Avg-Acc 69.0820)\n",
            "Epoch: [121][38/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.8670 (Avg-Loss 0.9045)\tAcc 68.7500 (Avg-Acc 68.3494)\n",
            "Epoch: [121][57/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9773 (Avg-Loss 0.9154)\tAcc 64.2578 (Avg-Acc 67.9115)\n",
            "Epoch: [121][76/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.9792 (Avg-Loss 0.9277)\tAcc 64.4531 (Avg-Acc 67.4361)\n",
            "Epoch: [121][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1552 (Avg-Loss 0.9288)\tAcc 60.9375 (Avg-Acc 67.3800)\n",
            "EPOCH: 121 train Results: Acc 67.380 Loss: 0.9288\n",
            "Epoch: [121][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1806 (Avg-Loss 1.1806)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [121][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2584 (Avg-Loss 1.2040)\tAcc 57.7206 (Avg-Acc 57.8700)\n",
            "EPOCH: 121 Validation Results: Acc 57.870 Loss: 1.2040\n",
            "Epoch: [122][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9159 (Avg-Loss 0.9159)\tAcc 66.9922 (Avg-Acc 66.9922)\n",
            "Epoch: [122][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8829 (Avg-Loss 0.8892)\tAcc 70.7031 (Avg-Acc 69.9609)\n",
            "Epoch: [122][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9148 (Avg-Loss 0.9103)\tAcc 69.3359 (Avg-Acc 68.7951)\n",
            "Epoch: [122][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9770 (Avg-Loss 0.9234)\tAcc 66.9922 (Avg-Acc 68.0664)\n",
            "Epoch: [122][76/78]\tTime 0.100 (Avg-Time 0.069)\t Loss 0.9196 (Avg-Loss 0.9328)\tAcc 67.5781 (Avg-Acc 67.5223)\n",
            "Epoch: [122][78/78]\tTime 0.033 (Avg-Time 0.068)\t Loss 1.0635 (Avg-Loss 0.9331)\tAcc 62.5000 (Avg-Acc 67.5275)\n",
            "EPOCH: 122 train Results: Acc 67.528 Loss: 0.9331\n",
            "Epoch: [122][0/19]\tTime 0.029 (Avg-Time 0.029)\t Loss 1.1909 (Avg-Loss 1.1909)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [122][19/19]\tTime 0.023 (Avg-Time 0.028)\t Loss 1.2847 (Avg-Loss 1.1980)\tAcc 55.1471 (Avg-Acc 57.8400)\n",
            "EPOCH: 122 Validation Results: Acc 57.840 Loss: 1.1980\n",
            "Epoch: [123][0/78]\tTime 0.098 (Avg-Time 0.098)\t Loss 0.8371 (Avg-Loss 0.8371)\tAcc 71.4844 (Avg-Acc 71.4844)\n",
            "Epoch: [123][19/78]\tTime 0.047 (Avg-Time 0.060)\t Loss 0.9141 (Avg-Loss 0.8956)\tAcc 69.5312 (Avg-Acc 69.1016)\n",
            "Epoch: [123][38/78]\tTime 0.047 (Avg-Time 0.055)\t Loss 0.8865 (Avg-Loss 0.9064)\tAcc 69.3359 (Avg-Acc 68.4195)\n",
            "Epoch: [123][57/78]\tTime 0.046 (Avg-Time 0.054)\t Loss 0.9797 (Avg-Loss 0.9189)\tAcc 64.4531 (Avg-Acc 67.8037)\n",
            "Epoch: [123][76/78]\tTime 0.047 (Avg-Time 0.053)\t Loss 0.9972 (Avg-Loss 0.9298)\tAcc 66.0156 (Avg-Acc 67.4589)\n",
            "Epoch: [123][78/78]\tTime 0.009 (Avg-Time 0.052)\t Loss 0.8739 (Avg-Loss 0.9297)\tAcc 73.4375 (Avg-Acc 67.4825)\n",
            "EPOCH: 123 train Results: Acc 67.483 Loss: 0.9297\n",
            "Epoch: [123][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1856 (Avg-Loss 1.1856)\tAcc 60.1562 (Avg-Acc 60.1562)\n",
            "Epoch: [123][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2863 (Avg-Loss 1.2035)\tAcc 54.4118 (Avg-Acc 58.0900)\n",
            "EPOCH: 123 Validation Results: Acc 58.090 Loss: 1.2035\n",
            "Epoch: [124][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8778 (Avg-Loss 0.8778)\tAcc 69.9219 (Avg-Acc 69.9219)\n",
            "Epoch: [124][19/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 0.8909 (Avg-Loss 0.8863)\tAcc 68.3594 (Avg-Acc 69.5703)\n",
            "Epoch: [124][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9656 (Avg-Loss 0.9009)\tAcc 65.8203 (Avg-Acc 68.7300)\n",
            "Epoch: [124][57/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.9184 (Avg-Loss 0.9064)\tAcc 68.5547 (Avg-Acc 68.5210)\n",
            "Epoch: [124][76/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9879 (Avg-Loss 0.9202)\tAcc 63.6719 (Avg-Acc 67.9282)\n",
            "Epoch: [124][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1318 (Avg-Loss 0.9212)\tAcc 60.9375 (Avg-Acc 67.9025)\n",
            "EPOCH: 124 train Results: Acc 67.903 Loss: 0.9212\n",
            "Epoch: [124][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1733 (Avg-Loss 1.1733)\tAcc 61.3281 (Avg-Acc 61.3281)\n",
            "Epoch: [124][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2709 (Avg-Loss 1.2070)\tAcc 56.9853 (Avg-Acc 57.9100)\n",
            "EPOCH: 124 Validation Results: Acc 57.910 Loss: 1.2070\n",
            "Epoch: [125][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8628 (Avg-Loss 0.8628)\tAcc 69.9219 (Avg-Acc 69.9219)\n",
            "Epoch: [125][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8941 (Avg-Loss 0.8881)\tAcc 69.1406 (Avg-Acc 69.3652)\n",
            "Epoch: [125][38/78]\tTime 0.059 (Avg-Time 0.050)\t Loss 0.9654 (Avg-Loss 0.8943)\tAcc 67.7734 (Avg-Acc 69.1757)\n",
            "Epoch: [125][57/78]\tTime 0.099 (Avg-Time 0.080)\t Loss 0.9619 (Avg-Loss 0.9172)\tAcc 67.1875 (Avg-Acc 68.1742)\n",
            "Epoch: [125][76/78]\tTime 0.064 (Avg-Time 0.076)\t Loss 1.0055 (Avg-Loss 0.9262)\tAcc 64.6484 (Avg-Acc 67.6821)\n",
            "Epoch: [125][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.1040 (Avg-Loss 0.9266)\tAcc 62.5000 (Avg-Acc 67.7025)\n",
            "EPOCH: 125 train Results: Acc 67.703 Loss: 0.9266\n",
            "Epoch: [125][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1761 (Avg-Loss 1.1761)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [125][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2906 (Avg-Loss 1.2136)\tAcc 57.7206 (Avg-Acc 57.2800)\n",
            "EPOCH: 125 Validation Results: Acc 57.280 Loss: 1.2136\n",
            "Epoch: [126][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8000 (Avg-Loss 0.8000)\tAcc 69.5312 (Avg-Acc 69.5312)\n",
            "Epoch: [126][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9634 (Avg-Loss 0.8952)\tAcc 65.0391 (Avg-Acc 68.8867)\n",
            "Epoch: [126][38/78]\tTime 0.050 (Avg-Time 0.049)\t Loss 0.8532 (Avg-Loss 0.9035)\tAcc 69.3359 (Avg-Acc 68.5797)\n",
            "Epoch: [126][57/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9185 (Avg-Loss 0.9102)\tAcc 65.2344 (Avg-Acc 68.3223)\n",
            "Epoch: [126][76/78]\tTime 0.054 (Avg-Time 0.049)\t Loss 0.9206 (Avg-Loss 0.9244)\tAcc 68.7500 (Avg-Acc 67.7988)\n",
            "Epoch: [126][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.8908 (Avg-Loss 0.9246)\tAcc 62.5000 (Avg-Acc 67.7800)\n",
            "EPOCH: 126 train Results: Acc 67.780 Loss: 0.9246\n",
            "Epoch: [126][0/19]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.1841 (Avg-Loss 1.1841)\tAcc 60.5469 (Avg-Acc 60.5469)\n",
            "Epoch: [126][19/19]\tTime 0.009 (Avg-Time 0.013)\t Loss 1.2834 (Avg-Loss 1.2172)\tAcc 56.9853 (Avg-Acc 57.5400)\n",
            "EPOCH: 126 Validation Results: Acc 57.540 Loss: 1.2172\n",
            "Epoch: [127][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8473 (Avg-Loss 0.8473)\tAcc 71.4844 (Avg-Acc 71.4844)\n",
            "Epoch: [127][19/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 0.8767 (Avg-Loss 0.8890)\tAcc 68.9453 (Avg-Acc 68.6426)\n",
            "Epoch: [127][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9743 (Avg-Loss 0.9066)\tAcc 69.1406 (Avg-Acc 67.9888)\n",
            "Epoch: [127][57/78]\tTime 0.057 (Avg-Time 0.050)\t Loss 0.8532 (Avg-Loss 0.9121)\tAcc 72.0703 (Avg-Acc 67.8442)\n",
            "Epoch: [127][76/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9856 (Avg-Loss 0.9238)\tAcc 67.1875 (Avg-Acc 67.4234)\n",
            "Epoch: [127][78/78]\tTime 0.025 (Avg-Time 0.049)\t Loss 0.9703 (Avg-Loss 0.9244)\tAcc 64.0625 (Avg-Acc 67.4200)\n",
            "EPOCH: 127 train Results: Acc 67.420 Loss: 0.9244\n",
            "Epoch: [127][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.1626 (Avg-Loss 1.1626)\tAcc 59.9609 (Avg-Acc 59.9609)\n",
            "Epoch: [127][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2577 (Avg-Loss 1.2051)\tAcc 57.7206 (Avg-Acc 57.5600)\n",
            "EPOCH: 127 Validation Results: Acc 57.560 Loss: 1.2051\n",
            "Epoch: [128][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8480 (Avg-Loss 0.8480)\tAcc 71.8750 (Avg-Acc 71.8750)\n",
            "Epoch: [128][19/78]\tTime 0.102 (Avg-Time 0.062)\t Loss 0.8431 (Avg-Loss 0.8813)\tAcc 70.3125 (Avg-Acc 69.7949)\n",
            "Epoch: [128][38/78]\tTime 0.091 (Avg-Time 0.100)\t Loss 0.9903 (Avg-Loss 0.8977)\tAcc 66.2109 (Avg-Acc 69.1456)\n",
            "Epoch: [128][57/78]\tTime 0.048 (Avg-Time 0.084)\t Loss 0.9787 (Avg-Loss 0.9106)\tAcc 65.0391 (Avg-Acc 68.4772)\n",
            "Epoch: [128][76/78]\tTime 0.049 (Avg-Time 0.075)\t Loss 0.9173 (Avg-Loss 0.9213)\tAcc 67.5781 (Avg-Acc 67.9789)\n",
            "Epoch: [128][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 0.9970 (Avg-Loss 0.9214)\tAcc 64.0625 (Avg-Acc 67.9775)\n",
            "EPOCH: 128 train Results: Acc 67.978 Loss: 0.9214\n",
            "Epoch: [128][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1834 (Avg-Loss 1.1834)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [128][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2808 (Avg-Loss 1.2173)\tAcc 53.6765 (Avg-Acc 57.7400)\n",
            "EPOCH: 128 Validation Results: Acc 57.740 Loss: 1.2173\n",
            "Epoch: [129][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9351 (Avg-Loss 0.9351)\tAcc 68.9453 (Avg-Acc 68.9453)\n",
            "Epoch: [129][19/78]\tTime 0.068 (Avg-Time 0.049)\t Loss 0.8773 (Avg-Loss 0.8795)\tAcc 69.9219 (Avg-Acc 69.5020)\n",
            "Epoch: [129][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9197 (Avg-Loss 0.9024)\tAcc 65.4297 (Avg-Acc 68.6949)\n",
            "Epoch: [129][57/78]\tTime 0.051 (Avg-Time 0.049)\t Loss 0.9216 (Avg-Loss 0.9117)\tAcc 71.0938 (Avg-Acc 68.1674)\n",
            "Epoch: [129][76/78]\tTime 0.052 (Avg-Time 0.050)\t Loss 0.9376 (Avg-Loss 0.9207)\tAcc 68.3594 (Avg-Acc 67.9510)\n",
            "Epoch: [129][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1136 (Avg-Loss 0.9214)\tAcc 59.3750 (Avg-Acc 67.9200)\n",
            "EPOCH: 129 train Results: Acc 67.920 Loss: 0.9214\n",
            "Epoch: [129][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1542 (Avg-Loss 1.1542)\tAcc 59.9609 (Avg-Acc 59.9609)\n",
            "Epoch: [129][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2565 (Avg-Loss 1.2060)\tAcc 55.1471 (Avg-Acc 57.5200)\n",
            "EPOCH: 129 Validation Results: Acc 57.520 Loss: 1.2060\n",
            "Epoch: [130][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9118 (Avg-Loss 0.9118)\tAcc 70.3125 (Avg-Acc 70.3125)\n",
            "Epoch: [130][19/78]\tTime 0.070 (Avg-Time 0.049)\t Loss 0.9245 (Avg-Loss 0.8959)\tAcc 67.5781 (Avg-Acc 68.7988)\n",
            "Epoch: [130][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9792 (Avg-Loss 0.9110)\tAcc 65.6250 (Avg-Acc 68.2292)\n",
            "Epoch: [130][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9540 (Avg-Loss 0.9149)\tAcc 65.2344 (Avg-Acc 68.1472)\n",
            "Epoch: [130][76/78]\tTime 0.090 (Avg-Time 0.051)\t Loss 1.0526 (Avg-Loss 0.9264)\tAcc 63.2812 (Avg-Acc 67.7582)\n",
            "Epoch: [130][78/78]\tTime 0.024 (Avg-Time 0.052)\t Loss 0.8703 (Avg-Loss 0.9261)\tAcc 71.8750 (Avg-Acc 67.7475)\n",
            "EPOCH: 130 train Results: Acc 67.748 Loss: 0.9261\n",
            "Epoch: [130][0/19]\tTime 0.035 (Avg-Time 0.035)\t Loss 1.1973 (Avg-Loss 1.1973)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [130][19/19]\tTime 0.012 (Avg-Time 0.031)\t Loss 1.3054 (Avg-Loss 1.2146)\tAcc 54.0441 (Avg-Acc 57.6300)\n",
            "EPOCH: 130 Validation Results: Acc 57.630 Loss: 1.2146\n",
            "Epoch: [131][0/78]\tTime 0.094 (Avg-Time 0.094)\t Loss 0.8649 (Avg-Loss 0.8649)\tAcc 71.8750 (Avg-Acc 71.8750)\n",
            "Epoch: [131][19/78]\tTime 0.046 (Avg-Time 0.121)\t Loss 0.9695 (Avg-Loss 0.8907)\tAcc 66.6016 (Avg-Acc 69.1016)\n",
            "Epoch: [131][38/78]\tTime 0.047 (Avg-Time 0.086)\t Loss 0.8742 (Avg-Loss 0.8978)\tAcc 70.7031 (Avg-Acc 68.6999)\n",
            "Epoch: [131][57/78]\tTime 0.046 (Avg-Time 0.074)\t Loss 0.9475 (Avg-Loss 0.9101)\tAcc 67.9688 (Avg-Acc 68.2045)\n",
            "Epoch: [131][76/78]\tTime 0.046 (Avg-Time 0.069)\t Loss 0.9034 (Avg-Loss 0.9146)\tAcc 68.9453 (Avg-Acc 68.1539)\n",
            "Epoch: [131][78/78]\tTime 0.009 (Avg-Time 0.068)\t Loss 1.0488 (Avg-Loss 0.9161)\tAcc 59.3750 (Avg-Acc 68.0775)\n",
            "EPOCH: 131 train Results: Acc 68.078 Loss: 0.9161\n",
            "Epoch: [131][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1735 (Avg-Loss 1.1735)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [131][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3043 (Avg-Loss 1.2029)\tAcc 53.6765 (Avg-Acc 57.9700)\n",
            "EPOCH: 131 Validation Results: Acc 57.970 Loss: 1.2029\n",
            "Epoch: [132][0/78]\tTime 0.070 (Avg-Time 0.070)\t Loss 0.7962 (Avg-Loss 0.7962)\tAcc 72.2656 (Avg-Acc 72.2656)\n",
            "Epoch: [132][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8887 (Avg-Loss 0.8860)\tAcc 69.3359 (Avg-Acc 69.3750)\n",
            "Epoch: [132][38/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9440 (Avg-Loss 0.8986)\tAcc 67.1875 (Avg-Acc 68.9253)\n",
            "Epoch: [132][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9418 (Avg-Loss 0.9094)\tAcc 66.0156 (Avg-Acc 68.2954)\n",
            "Epoch: [132][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9082 (Avg-Loss 0.9187)\tAcc 68.7500 (Avg-Acc 67.8292)\n",
            "Epoch: [132][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.3479 (Avg-Loss 0.9205)\tAcc 51.5625 (Avg-Acc 67.7775)\n",
            "EPOCH: 132 train Results: Acc 67.778 Loss: 0.9205\n",
            "Epoch: [132][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1860 (Avg-Loss 1.1860)\tAcc 60.5469 (Avg-Acc 60.5469)\n",
            "Epoch: [132][19/19]\tTime 0.010 (Avg-Time 0.013)\t Loss 1.2971 (Avg-Loss 1.2029)\tAcc 54.7794 (Avg-Acc 57.8200)\n",
            "EPOCH: 132 Validation Results: Acc 57.820 Loss: 1.2029\n",
            "Epoch: [133][0/78]\tTime 0.047 (Avg-Time 0.047)\t Loss 0.8610 (Avg-Loss 0.8610)\tAcc 69.3359 (Avg-Acc 69.3359)\n",
            "Epoch: [133][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.8681 (Avg-Loss 0.8940)\tAcc 68.5547 (Avg-Acc 68.6816)\n",
            "Epoch: [133][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9657 (Avg-Loss 0.9058)\tAcc 65.4297 (Avg-Acc 68.4145)\n",
            "Epoch: [133][57/78]\tTime 0.095 (Avg-Time 0.055)\t Loss 0.9015 (Avg-Loss 0.9116)\tAcc 68.9453 (Avg-Acc 68.3156)\n",
            "Epoch: [133][76/78]\tTime 0.097 (Avg-Time 0.074)\t Loss 0.9427 (Avg-Loss 0.9185)\tAcc 67.3828 (Avg-Acc 67.9764)\n",
            "Epoch: [133][78/78]\tTime 0.009 (Avg-Time 0.073)\t Loss 1.0762 (Avg-Loss 0.9207)\tAcc 65.6250 (Avg-Acc 67.8875)\n",
            "EPOCH: 133 train Results: Acc 67.888 Loss: 0.9207\n",
            "Epoch: [133][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1906 (Avg-Loss 1.1906)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [133][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3053 (Avg-Loss 1.2149)\tAcc 53.6765 (Avg-Acc 57.5100)\n",
            "EPOCH: 133 Validation Results: Acc 57.510 Loss: 1.2149\n",
            "Epoch: [134][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.8425 (Avg-Loss 0.8425)\tAcc 72.8516 (Avg-Acc 72.8516)\n",
            "Epoch: [134][19/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.7946 (Avg-Loss 0.8818)\tAcc 74.4141 (Avg-Acc 70.0195)\n",
            "Epoch: [134][38/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 0.9106 (Avg-Loss 0.8987)\tAcc 65.8203 (Avg-Acc 69.0905)\n",
            "Epoch: [134][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9414 (Avg-Loss 0.9112)\tAcc 66.2109 (Avg-Acc 68.5008)\n",
            "Epoch: [134][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9135 (Avg-Loss 0.9208)\tAcc 68.5547 (Avg-Acc 68.0322)\n",
            "Epoch: [134][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 0.8984 (Avg-Loss 0.9213)\tAcc 71.8750 (Avg-Acc 68.0275)\n",
            "EPOCH: 134 train Results: Acc 68.028 Loss: 0.9213\n",
            "Epoch: [134][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1879 (Avg-Loss 1.1879)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [134][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3156 (Avg-Loss 1.2126)\tAcc 55.8824 (Avg-Acc 57.3900)\n",
            "EPOCH: 134 Validation Results: Acc 57.390 Loss: 1.2126\n",
            "Epoch: [135][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8996 (Avg-Loss 0.8996)\tAcc 66.2109 (Avg-Acc 66.2109)\n",
            "Epoch: [135][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.8450 (Avg-Loss 0.8985)\tAcc 70.1172 (Avg-Acc 68.7012)\n",
            "Epoch: [135][38/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.8358 (Avg-Loss 0.8931)\tAcc 73.2422 (Avg-Acc 68.7500)\n",
            "Epoch: [135][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9739 (Avg-Loss 0.9026)\tAcc 64.8438 (Avg-Acc 68.3156)\n",
            "Epoch: [135][76/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9115 (Avg-Loss 0.9135)\tAcc 68.9453 (Avg-Acc 67.9611)\n",
            "Epoch: [135][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.2630 (Avg-Loss 0.9153)\tAcc 57.8125 (Avg-Acc 67.9175)\n",
            "EPOCH: 135 train Results: Acc 67.918 Loss: 0.9153\n",
            "Epoch: [135][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2017 (Avg-Loss 1.2017)\tAcc 60.3516 (Avg-Acc 60.3516)\n",
            "Epoch: [135][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2899 (Avg-Loss 1.2123)\tAcc 54.7794 (Avg-Acc 57.8400)\n",
            "EPOCH: 135 Validation Results: Acc 57.840 Loss: 1.2123\n",
            "Epoch: [136][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.8556 (Avg-Loss 0.8556)\tAcc 70.3125 (Avg-Acc 70.3125)\n",
            "Epoch: [136][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9531 (Avg-Loss 0.8953)\tAcc 66.0156 (Avg-Acc 69.1699)\n",
            "Epoch: [136][38/78]\tTime 0.207 (Avg-Time 0.074)\t Loss 0.8958 (Avg-Loss 0.9024)\tAcc 69.3359 (Avg-Acc 68.4545)\n",
            "Epoch: [136][57/78]\tTime 0.047 (Avg-Time 0.084)\t Loss 1.0103 (Avg-Loss 0.9141)\tAcc 63.6719 (Avg-Acc 68.1102)\n",
            "Epoch: [136][76/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 0.9948 (Avg-Loss 0.9237)\tAcc 68.7500 (Avg-Acc 67.6999)\n",
            "Epoch: [136][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 0.9782 (Avg-Loss 0.9237)\tAcc 67.1875 (Avg-Acc 67.6775)\n",
            "EPOCH: 136 train Results: Acc 67.677 Loss: 0.9237\n",
            "Epoch: [136][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1889 (Avg-Loss 1.1889)\tAcc 61.1328 (Avg-Acc 61.1328)\n",
            "Epoch: [136][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3007 (Avg-Loss 1.2143)\tAcc 52.2059 (Avg-Acc 57.4300)\n",
            "EPOCH: 136 Validation Results: Acc 57.430 Loss: 1.2143\n",
            "Epoch: [137][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.9321 (Avg-Loss 0.9321)\tAcc 65.8203 (Avg-Acc 65.8203)\n",
            "Epoch: [137][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8773 (Avg-Loss 0.8797)\tAcc 69.3359 (Avg-Acc 69.6973)\n",
            "Epoch: [137][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.8886 (Avg-Loss 0.8905)\tAcc 70.5078 (Avg-Acc 69.3009)\n",
            "Epoch: [137][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9226 (Avg-Loss 0.9016)\tAcc 66.2109 (Avg-Acc 68.6827)\n",
            "Epoch: [137][76/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.8665 (Avg-Loss 0.9119)\tAcc 71.0938 (Avg-Acc 68.2681)\n",
            "Epoch: [137][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.8226 (Avg-Loss 0.9123)\tAcc 76.5625 (Avg-Acc 68.2800)\n",
            "EPOCH: 137 train Results: Acc 68.280 Loss: 0.9123\n",
            "Epoch: [137][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1674 (Avg-Loss 1.1674)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [137][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2636 (Avg-Loss 1.2106)\tAcc 55.5147 (Avg-Acc 57.7300)\n",
            "EPOCH: 137 Validation Results: Acc 57.730 Loss: 1.2106\n",
            "Epoch: [138][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8445 (Avg-Loss 0.8445)\tAcc 71.2891 (Avg-Acc 71.2891)\n",
            "Epoch: [138][19/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 0.8583 (Avg-Loss 0.8822)\tAcc 71.6797 (Avg-Acc 69.4531)\n",
            "Epoch: [138][38/78]\tTime 0.050 (Avg-Time 0.051)\t Loss 0.9303 (Avg-Loss 0.8931)\tAcc 67.3828 (Avg-Acc 68.8852)\n",
            "Epoch: [138][57/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.8905 (Avg-Loss 0.9023)\tAcc 69.1406 (Avg-Acc 68.5715)\n",
            "Epoch: [138][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8945 (Avg-Loss 0.9105)\tAcc 67.1875 (Avg-Acc 68.0499)\n",
            "Epoch: [138][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 0.8937 (Avg-Loss 0.9110)\tAcc 71.8750 (Avg-Acc 68.0350)\n",
            "EPOCH: 138 train Results: Acc 68.035 Loss: 0.9110\n",
            "Epoch: [138][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2231 (Avg-Loss 1.2231)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [138][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3080 (Avg-Loss 1.2116)\tAcc 52.9412 (Avg-Acc 57.4500)\n",
            "EPOCH: 138 Validation Results: Acc 57.450 Loss: 1.2116\n",
            "Epoch: [139][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8377 (Avg-Loss 0.8377)\tAcc 71.4844 (Avg-Acc 71.4844)\n",
            "Epoch: [139][19/78]\tTime 0.088 (Avg-Time 0.132)\t Loss 0.8923 (Avg-Loss 0.8744)\tAcc 68.7500 (Avg-Acc 69.6777)\n",
            "Epoch: [139][38/78]\tTime 0.046 (Avg-Time 0.101)\t Loss 0.9639 (Avg-Loss 0.8934)\tAcc 66.6016 (Avg-Acc 69.1456)\n",
            "Epoch: [139][57/78]\tTime 0.066 (Avg-Time 0.084)\t Loss 0.9383 (Avg-Loss 0.9037)\tAcc 67.5781 (Avg-Acc 68.5783)\n",
            "Epoch: [139][76/78]\tTime 0.052 (Avg-Time 0.075)\t Loss 0.9323 (Avg-Loss 0.9130)\tAcc 66.2109 (Avg-Acc 68.2884)\n",
            "Epoch: [139][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 0.8451 (Avg-Loss 0.9142)\tAcc 64.0625 (Avg-Acc 68.2550)\n",
            "EPOCH: 139 train Results: Acc 68.255 Loss: 0.9142\n",
            "Epoch: [139][0/19]\tTime 0.033 (Avg-Time 0.033)\t Loss 1.1980 (Avg-Loss 1.1980)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [139][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3076 (Avg-Loss 1.2084)\tAcc 52.5735 (Avg-Acc 57.2200)\n",
            "EPOCH: 139 Validation Results: Acc 57.220 Loss: 1.2084\n",
            "Epoch: [140][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8184 (Avg-Loss 0.8184)\tAcc 70.1172 (Avg-Acc 70.1172)\n",
            "Epoch: [140][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9308 (Avg-Loss 0.8795)\tAcc 66.7969 (Avg-Acc 69.4238)\n",
            "Epoch: [140][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8116 (Avg-Loss 0.8889)\tAcc 72.8516 (Avg-Acc 69.1456)\n",
            "Epoch: [140][57/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8284 (Avg-Loss 0.8935)\tAcc 72.6562 (Avg-Acc 68.8409)\n",
            "Epoch: [140][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9838 (Avg-Loss 0.9084)\tAcc 65.6250 (Avg-Acc 68.2224)\n",
            "Epoch: [140][78/78]\tTime 0.010 (Avg-Time 0.049)\t Loss 0.9691 (Avg-Loss 0.9081)\tAcc 67.1875 (Avg-Acc 68.2450)\n",
            "EPOCH: 140 train Results: Acc 68.245 Loss: 0.9081\n",
            "Epoch: [140][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2056 (Avg-Loss 1.2056)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [140][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2908 (Avg-Loss 1.2064)\tAcc 55.5147 (Avg-Acc 57.8700)\n",
            "EPOCH: 140 Validation Results: Acc 57.870 Loss: 1.2064\n",
            "Epoch: [141][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.8388 (Avg-Loss 0.8388)\tAcc 71.8750 (Avg-Acc 71.8750)\n",
            "Epoch: [141][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9304 (Avg-Loss 0.8955)\tAcc 66.7969 (Avg-Acc 68.8672)\n",
            "Epoch: [141][38/78]\tTime 0.050 (Avg-Time 0.049)\t Loss 0.8744 (Avg-Loss 0.9002)\tAcc 70.1172 (Avg-Acc 68.7099)\n",
            "Epoch: [141][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9560 (Avg-Loss 0.9083)\tAcc 65.8203 (Avg-Acc 68.4806)\n",
            "Epoch: [141][76/78]\tTime 0.086 (Avg-Time 0.068)\t Loss 1.0378 (Avg-Loss 0.9120)\tAcc 63.2812 (Avg-Acc 68.3391)\n",
            "Epoch: [141][78/78]\tTime 0.033 (Avg-Time 0.068)\t Loss 1.0092 (Avg-Loss 0.9130)\tAcc 62.5000 (Avg-Acc 68.3300)\n",
            "EPOCH: 141 train Results: Acc 68.330 Loss: 0.9130\n",
            "Epoch: [141][0/19]\tTime 0.031 (Avg-Time 0.031)\t Loss 1.2280 (Avg-Loss 1.2280)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [141][19/19]\tTime 0.054 (Avg-Time 0.033)\t Loss 1.2771 (Avg-Loss 1.2088)\tAcc 54.7794 (Avg-Acc 57.7900)\n",
            "EPOCH: 141 Validation Results: Acc 57.790 Loss: 1.2088\n",
            "Epoch: [142][0/78]\tTime 0.092 (Avg-Time 0.092)\t Loss 0.8757 (Avg-Loss 0.8757)\tAcc 70.5078 (Avg-Acc 70.5078)\n",
            "Epoch: [142][19/78]\tTime 0.072 (Avg-Time 0.056)\t Loss 0.8925 (Avg-Loss 0.8798)\tAcc 67.5781 (Avg-Acc 69.2383)\n",
            "Epoch: [142][38/78]\tTime 0.048 (Avg-Time 0.052)\t Loss 0.8916 (Avg-Loss 0.8866)\tAcc 69.9219 (Avg-Acc 69.2007)\n",
            "Epoch: [142][57/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 0.9530 (Avg-Loss 0.9008)\tAcc 64.6484 (Avg-Acc 68.6355)\n",
            "Epoch: [142][76/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.0214 (Avg-Loss 0.9093)\tAcc 66.2109 (Avg-Acc 68.3289)\n",
            "Epoch: [142][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.0468 (Avg-Loss 0.9105)\tAcc 64.0625 (Avg-Acc 68.2725)\n",
            "EPOCH: 142 train Results: Acc 68.272 Loss: 0.9105\n",
            "Epoch: [142][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2083 (Avg-Loss 1.2083)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [142][19/19]\tTime 0.010 (Avg-Time 0.013)\t Loss 1.2857 (Avg-Loss 1.2155)\tAcc 54.7794 (Avg-Acc 57.6300)\n",
            "EPOCH: 142 Validation Results: Acc 57.630 Loss: 1.2155\n",
            "Epoch: [143][0/78]\tTime 0.064 (Avg-Time 0.064)\t Loss 0.8552 (Avg-Loss 0.8552)\tAcc 71.6797 (Avg-Acc 71.6797)\n",
            "Epoch: [143][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8928 (Avg-Loss 0.8739)\tAcc 68.3594 (Avg-Acc 69.8926)\n",
            "Epoch: [143][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8038 (Avg-Loss 0.8957)\tAcc 73.4375 (Avg-Acc 68.9052)\n",
            "Epoch: [143][57/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9493 (Avg-Loss 0.9031)\tAcc 67.7734 (Avg-Acc 68.6254)\n",
            "Epoch: [143][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9889 (Avg-Loss 0.9125)\tAcc 65.4297 (Avg-Acc 68.3289)\n",
            "Epoch: [143][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.8128 (Avg-Loss 0.9127)\tAcc 67.1875 (Avg-Acc 68.3200)\n",
            "EPOCH: 143 train Results: Acc 68.320 Loss: 0.9127\n",
            "Epoch: [143][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2099 (Avg-Loss 1.2099)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [143][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2587 (Avg-Loss 1.2114)\tAcc 56.9853 (Avg-Acc 57.5300)\n",
            "EPOCH: 143 Validation Results: Acc 57.530 Loss: 1.2114\n",
            "Epoch: [144][0/78]\tTime 0.063 (Avg-Time 0.063)\t Loss 0.8996 (Avg-Loss 0.8996)\tAcc 70.5078 (Avg-Acc 70.5078)\n",
            "Epoch: [144][19/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.8959 (Avg-Loss 0.8686)\tAcc 67.5781 (Avg-Acc 70.3320)\n",
            "Epoch: [144][38/78]\tTime 0.066 (Avg-Time 0.052)\t Loss 0.9280 (Avg-Loss 0.8846)\tAcc 67.9688 (Avg-Acc 69.3910)\n",
            "Epoch: [144][57/78]\tTime 0.097 (Avg-Time 0.081)\t Loss 0.9210 (Avg-Loss 0.8992)\tAcc 66.4062 (Avg-Acc 68.8173)\n",
            "Epoch: [144][76/78]\tTime 0.046 (Avg-Time 0.077)\t Loss 0.9237 (Avg-Loss 0.9079)\tAcc 68.3594 (Avg-Acc 68.5496)\n",
            "Epoch: [144][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.1999 (Avg-Loss 0.9097)\tAcc 57.8125 (Avg-Acc 68.4950)\n",
            "EPOCH: 144 train Results: Acc 68.495 Loss: 0.9097\n",
            "Epoch: [144][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2086 (Avg-Loss 1.2086)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [144][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2905 (Avg-Loss 1.2087)\tAcc 55.8824 (Avg-Acc 57.6900)\n",
            "EPOCH: 144 Validation Results: Acc 57.690 Loss: 1.2087\n",
            "Epoch: [145][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9003 (Avg-Loss 0.9003)\tAcc 67.9688 (Avg-Acc 67.9688)\n",
            "Epoch: [145][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8618 (Avg-Loss 0.8761)\tAcc 69.7266 (Avg-Acc 69.5898)\n",
            "Epoch: [145][38/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 0.8794 (Avg-Loss 0.8886)\tAcc 69.3359 (Avg-Acc 69.0855)\n",
            "Epoch: [145][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0041 (Avg-Loss 0.9034)\tAcc 63.4766 (Avg-Acc 68.4570)\n",
            "Epoch: [145][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9865 (Avg-Loss 0.9127)\tAcc 65.6250 (Avg-Acc 68.0753)\n",
            "Epoch: [145][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.8944 (Avg-Loss 0.9136)\tAcc 64.0625 (Avg-Acc 68.0225)\n",
            "EPOCH: 145 train Results: Acc 68.022 Loss: 0.9136\n",
            "Epoch: [145][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1996 (Avg-Loss 1.1996)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [145][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2887 (Avg-Loss 1.2090)\tAcc 52.9412 (Avg-Acc 57.7900)\n",
            "EPOCH: 145 Validation Results: Acc 57.790 Loss: 1.2090\n",
            "Epoch: [146][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8888 (Avg-Loss 0.8888)\tAcc 69.7266 (Avg-Acc 69.7266)\n",
            "Epoch: [146][19/78]\tTime 0.064 (Avg-Time 0.050)\t Loss 0.8653 (Avg-Loss 0.8599)\tAcc 70.1172 (Avg-Acc 70.2734)\n",
            "Epoch: [146][38/78]\tTime 0.051 (Avg-Time 0.052)\t Loss 0.9260 (Avg-Loss 0.8767)\tAcc 67.5781 (Avg-Acc 69.3610)\n",
            "Epoch: [146][57/78]\tTime 0.046 (Avg-Time 0.051)\t Loss 0.8757 (Avg-Loss 0.8940)\tAcc 69.5312 (Avg-Acc 68.6288)\n",
            "Epoch: [146][76/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 0.9732 (Avg-Loss 0.9078)\tAcc 68.3594 (Avg-Acc 68.1767)\n",
            "Epoch: [146][78/78]\tTime 0.010 (Avg-Time 0.050)\t Loss 0.9132 (Avg-Loss 0.9084)\tAcc 62.5000 (Avg-Acc 68.1700)\n",
            "EPOCH: 146 train Results: Acc 68.170 Loss: 0.9084\n",
            "Epoch: [146][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2064 (Avg-Loss 1.2064)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [146][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2895 (Avg-Loss 1.2119)\tAcc 55.5147 (Avg-Acc 57.5500)\n",
            "EPOCH: 146 Validation Results: Acc 57.550 Loss: 1.2119\n",
            "Epoch: [147][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8690 (Avg-Loss 0.8690)\tAcc 72.0703 (Avg-Acc 72.0703)\n",
            "Epoch: [147][19/78]\tTime 0.134 (Avg-Time 0.073)\t Loss 0.8703 (Avg-Loss 0.8770)\tAcc 70.1172 (Avg-Acc 69.2773)\n",
            "Epoch: [147][38/78]\tTime 0.047 (Avg-Time 0.102)\t Loss 0.9585 (Avg-Loss 0.8905)\tAcc 66.6016 (Avg-Acc 68.8702)\n",
            "Epoch: [147][57/78]\tTime 0.047 (Avg-Time 0.085)\t Loss 0.9566 (Avg-Loss 0.8978)\tAcc 63.4766 (Avg-Acc 68.5042)\n",
            "Epoch: [147][76/78]\tTime 0.050 (Avg-Time 0.076)\t Loss 0.9154 (Avg-Loss 0.9075)\tAcc 68.7500 (Avg-Acc 68.1463)\n",
            "Epoch: [147][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 0.8395 (Avg-Loss 0.9083)\tAcc 71.8750 (Avg-Acc 68.1500)\n",
            "EPOCH: 147 train Results: Acc 68.150 Loss: 0.9083\n",
            "Epoch: [147][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2145 (Avg-Loss 1.2145)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [147][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3128 (Avg-Loss 1.2176)\tAcc 53.6765 (Avg-Acc 57.5100)\n",
            "EPOCH: 147 Validation Results: Acc 57.510 Loss: 1.2176\n",
            "Epoch: [148][0/78]\tTime 0.063 (Avg-Time 0.063)\t Loss 0.8343 (Avg-Loss 0.8343)\tAcc 71.8750 (Avg-Acc 71.8750)\n",
            "Epoch: [148][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9058 (Avg-Loss 0.8647)\tAcc 67.3828 (Avg-Acc 69.9219)\n",
            "Epoch: [148][38/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.9136 (Avg-Loss 0.8829)\tAcc 67.9688 (Avg-Acc 69.0905)\n",
            "Epoch: [148][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9503 (Avg-Loss 0.8901)\tAcc 69.3359 (Avg-Acc 68.9116)\n",
            "Epoch: [148][76/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 1.0463 (Avg-Loss 0.9036)\tAcc 64.4531 (Avg-Acc 68.4938)\n",
            "Epoch: [148][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.9686 (Avg-Loss 0.9040)\tAcc 60.9375 (Avg-Acc 68.4775)\n",
            "EPOCH: 148 train Results: Acc 68.478 Loss: 0.9040\n",
            "Epoch: [148][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.2196 (Avg-Loss 1.2196)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [148][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2903 (Avg-Loss 1.2175)\tAcc 54.7794 (Avg-Acc 57.5900)\n",
            "EPOCH: 148 Validation Results: Acc 57.590 Loss: 1.2175\n",
            "Epoch: [149][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.8512 (Avg-Loss 0.8512)\tAcc 69.7266 (Avg-Acc 69.7266)\n",
            "Epoch: [149][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8811 (Avg-Loss 0.8801)\tAcc 70.1172 (Avg-Acc 69.2285)\n",
            "Epoch: [149][38/78]\tTime 0.052 (Avg-Time 0.049)\t Loss 0.9728 (Avg-Loss 0.8865)\tAcc 66.0156 (Avg-Acc 68.9253)\n",
            "Epoch: [149][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9449 (Avg-Loss 0.8984)\tAcc 68.3594 (Avg-Acc 68.6052)\n",
            "Epoch: [149][76/78]\tTime 0.256 (Avg-Time 0.057)\t Loss 0.8975 (Avg-Loss 0.9064)\tAcc 68.5547 (Avg-Acc 68.3289)\n",
            "Epoch: [149][78/78]\tTime 0.153 (Avg-Time 0.060)\t Loss 1.2886 (Avg-Loss 0.9073)\tAcc 46.8750 (Avg-Acc 68.3025)\n",
            "EPOCH: 149 train Results: Acc 68.302 Loss: 0.9073\n",
            "Epoch: [149][0/19]\tTime 0.027 (Avg-Time 0.027)\t Loss 1.1930 (Avg-Loss 1.1930)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [149][19/19]\tTime 0.013 (Avg-Time 0.045)\t Loss 1.2839 (Avg-Loss 1.2159)\tAcc 52.2059 (Avg-Acc 57.2800)\n",
            "EPOCH: 149 Validation Results: Acc 57.280 Loss: 1.2159\n",
            "Epoch: [150][0/78]\tTime 0.148 (Avg-Time 0.148)\t Loss 0.8269 (Avg-Loss 0.8269)\tAcc 68.7500 (Avg-Acc 68.7500)\n",
            "Epoch: [150][19/78]\tTime 0.047 (Avg-Time 0.077)\t Loss 0.9201 (Avg-Loss 0.8832)\tAcc 68.3594 (Avg-Acc 68.8770)\n",
            "Epoch: [150][38/78]\tTime 0.047 (Avg-Time 0.064)\t Loss 0.8861 (Avg-Loss 0.8853)\tAcc 67.5781 (Avg-Acc 69.0705)\n",
            "Epoch: [150][57/78]\tTime 0.047 (Avg-Time 0.059)\t Loss 0.8999 (Avg-Loss 0.8957)\tAcc 69.3359 (Avg-Acc 68.6894)\n",
            "Epoch: [150][76/78]\tTime 0.047 (Avg-Time 0.057)\t Loss 0.9180 (Avg-Loss 0.9061)\tAcc 67.7734 (Avg-Acc 68.3847)\n",
            "Epoch: [150][78/78]\tTime 0.009 (Avg-Time 0.056)\t Loss 1.2315 (Avg-Loss 0.9076)\tAcc 59.3750 (Avg-Acc 68.3425)\n",
            "EPOCH: 150 train Results: Acc 68.343 Loss: 0.9076\n",
            "Epoch: [150][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1954 (Avg-Loss 1.1954)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [150][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2935 (Avg-Loss 1.2175)\tAcc 56.2500 (Avg-Acc 57.3800)\n",
            "EPOCH: 150 Validation Results: Acc 57.380 Loss: 1.2175\n",
            "Epoch: [151][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8310 (Avg-Loss 0.8310)\tAcc 70.7031 (Avg-Acc 70.7031)\n",
            "Epoch: [151][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8306 (Avg-Loss 0.8812)\tAcc 73.4375 (Avg-Acc 69.4727)\n",
            "Epoch: [151][38/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8567 (Avg-Loss 0.8811)\tAcc 69.5312 (Avg-Acc 69.2157)\n",
            "Epoch: [151][57/78]\tTime 0.050 (Avg-Time 0.049)\t Loss 0.9711 (Avg-Loss 0.8994)\tAcc 68.5547 (Avg-Acc 68.5176)\n",
            "Epoch: [151][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8990 (Avg-Loss 0.9068)\tAcc 67.7734 (Avg-Acc 68.2224)\n",
            "Epoch: [151][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0453 (Avg-Loss 0.9072)\tAcc 64.0625 (Avg-Acc 68.2375)\n",
            "EPOCH: 151 train Results: Acc 68.237 Loss: 0.9072\n",
            "Epoch: [151][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1643 (Avg-Loss 1.1643)\tAcc 60.1562 (Avg-Acc 60.1562)\n",
            "Epoch: [151][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2786 (Avg-Loss 1.2055)\tAcc 53.3088 (Avg-Acc 57.7100)\n",
            "EPOCH: 151 Validation Results: Acc 57.710 Loss: 1.2055\n",
            "Epoch: [152][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.7879 (Avg-Loss 0.7879)\tAcc 72.4609 (Avg-Acc 72.4609)\n",
            "Epoch: [152][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8262 (Avg-Loss 0.8602)\tAcc 70.5078 (Avg-Acc 70.1074)\n",
            "Epoch: [152][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8598 (Avg-Loss 0.8760)\tAcc 69.3359 (Avg-Acc 69.5663)\n",
            "Epoch: [152][57/78]\tTime 0.083 (Avg-Time 0.075)\t Loss 0.9265 (Avg-Loss 0.8860)\tAcc 70.5078 (Avg-Acc 69.2315)\n",
            "Epoch: [152][76/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 0.8846 (Avg-Loss 0.8960)\tAcc 69.5312 (Avg-Acc 68.8515)\n",
            "Epoch: [152][78/78]\tTime 0.010 (Avg-Time 0.075)\t Loss 1.0437 (Avg-Loss 0.8963)\tAcc 62.5000 (Avg-Acc 68.8325)\n",
            "EPOCH: 152 train Results: Acc 68.832 Loss: 0.8963\n",
            "Epoch: [152][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1813 (Avg-Loss 1.1813)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [152][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2655 (Avg-Loss 1.2086)\tAcc 55.5147 (Avg-Acc 57.7900)\n",
            "EPOCH: 152 Validation Results: Acc 57.790 Loss: 1.2086\n",
            "Epoch: [153][0/78]\tTime 0.053 (Avg-Time 0.053)\t Loss 0.7972 (Avg-Loss 0.7972)\tAcc 71.8750 (Avg-Acc 71.8750)\n",
            "Epoch: [153][19/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.8772 (Avg-Loss 0.8589)\tAcc 70.7031 (Avg-Acc 69.7754)\n",
            "Epoch: [153][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8708 (Avg-Loss 0.8811)\tAcc 69.3359 (Avg-Acc 69.2007)\n",
            "Epoch: [153][57/78]\tTime 0.059 (Avg-Time 0.050)\t Loss 0.9014 (Avg-Loss 0.8928)\tAcc 66.2109 (Avg-Acc 68.7567)\n",
            "Epoch: [153][76/78]\tTime 0.045 (Avg-Time 0.049)\t Loss 0.9584 (Avg-Loss 0.9047)\tAcc 67.1875 (Avg-Acc 68.4862)\n",
            "Epoch: [153][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1142 (Avg-Loss 0.9059)\tAcc 59.3750 (Avg-Acc 68.4475)\n",
            "EPOCH: 153 train Results: Acc 68.448 Loss: 0.9059\n",
            "Epoch: [153][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1874 (Avg-Loss 1.1874)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [153][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2715 (Avg-Loss 1.2194)\tAcc 56.2500 (Avg-Acc 57.1000)\n",
            "EPOCH: 153 Validation Results: Acc 57.100 Loss: 1.2194\n",
            "Epoch: [154][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8409 (Avg-Loss 0.8409)\tAcc 69.1406 (Avg-Acc 69.1406)\n",
            "Epoch: [154][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9568 (Avg-Loss 0.8641)\tAcc 66.6016 (Avg-Acc 70.2051)\n",
            "Epoch: [154][38/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 0.8470 (Avg-Loss 0.8732)\tAcc 70.5078 (Avg-Acc 69.8618)\n",
            "Epoch: [154][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9075 (Avg-Loss 0.8895)\tAcc 69.5312 (Avg-Acc 69.2821)\n",
            "Epoch: [154][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9365 (Avg-Loss 0.8998)\tAcc 67.5781 (Avg-Acc 68.6790)\n",
            "Epoch: [154][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1882 (Avg-Loss 0.9004)\tAcc 62.5000 (Avg-Acc 68.6525)\n",
            "EPOCH: 154 train Results: Acc 68.653 Loss: 0.9004\n",
            "Epoch: [154][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1824 (Avg-Loss 1.1824)\tAcc 60.3516 (Avg-Acc 60.3516)\n",
            "Epoch: [154][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3125 (Avg-Loss 1.2106)\tAcc 52.2059 (Avg-Acc 57.6600)\n",
            "EPOCH: 154 Validation Results: Acc 57.660 Loss: 1.2106\n",
            "Epoch: [155][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.8253 (Avg-Loss 0.8253)\tAcc 71.6797 (Avg-Acc 71.6797)\n",
            "Epoch: [155][19/78]\tTime 0.078 (Avg-Time 0.057)\t Loss 0.9117 (Avg-Loss 0.8563)\tAcc 66.4062 (Avg-Acc 70.0098)\n",
            "Epoch: [155][38/78]\tTime 0.091 (Avg-Time 0.094)\t Loss 0.9295 (Avg-Loss 0.8807)\tAcc 68.5547 (Avg-Acc 68.9503)\n",
            "Epoch: [155][57/78]\tTime 0.049 (Avg-Time 0.083)\t Loss 0.9348 (Avg-Loss 0.8949)\tAcc 66.7969 (Avg-Acc 68.4739)\n",
            "Epoch: [155][76/78]\tTime 0.047 (Avg-Time 0.075)\t Loss 0.9728 (Avg-Loss 0.9046)\tAcc 65.2344 (Avg-Acc 68.2275)\n",
            "Epoch: [155][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 1.0586 (Avg-Loss 0.9050)\tAcc 62.5000 (Avg-Acc 68.2025)\n",
            "EPOCH: 155 train Results: Acc 68.203 Loss: 0.9050\n",
            "Epoch: [155][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1764 (Avg-Loss 1.1764)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [155][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2554 (Avg-Loss 1.2113)\tAcc 56.2500 (Avg-Acc 57.8200)\n",
            "EPOCH: 155 Validation Results: Acc 57.820 Loss: 1.2113\n",
            "Epoch: [156][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.8298 (Avg-Loss 0.8298)\tAcc 70.8984 (Avg-Acc 70.8984)\n",
            "Epoch: [156][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8556 (Avg-Loss 0.8727)\tAcc 68.5547 (Avg-Acc 69.9121)\n",
            "Epoch: [156][38/78]\tTime 0.093 (Avg-Time 0.051)\t Loss 0.9365 (Avg-Loss 0.8834)\tAcc 66.6016 (Avg-Acc 69.4712)\n",
            "Epoch: [156][57/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 1.0026 (Avg-Loss 0.8959)\tAcc 64.8438 (Avg-Acc 69.0059)\n",
            "Epoch: [156][76/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.9909 (Avg-Loss 0.9061)\tAcc 64.2578 (Avg-Acc 68.5319)\n",
            "Epoch: [156][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0807 (Avg-Loss 0.9065)\tAcc 60.9375 (Avg-Acc 68.5100)\n",
            "EPOCH: 156 train Results: Acc 68.510 Loss: 0.9065\n",
            "Epoch: [156][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1951 (Avg-Loss 1.1951)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [156][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2959 (Avg-Loss 1.2112)\tAcc 55.5147 (Avg-Acc 57.8000)\n",
            "EPOCH: 156 Validation Results: Acc 57.800 Loss: 1.2112\n",
            "Epoch: [157][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.9004 (Avg-Loss 0.9004)\tAcc 69.9219 (Avg-Acc 69.9219)\n",
            "Epoch: [157][19/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.9304 (Avg-Loss 0.8637)\tAcc 67.9688 (Avg-Acc 69.9902)\n",
            "Epoch: [157][38/78]\tTime 0.061 (Avg-Time 0.050)\t Loss 0.8931 (Avg-Loss 0.8789)\tAcc 70.3125 (Avg-Acc 69.4311)\n",
            "Epoch: [157][57/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 0.8913 (Avg-Loss 0.8921)\tAcc 67.5781 (Avg-Acc 68.7163)\n",
            "Epoch: [157][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9779 (Avg-Loss 0.9007)\tAcc 65.4297 (Avg-Acc 68.4279)\n",
            "Epoch: [157][78/78]\tTime 0.024 (Avg-Time 0.050)\t Loss 1.0558 (Avg-Loss 0.9012)\tAcc 70.3125 (Avg-Acc 68.4175)\n",
            "EPOCH: 157 train Results: Acc 68.418 Loss: 0.9012\n",
            "Epoch: [157][0/19]\tTime 0.022 (Avg-Time 0.022)\t Loss 1.1944 (Avg-Loss 1.1944)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [157][19/19]\tTime 0.085 (Avg-Time 0.041)\t Loss 1.3050 (Avg-Loss 1.2069)\tAcc 52.2059 (Avg-Acc 57.6400)\n",
            "EPOCH: 157 Validation Results: Acc 57.640 Loss: 1.2069\n",
            "Epoch: [158][0/78]\tTime 0.295 (Avg-Time 0.295)\t Loss 0.8524 (Avg-Loss 0.8524)\tAcc 71.8750 (Avg-Acc 71.8750)\n",
            "Epoch: [158][19/78]\tTime 0.047 (Avg-Time 0.121)\t Loss 0.9230 (Avg-Loss 0.8743)\tAcc 67.5781 (Avg-Acc 69.9609)\n",
            "Epoch: [158][38/78]\tTime 0.047 (Avg-Time 0.086)\t Loss 0.8915 (Avg-Loss 0.8753)\tAcc 69.1406 (Avg-Acc 69.7566)\n",
            "Epoch: [158][57/78]\tTime 0.049 (Avg-Time 0.074)\t Loss 0.9294 (Avg-Loss 0.8895)\tAcc 66.0156 (Avg-Acc 69.0430)\n",
            "Epoch: [158][76/78]\tTime 0.047 (Avg-Time 0.068)\t Loss 0.9600 (Avg-Loss 0.9030)\tAcc 65.4297 (Avg-Acc 68.5166)\n",
            "Epoch: [158][78/78]\tTime 0.013 (Avg-Time 0.067)\t Loss 1.1015 (Avg-Loss 0.9040)\tAcc 57.8125 (Avg-Acc 68.4450)\n",
            "EPOCH: 158 train Results: Acc 68.445 Loss: 0.9040\n",
            "Epoch: [158][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1888 (Avg-Loss 1.1888)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [158][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.3042 (Avg-Loss 1.2117)\tAcc 56.6176 (Avg-Acc 57.4700)\n",
            "EPOCH: 158 Validation Results: Acc 57.470 Loss: 1.2117\n",
            "Epoch: [159][0/78]\tTime 0.067 (Avg-Time 0.067)\t Loss 0.8412 (Avg-Loss 0.8412)\tAcc 71.2891 (Avg-Acc 71.2891)\n",
            "Epoch: [159][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.8736 (Avg-Loss 0.8573)\tAcc 69.5312 (Avg-Acc 70.3223)\n",
            "Epoch: [159][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9351 (Avg-Loss 0.8784)\tAcc 66.9922 (Avg-Acc 69.3960)\n",
            "Epoch: [159][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9295 (Avg-Loss 0.8863)\tAcc 67.5781 (Avg-Acc 69.0598)\n",
            "Epoch: [159][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0172 (Avg-Loss 0.8994)\tAcc 62.1094 (Avg-Acc 68.6130)\n",
            "Epoch: [159][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1317 (Avg-Loss 0.9000)\tAcc 60.9375 (Avg-Acc 68.5850)\n",
            "EPOCH: 159 train Results: Acc 68.585 Loss: 0.9000\n",
            "Epoch: [159][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1976 (Avg-Loss 1.1976)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [159][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2562 (Avg-Loss 1.2150)\tAcc 57.3529 (Avg-Acc 57.4100)\n",
            "EPOCH: 159 Validation Results: Acc 57.410 Loss: 1.2150\n",
            "Epoch: [160][0/78]\tTime 0.074 (Avg-Time 0.074)\t Loss 0.8163 (Avg-Loss 0.8163)\tAcc 72.0703 (Avg-Acc 72.0703)\n",
            "Epoch: [160][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8724 (Avg-Loss 0.8654)\tAcc 69.9219 (Avg-Acc 70.3809)\n",
            "Epoch: [160][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8817 (Avg-Loss 0.8746)\tAcc 68.9453 (Avg-Acc 69.7316)\n",
            "Epoch: [160][57/78]\tTime 0.109 (Avg-Time 0.055)\t Loss 0.9531 (Avg-Loss 0.8872)\tAcc 66.7969 (Avg-Acc 69.1709)\n",
            "Epoch: [160][76/78]\tTime 0.086 (Avg-Time 0.075)\t Loss 1.0136 (Avg-Loss 0.8995)\tAcc 65.6250 (Avg-Acc 68.7094)\n",
            "Epoch: [160][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 0.8430 (Avg-Loss 0.9005)\tAcc 68.7500 (Avg-Acc 68.6750)\n",
            "EPOCH: 160 train Results: Acc 68.675 Loss: 0.9005\n",
            "Epoch: [160][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1936 (Avg-Loss 1.1936)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [160][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3026 (Avg-Loss 1.2117)\tAcc 52.2059 (Avg-Acc 57.7900)\n",
            "EPOCH: 160 Validation Results: Acc 57.790 Loss: 1.2117\n",
            "Epoch: [161][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8101 (Avg-Loss 0.8101)\tAcc 73.0469 (Avg-Acc 73.0469)\n",
            "Epoch: [161][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8481 (Avg-Loss 0.8651)\tAcc 75.3906 (Avg-Acc 70.1953)\n",
            "Epoch: [161][38/78]\tTime 0.050 (Avg-Time 0.049)\t Loss 0.9145 (Avg-Loss 0.8768)\tAcc 66.0156 (Avg-Acc 69.5162)\n",
            "Epoch: [161][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8353 (Avg-Loss 0.8844)\tAcc 72.4609 (Avg-Acc 69.2720)\n",
            "Epoch: [161][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8478 (Avg-Loss 0.8920)\tAcc 70.8984 (Avg-Acc 68.8971)\n",
            "Epoch: [161][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.9508 (Avg-Loss 0.8921)\tAcc 70.3125 (Avg-Acc 68.8775)\n",
            "EPOCH: 161 train Results: Acc 68.877 Loss: 0.8921\n",
            "Epoch: [161][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1927 (Avg-Loss 1.1927)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [161][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3132 (Avg-Loss 1.2182)\tAcc 55.5147 (Avg-Acc 57.7200)\n",
            "EPOCH: 161 Validation Results: Acc 57.720 Loss: 1.2182\n",
            "Epoch: [162][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8360 (Avg-Loss 0.8360)\tAcc 71.4844 (Avg-Acc 71.4844)\n",
            "Epoch: [162][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9475 (Avg-Loss 0.8732)\tAcc 66.6016 (Avg-Acc 69.3848)\n",
            "Epoch: [162][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9145 (Avg-Loss 0.8879)\tAcc 71.4844 (Avg-Acc 69.1106)\n",
            "Epoch: [162][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8312 (Avg-Loss 0.8876)\tAcc 71.4844 (Avg-Acc 69.0800)\n",
            "Epoch: [162][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9382 (Avg-Loss 0.8968)\tAcc 65.2344 (Avg-Acc 68.7170)\n",
            "Epoch: [162][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.1525 (Avg-Loss 0.8977)\tAcc 57.8125 (Avg-Acc 68.6925)\n",
            "EPOCH: 162 train Results: Acc 68.692 Loss: 0.8977\n",
            "Epoch: [162][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1930 (Avg-Loss 1.1930)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [162][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3036 (Avg-Loss 1.2108)\tAcc 54.0441 (Avg-Acc 57.7200)\n",
            "EPOCH: 162 Validation Results: Acc 57.720 Loss: 1.2108\n",
            "Epoch: [163][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.8968 (Avg-Loss 0.8968)\tAcc 68.5547 (Avg-Acc 68.5547)\n",
            "Epoch: [163][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.8412 (Avg-Loss 0.8840)\tAcc 71.6797 (Avg-Acc 69.6582)\n",
            "Epoch: [163][38/78]\tTime 0.104 (Avg-Time 0.074)\t Loss 0.9011 (Avg-Loss 0.8851)\tAcc 67.3828 (Avg-Acc 69.2358)\n",
            "Epoch: [163][57/78]\tTime 0.047 (Avg-Time 0.083)\t Loss 0.8425 (Avg-Loss 0.8900)\tAcc 71.8750 (Avg-Acc 69.1474)\n",
            "Epoch: [163][76/78]\tTime 0.047 (Avg-Time 0.075)\t Loss 0.9644 (Avg-Loss 0.9016)\tAcc 67.9688 (Avg-Acc 68.6156)\n",
            "Epoch: [163][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 0.8907 (Avg-Loss 0.9019)\tAcc 65.6250 (Avg-Acc 68.5625)\n",
            "EPOCH: 163 train Results: Acc 68.562 Loss: 0.9019\n",
            "Epoch: [163][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1839 (Avg-Loss 1.1839)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [163][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2794 (Avg-Loss 1.2084)\tAcc 54.7794 (Avg-Acc 57.6100)\n",
            "EPOCH: 163 Validation Results: Acc 57.610 Loss: 1.2084\n",
            "Epoch: [164][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8773 (Avg-Loss 0.8773)\tAcc 70.5078 (Avg-Acc 70.5078)\n",
            "Epoch: [164][19/78]\tTime 0.050 (Avg-Time 0.049)\t Loss 0.8059 (Avg-Loss 0.8577)\tAcc 74.6094 (Avg-Acc 69.8535)\n",
            "Epoch: [164][38/78]\tTime 0.051 (Avg-Time 0.049)\t Loss 0.8941 (Avg-Loss 0.8747)\tAcc 66.9922 (Avg-Acc 69.3860)\n",
            "Epoch: [164][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9283 (Avg-Loss 0.8835)\tAcc 65.8203 (Avg-Acc 68.9655)\n",
            "Epoch: [164][76/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9032 (Avg-Loss 0.8924)\tAcc 68.3594 (Avg-Acc 68.7018)\n",
            "Epoch: [164][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.9424 (Avg-Loss 0.8925)\tAcc 62.5000 (Avg-Acc 68.6875)\n",
            "EPOCH: 164 train Results: Acc 68.688 Loss: 0.8925\n",
            "Epoch: [164][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1878 (Avg-Loss 1.1878)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [164][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2664 (Avg-Loss 1.2136)\tAcc 52.9412 (Avg-Acc 57.3900)\n",
            "EPOCH: 164 Validation Results: Acc 57.390 Loss: 1.2136\n",
            "Epoch: [165][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.7984 (Avg-Loss 0.7984)\tAcc 73.0469 (Avg-Acc 73.0469)\n",
            "Epoch: [165][19/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.8767 (Avg-Loss 0.8601)\tAcc 72.0703 (Avg-Acc 69.9121)\n",
            "Epoch: [165][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8916 (Avg-Loss 0.8709)\tAcc 68.1641 (Avg-Acc 69.7316)\n",
            "Epoch: [165][57/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9044 (Avg-Loss 0.8909)\tAcc 70.3125 (Avg-Acc 68.8948)\n",
            "Epoch: [165][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9179 (Avg-Loss 0.9008)\tAcc 68.7500 (Avg-Acc 68.5369)\n",
            "Epoch: [165][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0458 (Avg-Loss 0.9015)\tAcc 64.0625 (Avg-Acc 68.5050)\n",
            "EPOCH: 165 train Results: Acc 68.505 Loss: 0.9015\n",
            "Epoch: [165][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2097 (Avg-Loss 1.2097)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [165][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2492 (Avg-Loss 1.2088)\tAcc 52.9412 (Avg-Acc 57.7300)\n",
            "EPOCH: 165 Validation Results: Acc 57.730 Loss: 1.2088\n",
            "Epoch: [166][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8032 (Avg-Loss 0.8032)\tAcc 69.7266 (Avg-Acc 69.7266)\n",
            "Epoch: [166][19/78]\tTime 0.142 (Avg-Time 0.117)\t Loss 0.9066 (Avg-Loss 0.8674)\tAcc 67.7734 (Avg-Acc 69.6875)\n",
            "Epoch: [166][38/78]\tTime 0.047 (Avg-Time 0.101)\t Loss 0.8692 (Avg-Loss 0.8739)\tAcc 68.7500 (Avg-Acc 69.6615)\n",
            "Epoch: [166][57/78]\tTime 0.047 (Avg-Time 0.084)\t Loss 0.9123 (Avg-Loss 0.8809)\tAcc 67.3828 (Avg-Acc 69.2012)\n",
            "Epoch: [166][76/78]\tTime 0.049 (Avg-Time 0.075)\t Loss 1.0123 (Avg-Loss 0.8936)\tAcc 64.4531 (Avg-Acc 68.6663)\n",
            "Epoch: [166][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 0.9623 (Avg-Loss 0.8953)\tAcc 59.3750 (Avg-Acc 68.6000)\n",
            "EPOCH: 166 train Results: Acc 68.600 Loss: 0.8953\n",
            "Epoch: [166][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1852 (Avg-Loss 1.1852)\tAcc 60.3516 (Avg-Acc 60.3516)\n",
            "Epoch: [166][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2833 (Avg-Loss 1.2113)\tAcc 51.1029 (Avg-Acc 57.5400)\n",
            "EPOCH: 166 Validation Results: Acc 57.540 Loss: 1.2113\n",
            "Epoch: [167][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8146 (Avg-Loss 0.8146)\tAcc 72.0703 (Avg-Acc 72.0703)\n",
            "Epoch: [167][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8449 (Avg-Loss 0.8534)\tAcc 70.7031 (Avg-Acc 70.6641)\n",
            "Epoch: [167][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8449 (Avg-Loss 0.8698)\tAcc 71.8750 (Avg-Acc 69.7165)\n",
            "Epoch: [167][57/78]\tTime 0.069 (Avg-Time 0.050)\t Loss 0.9572 (Avg-Loss 0.8827)\tAcc 66.0156 (Avg-Acc 69.0127)\n",
            "Epoch: [167][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9495 (Avg-Loss 0.8941)\tAcc 66.6016 (Avg-Acc 68.5902)\n",
            "Epoch: [167][78/78]\tTime 0.018 (Avg-Time 0.049)\t Loss 1.0444 (Avg-Loss 0.8943)\tAcc 73.4375 (Avg-Acc 68.5775)\n",
            "EPOCH: 167 train Results: Acc 68.578 Loss: 0.8943\n",
            "Epoch: [167][0/19]\tTime 0.019 (Avg-Time 0.019)\t Loss 1.2168 (Avg-Loss 1.2168)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [167][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2738 (Avg-Loss 1.2229)\tAcc 52.9412 (Avg-Acc 57.2000)\n",
            "EPOCH: 167 Validation Results: Acc 57.200 Loss: 1.2229\n",
            "Epoch: [168][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8459 (Avg-Loss 0.8459)\tAcc 71.0938 (Avg-Acc 71.0938)\n",
            "Epoch: [168][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9026 (Avg-Loss 0.8688)\tAcc 68.7500 (Avg-Acc 69.7559)\n",
            "Epoch: [168][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8223 (Avg-Loss 0.8765)\tAcc 71.4844 (Avg-Acc 69.2608)\n",
            "Epoch: [168][57/78]\tTime 0.072 (Avg-Time 0.049)\t Loss 0.9162 (Avg-Loss 0.8860)\tAcc 67.5781 (Avg-Acc 69.0463)\n",
            "Epoch: [168][76/78]\tTime 0.160 (Avg-Time 0.064)\t Loss 1.0388 (Avg-Loss 0.8935)\tAcc 62.5000 (Avg-Acc 68.8337)\n",
            "Epoch: [168][78/78]\tTime 0.163 (Avg-Time 0.067)\t Loss 1.0248 (Avg-Loss 0.8942)\tAcc 62.5000 (Avg-Acc 68.8025)\n",
            "EPOCH: 168 train Results: Acc 68.802 Loss: 0.8942\n",
            "Epoch: [168][0/19]\tTime 0.032 (Avg-Time 0.032)\t Loss 1.2038 (Avg-Loss 1.2038)\tAcc 59.9609 (Avg-Acc 59.9609)\n",
            "Epoch: [168][19/19]\tTime 0.012 (Avg-Time 0.029)\t Loss 1.2709 (Avg-Loss 1.2169)\tAcc 55.1471 (Avg-Acc 57.3800)\n",
            "EPOCH: 168 Validation Results: Acc 57.380 Loss: 1.2169\n",
            "Epoch: [169][0/78]\tTime 0.100 (Avg-Time 0.100)\t Loss 0.8267 (Avg-Loss 0.8267)\tAcc 71.8750 (Avg-Acc 71.8750)\n",
            "Epoch: [169][19/78]\tTime 0.050 (Avg-Time 0.067)\t Loss 0.8372 (Avg-Loss 0.8643)\tAcc 71.0938 (Avg-Acc 70.2539)\n",
            "Epoch: [169][38/78]\tTime 0.051 (Avg-Time 0.059)\t Loss 0.9218 (Avg-Loss 0.8775)\tAcc 67.3828 (Avg-Acc 69.4561)\n",
            "Epoch: [169][57/78]\tTime 0.047 (Avg-Time 0.055)\t Loss 0.9392 (Avg-Loss 0.8879)\tAcc 67.7734 (Avg-Acc 69.0127)\n",
            "Epoch: [169][76/78]\tTime 0.048 (Avg-Time 0.054)\t Loss 0.9061 (Avg-Loss 0.8973)\tAcc 67.5781 (Avg-Acc 68.5750)\n",
            "Epoch: [169][78/78]\tTime 0.014 (Avg-Time 0.053)\t Loss 0.9243 (Avg-Loss 0.8975)\tAcc 71.8750 (Avg-Acc 68.5700)\n",
            "EPOCH: 169 train Results: Acc 68.570 Loss: 0.8975\n",
            "Epoch: [169][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2014 (Avg-Loss 1.2014)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [169][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2763 (Avg-Loss 1.2199)\tAcc 53.6765 (Avg-Acc 57.6100)\n",
            "EPOCH: 169 Validation Results: Acc 57.610 Loss: 1.2199\n",
            "Epoch: [170][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.7942 (Avg-Loss 0.7942)\tAcc 71.6797 (Avg-Acc 71.6797)\n",
            "Epoch: [170][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.8422 (Avg-Loss 0.8536)\tAcc 71.0938 (Avg-Acc 70.6348)\n",
            "Epoch: [170][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9024 (Avg-Loss 0.8689)\tAcc 67.9688 (Avg-Acc 69.6464)\n",
            "Epoch: [170][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9155 (Avg-Loss 0.8848)\tAcc 68.3594 (Avg-Acc 69.0733)\n",
            "Epoch: [170][76/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9313 (Avg-Loss 0.8920)\tAcc 68.1641 (Avg-Acc 68.7728)\n",
            "Epoch: [170][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.3064 (Avg-Loss 0.8928)\tAcc 48.4375 (Avg-Acc 68.7375)\n",
            "EPOCH: 170 train Results: Acc 68.737 Loss: 0.8928\n",
            "Epoch: [170][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2175 (Avg-Loss 1.2175)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [170][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2803 (Avg-Loss 1.2168)\tAcc 55.1471 (Avg-Acc 57.9900)\n",
            "EPOCH: 170 Validation Results: Acc 57.990 Loss: 1.2168\n",
            "Epoch: [171][0/78]\tTime 0.055 (Avg-Time 0.055)\t Loss 0.8534 (Avg-Loss 0.8534)\tAcc 70.1172 (Avg-Acc 70.1172)\n",
            "Epoch: [171][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9028 (Avg-Loss 0.8468)\tAcc 67.9688 (Avg-Acc 70.6836)\n",
            "Epoch: [171][38/78]\tTime 0.092 (Avg-Time 0.051)\t Loss 0.8764 (Avg-Loss 0.8644)\tAcc 68.5547 (Avg-Acc 70.0371)\n",
            "Epoch: [171][57/78]\tTime 0.132 (Avg-Time 0.078)\t Loss 0.8915 (Avg-Loss 0.8740)\tAcc 69.7266 (Avg-Acc 69.5885)\n",
            "Epoch: [171][76/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 0.9458 (Avg-Loss 0.8891)\tAcc 63.6719 (Avg-Acc 68.9047)\n",
            "Epoch: [171][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.1264 (Avg-Loss 0.8900)\tAcc 56.2500 (Avg-Acc 68.8650)\n",
            "EPOCH: 171 train Results: Acc 68.865 Loss: 0.8900\n",
            "Epoch: [171][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2324 (Avg-Loss 1.2324)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [171][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3042 (Avg-Loss 1.2182)\tAcc 51.4706 (Avg-Acc 57.8000)\n",
            "EPOCH: 171 Validation Results: Acc 57.800 Loss: 1.2182\n",
            "Epoch: [172][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.7601 (Avg-Loss 0.7601)\tAcc 75.1953 (Avg-Acc 75.1953)\n",
            "Epoch: [172][19/78]\tTime 0.070 (Avg-Time 0.049)\t Loss 0.7887 (Avg-Loss 0.8508)\tAcc 73.0469 (Avg-Acc 70.6055)\n",
            "Epoch: [172][38/78]\tTime 0.050 (Avg-Time 0.049)\t Loss 0.9427 (Avg-Loss 0.8729)\tAcc 67.3828 (Avg-Acc 69.4211)\n",
            "Epoch: [172][57/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9468 (Avg-Loss 0.8878)\tAcc 68.7500 (Avg-Acc 68.9150)\n",
            "Epoch: [172][76/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8393 (Avg-Loss 0.8933)\tAcc 70.8984 (Avg-Acc 68.6663)\n",
            "Epoch: [172][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1049 (Avg-Loss 0.8945)\tAcc 62.5000 (Avg-Acc 68.6400)\n",
            "EPOCH: 172 train Results: Acc 68.640 Loss: 0.8945\n",
            "Epoch: [172][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1951 (Avg-Loss 1.1951)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [172][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2896 (Avg-Loss 1.2119)\tAcc 50.7353 (Avg-Acc 57.7000)\n",
            "EPOCH: 172 Validation Results: Acc 57.700 Loss: 1.2119\n",
            "Epoch: [173][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9565 (Avg-Loss 0.9565)\tAcc 67.5781 (Avg-Acc 67.5781)\n",
            "Epoch: [173][19/78]\tTime 0.046 (Avg-Time 0.048)\t Loss 0.9077 (Avg-Loss 0.8591)\tAcc 68.3594 (Avg-Acc 70.4199)\n",
            "Epoch: [173][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8590 (Avg-Loss 0.8722)\tAcc 69.9219 (Avg-Acc 69.8918)\n",
            "Epoch: [173][57/78]\tTime 0.045 (Avg-Time 0.049)\t Loss 0.9655 (Avg-Loss 0.8864)\tAcc 64.2578 (Avg-Acc 69.3292)\n",
            "Epoch: [173][76/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9835 (Avg-Loss 0.8932)\tAcc 65.8203 (Avg-Acc 68.8743)\n",
            "Epoch: [173][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.9118 (Avg-Loss 0.8938)\tAcc 65.6250 (Avg-Acc 68.8500)\n",
            "EPOCH: 173 train Results: Acc 68.850 Loss: 0.8938\n",
            "Epoch: [173][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2080 (Avg-Loss 1.2080)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [173][19/19]\tTime 0.006 (Avg-Time 0.013)\t Loss 1.2917 (Avg-Loss 1.2136)\tAcc 52.9412 (Avg-Acc 57.9100)\n",
            "EPOCH: 173 Validation Results: Acc 57.910 Loss: 1.2136\n",
            "Epoch: [174][0/78]\tTime 0.072 (Avg-Time 0.072)\t Loss 0.8211 (Avg-Loss 0.8211)\tAcc 71.6797 (Avg-Acc 71.6797)\n",
            "Epoch: [174][19/78]\tTime 0.092 (Avg-Time 0.053)\t Loss 0.9272 (Avg-Loss 0.8439)\tAcc 66.7969 (Avg-Acc 71.1914)\n",
            "Epoch: [174][38/78]\tTime 0.087 (Avg-Time 0.095)\t Loss 0.8499 (Avg-Loss 0.8576)\tAcc 69.5312 (Avg-Acc 70.4978)\n",
            "Epoch: [174][57/78]\tTime 0.047 (Avg-Time 0.084)\t Loss 0.9288 (Avg-Loss 0.8718)\tAcc 65.4297 (Avg-Acc 69.9623)\n",
            "Epoch: [174][76/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 0.8671 (Avg-Loss 0.8838)\tAcc 67.7734 (Avg-Acc 69.3994)\n",
            "Epoch: [174][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.1538 (Avg-Loss 0.8845)\tAcc 53.1250 (Avg-Acc 69.3400)\n",
            "EPOCH: 174 train Results: Acc 69.340 Loss: 0.8845\n",
            "Epoch: [174][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1595 (Avg-Loss 1.1595)\tAcc 60.3516 (Avg-Acc 60.3516)\n",
            "Epoch: [174][19/19]\tTime 0.006 (Avg-Time 0.013)\t Loss 1.2092 (Avg-Loss 1.2056)\tAcc 53.3088 (Avg-Acc 57.9500)\n",
            "EPOCH: 174 Validation Results: Acc 57.950 Loss: 1.2056\n",
            "Epoch: [175][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8498 (Avg-Loss 0.8498)\tAcc 70.3125 (Avg-Acc 70.3125)\n",
            "Epoch: [175][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8248 (Avg-Loss 0.8619)\tAcc 71.2891 (Avg-Acc 70.4883)\n",
            "Epoch: [175][38/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.8572 (Avg-Loss 0.8666)\tAcc 71.8750 (Avg-Acc 70.0120)\n",
            "Epoch: [175][57/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9099 (Avg-Loss 0.8821)\tAcc 68.1641 (Avg-Acc 69.1507)\n",
            "Epoch: [175][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8620 (Avg-Loss 0.8942)\tAcc 70.1172 (Avg-Acc 68.7779)\n",
            "Epoch: [175][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.7150 (Avg-Loss 0.8951)\tAcc 78.1250 (Avg-Acc 68.7400)\n",
            "EPOCH: 175 train Results: Acc 68.740 Loss: 0.8951\n",
            "Epoch: [175][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1506 (Avg-Loss 1.1506)\tAcc 59.9609 (Avg-Acc 59.9609)\n",
            "Epoch: [175][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2866 (Avg-Loss 1.2123)\tAcc 51.1029 (Avg-Acc 56.9100)\n",
            "EPOCH: 175 Validation Results: Acc 56.910 Loss: 1.2123\n",
            "Epoch: [176][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9030 (Avg-Loss 0.9030)\tAcc 70.5078 (Avg-Acc 70.5078)\n",
            "Epoch: [176][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8347 (Avg-Loss 0.8541)\tAcc 71.2891 (Avg-Acc 70.2539)\n",
            "Epoch: [176][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8843 (Avg-Loss 0.8561)\tAcc 68.1641 (Avg-Acc 70.1623)\n",
            "Epoch: [176][57/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8762 (Avg-Loss 0.8719)\tAcc 67.7734 (Avg-Acc 69.5717)\n",
            "Epoch: [176][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9274 (Avg-Loss 0.8848)\tAcc 68.3594 (Avg-Acc 69.0366)\n",
            "Epoch: [176][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1524 (Avg-Loss 0.8851)\tAcc 60.9375 (Avg-Acc 69.0175)\n",
            "EPOCH: 176 train Results: Acc 69.017 Loss: 0.8851\n",
            "Epoch: [176][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1699 (Avg-Loss 1.1699)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [176][19/19]\tTime 0.018 (Avg-Time 0.023)\t Loss 1.2495 (Avg-Loss 1.2150)\tAcc 54.0441 (Avg-Acc 57.3100)\n",
            "EPOCH: 176 Validation Results: Acc 57.310 Loss: 1.2150\n",
            "Epoch: [177][0/78]\tTime 0.099 (Avg-Time 0.099)\t Loss 0.8772 (Avg-Loss 0.8772)\tAcc 69.7266 (Avg-Acc 69.7266)\n",
            "Epoch: [177][19/78]\tTime 0.100 (Avg-Time 0.131)\t Loss 0.8853 (Avg-Loss 0.8484)\tAcc 67.9688 (Avg-Acc 70.4883)\n",
            "Epoch: [177][38/78]\tTime 0.050 (Avg-Time 0.096)\t Loss 0.9203 (Avg-Loss 0.8635)\tAcc 66.0156 (Avg-Acc 70.1372)\n",
            "Epoch: [177][57/78]\tTime 0.047 (Avg-Time 0.081)\t Loss 0.8932 (Avg-Loss 0.8762)\tAcc 68.7500 (Avg-Acc 69.4807)\n",
            "Epoch: [177][76/78]\tTime 0.047 (Avg-Time 0.073)\t Loss 0.8524 (Avg-Loss 0.8852)\tAcc 70.1172 (Avg-Acc 69.0671)\n",
            "Epoch: [177][78/78]\tTime 0.009 (Avg-Time 0.072)\t Loss 0.8865 (Avg-Loss 0.8867)\tAcc 62.5000 (Avg-Acc 69.0100)\n",
            "EPOCH: 177 train Results: Acc 69.010 Loss: 0.8867\n",
            "Epoch: [177][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1709 (Avg-Loss 1.1709)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [177][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3146 (Avg-Loss 1.2095)\tAcc 52.9412 (Avg-Acc 58.0800)\n",
            "EPOCH: 177 Validation Results: Acc 58.080 Loss: 1.2095\n",
            "Epoch: [178][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.8574 (Avg-Loss 0.8574)\tAcc 72.8516 (Avg-Acc 72.8516)\n",
            "Epoch: [178][19/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.9075 (Avg-Loss 0.8626)\tAcc 67.7734 (Avg-Acc 70.2832)\n",
            "Epoch: [178][38/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8569 (Avg-Loss 0.8733)\tAcc 71.0938 (Avg-Acc 69.8067)\n",
            "Epoch: [178][57/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9378 (Avg-Loss 0.8762)\tAcc 66.7969 (Avg-Acc 69.5110)\n",
            "Epoch: [178][76/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0602 (Avg-Loss 0.8860)\tAcc 65.2344 (Avg-Acc 69.2877)\n",
            "Epoch: [178][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.8611 (Avg-Loss 0.8858)\tAcc 65.6250 (Avg-Acc 69.2650)\n",
            "EPOCH: 178 train Results: Acc 69.265 Loss: 0.8858\n",
            "Epoch: [178][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1811 (Avg-Loss 1.1811)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [178][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3075 (Avg-Loss 1.2144)\tAcc 54.0441 (Avg-Acc 57.5500)\n",
            "EPOCH: 178 Validation Results: Acc 57.550 Loss: 1.2144\n",
            "Epoch: [179][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8424 (Avg-Loss 0.8424)\tAcc 69.3359 (Avg-Acc 69.3359)\n",
            "Epoch: [179][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8501 (Avg-Loss 0.8571)\tAcc 69.7266 (Avg-Acc 69.8926)\n",
            "Epoch: [179][38/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.9414 (Avg-Loss 0.8679)\tAcc 67.7734 (Avg-Acc 69.6014)\n",
            "Epoch: [179][57/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.8149 (Avg-Loss 0.8777)\tAcc 71.6797 (Avg-Acc 69.3124)\n",
            "Epoch: [179][76/78]\tTime 0.086 (Avg-Time 0.072)\t Loss 0.8817 (Avg-Loss 0.8877)\tAcc 68.5547 (Avg-Acc 68.9758)\n",
            "Epoch: [179][78/78]\tTime 0.021 (Avg-Time 0.072)\t Loss 1.0659 (Avg-Loss 0.8883)\tAcc 65.6250 (Avg-Acc 68.9600)\n",
            "EPOCH: 179 train Results: Acc 68.960 Loss: 0.8883\n",
            "Epoch: [179][0/19]\tTime 0.035 (Avg-Time 0.035)\t Loss 1.1745 (Avg-Loss 1.1745)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [179][19/19]\tTime 0.007 (Avg-Time 0.025)\t Loss 1.2754 (Avg-Loss 1.2033)\tAcc 53.6765 (Avg-Acc 58.3200)\n",
            "EPOCH: 179 Validation Results: Acc 58.320 Loss: 1.2033\n",
            "Epoch: [180][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8180 (Avg-Loss 0.8180)\tAcc 72.2656 (Avg-Acc 72.2656)\n",
            "Epoch: [180][19/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.9473 (Avg-Loss 0.8603)\tAcc 66.4062 (Avg-Acc 70.6348)\n",
            "Epoch: [180][38/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.9341 (Avg-Loss 0.8662)\tAcc 66.6016 (Avg-Acc 70.0321)\n",
            "Epoch: [180][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9319 (Avg-Loss 0.8782)\tAcc 66.7969 (Avg-Acc 69.4774)\n",
            "Epoch: [180][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9023 (Avg-Loss 0.8900)\tAcc 66.9922 (Avg-Acc 68.9884)\n",
            "Epoch: [180][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1797 (Avg-Loss 0.8914)\tAcc 62.5000 (Avg-Acc 68.9175)\n",
            "EPOCH: 180 train Results: Acc 68.918 Loss: 0.8914\n",
            "Epoch: [180][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1477 (Avg-Loss 1.1477)\tAcc 62.5000 (Avg-Acc 62.5000)\n",
            "Epoch: [180][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2922 (Avg-Loss 1.2059)\tAcc 54.4118 (Avg-Acc 58.3000)\n",
            "EPOCH: 180 Validation Results: Acc 58.300 Loss: 1.2059\n",
            "Epoch: [181][0/78]\tTime 0.053 (Avg-Time 0.053)\t Loss 0.8054 (Avg-Loss 0.8054)\tAcc 71.4844 (Avg-Acc 71.4844)\n",
            "Epoch: [181][19/78]\tTime 0.047 (Avg-Time 0.052)\t Loss 0.9652 (Avg-Loss 0.8587)\tAcc 64.8438 (Avg-Acc 70.4492)\n",
            "Epoch: [181][38/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 0.9554 (Avg-Loss 0.8692)\tAcc 69.1406 (Avg-Acc 69.8067)\n",
            "Epoch: [181][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8911 (Avg-Loss 0.8810)\tAcc 64.6484 (Avg-Acc 69.4336)\n",
            "Epoch: [181][76/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.9304 (Avg-Loss 0.8891)\tAcc 65.2344 (Avg-Acc 68.9453)\n",
            "Epoch: [181][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 0.8459 (Avg-Loss 0.8889)\tAcc 67.1875 (Avg-Acc 68.9600)\n",
            "EPOCH: 181 train Results: Acc 68.960 Loss: 0.8889\n",
            "Epoch: [181][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1714 (Avg-Loss 1.1714)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [181][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2616 (Avg-Loss 1.2081)\tAcc 55.8824 (Avg-Acc 58.0100)\n",
            "EPOCH: 181 Validation Results: Acc 58.010 Loss: 1.2081\n",
            "Epoch: [182][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8649 (Avg-Loss 0.8649)\tAcc 68.9453 (Avg-Acc 68.9453)\n",
            "Epoch: [182][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8501 (Avg-Loss 0.8451)\tAcc 70.5078 (Avg-Acc 70.7227)\n",
            "Epoch: [182][38/78]\tTime 0.100 (Avg-Time 0.056)\t Loss 0.8309 (Avg-Loss 0.8552)\tAcc 73.8281 (Avg-Acc 70.3776)\n",
            "Epoch: [182][57/78]\tTime 0.063 (Avg-Time 0.084)\t Loss 0.8981 (Avg-Loss 0.8708)\tAcc 68.7500 (Avg-Acc 69.8242)\n",
            "Epoch: [182][76/78]\tTime 0.048 (Avg-Time 0.075)\t Loss 0.8898 (Avg-Loss 0.8794)\tAcc 68.1641 (Avg-Acc 69.3613)\n",
            "Epoch: [182][78/78]\tTime 0.024 (Avg-Time 0.074)\t Loss 1.1089 (Avg-Loss 0.8803)\tAcc 57.8125 (Avg-Acc 69.3200)\n",
            "EPOCH: 182 train Results: Acc 69.320 Loss: 0.8803\n",
            "Epoch: [182][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1859 (Avg-Loss 1.1859)\tAcc 60.1562 (Avg-Acc 60.1562)\n",
            "Epoch: [182][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2659 (Avg-Loss 1.2181)\tAcc 54.0441 (Avg-Acc 57.6800)\n",
            "EPOCH: 182 Validation Results: Acc 57.680 Loss: 1.2181\n",
            "Epoch: [183][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.7957 (Avg-Loss 0.7957)\tAcc 71.2891 (Avg-Acc 71.2891)\n",
            "Epoch: [183][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9235 (Avg-Loss 0.8535)\tAcc 67.3828 (Avg-Acc 70.0684)\n",
            "Epoch: [183][38/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.8053 (Avg-Loss 0.8592)\tAcc 74.2188 (Avg-Acc 69.9669)\n",
            "Epoch: [183][57/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.8558 (Avg-Loss 0.8764)\tAcc 71.8750 (Avg-Acc 69.2214)\n",
            "Epoch: [183][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9422 (Avg-Loss 0.8859)\tAcc 68.1641 (Avg-Acc 68.9631)\n",
            "Epoch: [183][78/78]\tTime 0.017 (Avg-Time 0.049)\t Loss 1.0604 (Avg-Loss 0.8874)\tAcc 65.6250 (Avg-Acc 68.9075)\n",
            "EPOCH: 183 train Results: Acc 68.907 Loss: 0.8874\n",
            "Epoch: [183][0/19]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.1803 (Avg-Loss 1.1803)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [183][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2638 (Avg-Loss 1.2079)\tAcc 54.4118 (Avg-Acc 58.2800)\n",
            "EPOCH: 183 Validation Results: Acc 58.280 Loss: 1.2079\n",
            "Epoch: [184][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8377 (Avg-Loss 0.8377)\tAcc 70.1172 (Avg-Acc 70.1172)\n",
            "Epoch: [184][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.8499 (Avg-Loss 0.8268)\tAcc 71.6797 (Avg-Acc 70.8984)\n",
            "Epoch: [184][38/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8645 (Avg-Loss 0.8499)\tAcc 69.7266 (Avg-Acc 69.9920)\n",
            "Epoch: [184][57/78]\tTime 0.060 (Avg-Time 0.049)\t Loss 0.9596 (Avg-Loss 0.8678)\tAcc 66.4062 (Avg-Acc 69.4336)\n",
            "Epoch: [184][76/78]\tTime 0.053 (Avg-Time 0.049)\t Loss 0.8937 (Avg-Loss 0.8744)\tAcc 67.3828 (Avg-Acc 69.1964)\n",
            "Epoch: [184][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.7332 (Avg-Loss 0.8743)\tAcc 71.8750 (Avg-Acc 69.2075)\n",
            "EPOCH: 184 train Results: Acc 69.207 Loss: 0.8743\n",
            "Epoch: [184][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1800 (Avg-Loss 1.1800)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [184][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2399 (Avg-Loss 1.2120)\tAcc 55.1471 (Avg-Acc 58.1000)\n",
            "EPOCH: 184 Validation Results: Acc 58.100 Loss: 1.2120\n",
            "Epoch: [185][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8878 (Avg-Loss 0.8878)\tAcc 68.9453 (Avg-Acc 68.9453)\n",
            "Epoch: [185][19/78]\tTime 0.088 (Avg-Time 0.119)\t Loss 0.8713 (Avg-Loss 0.8601)\tAcc 67.5781 (Avg-Acc 69.9902)\n",
            "Epoch: [185][38/78]\tTime 0.048 (Avg-Time 0.100)\t Loss 0.8423 (Avg-Loss 0.8604)\tAcc 70.8984 (Avg-Acc 70.0671)\n",
            "Epoch: [185][57/78]\tTime 0.046 (Avg-Time 0.084)\t Loss 0.8319 (Avg-Loss 0.8728)\tAcc 71.4844 (Avg-Acc 69.5818)\n",
            "Epoch: [185][76/78]\tTime 0.046 (Avg-Time 0.075)\t Loss 0.9386 (Avg-Loss 0.8840)\tAcc 67.9688 (Avg-Acc 69.2395)\n",
            "Epoch: [185][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 0.8504 (Avg-Loss 0.8849)\tAcc 67.1875 (Avg-Acc 69.1950)\n",
            "EPOCH: 185 train Results: Acc 69.195 Loss: 0.8849\n",
            "Epoch: [185][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1911 (Avg-Loss 1.1911)\tAcc 61.7188 (Avg-Acc 61.7188)\n",
            "Epoch: [185][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2688 (Avg-Loss 1.2213)\tAcc 54.4118 (Avg-Acc 57.7400)\n",
            "EPOCH: 185 Validation Results: Acc 57.740 Loss: 1.2213\n",
            "Epoch: [186][0/78]\tTime 0.067 (Avg-Time 0.067)\t Loss 0.7614 (Avg-Loss 0.7614)\tAcc 76.1719 (Avg-Acc 76.1719)\n",
            "Epoch: [186][19/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8123 (Avg-Loss 0.8395)\tAcc 71.8750 (Avg-Acc 71.0938)\n",
            "Epoch: [186][38/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8731 (Avg-Loss 0.8609)\tAcc 69.9219 (Avg-Acc 70.1222)\n",
            "Epoch: [186][57/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9347 (Avg-Loss 0.8704)\tAcc 67.3828 (Avg-Acc 69.7636)\n",
            "Epoch: [186][76/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.8013 (Avg-Loss 0.8753)\tAcc 74.4141 (Avg-Acc 69.5439)\n",
            "Epoch: [186][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0548 (Avg-Loss 0.8772)\tAcc 57.8125 (Avg-Acc 69.4325)\n",
            "EPOCH: 186 train Results: Acc 69.433 Loss: 0.8772\n",
            "Epoch: [186][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2119 (Avg-Loss 1.2119)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [186][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2799 (Avg-Loss 1.2176)\tAcc 55.5147 (Avg-Acc 57.4300)\n",
            "EPOCH: 186 Validation Results: Acc 57.430 Loss: 1.2176\n",
            "Epoch: [187][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8927 (Avg-Loss 0.8927)\tAcc 66.7969 (Avg-Acc 66.7969)\n",
            "Epoch: [187][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8348 (Avg-Loss 0.8542)\tAcc 71.2891 (Avg-Acc 70.7520)\n",
            "Epoch: [187][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8411 (Avg-Loss 0.8630)\tAcc 69.7266 (Avg-Acc 69.9519)\n",
            "Epoch: [187][57/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.8812 (Avg-Loss 0.8752)\tAcc 71.0938 (Avg-Acc 69.3427)\n",
            "Epoch: [187][76/78]\tTime 0.270 (Avg-Time 0.065)\t Loss 0.9276 (Avg-Loss 0.8809)\tAcc 69.9219 (Avg-Acc 69.2269)\n",
            "Epoch: [187][78/78]\tTime 0.018 (Avg-Time 0.067)\t Loss 0.8196 (Avg-Loss 0.8811)\tAcc 71.8750 (Avg-Acc 69.2175)\n",
            "EPOCH: 187 train Results: Acc 69.218 Loss: 0.8811\n",
            "Epoch: [187][0/19]\tTime 0.033 (Avg-Time 0.033)\t Loss 1.1989 (Avg-Loss 1.1989)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [187][19/19]\tTime 0.011 (Avg-Time 0.029)\t Loss 1.2627 (Avg-Loss 1.2160)\tAcc 55.8824 (Avg-Acc 57.7300)\n",
            "EPOCH: 187 Validation Results: Acc 57.730 Loss: 1.2160\n",
            "Epoch: [188][0/78]\tTime 0.132 (Avg-Time 0.132)\t Loss 0.8060 (Avg-Loss 0.8060)\tAcc 73.0469 (Avg-Acc 73.0469)\n",
            "Epoch: [188][19/78]\tTime 0.047 (Avg-Time 0.064)\t Loss 0.8178 (Avg-Loss 0.8440)\tAcc 70.7031 (Avg-Acc 70.4980)\n",
            "Epoch: [188][38/78]\tTime 0.047 (Avg-Time 0.057)\t Loss 0.8685 (Avg-Loss 0.8582)\tAcc 69.5312 (Avg-Acc 70.1122)\n",
            "Epoch: [188][57/78]\tTime 0.046 (Avg-Time 0.054)\t Loss 0.8868 (Avg-Loss 0.8654)\tAcc 70.1172 (Avg-Acc 69.7670)\n",
            "Epoch: [188][76/78]\tTime 0.047 (Avg-Time 0.053)\t Loss 0.9737 (Avg-Loss 0.8763)\tAcc 66.2109 (Avg-Acc 69.2548)\n",
            "Epoch: [188][78/78]\tTime 0.009 (Avg-Time 0.052)\t Loss 1.0920 (Avg-Loss 0.8772)\tAcc 57.8125 (Avg-Acc 69.2400)\n",
            "EPOCH: 188 train Results: Acc 69.240 Loss: 0.8772\n",
            "Epoch: [188][0/19]\tTime 0.011 (Avg-Time 0.011)\t Loss 1.1870 (Avg-Loss 1.1870)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [188][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2436 (Avg-Loss 1.2194)\tAcc 56.2500 (Avg-Acc 57.7700)\n",
            "EPOCH: 188 Validation Results: Acc 57.770 Loss: 1.2194\n",
            "Epoch: [189][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8052 (Avg-Loss 0.8052)\tAcc 72.0703 (Avg-Acc 72.0703)\n",
            "Epoch: [189][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8297 (Avg-Loss 0.8405)\tAcc 70.5078 (Avg-Acc 70.5176)\n",
            "Epoch: [189][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9098 (Avg-Loss 0.8574)\tAcc 68.5547 (Avg-Acc 70.0321)\n",
            "Epoch: [189][57/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8775 (Avg-Loss 0.8649)\tAcc 70.7031 (Avg-Acc 69.9252)\n",
            "Epoch: [189][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9152 (Avg-Loss 0.8779)\tAcc 66.7969 (Avg-Acc 69.4323)\n",
            "Epoch: [189][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0230 (Avg-Loss 0.8790)\tAcc 60.9375 (Avg-Acc 69.3775)\n",
            "EPOCH: 189 train Results: Acc 69.377 Loss: 0.8790\n",
            "Epoch: [189][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1759 (Avg-Loss 1.1759)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [189][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2514 (Avg-Loss 1.2069)\tAcc 58.8235 (Avg-Acc 57.7200)\n",
            "EPOCH: 189 Validation Results: Acc 57.720 Loss: 1.2069\n",
            "Epoch: [190][0/78]\tTime 0.053 (Avg-Time 0.053)\t Loss 0.8378 (Avg-Loss 0.8378)\tAcc 71.0938 (Avg-Acc 71.0938)\n",
            "Epoch: [190][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8188 (Avg-Loss 0.8729)\tAcc 73.2422 (Avg-Acc 69.3555)\n",
            "Epoch: [190][38/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8480 (Avg-Loss 0.8680)\tAcc 72.2656 (Avg-Acc 69.4962)\n",
            "Epoch: [190][57/78]\tTime 0.098 (Avg-Time 0.075)\t Loss 0.8549 (Avg-Loss 0.8757)\tAcc 70.5078 (Avg-Acc 69.2854)\n",
            "Epoch: [190][76/78]\tTime 0.046 (Avg-Time 0.075)\t Loss 0.8205 (Avg-Loss 0.8814)\tAcc 71.4844 (Avg-Acc 68.9326)\n",
            "Epoch: [190][78/78]\tTime 0.009 (Avg-Time 0.073)\t Loss 1.2667 (Avg-Loss 0.8813)\tAcc 54.6875 (Avg-Acc 68.9500)\n",
            "EPOCH: 190 train Results: Acc 68.950 Loss: 0.8813\n",
            "Epoch: [190][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1945 (Avg-Loss 1.1945)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [190][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2534 (Avg-Loss 1.2091)\tAcc 58.0882 (Avg-Acc 57.8000)\n",
            "EPOCH: 190 Validation Results: Acc 57.800 Loss: 1.2091\n",
            "Epoch: [191][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.8506 (Avg-Loss 0.8506)\tAcc 71.2891 (Avg-Acc 71.2891)\n",
            "Epoch: [191][19/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.9321 (Avg-Loss 0.8635)\tAcc 66.9922 (Avg-Acc 70.1855)\n",
            "Epoch: [191][38/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8790 (Avg-Loss 0.8647)\tAcc 70.7031 (Avg-Acc 70.0020)\n",
            "Epoch: [191][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8899 (Avg-Loss 0.8766)\tAcc 67.3828 (Avg-Acc 69.4841)\n",
            "Epoch: [191][76/78]\tTime 0.051 (Avg-Time 0.049)\t Loss 1.0020 (Avg-Loss 0.8871)\tAcc 64.2578 (Avg-Acc 69.0214)\n",
            "Epoch: [191][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.8919 (Avg-Loss 0.8875)\tAcc 68.7500 (Avg-Acc 69.0050)\n",
            "EPOCH: 191 train Results: Acc 69.005 Loss: 0.8875\n",
            "Epoch: [191][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2123 (Avg-Loss 1.2123)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [191][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2982 (Avg-Loss 1.2113)\tAcc 55.1471 (Avg-Acc 57.7100)\n",
            "EPOCH: 191 Validation Results: Acc 57.710 Loss: 1.2113\n",
            "Epoch: [192][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8540 (Avg-Loss 0.8540)\tAcc 72.0703 (Avg-Acc 72.0703)\n",
            "Epoch: [192][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9002 (Avg-Loss 0.8452)\tAcc 68.9453 (Avg-Acc 70.1953)\n",
            "Epoch: [192][38/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.8907 (Avg-Loss 0.8500)\tAcc 70.3125 (Avg-Acc 70.3826)\n",
            "Epoch: [192][57/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.8522 (Avg-Loss 0.8645)\tAcc 72.0703 (Avg-Acc 69.8242)\n",
            "Epoch: [192][76/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9135 (Avg-Loss 0.8763)\tAcc 67.1875 (Avg-Acc 69.4070)\n",
            "Epoch: [192][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0406 (Avg-Loss 0.8769)\tAcc 68.7500 (Avg-Acc 69.3600)\n",
            "EPOCH: 192 train Results: Acc 69.360 Loss: 0.8769\n",
            "Epoch: [192][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2057 (Avg-Loss 1.2057)\tAcc 61.3281 (Avg-Acc 61.3281)\n",
            "Epoch: [192][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2623 (Avg-Loss 1.2069)\tAcc 51.4706 (Avg-Acc 57.7600)\n",
            "EPOCH: 192 Validation Results: Acc 57.760 Loss: 1.2069\n",
            "Epoch: [193][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8135 (Avg-Loss 0.8135)\tAcc 72.4609 (Avg-Acc 72.4609)\n",
            "Epoch: [193][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.8920 (Avg-Loss 0.8353)\tAcc 69.7266 (Avg-Acc 71.0254)\n",
            "Epoch: [193][38/78]\tTime 0.171 (Avg-Time 0.088)\t Loss 0.9042 (Avg-Loss 0.8583)\tAcc 68.3594 (Avg-Acc 70.2023)\n",
            "Epoch: [193][57/78]\tTime 0.046 (Avg-Time 0.085)\t Loss 0.9193 (Avg-Loss 0.8650)\tAcc 67.1875 (Avg-Acc 69.8242)\n",
            "Epoch: [193][76/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 0.9620 (Avg-Loss 0.8785)\tAcc 66.0156 (Avg-Acc 69.3689)\n",
            "Epoch: [193][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.0574 (Avg-Loss 0.8789)\tAcc 60.9375 (Avg-Acc 69.3575)\n",
            "EPOCH: 193 train Results: Acc 69.358 Loss: 0.8789\n",
            "Epoch: [193][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1774 (Avg-Loss 1.1774)\tAcc 60.9375 (Avg-Acc 60.9375)\n",
            "Epoch: [193][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2732 (Avg-Loss 1.2133)\tAcc 57.7206 (Avg-Acc 57.6400)\n",
            "EPOCH: 193 Validation Results: Acc 57.640 Loss: 1.2133\n",
            "Epoch: [194][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.7977 (Avg-Loss 0.7977)\tAcc 71.6797 (Avg-Acc 71.6797)\n",
            "Epoch: [194][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8669 (Avg-Loss 0.8397)\tAcc 67.5781 (Avg-Acc 70.9668)\n",
            "Epoch: [194][38/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9507 (Avg-Loss 0.8551)\tAcc 66.6016 (Avg-Acc 70.4177)\n",
            "Epoch: [194][57/78]\tTime 0.061 (Avg-Time 0.049)\t Loss 0.8445 (Avg-Loss 0.8621)\tAcc 72.4609 (Avg-Acc 70.1071)\n",
            "Epoch: [194][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9276 (Avg-Loss 0.8721)\tAcc 68.3594 (Avg-Acc 69.6479)\n",
            "Epoch: [194][78/78]\tTime 0.025 (Avg-Time 0.049)\t Loss 0.9405 (Avg-Loss 0.8731)\tAcc 62.5000 (Avg-Acc 69.5925)\n",
            "EPOCH: 194 train Results: Acc 69.593 Loss: 0.8731\n",
            "Epoch: [194][0/19]\tTime 0.022 (Avg-Time 0.022)\t Loss 1.2032 (Avg-Loss 1.2032)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [194][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2403 (Avg-Loss 1.2171)\tAcc 56.2500 (Avg-Acc 57.6400)\n",
            "EPOCH: 194 Validation Results: Acc 57.640 Loss: 1.2171\n",
            "Epoch: [195][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8157 (Avg-Loss 0.8157)\tAcc 73.8281 (Avg-Acc 73.8281)\n",
            "Epoch: [195][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9075 (Avg-Loss 0.8504)\tAcc 66.2109 (Avg-Acc 70.5469)\n",
            "Epoch: [195][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9074 (Avg-Loss 0.8663)\tAcc 66.6016 (Avg-Acc 69.9920)\n",
            "Epoch: [195][57/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9120 (Avg-Loss 0.8700)\tAcc 67.9688 (Avg-Acc 69.6761)\n",
            "Epoch: [195][76/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9261 (Avg-Loss 0.8782)\tAcc 67.9688 (Avg-Acc 69.3613)\n",
            "Epoch: [195][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.8288 (Avg-Loss 0.8792)\tAcc 75.0000 (Avg-Acc 69.3550)\n",
            "EPOCH: 195 train Results: Acc 69.355 Loss: 0.8792\n",
            "Epoch: [195][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1561 (Avg-Loss 1.1561)\tAcc 63.4766 (Avg-Acc 63.4766)\n",
            "Epoch: [195][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3005 (Avg-Loss 1.2139)\tAcc 55.8824 (Avg-Acc 57.5500)\n",
            "EPOCH: 195 Validation Results: Acc 57.550 Loss: 1.2139\n",
            "Epoch: [196][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8694 (Avg-Loss 0.8694)\tAcc 66.7969 (Avg-Acc 66.7969)\n",
            "Epoch: [196][19/78]\tTime 0.106 (Avg-Time 0.138)\t Loss 0.9201 (Avg-Loss 0.8483)\tAcc 66.4062 (Avg-Acc 70.2734)\n",
            "Epoch: [196][38/78]\tTime 0.046 (Avg-Time 0.100)\t Loss 0.9108 (Avg-Loss 0.8570)\tAcc 66.4062 (Avg-Acc 70.1422)\n",
            "Epoch: [196][57/78]\tTime 0.047 (Avg-Time 0.083)\t Loss 0.9140 (Avg-Loss 0.8713)\tAcc 67.7734 (Avg-Acc 69.6289)\n",
            "Epoch: [196][76/78]\tTime 0.049 (Avg-Time 0.075)\t Loss 0.8671 (Avg-Loss 0.8793)\tAcc 70.3125 (Avg-Acc 69.3004)\n",
            "Epoch: [196][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 1.0469 (Avg-Loss 0.8798)\tAcc 64.0625 (Avg-Acc 69.2700)\n",
            "EPOCH: 196 train Results: Acc 69.270 Loss: 0.8798\n",
            "Epoch: [196][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1959 (Avg-Loss 1.1959)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [196][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2607 (Avg-Loss 1.2172)\tAcc 58.8235 (Avg-Acc 58.0500)\n",
            "EPOCH: 196 Validation Results: Acc 58.050 Loss: 1.2172\n",
            "Epoch: [197][0/78]\tTime 0.072 (Avg-Time 0.072)\t Loss 0.8168 (Avg-Loss 0.8168)\tAcc 72.6562 (Avg-Acc 72.6562)\n",
            "Epoch: [197][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8320 (Avg-Loss 0.8444)\tAcc 72.0703 (Avg-Acc 70.3418)\n",
            "Epoch: [197][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9112 (Avg-Loss 0.8540)\tAcc 67.9688 (Avg-Acc 69.9419)\n",
            "Epoch: [197][57/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8667 (Avg-Loss 0.8597)\tAcc 70.7031 (Avg-Acc 69.8613)\n",
            "Epoch: [197][76/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8689 (Avg-Loss 0.8703)\tAcc 68.5547 (Avg-Acc 69.5110)\n",
            "Epoch: [197][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1028 (Avg-Loss 0.8714)\tAcc 65.6250 (Avg-Acc 69.5000)\n",
            "EPOCH: 197 train Results: Acc 69.500 Loss: 0.8714\n",
            "Epoch: [197][0/19]\tTime 0.011 (Avg-Time 0.011)\t Loss 1.1798 (Avg-Loss 1.1798)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [197][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2671 (Avg-Loss 1.2119)\tAcc 54.4118 (Avg-Acc 57.7500)\n",
            "EPOCH: 197 Validation Results: Acc 57.750 Loss: 1.2119\n",
            "Epoch: [198][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8247 (Avg-Loss 0.8247)\tAcc 70.8984 (Avg-Acc 70.8984)\n",
            "Epoch: [198][19/78]\tTime 0.054 (Avg-Time 0.051)\t Loss 0.8786 (Avg-Loss 0.8492)\tAcc 69.1406 (Avg-Acc 70.3223)\n",
            "Epoch: [198][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8431 (Avg-Loss 0.8567)\tAcc 70.5078 (Avg-Acc 70.1172)\n",
            "Epoch: [198][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.8524 (Avg-Loss 0.8657)\tAcc 69.7266 (Avg-Acc 69.6693)\n",
            "Epoch: [198][76/78]\tTime 0.119 (Avg-Time 0.072)\t Loss 0.9151 (Avg-Loss 0.8721)\tAcc 66.6016 (Avg-Acc 69.3359)\n",
            "Epoch: [198][78/78]\tTime 0.024 (Avg-Time 0.071)\t Loss 1.0528 (Avg-Loss 0.8726)\tAcc 59.3750 (Avg-Acc 69.3275)\n",
            "EPOCH: 198 train Results: Acc 69.328 Loss: 0.8726\n",
            "Epoch: [198][0/19]\tTime 0.030 (Avg-Time 0.030)\t Loss 1.1676 (Avg-Loss 1.1676)\tAcc 59.9609 (Avg-Acc 59.9609)\n",
            "Epoch: [198][19/19]\tTime 0.007 (Avg-Time 0.026)\t Loss 1.2795 (Avg-Loss 1.2124)\tAcc 53.6765 (Avg-Acc 57.6600)\n",
            "EPOCH: 198 Validation Results: Acc 57.660 Loss: 1.2124\n",
            "Epoch: [199][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8396 (Avg-Loss 0.8396)\tAcc 71.2891 (Avg-Acc 71.2891)\n",
            "Epoch: [199][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.8625 (Avg-Loss 0.8344)\tAcc 70.3125 (Avg-Acc 71.0938)\n",
            "Epoch: [199][38/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.8784 (Avg-Loss 0.8476)\tAcc 69.5312 (Avg-Acc 70.2975)\n",
            "Epoch: [199][57/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.8910 (Avg-Loss 0.8615)\tAcc 70.3125 (Avg-Acc 69.8107)\n",
            "Epoch: [199][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9042 (Avg-Loss 0.8710)\tAcc 68.1641 (Avg-Acc 69.3613)\n",
            "Epoch: [199][78/78]\tTime 0.010 (Avg-Time 0.050)\t Loss 1.0983 (Avg-Loss 0.8727)\tAcc 70.3125 (Avg-Acc 69.3075)\n",
            "EPOCH: 199 train Results: Acc 69.308 Loss: 0.8727\n",
            "Epoch: [199][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1828 (Avg-Loss 1.1828)\tAcc 60.3516 (Avg-Acc 60.3516)\n",
            "Epoch: [199][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2735 (Avg-Loss 1.2090)\tAcc 55.1471 (Avg-Acc 58.1100)\n",
            "EPOCH: 199 Validation Results: Acc 58.110 Loss: 1.2090\n",
            "Epoch: [200][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8034 (Avg-Loss 0.8034)\tAcc 74.0234 (Avg-Acc 74.0234)\n",
            "Epoch: [200][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.8598 (Avg-Loss 0.8374)\tAcc 71.2891 (Avg-Acc 71.0254)\n",
            "Epoch: [200][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.8201 (Avg-Loss 0.8586)\tAcc 69.9219 (Avg-Acc 70.0321)\n",
            "Epoch: [200][57/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.8683 (Avg-Loss 0.8675)\tAcc 69.7266 (Avg-Acc 69.6020)\n",
            "Epoch: [200][76/78]\tTime 0.050 (Avg-Time 0.049)\t Loss 0.9335 (Avg-Loss 0.8753)\tAcc 68.5547 (Avg-Acc 69.2370)\n",
            "Epoch: [200][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1671 (Avg-Loss 0.8763)\tAcc 57.8125 (Avg-Acc 69.1850)\n",
            "EPOCH: 200 train Results: Acc 69.185 Loss: 0.8763\n",
            "Epoch: [200][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1985 (Avg-Loss 1.1985)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [200][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2624 (Avg-Loss 1.2159)\tAcc 56.9853 (Avg-Acc 58.0100)\n",
            "EPOCH: 200 Validation Results: Acc 58.010 Loss: 1.2159\n",
            "\n",
            "Test Accuracy: 58.25% (Best so far: 58.25%)\n",
            "\n",
            "\n",
            "=== Testing Configuration 30/864 ===\n",
            "Current Config: {'lr': 0.005, 'batch_size': 512, 'hidden_units': [512, 256], 'dropout_rates': [0.3, 0.3], 'pre-process': 'standardization', 'weight_decay': 0.0005, 'optimizer': 'adamw'}\n",
            "Pre-process: standardization\n",
            "Pre-process: standardization\n",
            "Pre-process: standardization\n",
            "Epoch: [1][0/78]\tTime 0.079 (Avg-Time 0.079)\t Loss 8.8888 (Avg-Loss 8.8888)\tAcc 9.1797 (Avg-Acc 9.1797)\n",
            "Epoch: [1][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 4.4016 (Avg-Loss 5.9292)\tAcc 29.2969 (Avg-Acc 22.4023)\n",
            "Epoch: [1][38/78]\tTime 0.099 (Avg-Time 0.098)\t Loss 2.4903 (Avg-Loss 4.7181)\tAcc 35.5469 (Avg-Acc 25.4908)\n",
            "Epoch: [1][57/78]\tTime 0.057 (Avg-Time 0.094)\t Loss 1.9572 (Avg-Loss 3.9263)\tAcc 35.9375 (Avg-Acc 28.0374)\n",
            "Epoch: [1][76/78]\tTime 0.057 (Avg-Time 0.086)\t Loss 1.7504 (Avg-Loss 3.4136)\tAcc 37.3047 (Avg-Acc 30.0249)\n",
            "Epoch: [1][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 1.8351 (Avg-Loss 3.3896)\tAcc 23.4375 (Avg-Acc 30.1525)\n",
            "EPOCH: 1 train Results: Acc 30.152 Loss: 3.3896\n",
            "Epoch: [1][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5563 (Avg-Loss 1.5563)\tAcc 47.4609 (Avg-Acc 47.4609)\n",
            "Epoch: [1][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5837 (Avg-Loss 1.5805)\tAcc 43.7500 (Avg-Acc 43.7800)\n",
            "EPOCH: 1 Validation Results: Acc 43.780 Loss: 1.5805\n",
            "Epoch: [2][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.5996 (Avg-Loss 1.5996)\tAcc 44.7266 (Avg-Acc 44.7266)\n",
            "Epoch: [2][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.5436 (Avg-Loss 1.5753)\tAcc 44.1406 (Avg-Acc 43.4961)\n",
            "Epoch: [2][38/78]\tTime 0.084 (Avg-Time 0.061)\t Loss 1.5302 (Avg-Loss 1.5680)\tAcc 44.7266 (Avg-Acc 43.9002)\n",
            "Epoch: [2][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.4524 (Avg-Loss 1.5501)\tAcc 46.2891 (Avg-Acc 44.6323)\n",
            "Epoch: [2][76/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.5072 (Avg-Loss 1.5418)\tAcc 45.7031 (Avg-Acc 44.7697)\n",
            "Epoch: [2][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 1.6273 (Avg-Loss 1.5407)\tAcc 42.1875 (Avg-Acc 44.7850)\n",
            "EPOCH: 2 train Results: Acc 44.785 Loss: 1.5407\n",
            "Epoch: [2][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4508 (Avg-Loss 1.4508)\tAcc 49.4141 (Avg-Acc 49.4141)\n",
            "Epoch: [2][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4577 (Avg-Loss 1.4628)\tAcc 48.8971 (Avg-Acc 48.0700)\n",
            "EPOCH: 2 Validation Results: Acc 48.070 Loss: 1.4628\n",
            "Epoch: [3][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.4112 (Avg-Loss 1.4112)\tAcc 48.0469 (Avg-Acc 48.0469)\n",
            "Epoch: [3][19/78]\tTime 0.057 (Avg-Time 0.062)\t Loss 1.4812 (Avg-Loss 1.4070)\tAcc 50.0000 (Avg-Acc 49.5801)\n",
            "Epoch: [3][38/78]\tTime 0.064 (Avg-Time 0.061)\t Loss 1.4079 (Avg-Loss 1.4073)\tAcc 47.6562 (Avg-Acc 49.5643)\n",
            "Epoch: [3][57/78]\tTime 0.126 (Avg-Time 0.088)\t Loss 1.3716 (Avg-Loss 1.4039)\tAcc 50.0000 (Avg-Acc 49.5690)\n",
            "Epoch: [3][76/78]\tTime 0.058 (Avg-Time 0.086)\t Loss 1.4494 (Avg-Loss 1.4025)\tAcc 49.0234 (Avg-Acc 49.7159)\n",
            "Epoch: [3][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 1.3891 (Avg-Loss 1.4034)\tAcc 53.1250 (Avg-Acc 49.6950)\n",
            "EPOCH: 3 train Results: Acc 49.695 Loss: 1.4034\n",
            "Epoch: [3][0/19]\tTime 0.011 (Avg-Time 0.011)\t Loss 1.3968 (Avg-Loss 1.3968)\tAcc 52.5391 (Avg-Acc 52.5391)\n",
            "Epoch: [3][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3965 (Avg-Loss 1.3994)\tAcc 50.3676 (Avg-Acc 50.4300)\n",
            "EPOCH: 3 Validation Results: Acc 50.430 Loss: 1.3994\n",
            "Epoch: [4][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.3344 (Avg-Loss 1.3344)\tAcc 52.5391 (Avg-Acc 52.5391)\n",
            "Epoch: [4][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.3459 (Avg-Loss 1.3271)\tAcc 50.9766 (Avg-Acc 52.8418)\n",
            "Epoch: [4][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2905 (Avg-Loss 1.3132)\tAcc 53.5156 (Avg-Acc 53.3954)\n",
            "Epoch: [4][57/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 1.2609 (Avg-Loss 1.3165)\tAcc 54.1016 (Avg-Acc 53.1216)\n",
            "Epoch: [4][76/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 1.3620 (Avg-Loss 1.3220)\tAcc 49.4141 (Avg-Acc 52.9601)\n",
            "Epoch: [4][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 1.4618 (Avg-Loss 1.3227)\tAcc 50.0000 (Avg-Acc 52.9100)\n",
            "EPOCH: 4 train Results: Acc 52.910 Loss: 1.3227\n",
            "Epoch: [4][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3447 (Avg-Loss 1.3447)\tAcc 51.7578 (Avg-Acc 51.7578)\n",
            "Epoch: [4][19/19]\tTime 0.018 (Avg-Time 0.014)\t Loss 1.3341 (Avg-Loss 1.3582)\tAcc 51.1029 (Avg-Acc 51.4000)\n",
            "EPOCH: 4 Validation Results: Acc 51.400 Loss: 1.3582\n",
            "Epoch: [5][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.3046 (Avg-Loss 1.3046)\tAcc 54.1016 (Avg-Acc 54.1016)\n",
            "Epoch: [5][19/78]\tTime 0.057 (Avg-Time 0.062)\t Loss 1.2993 (Avg-Loss 1.2450)\tAcc 51.3672 (Avg-Acc 54.5898)\n",
            "Epoch: [5][38/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.2166 (Avg-Loss 1.2489)\tAcc 55.4688 (Avg-Acc 54.7376)\n",
            "Epoch: [5][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2356 (Avg-Loss 1.2550)\tAcc 55.0781 (Avg-Acc 54.6841)\n",
            "Epoch: [5][76/78]\tTime 0.276 (Avg-Time 0.078)\t Loss 1.3367 (Avg-Loss 1.2624)\tAcc 52.9297 (Avg-Acc 54.6520)\n",
            "Epoch: [5][78/78]\tTime 0.027 (Avg-Time 0.078)\t Loss 1.6286 (Avg-Loss 1.2628)\tAcc 42.1875 (Avg-Acc 54.6625)\n",
            "EPOCH: 5 train Results: Acc 54.663 Loss: 1.2628\n",
            "Epoch: [5][0/19]\tTime 0.028 (Avg-Time 0.028)\t Loss 1.3154 (Avg-Loss 1.3154)\tAcc 52.5391 (Avg-Acc 52.5391)\n",
            "Epoch: [5][19/19]\tTime 0.026 (Avg-Time 0.030)\t Loss 1.3237 (Avg-Loss 1.3341)\tAcc 53.3088 (Avg-Acc 52.1600)\n",
            "EPOCH: 5 Validation Results: Acc 52.160 Loss: 1.3341\n",
            "Epoch: [6][0/78]\tTime 0.115 (Avg-Time 0.115)\t Loss 1.1110 (Avg-Loss 1.1110)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [6][19/78]\tTime 0.058 (Avg-Time 0.069)\t Loss 1.1293 (Avg-Loss 1.1796)\tAcc 60.3516 (Avg-Acc 57.2070)\n",
            "Epoch: [6][38/78]\tTime 0.057 (Avg-Time 0.065)\t Loss 1.1920 (Avg-Loss 1.1951)\tAcc 55.6641 (Avg-Acc 56.9010)\n",
            "Epoch: [6][57/78]\tTime 0.057 (Avg-Time 0.063)\t Loss 1.2575 (Avg-Loss 1.2032)\tAcc 55.4688 (Avg-Acc 56.7787)\n",
            "Epoch: [6][76/78]\tTime 0.076 (Avg-Time 0.063)\t Loss 1.2916 (Avg-Loss 1.2074)\tAcc 54.1016 (Avg-Acc 56.6914)\n",
            "Epoch: [6][78/78]\tTime 0.020 (Avg-Time 0.062)\t Loss 1.4193 (Avg-Loss 1.2099)\tAcc 51.5625 (Avg-Acc 56.5775)\n",
            "EPOCH: 6 train Results: Acc 56.578 Loss: 1.2099\n",
            "Epoch: [6][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.2942 (Avg-Loss 1.2942)\tAcc 53.1250 (Avg-Acc 53.1250)\n",
            "Epoch: [6][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2880 (Avg-Loss 1.3094)\tAcc 53.3088 (Avg-Acc 53.0700)\n",
            "EPOCH: 6 Validation Results: Acc 53.070 Loss: 1.3094\n",
            "Epoch: [7][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.0656 (Avg-Loss 1.0656)\tAcc 60.5469 (Avg-Acc 60.5469)\n",
            "Epoch: [7][19/78]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.1282 (Avg-Loss 1.1500)\tAcc 59.9609 (Avg-Acc 58.9551)\n",
            "Epoch: [7][38/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.2038 (Avg-Loss 1.1542)\tAcc 57.2266 (Avg-Acc 58.3333)\n",
            "Epoch: [7][57/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.2592 (Avg-Loss 1.1629)\tAcc 54.8828 (Avg-Acc 58.2940)\n",
            "Epoch: [7][76/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1348 (Avg-Loss 1.1681)\tAcc 58.2031 (Avg-Acc 58.2361)\n",
            "Epoch: [7][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 1.2394 (Avg-Loss 1.1684)\tAcc 60.9375 (Avg-Acc 58.2375)\n",
            "EPOCH: 7 train Results: Acc 58.237 Loss: 1.1684\n",
            "Epoch: [7][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2772 (Avg-Loss 1.2772)\tAcc 54.1016 (Avg-Acc 54.1016)\n",
            "Epoch: [7][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2668 (Avg-Loss 1.2904)\tAcc 57.7206 (Avg-Acc 53.9800)\n",
            "EPOCH: 7 Validation Results: Acc 53.980 Loss: 1.2904\n",
            "Epoch: [8][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.0776 (Avg-Loss 1.0776)\tAcc 61.7188 (Avg-Acc 61.7188)\n",
            "Epoch: [8][19/78]\tTime 0.097 (Avg-Time 0.128)\t Loss 1.1236 (Avg-Loss 1.1154)\tAcc 62.1094 (Avg-Acc 60.4297)\n",
            "Epoch: [8][38/78]\tTime 0.056 (Avg-Time 0.108)\t Loss 1.1388 (Avg-Loss 1.1186)\tAcc 59.9609 (Avg-Acc 60.1062)\n",
            "Epoch: [8][57/78]\tTime 0.057 (Avg-Time 0.092)\t Loss 1.1438 (Avg-Loss 1.1223)\tAcc 57.4219 (Avg-Acc 60.0653)\n",
            "Epoch: [8][76/78]\tTime 0.058 (Avg-Time 0.084)\t Loss 1.0868 (Avg-Loss 1.1292)\tAcc 62.5000 (Avg-Acc 59.8950)\n",
            "Epoch: [8][78/78]\tTime 0.019 (Avg-Time 0.083)\t Loss 1.2698 (Avg-Loss 1.1295)\tAcc 48.4375 (Avg-Acc 59.8525)\n",
            "EPOCH: 8 train Results: Acc 59.852 Loss: 1.1295\n",
            "Epoch: [8][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2679 (Avg-Loss 1.2679)\tAcc 54.6875 (Avg-Acc 54.6875)\n",
            "Epoch: [8][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2688 (Avg-Loss 1.2848)\tAcc 56.9853 (Avg-Acc 54.3700)\n",
            "EPOCH: 8 Validation Results: Acc 54.370 Loss: 1.2848\n",
            "Epoch: [9][0/78]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.0379 (Avg-Loss 1.0379)\tAcc 63.4766 (Avg-Acc 63.4766)\n",
            "Epoch: [9][19/78]\tTime 0.060 (Avg-Time 0.059)\t Loss 1.0047 (Avg-Loss 1.0536)\tAcc 65.6250 (Avg-Acc 62.4609)\n",
            "Epoch: [9][38/78]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.0857 (Avg-Loss 1.0778)\tAcc 58.7891 (Avg-Acc 61.4583)\n",
            "Epoch: [9][57/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1950 (Avg-Loss 1.0870)\tAcc 57.0312 (Avg-Acc 61.0857)\n",
            "Epoch: [9][76/78]\tTime 0.083 (Avg-Time 0.060)\t Loss 1.1801 (Avg-Loss 1.0977)\tAcc 60.1562 (Avg-Acc 60.6991)\n",
            "Epoch: [9][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 1.1712 (Avg-Loss 1.0980)\tAcc 60.9375 (Avg-Acc 60.6825)\n",
            "EPOCH: 9 train Results: Acc 60.682 Loss: 1.0980\n",
            "Epoch: [9][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2498 (Avg-Loss 1.2498)\tAcc 55.0781 (Avg-Acc 55.0781)\n",
            "Epoch: [9][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2756 (Avg-Loss 1.2813)\tAcc 54.0441 (Avg-Acc 54.4500)\n",
            "EPOCH: 9 Validation Results: Acc 54.450 Loss: 1.2813\n",
            "Epoch: [10][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.1127 (Avg-Loss 1.1127)\tAcc 61.7188 (Avg-Acc 61.7188)\n",
            "Epoch: [10][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.0739 (Avg-Loss 1.0371)\tAcc 61.3281 (Avg-Acc 62.5391)\n",
            "Epoch: [10][38/78]\tTime 0.149 (Avg-Time 0.085)\t Loss 1.0478 (Avg-Loss 1.0464)\tAcc 63.0859 (Avg-Acc 62.1044)\n",
            "Epoch: [10][57/78]\tTime 0.057 (Avg-Time 0.094)\t Loss 1.0841 (Avg-Loss 1.0579)\tAcc 60.7422 (Avg-Acc 61.8299)\n",
            "Epoch: [10][76/78]\tTime 0.057 (Avg-Time 0.085)\t Loss 1.1349 (Avg-Loss 1.0684)\tAcc 58.5938 (Avg-Acc 61.4550)\n",
            "Epoch: [10][78/78]\tTime 0.020 (Avg-Time 0.084)\t Loss 1.3703 (Avg-Loss 1.0689)\tAcc 56.2500 (Avg-Acc 61.4600)\n",
            "EPOCH: 10 train Results: Acc 61.460 Loss: 1.0689\n",
            "Epoch: [10][0/19]\tTime 0.011 (Avg-Time 0.011)\t Loss 1.2561 (Avg-Loss 1.2561)\tAcc 56.0547 (Avg-Acc 56.0547)\n",
            "Epoch: [10][19/19]\tTime 0.008 (Avg-Time 0.012)\t Loss 1.2960 (Avg-Loss 1.2764)\tAcc 52.5735 (Avg-Acc 55.1200)\n",
            "EPOCH: 10 Validation Results: Acc 55.120 Loss: 1.2764\n",
            "Epoch: [11][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.8872 (Avg-Loss 0.8872)\tAcc 67.7734 (Avg-Acc 67.7734)\n",
            "Epoch: [11][19/78]\tTime 0.079 (Avg-Time 0.060)\t Loss 1.0409 (Avg-Loss 1.0155)\tAcc 62.8906 (Avg-Acc 62.9102)\n",
            "Epoch: [11][38/78]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.0748 (Avg-Loss 1.0303)\tAcc 59.7656 (Avg-Acc 62.7404)\n",
            "Epoch: [11][57/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.0889 (Avg-Loss 1.0405)\tAcc 62.5000 (Avg-Acc 62.4899)\n",
            "Epoch: [11][76/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.0580 (Avg-Loss 1.0473)\tAcc 61.5234 (Avg-Acc 62.1601)\n",
            "Epoch: [11][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 1.0097 (Avg-Loss 1.0490)\tAcc 60.9375 (Avg-Acc 62.1050)\n",
            "EPOCH: 11 train Results: Acc 62.105 Loss: 1.0490\n",
            "Epoch: [11][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2540 (Avg-Loss 1.2540)\tAcc 55.0781 (Avg-Acc 55.0781)\n",
            "Epoch: [11][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3024 (Avg-Loss 1.2796)\tAcc 52.9412 (Avg-Acc 54.9300)\n",
            "EPOCH: 11 Validation Results: Acc 54.930 Loss: 1.2796\n",
            "Epoch: [12][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.9241 (Avg-Loss 0.9241)\tAcc 66.0156 (Avg-Acc 66.0156)\n",
            "Epoch: [12][19/78]\tTime 0.063 (Avg-Time 0.061)\t Loss 0.9720 (Avg-Loss 0.9822)\tAcc 66.0156 (Avg-Acc 64.5508)\n",
            "Epoch: [12][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.0268 (Avg-Loss 0.9887)\tAcc 62.5000 (Avg-Acc 64.1877)\n",
            "Epoch: [12][57/78]\tTime 0.099 (Avg-Time 0.063)\t Loss 1.0210 (Avg-Loss 1.0034)\tAcc 65.8203 (Avg-Acc 63.9547)\n",
            "Epoch: [12][76/78]\tTime 0.065 (Avg-Time 0.085)\t Loss 1.1021 (Avg-Loss 1.0143)\tAcc 60.9375 (Avg-Acc 63.4994)\n",
            "Epoch: [12][78/78]\tTime 0.020 (Avg-Time 0.084)\t Loss 1.2606 (Avg-Loss 1.0149)\tAcc 50.0000 (Avg-Acc 63.4400)\n",
            "EPOCH: 12 train Results: Acc 63.440 Loss: 1.0149\n",
            "Epoch: [12][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2647 (Avg-Loss 1.2647)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [12][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2818 (Avg-Loss 1.2825)\tAcc 54.7794 (Avg-Acc 55.2000)\n",
            "EPOCH: 12 Validation Results: Acc 55.200 Loss: 1.2825\n",
            "Epoch: [13][0/78]\tTime 0.063 (Avg-Time 0.063)\t Loss 0.8696 (Avg-Loss 0.8696)\tAcc 70.3125 (Avg-Acc 70.3125)\n",
            "Epoch: [13][19/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.9355 (Avg-Loss 0.9596)\tAcc 67.1875 (Avg-Acc 65.7129)\n",
            "Epoch: [13][38/78]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.0487 (Avg-Loss 0.9754)\tAcc 62.8906 (Avg-Acc 64.8137)\n",
            "Epoch: [13][57/78]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.0577 (Avg-Loss 0.9857)\tAcc 61.5234 (Avg-Acc 64.4093)\n",
            "Epoch: [13][76/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.0245 (Avg-Loss 0.9955)\tAcc 63.6719 (Avg-Acc 64.3567)\n",
            "Epoch: [13][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 1.2120 (Avg-Loss 0.9967)\tAcc 53.1250 (Avg-Acc 64.2800)\n",
            "EPOCH: 13 train Results: Acc 64.280 Loss: 0.9967\n",
            "Epoch: [13][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2328 (Avg-Loss 1.2328)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [13][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.3029 (Avg-Loss 1.2769)\tAcc 53.3088 (Avg-Acc 54.9100)\n",
            "EPOCH: 13 Validation Results: Acc 54.910 Loss: 1.2769\n",
            "Epoch: [14][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.9349 (Avg-Loss 0.9349)\tAcc 66.4062 (Avg-Acc 66.4062)\n",
            "Epoch: [14][19/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.9140 (Avg-Loss 0.9260)\tAcc 66.4062 (Avg-Acc 66.4062)\n",
            "Epoch: [14][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.9946 (Avg-Loss 0.9434)\tAcc 63.0859 (Avg-Acc 65.8854)\n",
            "Epoch: [14][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.9932 (Avg-Loss 0.9628)\tAcc 61.5234 (Avg-Acc 65.2344)\n",
            "Epoch: [14][76/78]\tTime 0.056 (Avg-Time 0.060)\t Loss 0.9783 (Avg-Loss 0.9745)\tAcc 62.8906 (Avg-Acc 64.8514)\n",
            "Epoch: [14][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 1.0300 (Avg-Loss 0.9763)\tAcc 56.2500 (Avg-Acc 64.7975)\n",
            "EPOCH: 14 train Results: Acc 64.797 Loss: 0.9763\n",
            "Epoch: [14][0/19]\tTime 0.029 (Avg-Time 0.029)\t Loss 1.2561 (Avg-Loss 1.2561)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [14][19/19]\tTime 0.021 (Avg-Time 0.033)\t Loss 1.3117 (Avg-Loss 1.2760)\tAcc 55.1471 (Avg-Acc 55.2300)\n",
            "EPOCH: 14 Validation Results: Acc 55.230 Loss: 1.2760\n",
            "Epoch: [15][0/78]\tTime 0.097 (Avg-Time 0.097)\t Loss 0.8418 (Avg-Loss 0.8418)\tAcc 69.7266 (Avg-Acc 69.7266)\n",
            "Epoch: [15][19/78]\tTime 0.056 (Avg-Time 0.138)\t Loss 0.9750 (Avg-Loss 0.9307)\tAcc 66.0156 (Avg-Acc 66.7969)\n",
            "Epoch: [15][38/78]\tTime 0.061 (Avg-Time 0.100)\t Loss 0.9778 (Avg-Loss 0.9349)\tAcc 64.8438 (Avg-Acc 66.5465)\n",
            "Epoch: [15][57/78]\tTime 0.058 (Avg-Time 0.087)\t Loss 0.8911 (Avg-Loss 0.9406)\tAcc 69.3359 (Avg-Acc 66.3524)\n",
            "Epoch: [15][76/78]\tTime 0.058 (Avg-Time 0.080)\t Loss 1.0374 (Avg-Loss 0.9530)\tAcc 62.6953 (Avg-Acc 65.8508)\n",
            "Epoch: [15][78/78]\tTime 0.020 (Avg-Time 0.079)\t Loss 1.2034 (Avg-Loss 0.9539)\tAcc 60.9375 (Avg-Acc 65.8300)\n",
            "EPOCH: 15 train Results: Acc 65.830 Loss: 0.9539\n",
            "Epoch: [15][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2380 (Avg-Loss 1.2380)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [15][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3123 (Avg-Loss 1.2804)\tAcc 52.5735 (Avg-Acc 55.3800)\n",
            "EPOCH: 15 Validation Results: Acc 55.380 Loss: 1.2804\n",
            "Epoch: [16][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.8505 (Avg-Loss 0.8505)\tAcc 71.8750 (Avg-Acc 71.8750)\n",
            "Epoch: [16][19/78]\tTime 0.060 (Avg-Time 0.059)\t Loss 0.8393 (Avg-Loss 0.8910)\tAcc 71.4844 (Avg-Acc 68.4180)\n",
            "Epoch: [16][38/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.8992 (Avg-Loss 0.9032)\tAcc 67.5781 (Avg-Acc 67.4629)\n",
            "Epoch: [16][57/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.9922 (Avg-Loss 0.9207)\tAcc 65.2344 (Avg-Acc 66.8945)\n",
            "Epoch: [16][76/78]\tTime 0.058 (Avg-Time 0.059)\t Loss 0.9700 (Avg-Loss 0.9372)\tAcc 66.9922 (Avg-Acc 66.2464)\n",
            "Epoch: [16][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 1.0791 (Avg-Loss 0.9380)\tAcc 59.3750 (Avg-Acc 66.1975)\n",
            "EPOCH: 16 train Results: Acc 66.198 Loss: 0.9380\n",
            "Epoch: [16][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2506 (Avg-Loss 1.2506)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [16][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3037 (Avg-Loss 1.2845)\tAcc 51.4706 (Avg-Acc 55.1500)\n",
            "EPOCH: 16 Validation Results: Acc 55.150 Loss: 1.2845\n",
            "Epoch: [17][0/78]\tTime 0.057 (Avg-Time 0.057)\t Loss 0.8863 (Avg-Loss 0.8863)\tAcc 69.5312 (Avg-Acc 69.5312)\n",
            "Epoch: [17][19/78]\tTime 0.108 (Avg-Time 0.064)\t Loss 0.8503 (Avg-Loss 0.8698)\tAcc 67.7734 (Avg-Acc 68.5645)\n",
            "Epoch: [17][38/78]\tTime 0.127 (Avg-Time 0.105)\t Loss 0.9962 (Avg-Loss 0.8939)\tAcc 65.4297 (Avg-Acc 67.9838)\n",
            "Epoch: [17][57/78]\tTime 0.057 (Avg-Time 0.092)\t Loss 0.9554 (Avg-Loss 0.9038)\tAcc 65.0391 (Avg-Acc 67.5849)\n",
            "Epoch: [17][76/78]\tTime 0.072 (Avg-Time 0.084)\t Loss 0.9194 (Avg-Loss 0.9128)\tAcc 67.3828 (Avg-Acc 67.1697)\n",
            "Epoch: [17][78/78]\tTime 0.020 (Avg-Time 0.083)\t Loss 1.0356 (Avg-Loss 0.9143)\tAcc 64.0625 (Avg-Acc 67.1500)\n",
            "EPOCH: 17 train Results: Acc 67.150 Loss: 0.9143\n",
            "Epoch: [17][0/19]\tTime 0.011 (Avg-Time 0.011)\t Loss 1.2889 (Avg-Loss 1.2889)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [17][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2969 (Avg-Loss 1.2947)\tAcc 55.8824 (Avg-Acc 55.5800)\n",
            "EPOCH: 17 Validation Results: Acc 55.580 Loss: 1.2947\n",
            "Epoch: [18][0/78]\tTime 0.057 (Avg-Time 0.057)\t Loss 0.7971 (Avg-Loss 0.7971)\tAcc 70.1172 (Avg-Acc 70.1172)\n",
            "Epoch: [18][19/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.9633 (Avg-Loss 0.8506)\tAcc 64.0625 (Avg-Acc 69.5801)\n",
            "Epoch: [18][38/78]\tTime 0.056 (Avg-Time 0.059)\t Loss 0.8432 (Avg-Loss 0.8712)\tAcc 68.9453 (Avg-Acc 68.5447)\n",
            "Epoch: [18][57/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.8721 (Avg-Loss 0.8859)\tAcc 67.9688 (Avg-Acc 67.9789)\n",
            "Epoch: [18][76/78]\tTime 0.058 (Avg-Time 0.059)\t Loss 0.9005 (Avg-Loss 0.8990)\tAcc 66.2109 (Avg-Acc 67.4310)\n",
            "Epoch: [18][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 1.1377 (Avg-Loss 0.8994)\tAcc 56.2500 (Avg-Acc 67.4100)\n",
            "EPOCH: 18 train Results: Acc 67.410 Loss: 0.8994\n",
            "Epoch: [18][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2703 (Avg-Loss 1.2703)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [18][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3269 (Avg-Loss 1.2991)\tAcc 54.7794 (Avg-Acc 55.5200)\n",
            "EPOCH: 18 Validation Results: Acc 55.520 Loss: 1.2991\n",
            "Epoch: [19][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.7852 (Avg-Loss 0.7852)\tAcc 72.0703 (Avg-Acc 72.0703)\n",
            "Epoch: [19][19/78]\tTime 0.060 (Avg-Time 0.059)\t Loss 0.9034 (Avg-Loss 0.8457)\tAcc 68.5547 (Avg-Acc 69.3945)\n",
            "Epoch: [19][38/78]\tTime 0.055 (Avg-Time 0.060)\t Loss 0.9581 (Avg-Loss 0.8617)\tAcc 65.8203 (Avg-Acc 69.0455)\n",
            "Epoch: [19][57/78]\tTime 0.098 (Avg-Time 0.085)\t Loss 0.9384 (Avg-Loss 0.8725)\tAcc 68.7500 (Avg-Acc 68.4200)\n",
            "Epoch: [19][76/78]\tTime 0.057 (Avg-Time 0.085)\t Loss 1.0168 (Avg-Loss 0.8850)\tAcc 61.7188 (Avg-Acc 68.0119)\n",
            "Epoch: [19][78/78]\tTime 0.021 (Avg-Time 0.084)\t Loss 0.7811 (Avg-Loss 0.8870)\tAcc 68.7500 (Avg-Acc 67.9550)\n",
            "EPOCH: 19 train Results: Acc 67.955 Loss: 0.8870\n",
            "Epoch: [19][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2565 (Avg-Loss 1.2565)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [19][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3328 (Avg-Loss 1.2970)\tAcc 52.9412 (Avg-Acc 55.7500)\n",
            "EPOCH: 19 Validation Results: Acc 55.750 Loss: 1.2970\n",
            "Epoch: [20][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.6904 (Avg-Loss 0.6904)\tAcc 74.4141 (Avg-Acc 74.4141)\n",
            "Epoch: [20][19/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.8758 (Avg-Loss 0.8212)\tAcc 68.3594 (Avg-Acc 70.5664)\n",
            "Epoch: [20][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.8865 (Avg-Loss 0.8390)\tAcc 66.7969 (Avg-Acc 69.9519)\n",
            "Epoch: [20][57/78]\tTime 0.081 (Avg-Time 0.061)\t Loss 0.8656 (Avg-Loss 0.8495)\tAcc 67.9688 (Avg-Acc 69.3292)\n",
            "Epoch: [20][76/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.9264 (Avg-Loss 0.8633)\tAcc 67.9688 (Avg-Acc 68.9935)\n",
            "Epoch: [20][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.9419 (Avg-Loss 0.8650)\tAcc 64.0625 (Avg-Acc 68.9475)\n",
            "EPOCH: 20 train Results: Acc 68.948 Loss: 0.8650\n",
            "Epoch: [20][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2762 (Avg-Loss 1.2762)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [20][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3554 (Avg-Loss 1.3085)\tAcc 52.2059 (Avg-Acc 54.9100)\n",
            "EPOCH: 20 Validation Results: Acc 54.910 Loss: 1.3085\n",
            "Epoch: [21][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.6914 (Avg-Loss 0.6914)\tAcc 74.2188 (Avg-Acc 74.2188)\n",
            "Epoch: [21][19/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.8662 (Avg-Loss 0.8174)\tAcc 67.9688 (Avg-Acc 70.2734)\n",
            "Epoch: [21][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.8181 (Avg-Loss 0.8395)\tAcc 71.6797 (Avg-Acc 69.6214)\n",
            "Epoch: [21][57/78]\tTime 0.056 (Avg-Time 0.060)\t Loss 0.9038 (Avg-Loss 0.8529)\tAcc 66.9922 (Avg-Acc 69.2080)\n",
            "Epoch: [21][76/78]\tTime 0.182 (Avg-Time 0.076)\t Loss 0.8710 (Avg-Loss 0.8607)\tAcc 70.7031 (Avg-Acc 69.0316)\n",
            "Epoch: [21][78/78]\tTime 0.079 (Avg-Time 0.076)\t Loss 0.9090 (Avg-Loss 0.8601)\tAcc 70.3125 (Avg-Acc 69.0675)\n",
            "EPOCH: 21 train Results: Acc 69.067 Loss: 0.8601\n",
            "Epoch: [21][0/19]\tTime 0.036 (Avg-Time 0.036)\t Loss 1.2801 (Avg-Loss 1.2801)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [21][19/19]\tTime 0.021 (Avg-Time 0.026)\t Loss 1.3727 (Avg-Loss 1.3088)\tAcc 52.9412 (Avg-Acc 55.1500)\n",
            "EPOCH: 21 Validation Results: Acc 55.150 Loss: 1.3088\n",
            "Epoch: [22][0/78]\tTime 0.115 (Avg-Time 0.115)\t Loss 0.7502 (Avg-Loss 0.7502)\tAcc 74.4141 (Avg-Acc 74.4141)\n",
            "Epoch: [22][19/78]\tTime 0.057 (Avg-Time 0.076)\t Loss 0.7781 (Avg-Loss 0.8024)\tAcc 70.7031 (Avg-Acc 71.4551)\n",
            "Epoch: [22][38/78]\tTime 0.058 (Avg-Time 0.068)\t Loss 0.8676 (Avg-Loss 0.8107)\tAcc 69.9219 (Avg-Acc 70.8734)\n",
            "Epoch: [22][57/78]\tTime 0.057 (Avg-Time 0.065)\t Loss 0.8676 (Avg-Loss 0.8239)\tAcc 68.9453 (Avg-Acc 70.3361)\n",
            "Epoch: [22][76/78]\tTime 0.056 (Avg-Time 0.064)\t Loss 0.8662 (Avg-Loss 0.8399)\tAcc 67.7734 (Avg-Acc 69.7874)\n",
            "Epoch: [22][78/78]\tTime 0.020 (Avg-Time 0.063)\t Loss 1.2147 (Avg-Loss 0.8421)\tAcc 53.1250 (Avg-Acc 69.6925)\n",
            "EPOCH: 22 train Results: Acc 69.692 Loss: 0.8421\n",
            "Epoch: [22][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3091 (Avg-Loss 1.3091)\tAcc 56.0547 (Avg-Acc 56.0547)\n",
            "Epoch: [22][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3901 (Avg-Loss 1.3242)\tAcc 50.7353 (Avg-Acc 55.0300)\n",
            "EPOCH: 22 Validation Results: Acc 55.030 Loss: 1.3242\n",
            "Epoch: [23][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.7838 (Avg-Loss 0.7838)\tAcc 71.4844 (Avg-Acc 71.4844)\n",
            "Epoch: [23][19/78]\tTime 0.056 (Avg-Time 0.059)\t Loss 0.8006 (Avg-Loss 0.8056)\tAcc 69.5312 (Avg-Acc 70.9961)\n",
            "Epoch: [23][38/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.7788 (Avg-Loss 0.8089)\tAcc 70.7031 (Avg-Acc 70.8684)\n",
            "Epoch: [23][57/78]\tTime 0.071 (Avg-Time 0.059)\t Loss 0.8923 (Avg-Loss 0.8240)\tAcc 67.5781 (Avg-Acc 70.2856)\n",
            "Epoch: [23][76/78]\tTime 0.056 (Avg-Time 0.059)\t Loss 0.9354 (Avg-Loss 0.8323)\tAcc 65.4297 (Avg-Acc 70.0411)\n",
            "Epoch: [23][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 1.0340 (Avg-Loss 0.8330)\tAcc 62.5000 (Avg-Acc 69.9875)\n",
            "EPOCH: 23 train Results: Acc 69.987 Loss: 0.8330\n",
            "Epoch: [23][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2780 (Avg-Loss 1.2780)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [23][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3769 (Avg-Loss 1.3055)\tAcc 54.7794 (Avg-Acc 55.2900)\n",
            "EPOCH: 23 Validation Results: Acc 55.290 Loss: 1.3055\n",
            "Epoch: [24][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.8415 (Avg-Loss 0.8415)\tAcc 68.3594 (Avg-Acc 68.3594)\n",
            "Epoch: [24][19/78]\tTime 0.134 (Avg-Time 0.131)\t Loss 0.8080 (Avg-Loss 0.7841)\tAcc 68.9453 (Avg-Acc 71.2500)\n",
            "Epoch: [24][38/78]\tTime 0.055 (Avg-Time 0.110)\t Loss 0.7916 (Avg-Loss 0.7941)\tAcc 68.7500 (Avg-Acc 70.9335)\n",
            "Epoch: [24][57/78]\tTime 0.058 (Avg-Time 0.093)\t Loss 0.7924 (Avg-Loss 0.8088)\tAcc 68.1641 (Avg-Acc 70.6324)\n",
            "Epoch: [24][76/78]\tTime 0.055 (Avg-Time 0.085)\t Loss 0.7673 (Avg-Loss 0.8196)\tAcc 72.6562 (Avg-Acc 70.2212)\n",
            "Epoch: [24][78/78]\tTime 0.020 (Avg-Time 0.084)\t Loss 0.8060 (Avg-Loss 0.8200)\tAcc 62.5000 (Avg-Acc 70.2275)\n",
            "EPOCH: 24 train Results: Acc 70.228 Loss: 0.8200\n",
            "Epoch: [24][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2836 (Avg-Loss 1.2836)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [24][19/19]\tTime 0.009 (Avg-Time 0.013)\t Loss 1.4192 (Avg-Loss 1.3194)\tAcc 52.5735 (Avg-Acc 54.9000)\n",
            "EPOCH: 24 Validation Results: Acc 54.900 Loss: 1.3194\n",
            "Epoch: [25][0/78]\tTime 0.057 (Avg-Time 0.057)\t Loss 0.6907 (Avg-Loss 0.6907)\tAcc 76.5625 (Avg-Acc 76.5625)\n",
            "Epoch: [25][19/78]\tTime 0.058 (Avg-Time 0.059)\t Loss 0.7573 (Avg-Loss 0.7749)\tAcc 70.1172 (Avg-Acc 72.2559)\n",
            "Epoch: [25][38/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.8445 (Avg-Loss 0.7840)\tAcc 69.9219 (Avg-Acc 71.6947)\n",
            "Epoch: [25][57/78]\tTime 0.058 (Avg-Time 0.059)\t Loss 0.7677 (Avg-Loss 0.7932)\tAcc 73.6328 (Avg-Acc 71.4372)\n",
            "Epoch: [25][76/78]\tTime 0.056 (Avg-Time 0.060)\t Loss 0.7598 (Avg-Loss 0.8042)\tAcc 71.8750 (Avg-Acc 70.9974)\n",
            "Epoch: [25][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 0.9118 (Avg-Loss 0.8053)\tAcc 65.6250 (Avg-Acc 70.9925)\n",
            "EPOCH: 25 train Results: Acc 70.993 Loss: 0.8053\n",
            "Epoch: [25][0/19]\tTime 0.017 (Avg-Time 0.017)\t Loss 1.3044 (Avg-Loss 1.3044)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [25][19/19]\tTime 0.006 (Avg-Time 0.013)\t Loss 1.4378 (Avg-Loss 1.3299)\tAcc 51.8382 (Avg-Acc 55.0100)\n",
            "EPOCH: 25 Validation Results: Acc 55.010 Loss: 1.3299\n",
            "Epoch: [26][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.7615 (Avg-Loss 0.7615)\tAcc 71.6797 (Avg-Acc 71.6797)\n",
            "Epoch: [26][19/78]\tTime 0.056 (Avg-Time 0.059)\t Loss 0.7441 (Avg-Loss 0.7632)\tAcc 74.6094 (Avg-Acc 72.4902)\n",
            "Epoch: [26][38/78]\tTime 0.211 (Avg-Time 0.074)\t Loss 0.7356 (Avg-Loss 0.7679)\tAcc 75.3906 (Avg-Acc 72.1605)\n",
            "Epoch: [26][57/78]\tTime 0.059 (Avg-Time 0.092)\t Loss 0.8661 (Avg-Loss 0.7820)\tAcc 70.3125 (Avg-Acc 71.7942)\n",
            "Epoch: [26][76/78]\tTime 0.057 (Avg-Time 0.084)\t Loss 0.8462 (Avg-Loss 0.7928)\tAcc 69.1406 (Avg-Acc 71.1800)\n",
            "Epoch: [26][78/78]\tTime 0.020 (Avg-Time 0.083)\t Loss 0.8164 (Avg-Loss 0.7930)\tAcc 67.1875 (Avg-Acc 71.1650)\n",
            "EPOCH: 26 train Results: Acc 71.165 Loss: 0.7930\n",
            "Epoch: [26][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3002 (Avg-Loss 1.3002)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [26][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4228 (Avg-Loss 1.3312)\tAcc 50.7353 (Avg-Acc 55.1500)\n",
            "EPOCH: 26 Validation Results: Acc 55.150 Loss: 1.3312\n",
            "Epoch: [27][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.7257 (Avg-Loss 0.7257)\tAcc 72.2656 (Avg-Acc 72.2656)\n",
            "Epoch: [27][19/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.6865 (Avg-Loss 0.7327)\tAcc 74.8047 (Avg-Acc 73.1641)\n",
            "Epoch: [27][38/78]\tTime 0.056 (Avg-Time 0.059)\t Loss 0.6946 (Avg-Loss 0.7510)\tAcc 76.3672 (Avg-Acc 72.9267)\n",
            "Epoch: [27][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.8409 (Avg-Loss 0.7702)\tAcc 70.1172 (Avg-Acc 72.2926)\n",
            "Epoch: [27][76/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.8229 (Avg-Loss 0.7832)\tAcc 72.4609 (Avg-Acc 71.7279)\n",
            "Epoch: [27][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 0.8723 (Avg-Loss 0.7838)\tAcc 62.5000 (Avg-Acc 71.6875)\n",
            "EPOCH: 27 train Results: Acc 71.688 Loss: 0.7838\n",
            "Epoch: [27][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3135 (Avg-Loss 1.3135)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [27][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.4445 (Avg-Loss 1.3277)\tAcc 51.4706 (Avg-Acc 54.9100)\n",
            "EPOCH: 27 Validation Results: Acc 54.910 Loss: 1.3277\n",
            "Epoch: [28][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.6467 (Avg-Loss 0.6467)\tAcc 76.9531 (Avg-Acc 76.9531)\n",
            "Epoch: [28][19/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.7741 (Avg-Loss 0.7427)\tAcc 72.0703 (Avg-Acc 73.6426)\n",
            "Epoch: [28][38/78]\tTime 0.056 (Avg-Time 0.059)\t Loss 0.7944 (Avg-Loss 0.7525)\tAcc 71.0938 (Avg-Acc 72.8966)\n",
            "Epoch: [28][57/78]\tTime 0.087 (Avg-Time 0.061)\t Loss 0.8241 (Avg-Loss 0.7562)\tAcc 71.6797 (Avg-Acc 72.7573)\n",
            "Epoch: [28][76/78]\tTime 0.057 (Avg-Time 0.085)\t Loss 0.9177 (Avg-Loss 0.7726)\tAcc 66.7969 (Avg-Acc 72.0678)\n",
            "Epoch: [28][78/78]\tTime 0.020 (Avg-Time 0.084)\t Loss 0.7370 (Avg-Loss 0.7737)\tAcc 68.7500 (Avg-Acc 72.0450)\n",
            "EPOCH: 28 train Results: Acc 72.045 Loss: 0.7737\n",
            "Epoch: [28][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3363 (Avg-Loss 1.3363)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [28][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4589 (Avg-Loss 1.3428)\tAcc 53.3088 (Avg-Acc 55.3700)\n",
            "EPOCH: 28 Validation Results: Acc 55.370 Loss: 1.3428\n",
            "Epoch: [29][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.7013 (Avg-Loss 0.7013)\tAcc 74.8047 (Avg-Acc 74.8047)\n",
            "Epoch: [29][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.7369 (Avg-Loss 0.7382)\tAcc 73.4375 (Avg-Acc 73.8086)\n",
            "Epoch: [29][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.6862 (Avg-Loss 0.7426)\tAcc 75.3906 (Avg-Acc 73.4024)\n",
            "Epoch: [29][57/78]\tTime 0.056 (Avg-Time 0.059)\t Loss 0.7271 (Avg-Loss 0.7568)\tAcc 73.2422 (Avg-Acc 72.8314)\n",
            "Epoch: [29][76/78]\tTime 0.081 (Avg-Time 0.059)\t Loss 0.8320 (Avg-Loss 0.7616)\tAcc 70.3125 (Avg-Acc 72.7121)\n",
            "Epoch: [29][78/78]\tTime 0.023 (Avg-Time 0.059)\t Loss 0.9260 (Avg-Loss 0.7626)\tAcc 68.7500 (Avg-Acc 72.6750)\n",
            "EPOCH: 29 train Results: Acc 72.675 Loss: 0.7626\n",
            "Epoch: [29][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3253 (Avg-Loss 1.3253)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [29][19/19]\tTime 0.006 (Avg-Time 0.012)\t Loss 1.4621 (Avg-Loss 1.3383)\tAcc 52.2059 (Avg-Acc 55.3900)\n",
            "EPOCH: 29 Validation Results: Acc 55.390 Loss: 1.3383\n",
            "Epoch: [30][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.7953 (Avg-Loss 0.7953)\tAcc 71.0938 (Avg-Acc 71.0938)\n",
            "Epoch: [30][19/78]\tTime 0.060 (Avg-Time 0.059)\t Loss 0.7540 (Avg-Loss 0.7179)\tAcc 73.4375 (Avg-Acc 73.8965)\n",
            "Epoch: [30][38/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.6714 (Avg-Loss 0.7284)\tAcc 76.1719 (Avg-Acc 73.4525)\n",
            "Epoch: [30][57/78]\tTime 0.058 (Avg-Time 0.059)\t Loss 0.7712 (Avg-Loss 0.7440)\tAcc 72.2656 (Avg-Acc 72.9762)\n",
            "Epoch: [30][76/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.8230 (Avg-Loss 0.7528)\tAcc 69.7266 (Avg-Acc 72.7121)\n",
            "Epoch: [30][78/78]\tTime 0.040 (Avg-Time 0.059)\t Loss 0.8994 (Avg-Loss 0.7546)\tAcc 68.7500 (Avg-Acc 72.6475)\n",
            "EPOCH: 30 train Results: Acc 72.647 Loss: 0.7546\n",
            "Epoch: [30][0/19]\tTime 0.020 (Avg-Time 0.020)\t Loss 1.3155 (Avg-Loss 1.3155)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [30][19/19]\tTime 0.024 (Avg-Time 0.026)\t Loss 1.4887 (Avg-Loss 1.3522)\tAcc 52.2059 (Avg-Acc 55.2900)\n",
            "EPOCH: 30 Validation Results: Acc 55.290 Loss: 1.3522\n",
            "Epoch: [31][0/78]\tTime 0.093 (Avg-Time 0.093)\t Loss 0.7482 (Avg-Loss 0.7482)\tAcc 76.1719 (Avg-Acc 76.1719)\n",
            "Epoch: [31][19/78]\tTime 0.142 (Avg-Time 0.141)\t Loss 0.6990 (Avg-Loss 0.7217)\tAcc 74.2188 (Avg-Acc 73.9941)\n",
            "Epoch: [31][38/78]\tTime 0.059 (Avg-Time 0.104)\t Loss 0.7209 (Avg-Loss 0.7324)\tAcc 73.0469 (Avg-Acc 73.4575)\n",
            "Epoch: [31][57/78]\tTime 0.057 (Avg-Time 0.089)\t Loss 0.7785 (Avg-Loss 0.7421)\tAcc 72.4609 (Avg-Acc 73.0738)\n",
            "Epoch: [31][76/78]\tTime 0.057 (Avg-Time 0.082)\t Loss 0.7780 (Avg-Loss 0.7514)\tAcc 72.4609 (Avg-Acc 72.7400)\n",
            "Epoch: [31][78/78]\tTime 0.020 (Avg-Time 0.081)\t Loss 0.8229 (Avg-Loss 0.7526)\tAcc 70.3125 (Avg-Acc 72.6925)\n",
            "EPOCH: 31 train Results: Acc 72.692 Loss: 0.7526\n",
            "Epoch: [31][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3440 (Avg-Loss 1.3440)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [31][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4920 (Avg-Loss 1.3607)\tAcc 51.8382 (Avg-Acc 55.4400)\n",
            "EPOCH: 31 Validation Results: Acc 55.440 Loss: 1.3607\n",
            "Epoch: [32][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.6122 (Avg-Loss 0.6122)\tAcc 78.1250 (Avg-Acc 78.1250)\n",
            "Epoch: [32][19/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.7533 (Avg-Loss 0.7006)\tAcc 70.8984 (Avg-Acc 74.4922)\n",
            "Epoch: [32][38/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.8219 (Avg-Loss 0.7220)\tAcc 69.7266 (Avg-Acc 73.9383)\n",
            "Epoch: [32][57/78]\tTime 0.061 (Avg-Time 0.060)\t Loss 0.7877 (Avg-Loss 0.7308)\tAcc 70.8984 (Avg-Acc 73.6665)\n",
            "Epoch: [32][76/78]\tTime 0.079 (Avg-Time 0.060)\t Loss 0.8072 (Avg-Loss 0.7379)\tAcc 71.6797 (Avg-Acc 73.3868)\n",
            "Epoch: [32][78/78]\tTime 0.021 (Avg-Time 0.059)\t Loss 1.1589 (Avg-Loss 0.7392)\tAcc 64.0625 (Avg-Acc 73.3450)\n",
            "EPOCH: 32 train Results: Acc 73.345 Loss: 0.7392\n",
            "Epoch: [32][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3466 (Avg-Loss 1.3466)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [32][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4907 (Avg-Loss 1.3588)\tAcc 52.9412 (Avg-Acc 55.3200)\n",
            "EPOCH: 32 Validation Results: Acc 55.320 Loss: 1.3588\n",
            "Epoch: [33][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.6301 (Avg-Loss 0.6301)\tAcc 76.5625 (Avg-Acc 76.5625)\n",
            "Epoch: [33][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.7346 (Avg-Loss 0.6878)\tAcc 72.8516 (Avg-Acc 75.1660)\n",
            "Epoch: [33][38/78]\tTime 0.104 (Avg-Time 0.101)\t Loss 0.7444 (Avg-Loss 0.7193)\tAcc 71.6797 (Avg-Acc 74.1236)\n",
            "Epoch: [33][57/78]\tTime 0.057 (Avg-Time 0.094)\t Loss 0.7858 (Avg-Loss 0.7247)\tAcc 71.6797 (Avg-Acc 74.0571)\n",
            "Epoch: [33][76/78]\tTime 0.057 (Avg-Time 0.085)\t Loss 0.7674 (Avg-Loss 0.7377)\tAcc 72.2656 (Avg-Acc 73.5085)\n",
            "Epoch: [33][78/78]\tTime 0.020 (Avg-Time 0.084)\t Loss 0.8674 (Avg-Loss 0.7388)\tAcc 59.3750 (Avg-Acc 73.4700)\n",
            "EPOCH: 33 train Results: Acc 73.470 Loss: 0.7388\n",
            "Epoch: [33][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3507 (Avg-Loss 1.3507)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [33][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4719 (Avg-Loss 1.3522)\tAcc 51.8382 (Avg-Acc 54.7500)\n",
            "EPOCH: 33 Validation Results: Acc 54.750 Loss: 1.3522\n",
            "Epoch: [34][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.6736 (Avg-Loss 0.6736)\tAcc 75.5859 (Avg-Acc 75.5859)\n",
            "Epoch: [34][19/78]\tTime 0.076 (Avg-Time 0.062)\t Loss 0.6596 (Avg-Loss 0.6801)\tAcc 74.2188 (Avg-Acc 75.4883)\n",
            "Epoch: [34][38/78]\tTime 0.056 (Avg-Time 0.061)\t Loss 0.7434 (Avg-Loss 0.6887)\tAcc 73.6328 (Avg-Acc 75.1002)\n",
            "Epoch: [34][57/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.7546 (Avg-Loss 0.7026)\tAcc 71.8750 (Avg-Acc 74.5723)\n",
            "Epoch: [34][76/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.7534 (Avg-Loss 0.7121)\tAcc 72.8516 (Avg-Acc 74.3177)\n",
            "Epoch: [34][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.6852 (Avg-Loss 0.7123)\tAcc 78.1250 (Avg-Acc 74.2950)\n",
            "EPOCH: 34 train Results: Acc 74.295 Loss: 0.7123\n",
            "Epoch: [34][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3498 (Avg-Loss 1.3498)\tAcc 60.1562 (Avg-Acc 60.1562)\n",
            "Epoch: [34][19/19]\tTime 0.008 (Avg-Time 0.013)\t Loss 1.5066 (Avg-Loss 1.3664)\tAcc 52.9412 (Avg-Acc 55.4600)\n",
            "EPOCH: 34 Validation Results: Acc 55.460 Loss: 1.3664\n",
            "Epoch: [35][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.6153 (Avg-Loss 0.6153)\tAcc 77.3438 (Avg-Acc 77.3438)\n",
            "Epoch: [35][19/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.6698 (Avg-Loss 0.6632)\tAcc 77.3438 (Avg-Acc 76.0645)\n",
            "Epoch: [35][38/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.6609 (Avg-Loss 0.6782)\tAcc 76.9531 (Avg-Acc 75.8213)\n",
            "Epoch: [35][57/78]\tTime 0.221 (Avg-Time 0.083)\t Loss 0.7374 (Avg-Loss 0.6939)\tAcc 73.4375 (Avg-Acc 75.0876)\n",
            "Epoch: [35][76/78]\tTime 0.057 (Avg-Time 0.085)\t Loss 0.7821 (Avg-Loss 0.7057)\tAcc 71.2891 (Avg-Acc 74.4623)\n",
            "Epoch: [35][78/78]\tTime 0.042 (Avg-Time 0.084)\t Loss 0.9754 (Avg-Loss 0.7070)\tAcc 60.9375 (Avg-Acc 74.4225)\n",
            "EPOCH: 35 train Results: Acc 74.422 Loss: 0.7070\n",
            "Epoch: [35][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3256 (Avg-Loss 1.3256)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [35][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5024 (Avg-Loss 1.3746)\tAcc 54.7794 (Avg-Acc 55.4800)\n",
            "EPOCH: 35 Validation Results: Acc 55.480 Loss: 1.3746\n",
            "Epoch: [36][0/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.6684 (Avg-Loss 0.6684)\tAcc 76.3672 (Avg-Acc 76.3672)\n",
            "Epoch: [36][19/78]\tTime 0.057 (Avg-Time 0.062)\t Loss 0.7050 (Avg-Loss 0.6773)\tAcc 74.0234 (Avg-Acc 75.5371)\n",
            "Epoch: [36][38/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.6787 (Avg-Loss 0.6904)\tAcc 75.3906 (Avg-Acc 74.9900)\n",
            "Epoch: [36][57/78]\tTime 0.056 (Avg-Time 0.060)\t Loss 0.7305 (Avg-Loss 0.6982)\tAcc 71.6797 (Avg-Acc 74.5824)\n",
            "Epoch: [36][76/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.7669 (Avg-Loss 0.7087)\tAcc 69.9219 (Avg-Acc 74.0539)\n",
            "Epoch: [36][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.6785 (Avg-Loss 0.7084)\tAcc 71.8750 (Avg-Acc 74.0775)\n",
            "EPOCH: 36 train Results: Acc 74.078 Loss: 0.7084\n",
            "Epoch: [36][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3477 (Avg-Loss 1.3477)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [36][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.4954 (Avg-Loss 1.3795)\tAcc 54.4118 (Avg-Acc 55.5600)\n",
            "EPOCH: 36 Validation Results: Acc 55.560 Loss: 1.3795\n",
            "Epoch: [37][0/78]\tTime 0.067 (Avg-Time 0.067)\t Loss 0.5876 (Avg-Loss 0.5876)\tAcc 80.0781 (Avg-Acc 80.0781)\n",
            "Epoch: [37][19/78]\tTime 0.056 (Avg-Time 0.060)\t Loss 0.6686 (Avg-Loss 0.6644)\tAcc 77.5391 (Avg-Acc 76.0742)\n",
            "Epoch: [37][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.6784 (Avg-Loss 0.6769)\tAcc 77.5391 (Avg-Acc 75.6911)\n",
            "Epoch: [37][57/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.7271 (Avg-Loss 0.6893)\tAcc 75.1953 (Avg-Acc 75.3266)\n",
            "Epoch: [37][76/78]\tTime 0.255 (Avg-Time 0.068)\t Loss 0.7231 (Avg-Loss 0.6984)\tAcc 75.1953 (Avg-Acc 74.9011)\n",
            "Epoch: [37][78/78]\tTime 0.030 (Avg-Time 0.070)\t Loss 0.7497 (Avg-Loss 0.6997)\tAcc 78.1250 (Avg-Acc 74.8475)\n",
            "EPOCH: 37 train Results: Acc 74.847 Loss: 0.6997\n",
            "Epoch: [37][0/19]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.3558 (Avg-Loss 1.3558)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [37][19/19]\tTime 0.015 (Avg-Time 0.050)\t Loss 1.4835 (Avg-Loss 1.3866)\tAcc 52.9412 (Avg-Acc 54.9200)\n",
            "EPOCH: 37 Validation Results: Acc 54.920 Loss: 1.3866\n",
            "Epoch: [38][0/78]\tTime 0.161 (Avg-Time 0.161)\t Loss 0.6210 (Avg-Loss 0.6210)\tAcc 75.5859 (Avg-Acc 75.5859)\n",
            "Epoch: [38][19/78]\tTime 0.061 (Avg-Time 0.080)\t Loss 0.6542 (Avg-Loss 0.6612)\tAcc 77.1484 (Avg-Acc 76.8359)\n",
            "Epoch: [38][38/78]\tTime 0.057 (Avg-Time 0.070)\t Loss 0.7034 (Avg-Loss 0.6753)\tAcc 73.0469 (Avg-Acc 76.0116)\n",
            "Epoch: [38][57/78]\tTime 0.057 (Avg-Time 0.067)\t Loss 0.7269 (Avg-Loss 0.6837)\tAcc 72.4609 (Avg-Acc 75.4984)\n",
            "Epoch: [38][76/78]\tTime 0.060 (Avg-Time 0.065)\t Loss 0.7285 (Avg-Loss 0.6930)\tAcc 76.1719 (Avg-Acc 75.1598)\n",
            "Epoch: [38][78/78]\tTime 0.020 (Avg-Time 0.064)\t Loss 0.8216 (Avg-Loss 0.6939)\tAcc 70.3125 (Avg-Acc 75.1075)\n",
            "EPOCH: 38 train Results: Acc 75.108 Loss: 0.6939\n",
            "Epoch: [38][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3720 (Avg-Loss 1.3720)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [38][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5246 (Avg-Loss 1.3888)\tAcc 52.2059 (Avg-Acc 54.9900)\n",
            "EPOCH: 38 Validation Results: Acc 54.990 Loss: 1.3888\n",
            "Epoch: [39][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.6686 (Avg-Loss 0.6686)\tAcc 75.0000 (Avg-Acc 75.0000)\n",
            "Epoch: [39][19/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.6970 (Avg-Loss 0.6669)\tAcc 75.5859 (Avg-Acc 76.0547)\n",
            "Epoch: [39][38/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.7197 (Avg-Loss 0.6698)\tAcc 73.2422 (Avg-Acc 75.9014)\n",
            "Epoch: [39][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.6477 (Avg-Loss 0.6782)\tAcc 76.5625 (Avg-Acc 75.6028)\n",
            "Epoch: [39][76/78]\tTime 0.056 (Avg-Time 0.059)\t Loss 0.7779 (Avg-Loss 0.6915)\tAcc 72.4609 (Avg-Acc 75.1928)\n",
            "Epoch: [39][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 1.0158 (Avg-Loss 0.6931)\tAcc 59.3750 (Avg-Acc 75.1100)\n",
            "EPOCH: 39 train Results: Acc 75.110 Loss: 0.6931\n",
            "Epoch: [39][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3721 (Avg-Loss 1.3721)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [39][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.5097 (Avg-Loss 1.3813)\tAcc 52.5735 (Avg-Acc 55.1600)\n",
            "EPOCH: 39 Validation Results: Acc 55.160 Loss: 1.3813\n",
            "Epoch: [40][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.6629 (Avg-Loss 0.6629)\tAcc 76.5625 (Avg-Acc 76.5625)\n",
            "Epoch: [40][19/78]\tTime 0.119 (Avg-Time 0.114)\t Loss 0.7148 (Avg-Loss 0.6728)\tAcc 74.0234 (Avg-Acc 76.0742)\n",
            "Epoch: [40][38/78]\tTime 0.061 (Avg-Time 0.110)\t Loss 0.6971 (Avg-Loss 0.6728)\tAcc 75.1953 (Avg-Acc 76.0166)\n",
            "Epoch: [40][57/78]\tTime 0.056 (Avg-Time 0.094)\t Loss 0.7018 (Avg-Loss 0.6814)\tAcc 73.8281 (Avg-Acc 75.5354)\n",
            "Epoch: [40][76/78]\tTime 0.058 (Avg-Time 0.085)\t Loss 0.7315 (Avg-Loss 0.6880)\tAcc 74.6094 (Avg-Acc 75.1395)\n",
            "Epoch: [40][78/78]\tTime 0.020 (Avg-Time 0.084)\t Loss 1.0650 (Avg-Loss 0.6887)\tAcc 64.0625 (Avg-Acc 75.1350)\n",
            "EPOCH: 40 train Results: Acc 75.135 Loss: 0.6887\n",
            "Epoch: [40][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3647 (Avg-Loss 1.3647)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [40][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5173 (Avg-Loss 1.3881)\tAcc 53.3088 (Avg-Acc 55.1700)\n",
            "EPOCH: 40 Validation Results: Acc 55.170 Loss: 1.3881\n",
            "Epoch: [41][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.6653 (Avg-Loss 0.6653)\tAcc 77.5391 (Avg-Acc 77.5391)\n",
            "Epoch: [41][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5974 (Avg-Loss 0.6341)\tAcc 77.9297 (Avg-Acc 77.5586)\n",
            "Epoch: [41][38/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.7016 (Avg-Loss 0.6539)\tAcc 75.5859 (Avg-Acc 76.5124)\n",
            "Epoch: [41][57/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.6600 (Avg-Loss 0.6606)\tAcc 78.9062 (Avg-Acc 76.2224)\n",
            "Epoch: [41][76/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.7639 (Avg-Loss 0.6750)\tAcc 71.4844 (Avg-Acc 75.6773)\n",
            "Epoch: [41][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.9392 (Avg-Loss 0.6752)\tAcc 68.7500 (Avg-Acc 75.6425)\n",
            "EPOCH: 41 train Results: Acc 75.642 Loss: 0.6752\n",
            "Epoch: [41][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3932 (Avg-Loss 1.3932)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [41][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5377 (Avg-Loss 1.4048)\tAcc 52.9412 (Avg-Acc 54.9400)\n",
            "EPOCH: 41 Validation Results: Acc 54.940 Loss: 1.4048\n",
            "Epoch: [42][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.6395 (Avg-Loss 0.6395)\tAcc 78.7109 (Avg-Acc 78.7109)\n",
            "Epoch: [42][19/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.7101 (Avg-Loss 0.6434)\tAcc 75.9766 (Avg-Acc 77.3535)\n",
            "Epoch: [42][38/78]\tTime 0.158 (Avg-Time 0.084)\t Loss 0.6852 (Avg-Loss 0.6529)\tAcc 74.4141 (Avg-Acc 76.6426)\n",
            "Epoch: [42][57/78]\tTime 0.057 (Avg-Time 0.094)\t Loss 0.7509 (Avg-Loss 0.6632)\tAcc 72.0703 (Avg-Acc 76.2392)\n",
            "Epoch: [42][76/78]\tTime 0.062 (Avg-Time 0.086)\t Loss 0.6911 (Avg-Loss 0.6738)\tAcc 73.2422 (Avg-Acc 75.8092)\n",
            "Epoch: [42][78/78]\tTime 0.020 (Avg-Time 0.084)\t Loss 0.9925 (Avg-Loss 0.6741)\tAcc 71.8750 (Avg-Acc 75.8075)\n",
            "EPOCH: 42 train Results: Acc 75.808 Loss: 0.6741\n",
            "Epoch: [42][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3790 (Avg-Loss 1.3790)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [42][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.4791 (Avg-Loss 1.3905)\tAcc 52.9412 (Avg-Acc 55.0900)\n",
            "EPOCH: 42 Validation Results: Acc 55.090 Loss: 1.3905\n",
            "Epoch: [43][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.6409 (Avg-Loss 0.6409)\tAcc 76.7578 (Avg-Acc 76.7578)\n",
            "Epoch: [43][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.6531 (Avg-Loss 0.6421)\tAcc 75.3906 (Avg-Acc 77.0117)\n",
            "Epoch: [43][38/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.7253 (Avg-Loss 0.6585)\tAcc 73.2422 (Avg-Acc 76.3722)\n",
            "Epoch: [43][57/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.7043 (Avg-Loss 0.6621)\tAcc 74.6094 (Avg-Acc 76.2493)\n",
            "Epoch: [43][76/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.7209 (Avg-Loss 0.6678)\tAcc 75.3906 (Avg-Acc 76.0095)\n",
            "Epoch: [43][78/78]\tTime 0.023 (Avg-Time 0.060)\t Loss 0.8945 (Avg-Loss 0.6683)\tAcc 68.7500 (Avg-Acc 75.9900)\n",
            "EPOCH: 43 train Results: Acc 75.990 Loss: 0.6683\n",
            "Epoch: [43][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3832 (Avg-Loss 1.3832)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [43][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4785 (Avg-Loss 1.3950)\tAcc 54.0441 (Avg-Acc 55.1300)\n",
            "EPOCH: 43 Validation Results: Acc 55.130 Loss: 1.3950\n",
            "Epoch: [44][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.6044 (Avg-Loss 0.6044)\tAcc 82.0312 (Avg-Acc 82.0312)\n",
            "Epoch: [44][19/78]\tTime 0.073 (Avg-Time 0.061)\t Loss 0.6865 (Avg-Loss 0.6333)\tAcc 77.7344 (Avg-Acc 77.5293)\n",
            "Epoch: [44][38/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.6896 (Avg-Loss 0.6477)\tAcc 74.6094 (Avg-Acc 77.1284)\n",
            "Epoch: [44][57/78]\tTime 0.122 (Avg-Time 0.068)\t Loss 0.6681 (Avg-Loss 0.6532)\tAcc 74.4141 (Avg-Acc 76.6096)\n",
            "Epoch: [44][76/78]\tTime 0.061 (Avg-Time 0.087)\t Loss 0.7587 (Avg-Loss 0.6608)\tAcc 71.2891 (Avg-Acc 76.3038)\n",
            "Epoch: [44][78/78]\tTime 0.021 (Avg-Time 0.085)\t Loss 0.8071 (Avg-Loss 0.6610)\tAcc 68.7500 (Avg-Acc 76.2900)\n",
            "EPOCH: 44 train Results: Acc 76.290 Loss: 0.6610\n",
            "Epoch: [44][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3913 (Avg-Loss 1.3913)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [44][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4948 (Avg-Loss 1.4240)\tAcc 54.7794 (Avg-Acc 54.5100)\n",
            "EPOCH: 44 Validation Results: Acc 54.510 Loss: 1.4240\n",
            "Epoch: [45][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.5716 (Avg-Loss 0.5716)\tAcc 77.7344 (Avg-Acc 77.7344)\n",
            "Epoch: [45][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.6091 (Avg-Loss 0.6258)\tAcc 76.9531 (Avg-Acc 77.2852)\n",
            "Epoch: [45][38/78]\tTime 0.061 (Avg-Time 0.060)\t Loss 0.5824 (Avg-Loss 0.6282)\tAcc 78.5156 (Avg-Acc 77.4139)\n",
            "Epoch: [45][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.6147 (Avg-Loss 0.6440)\tAcc 78.1250 (Avg-Acc 76.8420)\n",
            "Epoch: [45][76/78]\tTime 0.063 (Avg-Time 0.060)\t Loss 0.6604 (Avg-Loss 0.6558)\tAcc 76.9531 (Avg-Acc 76.4331)\n",
            "Epoch: [45][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 0.7047 (Avg-Loss 0.6567)\tAcc 75.0000 (Avg-Acc 76.4100)\n",
            "EPOCH: 45 train Results: Acc 76.410 Loss: 0.6567\n",
            "Epoch: [45][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3828 (Avg-Loss 1.3828)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [45][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4648 (Avg-Loss 1.4194)\tAcc 54.7794 (Avg-Acc 54.8900)\n",
            "EPOCH: 45 Validation Results: Acc 54.890 Loss: 1.4194\n",
            "Epoch: [46][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.5326 (Avg-Loss 0.5326)\tAcc 81.8359 (Avg-Acc 81.8359)\n",
            "Epoch: [46][19/78]\tTime 0.060 (Avg-Time 0.059)\t Loss 0.6871 (Avg-Loss 0.6168)\tAcc 73.6328 (Avg-Acc 77.8418)\n",
            "Epoch: [46][38/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.6736 (Avg-Loss 0.6353)\tAcc 73.8281 (Avg-Acc 77.0483)\n",
            "Epoch: [46][57/78]\tTime 0.061 (Avg-Time 0.060)\t Loss 0.6686 (Avg-Loss 0.6438)\tAcc 76.5625 (Avg-Acc 76.7848)\n",
            "Epoch: [46][76/78]\tTime 0.109 (Avg-Time 0.063)\t Loss 0.6726 (Avg-Loss 0.6507)\tAcc 74.6094 (Avg-Acc 76.5650)\n",
            "Epoch: [46][78/78]\tTime 0.040 (Avg-Time 0.063)\t Loss 0.8387 (Avg-Loss 0.6514)\tAcc 73.4375 (Avg-Acc 76.5750)\n",
            "EPOCH: 46 train Results: Acc 76.575 Loss: 0.6514\n",
            "Epoch: [46][0/19]\tTime 0.045 (Avg-Time 0.045)\t Loss 1.3933 (Avg-Loss 1.3933)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [46][19/19]\tTime 0.021 (Avg-Time 0.050)\t Loss 1.5227 (Avg-Loss 1.4255)\tAcc 50.3676 (Avg-Acc 54.4000)\n",
            "EPOCH: 46 Validation Results: Acc 54.400 Loss: 1.4255\n",
            "Epoch: [47][0/78]\tTime 0.251 (Avg-Time 0.251)\t Loss 0.6795 (Avg-Loss 0.6795)\tAcc 75.0000 (Avg-Acc 75.0000)\n",
            "Epoch: [47][19/78]\tTime 0.059 (Avg-Time 0.112)\t Loss 0.7065 (Avg-Loss 0.6213)\tAcc 74.6094 (Avg-Acc 77.7832)\n",
            "Epoch: [47][38/78]\tTime 0.062 (Avg-Time 0.088)\t Loss 0.6282 (Avg-Loss 0.6235)\tAcc 76.3672 (Avg-Acc 77.5090)\n",
            "Epoch: [47][57/78]\tTime 0.056 (Avg-Time 0.079)\t Loss 0.6309 (Avg-Loss 0.6352)\tAcc 77.9297 (Avg-Acc 77.0710)\n",
            "Epoch: [47][76/78]\tTime 0.057 (Avg-Time 0.074)\t Loss 0.6307 (Avg-Loss 0.6456)\tAcc 76.7578 (Avg-Acc 76.6995)\n",
            "Epoch: [47][78/78]\tTime 0.020 (Avg-Time 0.073)\t Loss 0.7721 (Avg-Loss 0.6462)\tAcc 70.3125 (Avg-Acc 76.6500)\n",
            "EPOCH: 47 train Results: Acc 76.650 Loss: 0.6462\n",
            "Epoch: [47][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4109 (Avg-Loss 1.4109)\tAcc 54.1016 (Avg-Acc 54.1016)\n",
            "Epoch: [47][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4885 (Avg-Loss 1.4268)\tAcc 52.9412 (Avg-Acc 54.4300)\n",
            "EPOCH: 47 Validation Results: Acc 54.430 Loss: 1.4268\n",
            "Epoch: [48][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.6265 (Avg-Loss 0.6265)\tAcc 77.9297 (Avg-Acc 77.9297)\n",
            "Epoch: [48][19/78]\tTime 0.082 (Avg-Time 0.061)\t Loss 0.6093 (Avg-Loss 0.6027)\tAcc 76.7578 (Avg-Acc 78.5352)\n",
            "Epoch: [48][38/78]\tTime 0.062 (Avg-Time 0.061)\t Loss 0.6456 (Avg-Loss 0.6207)\tAcc 78.3203 (Avg-Acc 77.6893)\n",
            "Epoch: [48][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.6619 (Avg-Loss 0.6295)\tAcc 75.7812 (Avg-Acc 77.1989)\n",
            "Epoch: [48][76/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.6613 (Avg-Loss 0.6386)\tAcc 76.5625 (Avg-Acc 76.9709)\n",
            "Epoch: [48][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 1.1503 (Avg-Loss 0.6397)\tAcc 56.2500 (Avg-Acc 76.9325)\n",
            "EPOCH: 48 train Results: Acc 76.933 Loss: 0.6397\n",
            "Epoch: [48][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4228 (Avg-Loss 1.4228)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [48][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4649 (Avg-Loss 1.4351)\tAcc 52.2059 (Avg-Acc 54.5500)\n",
            "EPOCH: 48 Validation Results: Acc 54.550 Loss: 1.4351\n",
            "Epoch: [49][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.5657 (Avg-Loss 0.5657)\tAcc 80.8594 (Avg-Acc 80.8594)\n",
            "Epoch: [49][19/78]\tTime 0.226 (Avg-Time 0.109)\t Loss 0.5697 (Avg-Loss 0.6203)\tAcc 80.2734 (Avg-Acc 77.9688)\n",
            "Epoch: [49][38/78]\tTime 0.058 (Avg-Time 0.110)\t Loss 0.6581 (Avg-Loss 0.6160)\tAcc 76.7578 (Avg-Acc 78.0298)\n",
            "Epoch: [49][57/78]\tTime 0.060 (Avg-Time 0.094)\t Loss 0.6921 (Avg-Loss 0.6313)\tAcc 75.1953 (Avg-Acc 77.2899)\n",
            "Epoch: [49][76/78]\tTime 0.060 (Avg-Time 0.085)\t Loss 0.6242 (Avg-Loss 0.6420)\tAcc 79.2969 (Avg-Acc 76.8922)\n",
            "Epoch: [49][78/78]\tTime 0.020 (Avg-Time 0.084)\t Loss 0.6521 (Avg-Loss 0.6428)\tAcc 73.4375 (Avg-Acc 76.8550)\n",
            "EPOCH: 49 train Results: Acc 76.855 Loss: 0.6428\n",
            "Epoch: [49][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4131 (Avg-Loss 1.4131)\tAcc 56.0547 (Avg-Acc 56.0547)\n",
            "Epoch: [49][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.4863 (Avg-Loss 1.4307)\tAcc 52.5735 (Avg-Acc 54.5700)\n",
            "EPOCH: 49 Validation Results: Acc 54.570 Loss: 1.4307\n",
            "Epoch: [50][0/78]\tTime 0.063 (Avg-Time 0.063)\t Loss 0.5816 (Avg-Loss 0.5816)\tAcc 79.2969 (Avg-Acc 79.2969)\n",
            "Epoch: [50][19/78]\tTime 0.056 (Avg-Time 0.060)\t Loss 0.6130 (Avg-Loss 0.5875)\tAcc 78.5156 (Avg-Acc 79.0527)\n",
            "Epoch: [50][38/78]\tTime 0.056 (Avg-Time 0.059)\t Loss 0.5969 (Avg-Loss 0.6044)\tAcc 77.7344 (Avg-Acc 78.1851)\n",
            "Epoch: [50][57/78]\tTime 0.056 (Avg-Time 0.059)\t Loss 0.6094 (Avg-Loss 0.6192)\tAcc 76.1719 (Avg-Acc 77.6771)\n",
            "Epoch: [50][76/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.6139 (Avg-Loss 0.6316)\tAcc 78.3203 (Avg-Acc 77.3869)\n",
            "Epoch: [50][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 0.7123 (Avg-Loss 0.6322)\tAcc 68.7500 (Avg-Acc 77.3375)\n",
            "EPOCH: 50 train Results: Acc 77.338 Loss: 0.6322\n",
            "Epoch: [50][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4155 (Avg-Loss 1.4155)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [50][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4760 (Avg-Loss 1.4344)\tAcc 52.5735 (Avg-Acc 54.7300)\n",
            "EPOCH: 50 Validation Results: Acc 54.730 Loss: 1.4344\n",
            "Epoch: [51][0/78]\tTime 0.081 (Avg-Time 0.081)\t Loss 0.6016 (Avg-Loss 0.6016)\tAcc 79.4922 (Avg-Acc 79.4922)\n",
            "Epoch: [51][19/78]\tTime 0.056 (Avg-Time 0.061)\t Loss 0.5556 (Avg-Loss 0.6054)\tAcc 79.1016 (Avg-Acc 78.3496)\n",
            "Epoch: [51][38/78]\tTime 0.210 (Avg-Time 0.075)\t Loss 0.6022 (Avg-Loss 0.6111)\tAcc 79.1016 (Avg-Acc 78.0549)\n",
            "Epoch: [51][57/78]\tTime 0.057 (Avg-Time 0.095)\t Loss 0.6844 (Avg-Loss 0.6202)\tAcc 75.5859 (Avg-Acc 77.6839)\n",
            "Epoch: [51][76/78]\tTime 0.057 (Avg-Time 0.086)\t Loss 0.7487 (Avg-Loss 0.6287)\tAcc 74.8047 (Avg-Acc 77.3996)\n",
            "Epoch: [51][78/78]\tTime 0.021 (Avg-Time 0.085)\t Loss 0.8823 (Avg-Loss 0.6291)\tAcc 68.7500 (Avg-Acc 77.3600)\n",
            "EPOCH: 51 train Results: Acc 77.360 Loss: 0.6291\n",
            "Epoch: [51][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4251 (Avg-Loss 1.4251)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [51][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4544 (Avg-Loss 1.4305)\tAcc 53.6765 (Avg-Acc 54.8600)\n",
            "EPOCH: 51 Validation Results: Acc 54.860 Loss: 1.4305\n",
            "Epoch: [52][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.6653 (Avg-Loss 0.6653)\tAcc 75.9766 (Avg-Acc 75.9766)\n",
            "Epoch: [52][19/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.5608 (Avg-Loss 0.5929)\tAcc 78.5156 (Avg-Acc 78.2910)\n",
            "Epoch: [52][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.6070 (Avg-Loss 0.6053)\tAcc 78.9062 (Avg-Acc 77.9397)\n",
            "Epoch: [52][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.6265 (Avg-Loss 0.6184)\tAcc 78.9062 (Avg-Acc 77.6165)\n",
            "Epoch: [52][76/78]\tTime 0.080 (Avg-Time 0.060)\t Loss 0.6131 (Avg-Loss 0.6268)\tAcc 78.1250 (Avg-Acc 77.4452)\n",
            "Epoch: [52][78/78]\tTime 0.024 (Avg-Time 0.060)\t Loss 0.9593 (Avg-Loss 0.6278)\tAcc 67.1875 (Avg-Acc 77.4100)\n",
            "EPOCH: 52 train Results: Acc 77.410 Loss: 0.6278\n",
            "Epoch: [52][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4165 (Avg-Loss 1.4165)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [52][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4860 (Avg-Loss 1.4345)\tAcc 51.1029 (Avg-Acc 54.4700)\n",
            "EPOCH: 52 Validation Results: Acc 54.470 Loss: 1.4345\n",
            "Epoch: [53][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.5749 (Avg-Loss 0.5749)\tAcc 79.4922 (Avg-Acc 79.4922)\n",
            "Epoch: [53][19/78]\tTime 0.056 (Avg-Time 0.059)\t Loss 0.6911 (Avg-Loss 0.5909)\tAcc 75.0000 (Avg-Acc 78.7207)\n",
            "Epoch: [53][38/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.6743 (Avg-Loss 0.5981)\tAcc 75.3906 (Avg-Acc 78.4505)\n",
            "Epoch: [53][57/78]\tTime 0.116 (Avg-Time 0.062)\t Loss 0.6315 (Avg-Loss 0.6047)\tAcc 75.9766 (Avg-Acc 78.0745)\n",
            "Epoch: [53][76/78]\tTime 0.056 (Avg-Time 0.085)\t Loss 0.7188 (Avg-Loss 0.6159)\tAcc 74.8047 (Avg-Acc 77.7800)\n",
            "Epoch: [53][78/78]\tTime 0.020 (Avg-Time 0.084)\t Loss 1.0061 (Avg-Loss 0.6164)\tAcc 60.9375 (Avg-Acc 77.7525)\n",
            "EPOCH: 53 train Results: Acc 77.752 Loss: 0.6164\n",
            "Epoch: [53][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4240 (Avg-Loss 1.4240)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [53][19/19]\tTime 0.009 (Avg-Time 0.013)\t Loss 1.5087 (Avg-Loss 1.4479)\tAcc 55.1471 (Avg-Acc 54.7900)\n",
            "EPOCH: 53 Validation Results: Acc 54.790 Loss: 1.4479\n",
            "Epoch: [54][0/78]\tTime 0.057 (Avg-Time 0.057)\t Loss 0.6072 (Avg-Loss 0.6072)\tAcc 78.1250 (Avg-Acc 78.1250)\n",
            "Epoch: [54][19/78]\tTime 0.076 (Avg-Time 0.060)\t Loss 0.6628 (Avg-Loss 0.5825)\tAcc 75.5859 (Avg-Acc 78.8770)\n",
            "Epoch: [54][38/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.5558 (Avg-Loss 0.5952)\tAcc 79.8828 (Avg-Acc 78.4756)\n",
            "Epoch: [54][57/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.6404 (Avg-Loss 0.6038)\tAcc 76.1719 (Avg-Acc 77.9836)\n",
            "Epoch: [54][76/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.6293 (Avg-Loss 0.6132)\tAcc 78.3203 (Avg-Acc 77.6583)\n",
            "Epoch: [54][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.8483 (Avg-Loss 0.6150)\tAcc 68.7500 (Avg-Acc 77.5750)\n",
            "EPOCH: 54 train Results: Acc 77.575 Loss: 0.6150\n",
            "Epoch: [54][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4243 (Avg-Loss 1.4243)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [54][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4962 (Avg-Loss 1.4475)\tAcc 54.0441 (Avg-Acc 54.7200)\n",
            "EPOCH: 54 Validation Results: Acc 54.720 Loss: 1.4475\n",
            "Epoch: [55][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.5790 (Avg-Loss 0.5790)\tAcc 78.5156 (Avg-Acc 78.5156)\n",
            "Epoch: [55][19/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.6228 (Avg-Loss 0.5890)\tAcc 78.1250 (Avg-Acc 79.0137)\n",
            "Epoch: [55][38/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5650 (Avg-Loss 0.5876)\tAcc 80.2734 (Avg-Acc 78.9613)\n",
            "Epoch: [55][57/78]\tTime 0.079 (Avg-Time 0.060)\t Loss 0.6593 (Avg-Loss 0.5990)\tAcc 77.9297 (Avg-Acc 78.5560)\n",
            "Epoch: [55][76/78]\tTime 0.125 (Avg-Time 0.062)\t Loss 0.6144 (Avg-Loss 0.6050)\tAcc 76.1719 (Avg-Acc 78.1935)\n",
            "Epoch: [55][78/78]\tTime 0.034 (Avg-Time 0.062)\t Loss 1.0475 (Avg-Loss 0.6065)\tAcc 62.5000 (Avg-Acc 78.1250)\n",
            "EPOCH: 55 train Results: Acc 78.125 Loss: 0.6065\n",
            "Epoch: [55][0/19]\tTime 0.026 (Avg-Time 0.026)\t Loss 1.4352 (Avg-Loss 1.4352)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [55][19/19]\tTime 0.018 (Avg-Time 0.031)\t Loss 1.5065 (Avg-Loss 1.4659)\tAcc 52.2059 (Avg-Acc 54.9700)\n",
            "EPOCH: 55 Validation Results: Acc 54.970 Loss: 1.4659\n",
            "Epoch: [56][0/78]\tTime 0.126 (Avg-Time 0.126)\t Loss 0.5632 (Avg-Loss 0.5632)\tAcc 79.1016 (Avg-Acc 79.1016)\n",
            "Epoch: [56][19/78]\tTime 0.067 (Avg-Time 0.134)\t Loss 0.5444 (Avg-Loss 0.5870)\tAcc 80.8594 (Avg-Acc 78.9258)\n",
            "Epoch: [56][38/78]\tTime 0.061 (Avg-Time 0.099)\t Loss 0.6513 (Avg-Loss 0.5994)\tAcc 75.5859 (Avg-Acc 78.3804)\n",
            "Epoch: [56][57/78]\tTime 0.058 (Avg-Time 0.086)\t Loss 0.6097 (Avg-Loss 0.6011)\tAcc 78.5156 (Avg-Acc 78.1284)\n",
            "Epoch: [56][76/78]\tTime 0.058 (Avg-Time 0.080)\t Loss 0.5525 (Avg-Loss 0.6097)\tAcc 80.0781 (Avg-Acc 77.8333)\n",
            "Epoch: [56][78/78]\tTime 0.021 (Avg-Time 0.079)\t Loss 0.6874 (Avg-Loss 0.6125)\tAcc 75.0000 (Avg-Acc 77.7350)\n",
            "EPOCH: 56 train Results: Acc 77.735 Loss: 0.6125\n",
            "Epoch: [56][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4443 (Avg-Loss 1.4443)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [56][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.5103 (Avg-Loss 1.4698)\tAcc 51.4706 (Avg-Acc 54.8300)\n",
            "EPOCH: 56 Validation Results: Acc 54.830 Loss: 1.4698\n",
            "Epoch: [57][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.5603 (Avg-Loss 0.5603)\tAcc 79.4922 (Avg-Acc 79.4922)\n",
            "Epoch: [57][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.6224 (Avg-Loss 0.5863)\tAcc 76.5625 (Avg-Acc 78.7500)\n",
            "Epoch: [57][38/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.6431 (Avg-Loss 0.5920)\tAcc 76.3672 (Avg-Acc 78.6709)\n",
            "Epoch: [57][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.6732 (Avg-Loss 0.5966)\tAcc 76.1719 (Avg-Acc 78.5998)\n",
            "Epoch: [57][76/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5769 (Avg-Loss 0.6027)\tAcc 78.5156 (Avg-Acc 78.3939)\n",
            "Epoch: [57][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.5702 (Avg-Loss 0.6031)\tAcc 76.5625 (Avg-Acc 78.3700)\n",
            "EPOCH: 57 train Results: Acc 78.370 Loss: 0.6031\n",
            "Epoch: [57][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5058 (Avg-Loss 1.5058)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [57][19/19]\tTime 0.007 (Avg-Time 0.015)\t Loss 1.4855 (Avg-Loss 1.4662)\tAcc 52.2059 (Avg-Acc 54.7400)\n",
            "EPOCH: 57 Validation Results: Acc 54.740 Loss: 1.4662\n",
            "Epoch: [58][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.6064 (Avg-Loss 0.6064)\tAcc 77.9297 (Avg-Acc 77.9297)\n",
            "Epoch: [58][19/78]\tTime 0.333 (Avg-Time 0.094)\t Loss 0.5876 (Avg-Loss 0.5688)\tAcc 76.5625 (Avg-Acc 79.4629)\n",
            "Epoch: [58][38/78]\tTime 0.056 (Avg-Time 0.113)\t Loss 0.5786 (Avg-Loss 0.5761)\tAcc 77.3438 (Avg-Acc 79.0315)\n",
            "Epoch: [58][57/78]\tTime 0.058 (Avg-Time 0.095)\t Loss 0.6204 (Avg-Loss 0.5806)\tAcc 78.3203 (Avg-Acc 78.9062)\n",
            "Epoch: [58][76/78]\tTime 0.058 (Avg-Time 0.087)\t Loss 0.6389 (Avg-Loss 0.5963)\tAcc 77.1484 (Avg-Acc 78.3787)\n",
            "Epoch: [58][78/78]\tTime 0.020 (Avg-Time 0.086)\t Loss 0.7069 (Avg-Loss 0.5971)\tAcc 76.5625 (Avg-Acc 78.3650)\n",
            "EPOCH: 58 train Results: Acc 78.365 Loss: 0.5971\n",
            "Epoch: [58][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4821 (Avg-Loss 1.4821)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [58][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4835 (Avg-Loss 1.4674)\tAcc 50.3676 (Avg-Acc 54.6800)\n",
            "EPOCH: 58 Validation Results: Acc 54.680 Loss: 1.4674\n",
            "Epoch: [59][0/78]\tTime 0.064 (Avg-Time 0.064)\t Loss 0.6124 (Avg-Loss 0.6124)\tAcc 77.7344 (Avg-Acc 77.7344)\n",
            "Epoch: [59][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5778 (Avg-Loss 0.5836)\tAcc 78.5156 (Avg-Acc 78.5742)\n",
            "Epoch: [59][38/78]\tTime 0.082 (Avg-Time 0.061)\t Loss 0.6320 (Avg-Loss 0.5899)\tAcc 77.1484 (Avg-Acc 78.5958)\n",
            "Epoch: [59][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.6381 (Avg-Loss 0.5924)\tAcc 76.1719 (Avg-Acc 78.5426)\n",
            "Epoch: [59][76/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5902 (Avg-Loss 0.5980)\tAcc 78.7109 (Avg-Acc 78.4192)\n",
            "Epoch: [59][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.6995 (Avg-Loss 0.5981)\tAcc 75.0000 (Avg-Acc 78.4400)\n",
            "EPOCH: 59 train Results: Acc 78.440 Loss: 0.5981\n",
            "Epoch: [59][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4826 (Avg-Loss 1.4826)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [59][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4641 (Avg-Loss 1.4625)\tAcc 54.0441 (Avg-Acc 54.6200)\n",
            "EPOCH: 59 Validation Results: Acc 54.620 Loss: 1.4625\n",
            "Epoch: [60][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.5809 (Avg-Loss 0.5809)\tAcc 79.8828 (Avg-Acc 79.8828)\n",
            "Epoch: [60][19/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.5296 (Avg-Loss 0.5648)\tAcc 81.8359 (Avg-Acc 79.3555)\n",
            "Epoch: [60][38/78]\tTime 0.247 (Avg-Time 0.083)\t Loss 0.6141 (Avg-Loss 0.5687)\tAcc 79.2969 (Avg-Acc 79.2618)\n",
            "Epoch: [60][57/78]\tTime 0.058 (Avg-Time 0.095)\t Loss 0.6410 (Avg-Loss 0.5793)\tAcc 77.5391 (Avg-Acc 78.9770)\n",
            "Epoch: [60][76/78]\tTime 0.058 (Avg-Time 0.086)\t Loss 0.6606 (Avg-Loss 0.5932)\tAcc 74.4141 (Avg-Acc 78.4725)\n",
            "Epoch: [60][78/78]\tTime 0.022 (Avg-Time 0.085)\t Loss 1.0425 (Avg-Loss 0.5950)\tAcc 62.5000 (Avg-Acc 78.4400)\n",
            "EPOCH: 60 train Results: Acc 78.440 Loss: 0.5950\n",
            "Epoch: [60][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.4855 (Avg-Loss 1.4855)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [60][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.4581 (Avg-Loss 1.4599)\tAcc 54.7794 (Avg-Acc 54.7900)\n",
            "EPOCH: 60 Validation Results: Acc 54.790 Loss: 1.4599\n",
            "Epoch: [61][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.6127 (Avg-Loss 0.6127)\tAcc 77.9297 (Avg-Acc 77.9297)\n",
            "Epoch: [61][19/78]\tTime 0.068 (Avg-Time 0.061)\t Loss 0.6196 (Avg-Loss 0.5715)\tAcc 78.5156 (Avg-Acc 79.3652)\n",
            "Epoch: [61][38/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.5712 (Avg-Loss 0.5865)\tAcc 79.2969 (Avg-Acc 78.9814)\n",
            "Epoch: [61][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.5986 (Avg-Loss 0.5951)\tAcc 77.3438 (Avg-Acc 78.6301)\n",
            "Epoch: [61][76/78]\tTime 0.063 (Avg-Time 0.061)\t Loss 0.6676 (Avg-Loss 0.5997)\tAcc 75.9766 (Avg-Acc 78.4877)\n",
            "Epoch: [61][78/78]\tTime 0.020 (Avg-Time 0.061)\t Loss 0.9922 (Avg-Loss 0.6003)\tAcc 65.6250 (Avg-Acc 78.4275)\n",
            "EPOCH: 61 train Results: Acc 78.427 Loss: 0.6003\n",
            "Epoch: [61][0/19]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4893 (Avg-Loss 1.4893)\tAcc 54.6875 (Avg-Acc 54.6875)\n",
            "Epoch: [61][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.4803 (Avg-Loss 1.4662)\tAcc 54.4118 (Avg-Acc 54.8300)\n",
            "EPOCH: 61 Validation Results: Acc 54.830 Loss: 1.4662\n",
            "Epoch: [62][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4608 (Avg-Loss 0.4608)\tAcc 82.4219 (Avg-Acc 82.4219)\n",
            "Epoch: [62][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.6164 (Avg-Loss 0.5717)\tAcc 78.7109 (Avg-Acc 79.2578)\n",
            "Epoch: [62][38/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5077 (Avg-Loss 0.5806)\tAcc 81.8359 (Avg-Acc 79.1416)\n",
            "Epoch: [62][57/78]\tTime 0.282 (Avg-Time 0.073)\t Loss 0.6046 (Avg-Loss 0.5834)\tAcc 78.9062 (Avg-Acc 79.1487)\n",
            "Epoch: [62][76/78]\tTime 0.058 (Avg-Time 0.086)\t Loss 0.6524 (Avg-Loss 0.5906)\tAcc 76.3672 (Avg-Acc 78.7744)\n",
            "Epoch: [62][78/78]\tTime 0.023 (Avg-Time 0.085)\t Loss 0.7557 (Avg-Loss 0.5913)\tAcc 76.5625 (Avg-Acc 78.7300)\n",
            "EPOCH: 62 train Results: Acc 78.730 Loss: 0.5913\n",
            "Epoch: [62][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4698 (Avg-Loss 1.4698)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [62][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4724 (Avg-Loss 1.4658)\tAcc 53.3088 (Avg-Acc 54.9600)\n",
            "EPOCH: 62 Validation Results: Acc 54.960 Loss: 1.4658\n",
            "Epoch: [63][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4381 (Avg-Loss 0.4381)\tAcc 83.9844 (Avg-Acc 83.9844)\n",
            "Epoch: [63][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5798 (Avg-Loss 0.5441)\tAcc 79.6875 (Avg-Acc 80.4199)\n",
            "Epoch: [63][38/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5748 (Avg-Loss 0.5567)\tAcc 76.5625 (Avg-Acc 79.8978)\n",
            "Epoch: [63][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.6393 (Avg-Loss 0.5683)\tAcc 79.2969 (Avg-Acc 79.5562)\n",
            "Epoch: [63][76/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.6240 (Avg-Loss 0.5743)\tAcc 77.3438 (Avg-Acc 79.3628)\n",
            "Epoch: [63][78/78]\tTime 0.020 (Avg-Time 0.061)\t Loss 0.5660 (Avg-Loss 0.5744)\tAcc 79.6875 (Avg-Acc 79.3475)\n",
            "EPOCH: 63 train Results: Acc 79.347 Loss: 0.5744\n",
            "Epoch: [63][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4709 (Avg-Loss 1.4709)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [63][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4650 (Avg-Loss 1.4835)\tAcc 52.9412 (Avg-Acc 54.9000)\n",
            "EPOCH: 63 Validation Results: Acc 54.900 Loss: 1.4835\n",
            "Epoch: [64][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.5479 (Avg-Loss 0.5479)\tAcc 79.6875 (Avg-Acc 79.6875)\n",
            "Epoch: [64][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.6306 (Avg-Loss 0.5532)\tAcc 77.7344 (Avg-Acc 80.3613)\n",
            "Epoch: [64][38/78]\tTime 0.062 (Avg-Time 0.061)\t Loss 0.6164 (Avg-Loss 0.5650)\tAcc 78.5156 (Avg-Acc 79.7676)\n",
            "Epoch: [64][57/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.6944 (Avg-Loss 0.5747)\tAcc 77.9297 (Avg-Acc 79.4922)\n",
            "Epoch: [64][76/78]\tTime 0.171 (Avg-Time 0.069)\t Loss 0.5997 (Avg-Loss 0.5756)\tAcc 78.1250 (Avg-Acc 79.3933)\n",
            "Epoch: [64][78/78]\tTime 0.049 (Avg-Time 0.071)\t Loss 0.8775 (Avg-Loss 0.5780)\tAcc 71.8750 (Avg-Acc 79.3225)\n",
            "EPOCH: 64 train Results: Acc 79.323 Loss: 0.5780\n",
            "Epoch: [64][0/19]\tTime 0.079 (Avg-Time 0.079)\t Loss 1.4489 (Avg-Loss 1.4489)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [64][19/19]\tTime 0.010 (Avg-Time 0.055)\t Loss 1.4802 (Avg-Loss 1.4786)\tAcc 55.1471 (Avg-Acc 54.8100)\n",
            "EPOCH: 64 Validation Results: Acc 54.810 Loss: 1.4786\n",
            "Epoch: [65][0/78]\tTime 0.130 (Avg-Time 0.130)\t Loss 0.5514 (Avg-Loss 0.5514)\tAcc 81.4453 (Avg-Acc 81.4453)\n",
            "Epoch: [65][19/78]\tTime 0.058 (Avg-Time 0.083)\t Loss 0.5470 (Avg-Loss 0.5592)\tAcc 79.1016 (Avg-Acc 79.5703)\n",
            "Epoch: [65][38/78]\tTime 0.059 (Avg-Time 0.073)\t Loss 0.5479 (Avg-Loss 0.5659)\tAcc 77.9297 (Avg-Acc 79.3169)\n",
            "Epoch: [65][57/78]\tTime 0.059 (Avg-Time 0.069)\t Loss 0.5500 (Avg-Loss 0.5689)\tAcc 80.0781 (Avg-Acc 79.3070)\n",
            "Epoch: [65][76/78]\tTime 0.059 (Avg-Time 0.067)\t Loss 0.6111 (Avg-Loss 0.5766)\tAcc 78.7109 (Avg-Acc 79.1295)\n",
            "Epoch: [65][78/78]\tTime 0.021 (Avg-Time 0.067)\t Loss 0.7637 (Avg-Loss 0.5769)\tAcc 81.2500 (Avg-Acc 79.1350)\n",
            "EPOCH: 65 train Results: Acc 79.135 Loss: 0.5769\n",
            "Epoch: [65][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4698 (Avg-Loss 1.4698)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [65][19/19]\tTime 0.007 (Avg-Time 0.015)\t Loss 1.4669 (Avg-Loss 1.4827)\tAcc 55.1471 (Avg-Acc 54.5000)\n",
            "EPOCH: 65 Validation Results: Acc 54.500 Loss: 1.4827\n",
            "Epoch: [66][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.5861 (Avg-Loss 0.5861)\tAcc 78.9062 (Avg-Acc 78.9062)\n",
            "Epoch: [66][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5066 (Avg-Loss 0.5501)\tAcc 82.2266 (Avg-Acc 80.2832)\n",
            "Epoch: [66][38/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.5266 (Avg-Loss 0.5540)\tAcc 80.4688 (Avg-Acc 80.1032)\n",
            "Epoch: [66][57/78]\tTime 0.063 (Avg-Time 0.061)\t Loss 0.5241 (Avg-Loss 0.5603)\tAcc 80.0781 (Avg-Acc 79.8188)\n",
            "Epoch: [66][76/78]\tTime 0.060 (Avg-Time 0.062)\t Loss 0.5783 (Avg-Loss 0.5677)\tAcc 78.7109 (Avg-Acc 79.5353)\n",
            "Epoch: [66][78/78]\tTime 0.020 (Avg-Time 0.061)\t Loss 0.7154 (Avg-Loss 0.5680)\tAcc 70.3125 (Avg-Acc 79.5125)\n",
            "EPOCH: 66 train Results: Acc 79.513 Loss: 0.5680\n",
            "Epoch: [66][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4770 (Avg-Loss 1.4770)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [66][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.5093 (Avg-Loss 1.4887)\tAcc 54.4118 (Avg-Acc 54.5300)\n",
            "EPOCH: 66 Validation Results: Acc 54.530 Loss: 1.4887\n",
            "Epoch: [67][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.5520 (Avg-Loss 0.5520)\tAcc 78.9062 (Avg-Acc 78.9062)\n",
            "Epoch: [67][19/78]\tTime 0.096 (Avg-Time 0.153)\t Loss 0.5047 (Avg-Loss 0.5377)\tAcc 80.6641 (Avg-Acc 80.3613)\n",
            "Epoch: [67][38/78]\tTime 0.058 (Avg-Time 0.114)\t Loss 0.6983 (Avg-Loss 0.5474)\tAcc 75.3906 (Avg-Acc 80.2083)\n",
            "Epoch: [67][57/78]\tTime 0.061 (Avg-Time 0.097)\t Loss 0.5675 (Avg-Loss 0.5589)\tAcc 78.1250 (Avg-Acc 79.6841)\n",
            "Epoch: [67][76/78]\tTime 0.057 (Avg-Time 0.088)\t Loss 0.5812 (Avg-Loss 0.5674)\tAcc 78.9062 (Avg-Acc 79.4516)\n",
            "Epoch: [67][78/78]\tTime 0.020 (Avg-Time 0.087)\t Loss 0.9435 (Avg-Loss 0.5683)\tAcc 64.0625 (Avg-Acc 79.3875)\n",
            "EPOCH: 67 train Results: Acc 79.388 Loss: 0.5683\n",
            "Epoch: [67][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4929 (Avg-Loss 1.4929)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [67][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5237 (Avg-Loss 1.5083)\tAcc 53.6765 (Avg-Acc 54.8800)\n",
            "EPOCH: 67 Validation Results: Acc 54.880 Loss: 1.5083\n",
            "Epoch: [68][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.5179 (Avg-Loss 0.5179)\tAcc 81.0547 (Avg-Acc 81.0547)\n",
            "Epoch: [68][19/78]\tTime 0.078 (Avg-Time 0.061)\t Loss 0.6137 (Avg-Loss 0.5523)\tAcc 79.4922 (Avg-Acc 80.4785)\n",
            "Epoch: [68][38/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.5906 (Avg-Loss 0.5511)\tAcc 79.2969 (Avg-Acc 80.2935)\n",
            "Epoch: [68][57/78]\tTime 0.065 (Avg-Time 0.062)\t Loss 0.6048 (Avg-Loss 0.5615)\tAcc 79.8828 (Avg-Acc 79.8727)\n",
            "Epoch: [68][76/78]\tTime 0.059 (Avg-Time 0.062)\t Loss 0.6046 (Avg-Loss 0.5645)\tAcc 78.5156 (Avg-Acc 79.7433)\n",
            "Epoch: [68][78/78]\tTime 0.021 (Avg-Time 0.061)\t Loss 0.4972 (Avg-Loss 0.5654)\tAcc 82.8125 (Avg-Acc 79.7125)\n",
            "EPOCH: 68 train Results: Acc 79.713 Loss: 0.5654\n",
            "Epoch: [68][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4811 (Avg-Loss 1.4811)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [68][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5604 (Avg-Loss 1.5158)\tAcc 52.9412 (Avg-Acc 54.8200)\n",
            "EPOCH: 68 Validation Results: Acc 54.820 Loss: 1.5158\n",
            "Epoch: [69][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.5070 (Avg-Loss 0.5070)\tAcc 81.0547 (Avg-Acc 81.0547)\n",
            "Epoch: [69][19/78]\tTime 0.060 (Avg-Time 0.061)\t Loss 0.5548 (Avg-Loss 0.5379)\tAcc 78.7109 (Avg-Acc 80.4883)\n",
            "Epoch: [69][38/78]\tTime 0.099 (Avg-Time 0.101)\t Loss 0.6446 (Avg-Loss 0.5433)\tAcc 77.3438 (Avg-Acc 80.3736)\n",
            "Epoch: [69][57/78]\tTime 0.063 (Avg-Time 0.095)\t Loss 0.5450 (Avg-Loss 0.5533)\tAcc 81.8359 (Avg-Acc 80.0512)\n",
            "Epoch: [69][76/78]\tTime 0.058 (Avg-Time 0.087)\t Loss 0.6904 (Avg-Loss 0.5671)\tAcc 74.6094 (Avg-Acc 79.4744)\n",
            "Epoch: [69][78/78]\tTime 0.020 (Avg-Time 0.086)\t Loss 1.2128 (Avg-Loss 0.5684)\tAcc 65.6250 (Avg-Acc 79.4075)\n",
            "EPOCH: 69 train Results: Acc 79.407 Loss: 0.5684\n",
            "Epoch: [69][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.4580 (Avg-Loss 1.4580)\tAcc 55.2734 (Avg-Acc 55.2734)\n",
            "Epoch: [69][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5209 (Avg-Loss 1.4882)\tAcc 52.9412 (Avg-Acc 54.4200)\n",
            "EPOCH: 69 Validation Results: Acc 54.420 Loss: 1.4882\n",
            "Epoch: [70][0/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.5863 (Avg-Loss 0.5863)\tAcc 79.4922 (Avg-Acc 79.4922)\n",
            "Epoch: [70][19/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.5400 (Avg-Loss 0.5474)\tAcc 82.4219 (Avg-Acc 80.3516)\n",
            "Epoch: [70][38/78]\tTime 0.066 (Avg-Time 0.061)\t Loss 0.5668 (Avg-Loss 0.5506)\tAcc 81.6406 (Avg-Acc 80.2384)\n",
            "Epoch: [70][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.5745 (Avg-Loss 0.5561)\tAcc 80.6641 (Avg-Acc 80.0310)\n",
            "Epoch: [70][76/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5702 (Avg-Loss 0.5661)\tAcc 80.4688 (Avg-Acc 79.5987)\n",
            "Epoch: [70][78/78]\tTime 0.021 (Avg-Time 0.060)\t Loss 0.8611 (Avg-Loss 0.5671)\tAcc 70.3125 (Avg-Acc 79.5650)\n",
            "EPOCH: 70 train Results: Acc 79.565 Loss: 0.5671\n",
            "Epoch: [70][0/19]\tTime 0.018 (Avg-Time 0.018)\t Loss 1.4575 (Avg-Loss 1.4575)\tAcc 54.8828 (Avg-Acc 54.8828)\n",
            "Epoch: [70][19/19]\tTime 0.009 (Avg-Time 0.014)\t Loss 1.5279 (Avg-Loss 1.4960)\tAcc 54.4118 (Avg-Acc 54.6800)\n",
            "EPOCH: 70 Validation Results: Acc 54.680 Loss: 1.4960\n",
            "Epoch: [71][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4560 (Avg-Loss 0.4560)\tAcc 82.8125 (Avg-Acc 82.8125)\n",
            "Epoch: [71][19/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.5000 (Avg-Loss 0.5272)\tAcc 82.8125 (Avg-Acc 81.1035)\n",
            "Epoch: [71][38/78]\tTime 0.063 (Avg-Time 0.061)\t Loss 0.5625 (Avg-Loss 0.5388)\tAcc 80.0781 (Avg-Acc 80.6340)\n",
            "Epoch: [71][57/78]\tTime 0.120 (Avg-Time 0.090)\t Loss 0.6037 (Avg-Loss 0.5490)\tAcc 80.0781 (Avg-Acc 80.3576)\n",
            "Epoch: [71][76/78]\tTime 0.057 (Avg-Time 0.087)\t Loss 0.6032 (Avg-Loss 0.5605)\tAcc 79.2969 (Avg-Acc 79.9741)\n",
            "Epoch: [71][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.8421 (Avg-Loss 0.5606)\tAcc 71.8750 (Avg-Acc 79.9925)\n",
            "EPOCH: 71 train Results: Acc 79.993 Loss: 0.5606\n",
            "Epoch: [71][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4645 (Avg-Loss 1.4645)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [71][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5391 (Avg-Loss 1.4950)\tAcc 52.9412 (Avg-Acc 54.5300)\n",
            "EPOCH: 71 Validation Results: Acc 54.530 Loss: 1.4950\n",
            "Epoch: [72][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.5048 (Avg-Loss 0.5048)\tAcc 80.4688 (Avg-Acc 80.4688)\n",
            "Epoch: [72][19/78]\tTime 0.071 (Avg-Time 0.061)\t Loss 0.5329 (Avg-Loss 0.5363)\tAcc 81.4453 (Avg-Acc 80.8594)\n",
            "Epoch: [72][38/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5299 (Avg-Loss 0.5469)\tAcc 80.8594 (Avg-Acc 80.6891)\n",
            "Epoch: [72][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.7133 (Avg-Loss 0.5533)\tAcc 74.6094 (Avg-Acc 80.3812)\n",
            "Epoch: [72][76/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5031 (Avg-Loss 0.5571)\tAcc 83.3984 (Avg-Acc 80.2557)\n",
            "Epoch: [72][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.7764 (Avg-Loss 0.5586)\tAcc 71.8750 (Avg-Acc 80.1900)\n",
            "EPOCH: 72 train Results: Acc 80.190 Loss: 0.5586\n",
            "Epoch: [72][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4827 (Avg-Loss 1.4827)\tAcc 55.0781 (Avg-Acc 55.0781)\n",
            "Epoch: [72][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5844 (Avg-Loss 1.5104)\tAcc 50.7353 (Avg-Acc 54.6500)\n",
            "EPOCH: 72 Validation Results: Acc 54.650 Loss: 1.5104\n",
            "Epoch: [73][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.5595 (Avg-Loss 0.5595)\tAcc 80.4688 (Avg-Acc 80.4688)\n",
            "Epoch: [73][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5431 (Avg-Loss 0.5227)\tAcc 79.4922 (Avg-Acc 81.7188)\n",
            "Epoch: [73][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5700 (Avg-Loss 0.5305)\tAcc 79.4922 (Avg-Acc 81.0847)\n",
            "Epoch: [73][57/78]\tTime 0.085 (Avg-Time 0.060)\t Loss 0.5202 (Avg-Loss 0.5447)\tAcc 81.6406 (Avg-Acc 80.5024)\n",
            "Epoch: [73][76/78]\tTime 0.144 (Avg-Time 0.081)\t Loss 0.6161 (Avg-Loss 0.5500)\tAcc 79.4922 (Avg-Acc 80.2481)\n",
            "Epoch: [73][78/78]\tTime 0.048 (Avg-Time 0.081)\t Loss 0.7832 (Avg-Loss 0.5520)\tAcc 73.4375 (Avg-Acc 80.1825)\n",
            "EPOCH: 73 train Results: Acc 80.183 Loss: 0.5520\n",
            "Epoch: [73][0/19]\tTime 0.030 (Avg-Time 0.030)\t Loss 1.5134 (Avg-Loss 1.5134)\tAcc 55.0781 (Avg-Acc 55.0781)\n",
            "Epoch: [73][19/19]\tTime 0.019 (Avg-Time 0.026)\t Loss 1.5632 (Avg-Loss 1.5170)\tAcc 54.0441 (Avg-Acc 54.5400)\n",
            "EPOCH: 73 Validation Results: Acc 54.540 Loss: 1.5170\n",
            "Epoch: [74][0/78]\tTime 0.112 (Avg-Time 0.112)\t Loss 0.4682 (Avg-Loss 0.4682)\tAcc 83.9844 (Avg-Acc 83.9844)\n",
            "Epoch: [74][19/78]\tTime 0.060 (Avg-Time 0.066)\t Loss 0.5130 (Avg-Loss 0.5284)\tAcc 80.6641 (Avg-Acc 80.9277)\n",
            "Epoch: [74][38/78]\tTime 0.059 (Avg-Time 0.064)\t Loss 0.5609 (Avg-Loss 0.5433)\tAcc 79.4922 (Avg-Acc 80.4838)\n",
            "Epoch: [74][57/78]\tTime 0.065 (Avg-Time 0.063)\t Loss 0.5419 (Avg-Loss 0.5530)\tAcc 80.8594 (Avg-Acc 80.2297)\n",
            "Epoch: [74][76/78]\tTime 0.058 (Avg-Time 0.062)\t Loss 0.4843 (Avg-Loss 0.5546)\tAcc 82.6172 (Avg-Acc 80.0147)\n",
            "Epoch: [74][78/78]\tTime 0.020 (Avg-Time 0.062)\t Loss 0.7556 (Avg-Loss 0.5551)\tAcc 73.4375 (Avg-Acc 79.9850)\n",
            "EPOCH: 74 train Results: Acc 79.985 Loss: 0.5551\n",
            "Epoch: [74][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5191 (Avg-Loss 1.5191)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [74][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.5363 (Avg-Loss 1.5158)\tAcc 54.7794 (Avg-Acc 54.5600)\n",
            "EPOCH: 74 Validation Results: Acc 54.560 Loss: 1.5158\n",
            "Epoch: [75][0/78]\tTime 0.069 (Avg-Time 0.069)\t Loss 0.4967 (Avg-Loss 0.4967)\tAcc 81.2500 (Avg-Acc 81.2500)\n",
            "Epoch: [75][19/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.5320 (Avg-Loss 0.5323)\tAcc 80.8594 (Avg-Acc 80.9375)\n",
            "Epoch: [75][38/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5470 (Avg-Loss 0.5347)\tAcc 77.3438 (Avg-Acc 81.0647)\n",
            "Epoch: [75][57/78]\tTime 0.062 (Avg-Time 0.061)\t Loss 0.5375 (Avg-Loss 0.5392)\tAcc 80.0781 (Avg-Acc 80.7280)\n",
            "Epoch: [75][76/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.6274 (Avg-Loss 0.5471)\tAcc 79.4922 (Avg-Acc 80.4713)\n",
            "Epoch: [75][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.9520 (Avg-Loss 0.5475)\tAcc 73.4375 (Avg-Acc 80.4775)\n",
            "EPOCH: 75 train Results: Acc 80.478 Loss: 0.5475\n",
            "Epoch: [75][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4919 (Avg-Loss 1.4919)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [75][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.5476 (Avg-Loss 1.5230)\tAcc 55.1471 (Avg-Acc 54.9300)\n",
            "EPOCH: 75 Validation Results: Acc 54.930 Loss: 1.5230\n",
            "Epoch: [76][0/78]\tTime 0.120 (Avg-Time 0.120)\t Loss 0.4855 (Avg-Loss 0.4855)\tAcc 83.0078 (Avg-Acc 83.0078)\n",
            "Epoch: [76][19/78]\tTime 0.142 (Avg-Time 0.156)\t Loss 0.5959 (Avg-Loss 0.5386)\tAcc 77.1484 (Avg-Acc 80.6250)\n",
            "Epoch: [76][38/78]\tTime 0.059 (Avg-Time 0.112)\t Loss 0.5124 (Avg-Loss 0.5372)\tAcc 83.7891 (Avg-Acc 80.8594)\n",
            "Epoch: [76][57/78]\tTime 0.061 (Avg-Time 0.096)\t Loss 0.5377 (Avg-Loss 0.5419)\tAcc 81.2500 (Avg-Acc 80.6001)\n",
            "Epoch: [76][76/78]\tTime 0.061 (Avg-Time 0.087)\t Loss 0.6040 (Avg-Loss 0.5506)\tAcc 77.5391 (Avg-Acc 80.2608)\n",
            "Epoch: [76][78/78]\tTime 0.020 (Avg-Time 0.086)\t Loss 0.6912 (Avg-Loss 0.5508)\tAcc 76.5625 (Avg-Acc 80.2400)\n",
            "EPOCH: 76 train Results: Acc 80.240 Loss: 0.5508\n",
            "Epoch: [76][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4599 (Avg-Loss 1.4599)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [76][19/19]\tTime 0.007 (Avg-Time 0.016)\t Loss 1.4975 (Avg-Loss 1.5030)\tAcc 55.1471 (Avg-Acc 54.8900)\n",
            "EPOCH: 76 Validation Results: Acc 54.890 Loss: 1.5030\n",
            "Epoch: [77][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.5578 (Avg-Loss 0.5578)\tAcc 81.8359 (Avg-Acc 81.8359)\n",
            "Epoch: [77][19/78]\tTime 0.082 (Avg-Time 0.062)\t Loss 0.5241 (Avg-Loss 0.5357)\tAcc 80.6641 (Avg-Acc 80.9766)\n",
            "Epoch: [77][38/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.5838 (Avg-Loss 0.5358)\tAcc 78.7109 (Avg-Acc 80.7592)\n",
            "Epoch: [77][57/78]\tTime 0.059 (Avg-Time 0.062)\t Loss 0.5769 (Avg-Loss 0.5435)\tAcc 79.1016 (Avg-Acc 80.5159)\n",
            "Epoch: [77][76/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5125 (Avg-Loss 0.5490)\tAcc 80.6641 (Avg-Acc 80.3166)\n",
            "Epoch: [77][78/78]\tTime 0.021 (Avg-Time 0.061)\t Loss 0.7991 (Avg-Loss 0.5495)\tAcc 70.3125 (Avg-Acc 80.2975)\n",
            "EPOCH: 77 train Results: Acc 80.297 Loss: 0.5495\n",
            "Epoch: [77][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.4831 (Avg-Loss 1.4831)\tAcc 54.6875 (Avg-Acc 54.6875)\n",
            "Epoch: [77][19/19]\tTime 0.008 (Avg-Time 0.013)\t Loss 1.5415 (Avg-Loss 1.5195)\tAcc 56.2500 (Avg-Acc 54.8200)\n",
            "EPOCH: 77 Validation Results: Acc 54.820 Loss: 1.5195\n",
            "Epoch: [78][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4987 (Avg-Loss 0.4987)\tAcc 82.4219 (Avg-Acc 82.4219)\n",
            "Epoch: [78][19/78]\tTime 0.113 (Avg-Time 0.066)\t Loss 0.5451 (Avg-Loss 0.5222)\tAcc 78.9062 (Avg-Acc 81.5918)\n",
            "Epoch: [78][38/78]\tTime 0.062 (Avg-Time 0.113)\t Loss 0.5188 (Avg-Loss 0.5331)\tAcc 80.0781 (Avg-Acc 80.9595)\n",
            "Epoch: [78][57/78]\tTime 0.065 (Avg-Time 0.097)\t Loss 0.6167 (Avg-Loss 0.5407)\tAcc 78.5156 (Avg-Acc 80.8627)\n",
            "Epoch: [78][76/78]\tTime 0.058 (Avg-Time 0.088)\t Loss 0.5909 (Avg-Loss 0.5466)\tAcc 80.4688 (Avg-Acc 80.5804)\n",
            "Epoch: [78][78/78]\tTime 0.020 (Avg-Time 0.087)\t Loss 0.5790 (Avg-Loss 0.5482)\tAcc 76.5625 (Avg-Acc 80.5100)\n",
            "EPOCH: 78 train Results: Acc 80.510 Loss: 0.5482\n",
            "Epoch: [78][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4536 (Avg-Loss 1.4536)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [78][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5268 (Avg-Loss 1.5031)\tAcc 55.5147 (Avg-Acc 55.0300)\n",
            "EPOCH: 78 Validation Results: Acc 55.030 Loss: 1.5031\n",
            "Epoch: [79][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4672 (Avg-Loss 0.4672)\tAcc 83.3984 (Avg-Acc 83.3984)\n",
            "Epoch: [79][19/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.5047 (Avg-Loss 0.5053)\tAcc 82.8125 (Avg-Acc 81.9629)\n",
            "Epoch: [79][38/78]\tTime 0.062 (Avg-Time 0.061)\t Loss 0.5772 (Avg-Loss 0.5227)\tAcc 79.8828 (Avg-Acc 81.2700)\n",
            "Epoch: [79][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5684 (Avg-Loss 0.5317)\tAcc 80.2734 (Avg-Acc 81.0513)\n",
            "Epoch: [79][76/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5302 (Avg-Loss 0.5368)\tAcc 80.6641 (Avg-Acc 80.7503)\n",
            "Epoch: [79][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.7235 (Avg-Loss 0.5370)\tAcc 70.3125 (Avg-Acc 80.7450)\n",
            "EPOCH: 79 train Results: Acc 80.745 Loss: 0.5370\n",
            "Epoch: [79][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4692 (Avg-Loss 1.4692)\tAcc 56.0547 (Avg-Acc 56.0547)\n",
            "Epoch: [79][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.5220 (Avg-Loss 1.5210)\tAcc 56.2500 (Avg-Acc 55.0100)\n",
            "EPOCH: 79 Validation Results: Acc 55.010 Loss: 1.5210\n",
            "Epoch: [80][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4621 (Avg-Loss 0.4621)\tAcc 85.1562 (Avg-Acc 85.1562)\n",
            "Epoch: [80][19/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.5645 (Avg-Loss 0.5390)\tAcc 81.8359 (Avg-Acc 80.6738)\n",
            "Epoch: [80][38/78]\tTime 0.261 (Avg-Time 0.072)\t Loss 0.5749 (Avg-Loss 0.5364)\tAcc 80.0781 (Avg-Acc 80.8293)\n",
            "Epoch: [80][57/78]\tTime 0.058 (Avg-Time 0.095)\t Loss 0.5491 (Avg-Loss 0.5366)\tAcc 79.8828 (Avg-Acc 80.7348)\n",
            "Epoch: [80][76/78]\tTime 0.058 (Avg-Time 0.087)\t Loss 0.5973 (Avg-Loss 0.5441)\tAcc 77.3438 (Avg-Acc 80.4028)\n",
            "Epoch: [80][78/78]\tTime 0.021 (Avg-Time 0.085)\t Loss 1.0257 (Avg-Loss 0.5457)\tAcc 75.0000 (Avg-Acc 80.3450)\n",
            "EPOCH: 80 train Results: Acc 80.345 Loss: 0.5457\n",
            "Epoch: [80][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4831 (Avg-Loss 1.4831)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [80][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5272 (Avg-Loss 1.5216)\tAcc 55.5147 (Avg-Acc 54.8800)\n",
            "EPOCH: 80 Validation Results: Acc 54.880 Loss: 1.5216\n",
            "Epoch: [81][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4614 (Avg-Loss 0.4614)\tAcc 83.9844 (Avg-Acc 83.9844)\n",
            "Epoch: [81][19/78]\tTime 0.061 (Avg-Time 0.062)\t Loss 0.5462 (Avg-Loss 0.5076)\tAcc 81.0547 (Avg-Acc 81.5625)\n",
            "Epoch: [81][38/78]\tTime 0.063 (Avg-Time 0.062)\t Loss 0.5449 (Avg-Loss 0.5215)\tAcc 82.4219 (Avg-Acc 81.3902)\n",
            "Epoch: [81][57/78]\tTime 0.059 (Avg-Time 0.062)\t Loss 0.5661 (Avg-Loss 0.5327)\tAcc 79.2969 (Avg-Acc 80.9638)\n",
            "Epoch: [81][76/78]\tTime 0.059 (Avg-Time 0.062)\t Loss 0.5568 (Avg-Loss 0.5362)\tAcc 77.7344 (Avg-Acc 80.8163)\n",
            "Epoch: [81][78/78]\tTime 0.020 (Avg-Time 0.061)\t Loss 0.6843 (Avg-Loss 0.5365)\tAcc 73.4375 (Avg-Acc 80.8075)\n",
            "EPOCH: 81 train Results: Acc 80.808 Loss: 0.5365\n",
            "Epoch: [81][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4805 (Avg-Loss 1.4805)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [81][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5295 (Avg-Loss 1.5216)\tAcc 54.4118 (Avg-Acc 54.6300)\n",
            "EPOCH: 81 Validation Results: Acc 54.630 Loss: 1.5216\n",
            "Epoch: [82][0/78]\tTime 0.063 (Avg-Time 0.063)\t Loss 0.5714 (Avg-Loss 0.5714)\tAcc 79.1016 (Avg-Acc 79.1016)\n",
            "Epoch: [82][19/78]\tTime 0.071 (Avg-Time 0.064)\t Loss 0.5206 (Avg-Loss 0.5268)\tAcc 78.9062 (Avg-Acc 81.4160)\n",
            "Epoch: [82][38/78]\tTime 0.059 (Avg-Time 0.063)\t Loss 0.5650 (Avg-Loss 0.5320)\tAcc 80.4688 (Avg-Acc 81.1098)\n",
            "Epoch: [82][57/78]\tTime 0.191 (Avg-Time 0.069)\t Loss 0.5177 (Avg-Loss 0.5309)\tAcc 80.4688 (Avg-Acc 81.1591)\n",
            "Epoch: [82][76/78]\tTime 0.089 (Avg-Time 0.087)\t Loss 0.5281 (Avg-Loss 0.5371)\tAcc 80.4688 (Avg-Acc 80.9684)\n",
            "Epoch: [82][78/78]\tTime 0.020 (Avg-Time 0.086)\t Loss 0.6561 (Avg-Loss 0.5373)\tAcc 75.0000 (Avg-Acc 80.9600)\n",
            "EPOCH: 82 train Results: Acc 80.960 Loss: 0.5373\n",
            "Epoch: [82][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4911 (Avg-Loss 1.4911)\tAcc 55.2734 (Avg-Acc 55.2734)\n",
            "Epoch: [82][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5333 (Avg-Loss 1.5382)\tAcc 56.2500 (Avg-Acc 54.7800)\n",
            "EPOCH: 82 Validation Results: Acc 54.780 Loss: 1.5382\n",
            "Epoch: [83][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.5552 (Avg-Loss 0.5552)\tAcc 79.8828 (Avg-Acc 79.8828)\n",
            "Epoch: [83][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5782 (Avg-Loss 0.5117)\tAcc 81.6406 (Avg-Acc 81.7871)\n",
            "Epoch: [83][38/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5561 (Avg-Loss 0.5188)\tAcc 79.8828 (Avg-Acc 81.2700)\n",
            "Epoch: [83][57/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5595 (Avg-Loss 0.5290)\tAcc 80.0781 (Avg-Acc 81.0075)\n",
            "Epoch: [83][76/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5067 (Avg-Loss 0.5288)\tAcc 79.6875 (Avg-Acc 80.9228)\n",
            "Epoch: [83][78/78]\tTime 0.021 (Avg-Time 0.060)\t Loss 0.5637 (Avg-Loss 0.5291)\tAcc 81.2500 (Avg-Acc 80.9050)\n",
            "EPOCH: 83 train Results: Acc 80.905 Loss: 0.5291\n",
            "Epoch: [83][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.4586 (Avg-Loss 1.4586)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [83][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5348 (Avg-Loss 1.5316)\tAcc 56.2500 (Avg-Acc 54.6800)\n",
            "EPOCH: 83 Validation Results: Acc 54.680 Loss: 1.5316\n",
            "Epoch: [84][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4514 (Avg-Loss 0.4514)\tAcc 84.7656 (Avg-Acc 84.7656)\n",
            "Epoch: [84][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4442 (Avg-Loss 0.4954)\tAcc 84.9609 (Avg-Acc 82.4707)\n",
            "Epoch: [84][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4581 (Avg-Loss 0.5071)\tAcc 83.2031 (Avg-Acc 81.8409)\n",
            "Epoch: [84][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5611 (Avg-Loss 0.5123)\tAcc 81.2500 (Avg-Acc 81.5834)\n",
            "Epoch: [84][76/78]\tTime 0.078 (Avg-Time 0.060)\t Loss 0.5093 (Avg-Loss 0.5209)\tAcc 83.7891 (Avg-Acc 81.2830)\n",
            "Epoch: [84][78/78]\tTime 0.046 (Avg-Time 0.061)\t Loss 0.4837 (Avg-Loss 0.5213)\tAcc 78.1250 (Avg-Acc 81.2575)\n",
            "EPOCH: 84 train Results: Acc 81.257 Loss: 0.5213\n",
            "Epoch: [84][0/19]\tTime 0.026 (Avg-Time 0.026)\t Loss 1.4906 (Avg-Loss 1.4906)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [84][19/19]\tTime 0.014 (Avg-Time 0.032)\t Loss 1.5742 (Avg-Loss 1.5422)\tAcc 55.1471 (Avg-Acc 54.5400)\n",
            "EPOCH: 84 Validation Results: Acc 54.540 Loss: 1.5422\n",
            "Epoch: [85][0/78]\tTime 0.216 (Avg-Time 0.216)\t Loss 0.5580 (Avg-Loss 0.5580)\tAcc 79.2969 (Avg-Acc 79.2969)\n",
            "Epoch: [85][19/78]\tTime 0.056 (Avg-Time 0.138)\t Loss 0.5307 (Avg-Loss 0.5039)\tAcc 78.9062 (Avg-Acc 82.1680)\n",
            "Epoch: [85][38/78]\tTime 0.062 (Avg-Time 0.100)\t Loss 0.5348 (Avg-Loss 0.5055)\tAcc 79.2969 (Avg-Acc 81.8009)\n",
            "Epoch: [85][57/78]\tTime 0.057 (Avg-Time 0.087)\t Loss 0.5553 (Avg-Loss 0.5143)\tAcc 80.6641 (Avg-Acc 81.3914)\n",
            "Epoch: [85][76/78]\tTime 0.057 (Avg-Time 0.081)\t Loss 0.5273 (Avg-Loss 0.5248)\tAcc 82.8125 (Avg-Acc 81.1308)\n",
            "Epoch: [85][78/78]\tTime 0.020 (Avg-Time 0.080)\t Loss 0.6624 (Avg-Loss 0.5253)\tAcc 79.6875 (Avg-Acc 81.1125)\n",
            "EPOCH: 85 train Results: Acc 81.112 Loss: 0.5253\n",
            "Epoch: [85][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4904 (Avg-Loss 1.4904)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [85][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5228 (Avg-Loss 1.5227)\tAcc 52.9412 (Avg-Acc 54.7300)\n",
            "EPOCH: 85 Validation Results: Acc 54.730 Loss: 1.5227\n",
            "Epoch: [86][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4733 (Avg-Loss 0.4733)\tAcc 81.6406 (Avg-Acc 81.6406)\n",
            "Epoch: [86][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5340 (Avg-Loss 0.5061)\tAcc 83.7891 (Avg-Acc 82.4512)\n",
            "Epoch: [86][38/78]\tTime 0.062 (Avg-Time 0.060)\t Loss 0.5393 (Avg-Loss 0.5097)\tAcc 79.2969 (Avg-Acc 82.2616)\n",
            "Epoch: [86][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5560 (Avg-Loss 0.5170)\tAcc 80.8594 (Avg-Acc 81.8124)\n",
            "Epoch: [86][76/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.5870 (Avg-Loss 0.5242)\tAcc 79.6875 (Avg-Acc 81.3591)\n",
            "Epoch: [86][78/78]\tTime 0.021 (Avg-Time 0.060)\t Loss 0.6077 (Avg-Loss 0.5241)\tAcc 73.4375 (Avg-Acc 81.3675)\n",
            "EPOCH: 86 train Results: Acc 81.368 Loss: 0.5241\n",
            "Epoch: [86][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4873 (Avg-Loss 1.4873)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [86][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5527 (Avg-Loss 1.5451)\tAcc 54.0441 (Avg-Acc 54.7700)\n",
            "EPOCH: 86 Validation Results: Acc 54.770 Loss: 1.5451\n",
            "Epoch: [87][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4992 (Avg-Loss 0.4992)\tAcc 81.4453 (Avg-Acc 81.4453)\n",
            "Epoch: [87][19/78]\tTime 0.154 (Avg-Time 0.070)\t Loss 0.5868 (Avg-Loss 0.5079)\tAcc 79.8828 (Avg-Acc 81.4844)\n",
            "Epoch: [87][38/78]\tTime 0.057 (Avg-Time 0.112)\t Loss 0.5638 (Avg-Loss 0.5145)\tAcc 78.3203 (Avg-Acc 81.1248)\n",
            "Epoch: [87][57/78]\tTime 0.057 (Avg-Time 0.095)\t Loss 0.4958 (Avg-Loss 0.5124)\tAcc 82.2266 (Avg-Acc 81.2702)\n",
            "Epoch: [87][76/78]\tTime 0.058 (Avg-Time 0.086)\t Loss 0.4910 (Avg-Loss 0.5195)\tAcc 82.0312 (Avg-Acc 81.0369)\n",
            "Epoch: [87][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.6779 (Avg-Loss 0.5193)\tAcc 81.2500 (Avg-Acc 81.0500)\n",
            "EPOCH: 87 train Results: Acc 81.050 Loss: 0.5193\n",
            "Epoch: [87][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4916 (Avg-Loss 1.4916)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [87][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.5697 (Avg-Loss 1.5453)\tAcc 54.4118 (Avg-Acc 54.6500)\n",
            "EPOCH: 87 Validation Results: Acc 54.650 Loss: 1.5453\n",
            "Epoch: [88][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.5312 (Avg-Loss 0.5312)\tAcc 81.8359 (Avg-Acc 81.8359)\n",
            "Epoch: [88][19/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4820 (Avg-Loss 0.5100)\tAcc 81.8359 (Avg-Acc 81.4941)\n",
            "Epoch: [88][38/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5327 (Avg-Loss 0.5190)\tAcc 79.8828 (Avg-Acc 81.1649)\n",
            "Epoch: [88][57/78]\tTime 0.061 (Avg-Time 0.060)\t Loss 0.5068 (Avg-Loss 0.5173)\tAcc 81.6406 (Avg-Acc 81.2870)\n",
            "Epoch: [88][76/78]\tTime 0.065 (Avg-Time 0.060)\t Loss 0.5288 (Avg-Loss 0.5230)\tAcc 82.6172 (Avg-Acc 81.0319)\n",
            "Epoch: [88][78/78]\tTime 0.021 (Avg-Time 0.060)\t Loss 0.5745 (Avg-Loss 0.5222)\tAcc 75.0000 (Avg-Acc 81.0650)\n",
            "EPOCH: 88 train Results: Acc 81.065 Loss: 0.5222\n",
            "Epoch: [88][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5193 (Avg-Loss 1.5193)\tAcc 54.6875 (Avg-Acc 54.6875)\n",
            "Epoch: [88][19/19]\tTime 0.014 (Avg-Time 0.015)\t Loss 1.5688 (Avg-Loss 1.5468)\tAcc 55.8824 (Avg-Acc 54.5200)\n",
            "EPOCH: 88 Validation Results: Acc 54.520 Loss: 1.5468\n",
            "Epoch: [89][0/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.4735 (Avg-Loss 0.4735)\tAcc 86.1328 (Avg-Acc 86.1328)\n",
            "Epoch: [89][19/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.5681 (Avg-Loss 0.4905)\tAcc 78.9062 (Avg-Acc 82.5586)\n",
            "Epoch: [89][38/78]\tTime 0.102 (Avg-Time 0.064)\t Loss 0.5131 (Avg-Loss 0.5025)\tAcc 80.2734 (Avg-Acc 81.9862)\n",
            "Epoch: [89][57/78]\tTime 0.133 (Avg-Time 0.092)\t Loss 0.4931 (Avg-Loss 0.5070)\tAcc 80.4688 (Avg-Acc 81.7955)\n",
            "Epoch: [89][76/78]\tTime 0.059 (Avg-Time 0.086)\t Loss 0.5324 (Avg-Loss 0.5176)\tAcc 80.0781 (Avg-Acc 81.5011)\n",
            "Epoch: [89][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.9312 (Avg-Loss 0.5182)\tAcc 70.3125 (Avg-Acc 81.5025)\n",
            "EPOCH: 89 train Results: Acc 81.502 Loss: 0.5182\n",
            "Epoch: [89][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5278 (Avg-Loss 1.5278)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [89][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6177 (Avg-Loss 1.5560)\tAcc 54.7794 (Avg-Acc 54.5500)\n",
            "EPOCH: 89 Validation Results: Acc 54.550 Loss: 1.5560\n",
            "Epoch: [90][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4566 (Avg-Loss 0.4566)\tAcc 82.8125 (Avg-Acc 82.8125)\n",
            "Epoch: [90][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5054 (Avg-Loss 0.4978)\tAcc 83.5938 (Avg-Acc 82.1387)\n",
            "Epoch: [90][38/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5660 (Avg-Loss 0.5056)\tAcc 78.5156 (Avg-Acc 81.8109)\n",
            "Epoch: [90][57/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4444 (Avg-Loss 0.5086)\tAcc 83.9844 (Avg-Acc 81.7383)\n",
            "Epoch: [90][76/78]\tTime 0.062 (Avg-Time 0.060)\t Loss 0.5198 (Avg-Loss 0.5157)\tAcc 80.2734 (Avg-Acc 81.4808)\n",
            "Epoch: [90][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 0.7420 (Avg-Loss 0.5166)\tAcc 78.1250 (Avg-Acc 81.4750)\n",
            "EPOCH: 90 train Results: Acc 81.475 Loss: 0.5166\n",
            "Epoch: [90][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4917 (Avg-Loss 1.4917)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [90][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5950 (Avg-Loss 1.5394)\tAcc 53.3088 (Avg-Acc 54.9700)\n",
            "EPOCH: 90 Validation Results: Acc 54.970 Loss: 1.5394\n",
            "Epoch: [91][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.5437 (Avg-Loss 0.5437)\tAcc 82.2266 (Avg-Acc 82.2266)\n",
            "Epoch: [91][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4459 (Avg-Loss 0.5018)\tAcc 83.5938 (Avg-Acc 82.3535)\n",
            "Epoch: [91][38/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.5246 (Avg-Loss 0.5036)\tAcc 82.2266 (Avg-Acc 82.2967)\n",
            "Epoch: [91][57/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5319 (Avg-Loss 0.5129)\tAcc 80.4688 (Avg-Acc 81.8831)\n",
            "Epoch: [91][76/78]\tTime 0.150 (Avg-Time 0.082)\t Loss 0.6199 (Avg-Loss 0.5188)\tAcc 76.9531 (Avg-Acc 81.5087)\n",
            "Epoch: [91][78/78]\tTime 0.089 (Avg-Time 0.082)\t Loss 0.5851 (Avg-Loss 0.5196)\tAcc 78.1250 (Avg-Acc 81.4850)\n",
            "EPOCH: 91 train Results: Acc 81.485 Loss: 0.5196\n",
            "Epoch: [91][0/19]\tTime 0.028 (Avg-Time 0.028)\t Loss 1.4936 (Avg-Loss 1.4936)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [91][19/19]\tTime 0.007 (Avg-Time 0.025)\t Loss 1.6144 (Avg-Loss 1.5368)\tAcc 55.8824 (Avg-Acc 54.5400)\n",
            "EPOCH: 91 Validation Results: Acc 54.540 Loss: 1.5368\n",
            "Epoch: [92][0/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.4523 (Avg-Loss 0.4523)\tAcc 83.9844 (Avg-Acc 83.9844)\n",
            "Epoch: [92][19/78]\tTime 0.057 (Avg-Time 0.062)\t Loss 0.4698 (Avg-Loss 0.4914)\tAcc 83.5938 (Avg-Acc 82.5977)\n",
            "Epoch: [92][38/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.5304 (Avg-Loss 0.5043)\tAcc 80.6641 (Avg-Acc 82.0863)\n",
            "Epoch: [92][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5267 (Avg-Loss 0.5092)\tAcc 79.8828 (Avg-Acc 81.8393)\n",
            "Epoch: [92][76/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5446 (Avg-Loss 0.5126)\tAcc 81.0547 (Avg-Acc 81.6711)\n",
            "Epoch: [92][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.6189 (Avg-Loss 0.5141)\tAcc 75.0000 (Avg-Acc 81.6325)\n",
            "EPOCH: 92 train Results: Acc 81.632 Loss: 0.5141\n",
            "Epoch: [92][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5144 (Avg-Loss 1.5144)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [92][19/19]\tTime 0.010 (Avg-Time 0.013)\t Loss 1.6529 (Avg-Loss 1.5684)\tAcc 55.1471 (Avg-Acc 54.7500)\n",
            "EPOCH: 92 Validation Results: Acc 54.750 Loss: 1.5684\n",
            "Epoch: [93][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4997 (Avg-Loss 0.4997)\tAcc 83.2031 (Avg-Acc 83.2031)\n",
            "Epoch: [93][19/78]\tTime 0.079 (Avg-Time 0.061)\t Loss 0.5206 (Avg-Loss 0.4902)\tAcc 81.8359 (Avg-Acc 82.5977)\n",
            "Epoch: [93][38/78]\tTime 0.062 (Avg-Time 0.061)\t Loss 0.4751 (Avg-Loss 0.4941)\tAcc 82.2266 (Avg-Acc 82.4169)\n",
            "Epoch: [93][57/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.5060 (Avg-Loss 0.5075)\tAcc 82.0312 (Avg-Acc 81.8494)\n",
            "Epoch: [93][76/78]\tTime 0.062 (Avg-Time 0.060)\t Loss 0.4458 (Avg-Loss 0.5101)\tAcc 84.1797 (Avg-Acc 81.7193)\n",
            "Epoch: [93][78/78]\tTime 0.025 (Avg-Time 0.060)\t Loss 0.7172 (Avg-Loss 0.5115)\tAcc 78.1250 (Avg-Acc 81.6450)\n",
            "EPOCH: 93 train Results: Acc 81.645 Loss: 0.5115\n",
            "Epoch: [93][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5034 (Avg-Loss 1.5034)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [93][19/19]\tTime 0.009 (Avg-Time 0.015)\t Loss 1.6110 (Avg-Loss 1.5600)\tAcc 55.8824 (Avg-Acc 54.4300)\n",
            "EPOCH: 93 Validation Results: Acc 54.430 Loss: 1.5600\n",
            "Epoch: [94][0/78]\tTime 0.092 (Avg-Time 0.092)\t Loss 0.5049 (Avg-Loss 0.5049)\tAcc 81.0547 (Avg-Acc 81.0547)\n",
            "Epoch: [94][19/78]\tTime 0.117 (Avg-Time 0.154)\t Loss 0.5378 (Avg-Loss 0.5016)\tAcc 80.8594 (Avg-Acc 82.0508)\n",
            "Epoch: [94][38/78]\tTime 0.058 (Avg-Time 0.109)\t Loss 0.5140 (Avg-Loss 0.5084)\tAcc 83.5938 (Avg-Acc 81.7458)\n",
            "Epoch: [94][57/78]\tTime 0.058 (Avg-Time 0.093)\t Loss 0.5593 (Avg-Loss 0.5146)\tAcc 82.4219 (Avg-Acc 81.5362)\n",
            "Epoch: [94][76/78]\tTime 0.061 (Avg-Time 0.085)\t Loss 0.5858 (Avg-Loss 0.5188)\tAcc 80.0781 (Avg-Acc 81.3185)\n",
            "Epoch: [94][78/78]\tTime 0.020 (Avg-Time 0.084)\t Loss 0.6934 (Avg-Loss 0.5196)\tAcc 70.3125 (Avg-Acc 81.2825)\n",
            "EPOCH: 94 train Results: Acc 81.282 Loss: 0.5196\n",
            "Epoch: [94][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5255 (Avg-Loss 1.5255)\tAcc 56.0547 (Avg-Acc 56.0547)\n",
            "Epoch: [94][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6458 (Avg-Loss 1.5627)\tAcc 52.2059 (Avg-Acc 54.4500)\n",
            "EPOCH: 94 Validation Results: Acc 54.450 Loss: 1.5627\n",
            "Epoch: [95][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.5034 (Avg-Loss 0.5034)\tAcc 80.8594 (Avg-Acc 80.8594)\n",
            "Epoch: [95][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5203 (Avg-Loss 0.4807)\tAcc 81.2500 (Avg-Acc 82.8418)\n",
            "Epoch: [95][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5395 (Avg-Loss 0.4954)\tAcc 81.0547 (Avg-Acc 82.4669)\n",
            "Epoch: [95][57/78]\tTime 0.061 (Avg-Time 0.060)\t Loss 0.4591 (Avg-Loss 0.5011)\tAcc 81.8359 (Avg-Acc 82.1289)\n",
            "Epoch: [95][76/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.5335 (Avg-Loss 0.5090)\tAcc 81.4453 (Avg-Acc 81.8106)\n",
            "Epoch: [95][78/78]\tTime 0.022 (Avg-Time 0.060)\t Loss 0.6531 (Avg-Loss 0.5093)\tAcc 79.6875 (Avg-Acc 81.7975)\n",
            "EPOCH: 95 train Results: Acc 81.797 Loss: 0.5093\n",
            "Epoch: [95][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5249 (Avg-Loss 1.5249)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [95][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.6253 (Avg-Loss 1.5572)\tAcc 54.7794 (Avg-Acc 54.3100)\n",
            "EPOCH: 95 Validation Results: Acc 54.310 Loss: 1.5572\n",
            "Epoch: [96][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4802 (Avg-Loss 0.4802)\tAcc 82.4219 (Avg-Acc 82.4219)\n",
            "Epoch: [96][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4835 (Avg-Loss 0.4878)\tAcc 82.4219 (Avg-Acc 82.4512)\n",
            "Epoch: [96][38/78]\tTime 0.123 (Avg-Time 0.107)\t Loss 0.5153 (Avg-Loss 0.4961)\tAcc 82.4219 (Avg-Acc 82.2566)\n",
            "Epoch: [96][57/78]\tTime 0.058 (Avg-Time 0.094)\t Loss 0.5200 (Avg-Loss 0.4997)\tAcc 81.6406 (Avg-Acc 82.1828)\n",
            "Epoch: [96][76/78]\tTime 0.057 (Avg-Time 0.086)\t Loss 0.4992 (Avg-Loss 0.5048)\tAcc 81.8359 (Avg-Acc 81.9602)\n",
            "Epoch: [96][78/78]\tTime 0.020 (Avg-Time 0.084)\t Loss 0.6442 (Avg-Loss 0.5060)\tAcc 79.6875 (Avg-Acc 81.9450)\n",
            "EPOCH: 96 train Results: Acc 81.945 Loss: 0.5060\n",
            "Epoch: [96][0/19]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.5340 (Avg-Loss 1.5340)\tAcc 55.0781 (Avg-Acc 55.0781)\n",
            "Epoch: [96][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6002 (Avg-Loss 1.5537)\tAcc 54.7794 (Avg-Acc 54.3100)\n",
            "EPOCH: 96 Validation Results: Acc 54.310 Loss: 1.5537\n",
            "Epoch: [97][0/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.4746 (Avg-Loss 0.4746)\tAcc 83.5938 (Avg-Acc 83.5938)\n",
            "Epoch: [97][19/78]\tTime 0.060 (Avg-Time 0.061)\t Loss 0.5521 (Avg-Loss 0.4758)\tAcc 80.4688 (Avg-Acc 82.9395)\n",
            "Epoch: [97][38/78]\tTime 0.068 (Avg-Time 0.062)\t Loss 0.5103 (Avg-Loss 0.4830)\tAcc 82.4219 (Avg-Acc 82.5270)\n",
            "Epoch: [97][57/78]\tTime 0.058 (Avg-Time 0.062)\t Loss 0.5129 (Avg-Loss 0.4911)\tAcc 82.0312 (Avg-Acc 82.2165)\n",
            "Epoch: [97][76/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.5060 (Avg-Loss 0.5016)\tAcc 80.6641 (Avg-Acc 81.8714)\n",
            "Epoch: [97][78/78]\tTime 0.020 (Avg-Time 0.061)\t Loss 0.5708 (Avg-Loss 0.5023)\tAcc 67.1875 (Avg-Acc 81.8125)\n",
            "EPOCH: 97 train Results: Acc 81.812 Loss: 0.5023\n",
            "Epoch: [97][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5091 (Avg-Loss 1.5091)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [97][19/19]\tTime 0.009 (Avg-Time 0.013)\t Loss 1.6461 (Avg-Loss 1.5690)\tAcc 54.0441 (Avg-Acc 54.7200)\n",
            "EPOCH: 97 Validation Results: Acc 54.720 Loss: 1.5690\n",
            "Epoch: [98][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4944 (Avg-Loss 0.4944)\tAcc 82.6172 (Avg-Acc 82.6172)\n",
            "Epoch: [98][19/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.5033 (Avg-Loss 0.4796)\tAcc 82.4219 (Avg-Acc 82.6172)\n",
            "Epoch: [98][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5539 (Avg-Loss 0.4898)\tAcc 79.4922 (Avg-Acc 82.3267)\n",
            "Epoch: [98][57/78]\tTime 0.140 (Avg-Time 0.091)\t Loss 0.5739 (Avg-Loss 0.4958)\tAcc 78.7109 (Avg-Acc 82.1457)\n",
            "Epoch: [98][76/78]\tTime 0.058 (Avg-Time 0.086)\t Loss 0.5698 (Avg-Loss 0.4998)\tAcc 80.8594 (Avg-Acc 81.9754)\n",
            "Epoch: [98][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.9365 (Avg-Loss 0.5010)\tAcc 67.1875 (Avg-Acc 81.9475)\n",
            "EPOCH: 98 train Results: Acc 81.948 Loss: 0.5010\n",
            "Epoch: [98][0/19]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.5065 (Avg-Loss 1.5065)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [98][19/19]\tTime 0.007 (Avg-Time 0.015)\t Loss 1.6724 (Avg-Loss 1.5548)\tAcc 55.5147 (Avg-Acc 54.4700)\n",
            "EPOCH: 98 Validation Results: Acc 54.470 Loss: 1.5548\n",
            "Epoch: [99][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4706 (Avg-Loss 0.4706)\tAcc 81.8359 (Avg-Acc 81.8359)\n",
            "Epoch: [99][19/78]\tTime 0.058 (Avg-Time 0.062)\t Loss 0.4985 (Avg-Loss 0.4698)\tAcc 83.5938 (Avg-Acc 82.7148)\n",
            "Epoch: [99][38/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.5059 (Avg-Loss 0.4851)\tAcc 81.4453 (Avg-Acc 82.2416)\n",
            "Epoch: [99][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4588 (Avg-Loss 0.4925)\tAcc 82.2266 (Avg-Acc 82.0750)\n",
            "Epoch: [99][76/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5404 (Avg-Loss 0.5022)\tAcc 82.8125 (Avg-Acc 81.8359)\n",
            "Epoch: [99][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.6239 (Avg-Loss 0.5036)\tAcc 82.8125 (Avg-Acc 81.7975)\n",
            "EPOCH: 99 train Results: Acc 81.797 Loss: 0.5036\n",
            "Epoch: [99][0/19]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.5433 (Avg-Loss 1.5433)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [99][19/19]\tTime 0.013 (Avg-Time 0.014)\t Loss 1.6676 (Avg-Loss 1.5853)\tAcc 54.7794 (Avg-Acc 54.6200)\n",
            "EPOCH: 99 Validation Results: Acc 54.620 Loss: 1.5853\n",
            "Epoch: [100][0/78]\tTime 0.069 (Avg-Time 0.069)\t Loss 0.4691 (Avg-Loss 0.4691)\tAcc 83.7891 (Avg-Acc 83.7891)\n",
            "Epoch: [100][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5554 (Avg-Loss 0.4833)\tAcc 81.0547 (Avg-Acc 82.2461)\n",
            "Epoch: [100][38/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4845 (Avg-Loss 0.4993)\tAcc 81.0547 (Avg-Acc 81.9712)\n",
            "Epoch: [100][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.6010 (Avg-Loss 0.5045)\tAcc 78.5156 (Avg-Acc 81.8966)\n",
            "Epoch: [100][76/78]\tTime 0.106 (Avg-Time 0.083)\t Loss 0.5681 (Avg-Loss 0.5123)\tAcc 79.6875 (Avg-Acc 81.5747)\n",
            "Epoch: [100][78/78]\tTime 0.033 (Avg-Time 0.083)\t Loss 0.5605 (Avg-Loss 0.5125)\tAcc 79.6875 (Avg-Acc 81.5600)\n",
            "EPOCH: 100 train Results: Acc 81.560 Loss: 0.5125\n",
            "Epoch: [100][0/19]\tTime 0.027 (Avg-Time 0.027)\t Loss 1.5058 (Avg-Loss 1.5058)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [100][19/19]\tTime 0.007 (Avg-Time 0.021)\t Loss 1.6981 (Avg-Loss 1.5646)\tAcc 53.6765 (Avg-Acc 54.2100)\n",
            "EPOCH: 100 Validation Results: Acc 54.210 Loss: 1.5646\n",
            "Epoch: [101][0/78]\tTime 0.064 (Avg-Time 0.064)\t Loss 0.4716 (Avg-Loss 0.4716)\tAcc 85.1562 (Avg-Acc 85.1562)\n",
            "Epoch: [101][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5613 (Avg-Loss 0.4855)\tAcc 80.8594 (Avg-Acc 82.3926)\n",
            "Epoch: [101][38/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4578 (Avg-Loss 0.4939)\tAcc 82.0312 (Avg-Acc 82.0262)\n",
            "Epoch: [101][57/78]\tTime 0.067 (Avg-Time 0.060)\t Loss 0.5331 (Avg-Loss 0.4965)\tAcc 80.6641 (Avg-Acc 82.0784)\n",
            "Epoch: [101][76/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5534 (Avg-Loss 0.5033)\tAcc 80.0781 (Avg-Acc 81.8486)\n",
            "Epoch: [101][78/78]\tTime 0.021 (Avg-Time 0.060)\t Loss 0.8587 (Avg-Loss 0.5045)\tAcc 73.4375 (Avg-Acc 81.8375)\n",
            "EPOCH: 101 train Results: Acc 81.838 Loss: 0.5045\n",
            "Epoch: [101][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5456 (Avg-Loss 1.5456)\tAcc 54.4922 (Avg-Acc 54.4922)\n",
            "Epoch: [101][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6826 (Avg-Loss 1.5739)\tAcc 54.7794 (Avg-Acc 54.8600)\n",
            "EPOCH: 101 Validation Results: Acc 54.860 Loss: 1.5739\n",
            "Epoch: [102][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4412 (Avg-Loss 0.4412)\tAcc 84.1797 (Avg-Acc 84.1797)\n",
            "Epoch: [102][19/78]\tTime 0.065 (Avg-Time 0.061)\t Loss 0.5245 (Avg-Loss 0.4788)\tAcc 79.6875 (Avg-Acc 82.5293)\n",
            "Epoch: [102][38/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4957 (Avg-Loss 0.4859)\tAcc 83.9844 (Avg-Acc 82.5371)\n",
            "Epoch: [102][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.5361 (Avg-Loss 0.4906)\tAcc 81.6406 (Avg-Acc 82.4690)\n",
            "Epoch: [102][76/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.4819 (Avg-Loss 0.4973)\tAcc 83.2031 (Avg-Acc 82.3356)\n",
            "Epoch: [102][78/78]\tTime 0.023 (Avg-Time 0.060)\t Loss 0.7208 (Avg-Loss 0.4979)\tAcc 76.5625 (Avg-Acc 82.3100)\n",
            "EPOCH: 102 train Results: Acc 82.310 Loss: 0.4979\n",
            "Epoch: [102][0/19]\tTime 0.024 (Avg-Time 0.024)\t Loss 1.5265 (Avg-Loss 1.5265)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [102][19/19]\tTime 0.015 (Avg-Time 0.020)\t Loss 1.6879 (Avg-Loss 1.5644)\tAcc 54.4118 (Avg-Acc 54.6900)\n",
            "EPOCH: 102 Validation Results: Acc 54.690 Loss: 1.5644\n",
            "Epoch: [103][0/78]\tTime 0.105 (Avg-Time 0.105)\t Loss 0.4343 (Avg-Loss 0.4343)\tAcc 83.3984 (Avg-Acc 83.3984)\n",
            "Epoch: [103][19/78]\tTime 0.119 (Avg-Time 0.152)\t Loss 0.5144 (Avg-Loss 0.4862)\tAcc 81.2500 (Avg-Acc 82.4316)\n",
            "Epoch: [103][38/78]\tTime 0.057 (Avg-Time 0.107)\t Loss 0.4612 (Avg-Loss 0.4901)\tAcc 82.2266 (Avg-Acc 82.3267)\n",
            "Epoch: [103][57/78]\tTime 0.058 (Avg-Time 0.092)\t Loss 0.5601 (Avg-Loss 0.4967)\tAcc 78.7109 (Avg-Acc 82.1424)\n",
            "Epoch: [103][76/78]\tTime 0.060 (Avg-Time 0.084)\t Loss 0.5455 (Avg-Loss 0.5026)\tAcc 81.4453 (Avg-Acc 81.9298)\n",
            "Epoch: [103][78/78]\tTime 0.020 (Avg-Time 0.083)\t Loss 0.6663 (Avg-Loss 0.5026)\tAcc 78.1250 (Avg-Acc 81.9175)\n",
            "EPOCH: 103 train Results: Acc 81.918 Loss: 0.5026\n",
            "Epoch: [103][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5639 (Avg-Loss 1.5639)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [103][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6948 (Avg-Loss 1.5811)\tAcc 54.4118 (Avg-Acc 54.8700)\n",
            "EPOCH: 103 Validation Results: Acc 54.870 Loss: 1.5811\n",
            "Epoch: [104][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.4366 (Avg-Loss 0.4366)\tAcc 85.9375 (Avg-Acc 85.9375)\n",
            "Epoch: [104][19/78]\tTime 0.086 (Avg-Time 0.063)\t Loss 0.4960 (Avg-Loss 0.4764)\tAcc 82.8125 (Avg-Acc 82.8516)\n",
            "Epoch: [104][38/78]\tTime 0.059 (Avg-Time 0.062)\t Loss 0.4824 (Avg-Loss 0.4831)\tAcc 82.4219 (Avg-Acc 82.5571)\n",
            "Epoch: [104][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4490 (Avg-Loss 0.4885)\tAcc 83.0078 (Avg-Acc 82.4892)\n",
            "Epoch: [104][76/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.5597 (Avg-Loss 0.4957)\tAcc 79.1016 (Avg-Acc 82.2266)\n",
            "Epoch: [104][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.6671 (Avg-Loss 0.4962)\tAcc 78.1250 (Avg-Acc 82.1925)\n",
            "EPOCH: 104 train Results: Acc 82.192 Loss: 0.4962\n",
            "Epoch: [104][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5394 (Avg-Loss 1.5394)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [104][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6609 (Avg-Loss 1.5747)\tAcc 55.8824 (Avg-Acc 54.7300)\n",
            "EPOCH: 104 Validation Results: Acc 54.730 Loss: 1.5747\n",
            "Epoch: [105][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.4882 (Avg-Loss 0.4882)\tAcc 82.8125 (Avg-Acc 82.8125)\n",
            "Epoch: [105][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5298 (Avg-Loss 0.4841)\tAcc 81.6406 (Avg-Acc 82.5293)\n",
            "Epoch: [105][38/78]\tTime 0.136 (Avg-Time 0.106)\t Loss 0.4657 (Avg-Loss 0.4852)\tAcc 83.5938 (Avg-Acc 82.7123)\n",
            "Epoch: [105][57/78]\tTime 0.057 (Avg-Time 0.094)\t Loss 0.5805 (Avg-Loss 0.4931)\tAcc 78.7109 (Avg-Acc 82.5162)\n",
            "Epoch: [105][76/78]\tTime 0.058 (Avg-Time 0.086)\t Loss 0.6442 (Avg-Loss 0.4951)\tAcc 78.9062 (Avg-Acc 82.5309)\n",
            "Epoch: [105][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.6754 (Avg-Loss 0.4956)\tAcc 75.0000 (Avg-Acc 82.4800)\n",
            "EPOCH: 105 train Results: Acc 82.480 Loss: 0.4956\n",
            "Epoch: [105][0/19]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.5281 (Avg-Loss 1.5281)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [105][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.6627 (Avg-Loss 1.5707)\tAcc 52.9412 (Avg-Acc 54.5300)\n",
            "EPOCH: 105 Validation Results: Acc 54.530 Loss: 1.5707\n",
            "Epoch: [106][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.5083 (Avg-Loss 0.5083)\tAcc 81.4453 (Avg-Acc 81.4453)\n",
            "Epoch: [106][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4516 (Avg-Loss 0.4811)\tAcc 83.7891 (Avg-Acc 82.7539)\n",
            "Epoch: [106][38/78]\tTime 0.056 (Avg-Time 0.060)\t Loss 0.4724 (Avg-Loss 0.4826)\tAcc 82.0312 (Avg-Acc 82.7825)\n",
            "Epoch: [106][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5122 (Avg-Loss 0.4907)\tAcc 80.6641 (Avg-Acc 82.3276)\n",
            "Epoch: [106][76/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4532 (Avg-Loss 0.4958)\tAcc 83.3984 (Avg-Acc 82.1885)\n",
            "Epoch: [106][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 0.6738 (Avg-Loss 0.4961)\tAcc 76.5625 (Avg-Acc 82.1700)\n",
            "EPOCH: 106 train Results: Acc 82.170 Loss: 0.4961\n",
            "Epoch: [106][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5586 (Avg-Loss 1.5586)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [106][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6629 (Avg-Loss 1.5774)\tAcc 53.3088 (Avg-Acc 54.2500)\n",
            "EPOCH: 106 Validation Results: Acc 54.250 Loss: 1.5774\n",
            "Epoch: [107][0/78]\tTime 0.081 (Avg-Time 0.081)\t Loss 0.4955 (Avg-Loss 0.4955)\tAcc 81.2500 (Avg-Acc 81.2500)\n",
            "Epoch: [107][19/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4850 (Avg-Loss 0.4886)\tAcc 82.2266 (Avg-Acc 82.2656)\n",
            "Epoch: [107][38/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4830 (Avg-Loss 0.4923)\tAcc 83.0078 (Avg-Acc 82.2266)\n",
            "Epoch: [107][57/78]\tTime 0.244 (Avg-Time 0.084)\t Loss 0.5439 (Avg-Loss 0.4939)\tAcc 81.0547 (Avg-Acc 82.3040)\n",
            "Epoch: [107][76/78]\tTime 0.057 (Avg-Time 0.086)\t Loss 0.5171 (Avg-Loss 0.4949)\tAcc 81.0547 (Avg-Acc 82.2874)\n",
            "Epoch: [107][78/78]\tTime 0.022 (Avg-Time 0.085)\t Loss 0.6470 (Avg-Loss 0.4963)\tAcc 78.1250 (Avg-Acc 82.2200)\n",
            "EPOCH: 107 train Results: Acc 82.220 Loss: 0.4963\n",
            "Epoch: [107][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5672 (Avg-Loss 1.5672)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [107][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6778 (Avg-Loss 1.6016)\tAcc 55.1471 (Avg-Acc 54.4500)\n",
            "EPOCH: 107 Validation Results: Acc 54.450 Loss: 1.6016\n",
            "Epoch: [108][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4627 (Avg-Loss 0.4627)\tAcc 83.3984 (Avg-Acc 83.3984)\n",
            "Epoch: [108][19/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.5204 (Avg-Loss 0.4694)\tAcc 81.8359 (Avg-Acc 83.1152)\n",
            "Epoch: [108][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4748 (Avg-Loss 0.4865)\tAcc 82.6172 (Avg-Acc 82.7324)\n",
            "Epoch: [108][57/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5326 (Avg-Loss 0.4912)\tAcc 81.4453 (Avg-Acc 82.5936)\n",
            "Epoch: [108][76/78]\tTime 0.070 (Avg-Time 0.060)\t Loss 0.4545 (Avg-Loss 0.4911)\tAcc 85.1562 (Avg-Acc 82.5487)\n",
            "Epoch: [108][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.5955 (Avg-Loss 0.4925)\tAcc 79.6875 (Avg-Acc 82.5050)\n",
            "EPOCH: 108 train Results: Acc 82.505 Loss: 0.4925\n",
            "Epoch: [108][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5468 (Avg-Loss 1.5468)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [108][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6250 (Avg-Loss 1.5801)\tAcc 55.5147 (Avg-Acc 54.5000)\n",
            "EPOCH: 108 Validation Results: Acc 54.500 Loss: 1.5801\n",
            "Epoch: [109][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4323 (Avg-Loss 0.4323)\tAcc 84.5703 (Avg-Acc 84.5703)\n",
            "Epoch: [109][19/78]\tTime 0.056 (Avg-Time 0.060)\t Loss 0.4155 (Avg-Loss 0.4559)\tAcc 85.1562 (Avg-Acc 83.5352)\n",
            "Epoch: [109][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4574 (Avg-Loss 0.4637)\tAcc 83.2031 (Avg-Acc 83.3433)\n",
            "Epoch: [109][57/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4479 (Avg-Loss 0.4709)\tAcc 83.3984 (Avg-Acc 83.0415)\n",
            "Epoch: [109][76/78]\tTime 0.303 (Avg-Time 0.069)\t Loss 0.5254 (Avg-Loss 0.4780)\tAcc 81.8359 (Avg-Acc 82.9596)\n",
            "Epoch: [109][78/78]\tTime 0.053 (Avg-Time 0.072)\t Loss 0.6469 (Avg-Loss 0.4787)\tAcc 79.6875 (Avg-Acc 82.9400)\n",
            "EPOCH: 109 train Results: Acc 82.940 Loss: 0.4787\n",
            "Epoch: [109][0/19]\tTime 0.034 (Avg-Time 0.034)\t Loss 1.5720 (Avg-Loss 1.5720)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [109][19/19]\tTime 0.015 (Avg-Time 0.043)\t Loss 1.6626 (Avg-Loss 1.6040)\tAcc 55.1471 (Avg-Acc 54.3400)\n",
            "EPOCH: 109 Validation Results: Acc 54.340 Loss: 1.6040\n",
            "Epoch: [110][0/78]\tTime 0.119 (Avg-Time 0.119)\t Loss 0.4429 (Avg-Loss 0.4429)\tAcc 83.9844 (Avg-Acc 83.9844)\n",
            "Epoch: [110][19/78]\tTime 0.060 (Avg-Time 0.085)\t Loss 0.4552 (Avg-Loss 0.4633)\tAcc 84.9609 (Avg-Acc 83.6230)\n",
            "Epoch: [110][38/78]\tTime 0.058 (Avg-Time 0.073)\t Loss 0.4385 (Avg-Loss 0.4711)\tAcc 84.9609 (Avg-Acc 83.1631)\n",
            "Epoch: [110][57/78]\tTime 0.057 (Avg-Time 0.069)\t Loss 0.5856 (Avg-Loss 0.4818)\tAcc 78.5156 (Avg-Acc 82.8361)\n",
            "Epoch: [110][76/78]\tTime 0.057 (Avg-Time 0.067)\t Loss 0.5198 (Avg-Loss 0.4917)\tAcc 79.4922 (Avg-Acc 82.4472)\n",
            "Epoch: [110][78/78]\tTime 0.020 (Avg-Time 0.066)\t Loss 0.7632 (Avg-Loss 0.4927)\tAcc 79.6875 (Avg-Acc 82.4200)\n",
            "EPOCH: 110 train Results: Acc 82.420 Loss: 0.4927\n",
            "Epoch: [110][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5468 (Avg-Loss 1.5468)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [110][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6779 (Avg-Loss 1.5995)\tAcc 53.6765 (Avg-Acc 54.6500)\n",
            "EPOCH: 110 Validation Results: Acc 54.650 Loss: 1.5995\n",
            "Epoch: [111][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4632 (Avg-Loss 0.4632)\tAcc 82.2266 (Avg-Acc 82.2266)\n",
            "Epoch: [111][19/78]\tTime 0.077 (Avg-Time 0.061)\t Loss 0.5067 (Avg-Loss 0.4658)\tAcc 81.6406 (Avg-Acc 83.6523)\n",
            "Epoch: [111][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4889 (Avg-Loss 0.4794)\tAcc 83.5938 (Avg-Acc 83.0829)\n",
            "Epoch: [111][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4679 (Avg-Loss 0.4846)\tAcc 81.8359 (Avg-Acc 82.8226)\n",
            "Epoch: [111][76/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.4518 (Avg-Loss 0.4876)\tAcc 84.5703 (Avg-Acc 82.7009)\n",
            "Epoch: [111][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.6412 (Avg-Loss 0.4874)\tAcc 78.1250 (Avg-Acc 82.7025)\n",
            "EPOCH: 111 train Results: Acc 82.703 Loss: 0.4874\n",
            "Epoch: [111][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.5630 (Avg-Loss 1.5630)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [111][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6727 (Avg-Loss 1.5885)\tAcc 53.6765 (Avg-Acc 54.4600)\n",
            "EPOCH: 111 Validation Results: Acc 54.460 Loss: 1.5885\n",
            "Epoch: [112][0/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.4441 (Avg-Loss 0.4441)\tAcc 83.9844 (Avg-Acc 83.9844)\n",
            "Epoch: [112][19/78]\tTime 0.115 (Avg-Time 0.132)\t Loss 0.5678 (Avg-Loss 0.4697)\tAcc 82.4219 (Avg-Acc 83.4375)\n",
            "Epoch: [112][38/78]\tTime 0.057 (Avg-Time 0.110)\t Loss 0.4804 (Avg-Loss 0.4783)\tAcc 84.1797 (Avg-Acc 82.7774)\n",
            "Epoch: [112][57/78]\tTime 0.057 (Avg-Time 0.094)\t Loss 0.5142 (Avg-Loss 0.4853)\tAcc 81.8359 (Avg-Acc 82.6037)\n",
            "Epoch: [112][76/78]\tTime 0.057 (Avg-Time 0.086)\t Loss 0.4932 (Avg-Loss 0.4882)\tAcc 82.0312 (Avg-Acc 82.5081)\n",
            "Epoch: [112][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.9003 (Avg-Loss 0.4889)\tAcc 75.0000 (Avg-Acc 82.5200)\n",
            "EPOCH: 112 train Results: Acc 82.520 Loss: 0.4889\n",
            "Epoch: [112][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5394 (Avg-Loss 1.5394)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [112][19/19]\tTime 0.010 (Avg-Time 0.013)\t Loss 1.6580 (Avg-Loss 1.5971)\tAcc 56.9853 (Avg-Acc 54.4000)\n",
            "EPOCH: 112 Validation Results: Acc 54.400 Loss: 1.5971\n",
            "Epoch: [113][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4281 (Avg-Loss 0.4281)\tAcc 83.5938 (Avg-Acc 83.5938)\n",
            "Epoch: [113][19/78]\tTime 0.058 (Avg-Time 0.059)\t Loss 0.5151 (Avg-Loss 0.4797)\tAcc 82.2266 (Avg-Acc 82.6660)\n",
            "Epoch: [113][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4953 (Avg-Loss 0.4819)\tAcc 83.9844 (Avg-Acc 82.7073)\n",
            "Epoch: [113][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5202 (Avg-Loss 0.4882)\tAcc 81.0547 (Avg-Acc 82.4758)\n",
            "Epoch: [113][76/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4811 (Avg-Loss 0.4914)\tAcc 83.7891 (Avg-Acc 82.4244)\n",
            "Epoch: [113][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 0.7388 (Avg-Loss 0.4916)\tAcc 81.2500 (Avg-Acc 82.4175)\n",
            "EPOCH: 113 train Results: Acc 82.418 Loss: 0.4916\n",
            "Epoch: [113][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5666 (Avg-Loss 1.5666)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [113][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.6892 (Avg-Loss 1.5990)\tAcc 55.1471 (Avg-Acc 54.5400)\n",
            "EPOCH: 113 Validation Results: Acc 54.540 Loss: 1.5990\n",
            "Epoch: [114][0/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.4681 (Avg-Loss 0.4681)\tAcc 83.0078 (Avg-Acc 83.0078)\n",
            "Epoch: [114][19/78]\tTime 0.056 (Avg-Time 0.060)\t Loss 0.5049 (Avg-Loss 0.4801)\tAcc 83.3984 (Avg-Acc 82.7832)\n",
            "Epoch: [114][38/78]\tTime 0.150 (Avg-Time 0.083)\t Loss 0.4612 (Avg-Loss 0.4783)\tAcc 85.5469 (Avg-Acc 82.9327)\n",
            "Epoch: [114][57/78]\tTime 0.056 (Avg-Time 0.094)\t Loss 0.4699 (Avg-Loss 0.4843)\tAcc 84.7656 (Avg-Acc 82.7755)\n",
            "Epoch: [114][76/78]\tTime 0.057 (Avg-Time 0.086)\t Loss 0.5342 (Avg-Loss 0.4915)\tAcc 81.8359 (Avg-Acc 82.4244)\n",
            "Epoch: [114][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.5046 (Avg-Loss 0.4905)\tAcc 81.2500 (Avg-Acc 82.4600)\n",
            "EPOCH: 114 train Results: Acc 82.460 Loss: 0.4905\n",
            "Epoch: [114][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5623 (Avg-Loss 1.5623)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [114][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6826 (Avg-Loss 1.6046)\tAcc 56.2500 (Avg-Acc 54.9400)\n",
            "EPOCH: 114 Validation Results: Acc 54.940 Loss: 1.6046\n",
            "Epoch: [115][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.4313 (Avg-Loss 0.4313)\tAcc 86.7188 (Avg-Acc 86.7188)\n",
            "Epoch: [115][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4453 (Avg-Loss 0.4612)\tAcc 83.3984 (Avg-Acc 83.2910)\n",
            "Epoch: [115][38/78]\tTime 0.061 (Avg-Time 0.060)\t Loss 0.4432 (Avg-Loss 0.4651)\tAcc 83.5938 (Avg-Acc 83.1781)\n",
            "Epoch: [115][57/78]\tTime 0.066 (Avg-Time 0.060)\t Loss 0.4754 (Avg-Loss 0.4742)\tAcc 80.8594 (Avg-Acc 82.9910)\n",
            "Epoch: [115][76/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4686 (Avg-Loss 0.4814)\tAcc 84.7656 (Avg-Acc 82.7668)\n",
            "Epoch: [115][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.5268 (Avg-Loss 0.4819)\tAcc 79.6875 (Avg-Acc 82.7575)\n",
            "EPOCH: 115 train Results: Acc 82.757 Loss: 0.4819\n",
            "Epoch: [115][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5303 (Avg-Loss 1.5303)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [115][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6457 (Avg-Loss 1.5944)\tAcc 54.4118 (Avg-Acc 54.9300)\n",
            "EPOCH: 115 Validation Results: Acc 54.930 Loss: 1.5944\n",
            "Epoch: [116][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.4108 (Avg-Loss 0.4108)\tAcc 85.9375 (Avg-Acc 85.9375)\n",
            "Epoch: [116][19/78]\tTime 0.062 (Avg-Time 0.060)\t Loss 0.4762 (Avg-Loss 0.4623)\tAcc 84.7656 (Avg-Acc 83.7012)\n",
            "Epoch: [116][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5457 (Avg-Loss 0.4743)\tAcc 80.0781 (Avg-Acc 83.1581)\n",
            "Epoch: [116][57/78]\tTime 0.182 (Avg-Time 0.076)\t Loss 0.4292 (Avg-Loss 0.4794)\tAcc 84.3750 (Avg-Acc 82.8260)\n",
            "Epoch: [116][76/78]\tTime 0.057 (Avg-Time 0.087)\t Loss 0.5228 (Avg-Loss 0.4873)\tAcc 80.0781 (Avg-Acc 82.5183)\n",
            "Epoch: [116][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.5953 (Avg-Loss 0.4881)\tAcc 71.8750 (Avg-Acc 82.4825)\n",
            "EPOCH: 116 train Results: Acc 82.483 Loss: 0.4881\n",
            "Epoch: [116][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5270 (Avg-Loss 1.5270)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [116][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.6745 (Avg-Loss 1.5859)\tAcc 56.6176 (Avg-Acc 54.6800)\n",
            "EPOCH: 116 Validation Results: Acc 54.680 Loss: 1.5859\n",
            "Epoch: [117][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.5249 (Avg-Loss 0.5249)\tAcc 81.8359 (Avg-Acc 81.8359)\n",
            "Epoch: [117][19/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4952 (Avg-Loss 0.4701)\tAcc 81.4453 (Avg-Acc 83.4668)\n",
            "Epoch: [117][38/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4980 (Avg-Loss 0.4734)\tAcc 81.6406 (Avg-Acc 83.1681)\n",
            "Epoch: [117][57/78]\tTime 0.056 (Avg-Time 0.060)\t Loss 0.5050 (Avg-Loss 0.4785)\tAcc 82.2266 (Avg-Acc 83.0752)\n",
            "Epoch: [117][76/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.5196 (Avg-Loss 0.4814)\tAcc 81.6406 (Avg-Acc 82.8937)\n",
            "Epoch: [117][78/78]\tTime 0.021 (Avg-Time 0.060)\t Loss 0.4201 (Avg-Loss 0.4822)\tAcc 84.3750 (Avg-Acc 82.8325)\n",
            "EPOCH: 117 train Results: Acc 82.832 Loss: 0.4822\n",
            "Epoch: [117][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6014 (Avg-Loss 1.6014)\tAcc 54.2969 (Avg-Acc 54.2969)\n",
            "Epoch: [117][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6829 (Avg-Loss 1.6233)\tAcc 53.6765 (Avg-Acc 54.5200)\n",
            "EPOCH: 117 Validation Results: Acc 54.520 Loss: 1.6233\n",
            "Epoch: [118][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4953 (Avg-Loss 0.4953)\tAcc 83.2031 (Avg-Acc 83.2031)\n",
            "Epoch: [118][19/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4810 (Avg-Loss 0.4542)\tAcc 82.4219 (Avg-Acc 84.1699)\n",
            "Epoch: [118][38/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4579 (Avg-Loss 0.4640)\tAcc 82.8125 (Avg-Acc 83.6639)\n",
            "Epoch: [118][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4892 (Avg-Loss 0.4722)\tAcc 84.1797 (Avg-Acc 83.3917)\n",
            "Epoch: [118][76/78]\tTime 0.237 (Avg-Time 0.072)\t Loss 0.4560 (Avg-Loss 0.4751)\tAcc 85.3516 (Avg-Acc 83.1346)\n",
            "Epoch: [118][78/78]\tTime 0.211 (Avg-Time 0.076)\t Loss 0.5480 (Avg-Loss 0.4742)\tAcc 82.8125 (Avg-Acc 83.1650)\n",
            "EPOCH: 118 train Results: Acc 83.165 Loss: 0.4742\n",
            "Epoch: [118][0/19]\tTime 0.083 (Avg-Time 0.083)\t Loss 1.5481 (Avg-Loss 1.5481)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [118][19/19]\tTime 0.043 (Avg-Time 0.032)\t Loss 1.7152 (Avg-Loss 1.6198)\tAcc 53.3088 (Avg-Acc 54.4200)\n",
            "EPOCH: 118 Validation Results: Acc 54.420 Loss: 1.6198\n",
            "Epoch: [119][0/78]\tTime 0.195 (Avg-Time 0.195)\t Loss 0.4681 (Avg-Loss 0.4681)\tAcc 81.4453 (Avg-Acc 81.4453)\n",
            "Epoch: [119][19/78]\tTime 0.061 (Avg-Time 0.088)\t Loss 0.4606 (Avg-Loss 0.4657)\tAcc 81.8359 (Avg-Acc 83.4180)\n",
            "Epoch: [119][38/78]\tTime 0.057 (Avg-Time 0.075)\t Loss 0.4381 (Avg-Loss 0.4657)\tAcc 84.9609 (Avg-Acc 83.5136)\n",
            "Epoch: [119][57/78]\tTime 0.058 (Avg-Time 0.070)\t Loss 0.4844 (Avg-Loss 0.4666)\tAcc 82.8125 (Avg-Acc 83.4220)\n",
            "Epoch: [119][76/78]\tTime 0.059 (Avg-Time 0.068)\t Loss 0.4728 (Avg-Loss 0.4698)\tAcc 82.6172 (Avg-Acc 83.2462)\n",
            "Epoch: [119][78/78]\tTime 0.020 (Avg-Time 0.067)\t Loss 0.5257 (Avg-Loss 0.4706)\tAcc 81.2500 (Avg-Acc 83.2150)\n",
            "EPOCH: 119 train Results: Acc 83.215 Loss: 0.4706\n",
            "Epoch: [119][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5885 (Avg-Loss 1.5885)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [119][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7349 (Avg-Loss 1.6146)\tAcc 52.9412 (Avg-Acc 54.4400)\n",
            "EPOCH: 119 Validation Results: Acc 54.440 Loss: 1.6146\n",
            "Epoch: [120][0/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.5160 (Avg-Loss 0.5160)\tAcc 81.8359 (Avg-Acc 81.8359)\n",
            "Epoch: [120][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5198 (Avg-Loss 0.4646)\tAcc 80.8594 (Avg-Acc 83.3203)\n",
            "Epoch: [120][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4642 (Avg-Loss 0.4697)\tAcc 83.0078 (Avg-Acc 83.1230)\n",
            "Epoch: [120][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5357 (Avg-Loss 0.4727)\tAcc 82.0312 (Avg-Acc 83.1055)\n",
            "Epoch: [120][76/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5027 (Avg-Loss 0.4774)\tAcc 82.8125 (Avg-Acc 82.9292)\n",
            "Epoch: [120][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.8820 (Avg-Loss 0.4787)\tAcc 76.5625 (Avg-Acc 82.9100)\n",
            "EPOCH: 120 train Results: Acc 82.910 Loss: 0.4787\n",
            "Epoch: [120][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5846 (Avg-Loss 1.5846)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [120][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6984 (Avg-Loss 1.6152)\tAcc 54.0441 (Avg-Acc 54.6700)\n",
            "EPOCH: 120 Validation Results: Acc 54.670 Loss: 1.6152\n",
            "Epoch: [121][0/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.4538 (Avg-Loss 0.4538)\tAcc 84.5703 (Avg-Acc 84.5703)\n",
            "Epoch: [121][19/78]\tTime 0.127 (Avg-Time 0.134)\t Loss 0.4445 (Avg-Loss 0.4616)\tAcc 84.1797 (Avg-Acc 83.8965)\n",
            "Epoch: [121][38/78]\tTime 0.057 (Avg-Time 0.110)\t Loss 0.4403 (Avg-Loss 0.4704)\tAcc 83.9844 (Avg-Acc 83.4335)\n",
            "Epoch: [121][57/78]\tTime 0.057 (Avg-Time 0.094)\t Loss 0.5230 (Avg-Loss 0.4748)\tAcc 82.0312 (Avg-Acc 83.2772)\n",
            "Epoch: [121][76/78]\tTime 0.058 (Avg-Time 0.085)\t Loss 0.5284 (Avg-Loss 0.4783)\tAcc 81.0547 (Avg-Acc 83.0332)\n",
            "Epoch: [121][78/78]\tTime 0.021 (Avg-Time 0.084)\t Loss 0.3883 (Avg-Loss 0.4783)\tAcc 89.0625 (Avg-Acc 83.0500)\n",
            "EPOCH: 121 train Results: Acc 83.050 Loss: 0.4783\n",
            "Epoch: [121][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5524 (Avg-Loss 1.5524)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [121][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6840 (Avg-Loss 1.5889)\tAcc 54.7794 (Avg-Acc 54.6100)\n",
            "EPOCH: 121 Validation Results: Acc 54.610 Loss: 1.5889\n",
            "Epoch: [122][0/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.4266 (Avg-Loss 0.4266)\tAcc 84.9609 (Avg-Acc 84.9609)\n",
            "Epoch: [122][19/78]\tTime 0.065 (Avg-Time 0.060)\t Loss 0.4343 (Avg-Loss 0.4522)\tAcc 85.1562 (Avg-Acc 83.7207)\n",
            "Epoch: [122][38/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5680 (Avg-Loss 0.4588)\tAcc 79.6875 (Avg-Acc 83.5887)\n",
            "Epoch: [122][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4920 (Avg-Loss 0.4663)\tAcc 81.6406 (Avg-Acc 83.3648)\n",
            "Epoch: [122][76/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4772 (Avg-Loss 0.4726)\tAcc 84.5703 (Avg-Acc 83.2868)\n",
            "Epoch: [122][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.6099 (Avg-Loss 0.4725)\tAcc 81.2500 (Avg-Acc 83.2675)\n",
            "EPOCH: 122 train Results: Acc 83.267 Loss: 0.4725\n",
            "Epoch: [122][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5773 (Avg-Loss 1.5773)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [122][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7033 (Avg-Loss 1.6126)\tAcc 54.0441 (Avg-Acc 54.5800)\n",
            "EPOCH: 122 Validation Results: Acc 54.580 Loss: 1.6126\n",
            "Epoch: [123][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4726 (Avg-Loss 0.4726)\tAcc 82.2266 (Avg-Acc 82.2266)\n",
            "Epoch: [123][19/78]\tTime 0.057 (Avg-Time 0.062)\t Loss 0.3967 (Avg-Loss 0.4424)\tAcc 85.1562 (Avg-Acc 83.9844)\n",
            "Epoch: [123][38/78]\tTime 0.150 (Avg-Time 0.085)\t Loss 0.4280 (Avg-Loss 0.4566)\tAcc 83.5938 (Avg-Acc 83.7139)\n",
            "Epoch: [123][57/78]\tTime 0.056 (Avg-Time 0.094)\t Loss 0.4496 (Avg-Loss 0.4645)\tAcc 84.1797 (Avg-Acc 83.3715)\n",
            "Epoch: [123][76/78]\tTime 0.058 (Avg-Time 0.086)\t Loss 0.4717 (Avg-Loss 0.4727)\tAcc 81.8359 (Avg-Acc 83.0864)\n",
            "Epoch: [123][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.6030 (Avg-Loss 0.4735)\tAcc 82.8125 (Avg-Acc 83.0700)\n",
            "EPOCH: 123 train Results: Acc 83.070 Loss: 0.4735\n",
            "Epoch: [123][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5469 (Avg-Loss 1.5469)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [123][19/19]\tTime 0.006 (Avg-Time 0.013)\t Loss 1.6987 (Avg-Loss 1.5969)\tAcc 54.0441 (Avg-Acc 54.8600)\n",
            "EPOCH: 123 Validation Results: Acc 54.860 Loss: 1.5969\n",
            "Epoch: [124][0/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.5299 (Avg-Loss 0.5299)\tAcc 81.0547 (Avg-Acc 81.0547)\n",
            "Epoch: [124][19/78]\tTime 0.061 (Avg-Time 0.060)\t Loss 0.5073 (Avg-Loss 0.4670)\tAcc 82.8125 (Avg-Acc 83.4766)\n",
            "Epoch: [124][38/78]\tTime 0.063 (Avg-Time 0.060)\t Loss 0.4893 (Avg-Loss 0.4675)\tAcc 82.0312 (Avg-Acc 83.4285)\n",
            "Epoch: [124][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5424 (Avg-Loss 0.4686)\tAcc 81.2500 (Avg-Acc 83.4153)\n",
            "Epoch: [124][76/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4972 (Avg-Loss 0.4736)\tAcc 82.6172 (Avg-Acc 83.1828)\n",
            "Epoch: [124][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 1.0755 (Avg-Loss 0.4748)\tAcc 65.6250 (Avg-Acc 83.1275)\n",
            "EPOCH: 124 train Results: Acc 83.127 Loss: 0.4748\n",
            "Epoch: [124][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5779 (Avg-Loss 1.5779)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [124][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.7491 (Avg-Loss 1.6268)\tAcc 54.4118 (Avg-Acc 54.9100)\n",
            "EPOCH: 124 Validation Results: Acc 54.910 Loss: 1.6268\n",
            "Epoch: [125][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.4325 (Avg-Loss 0.4325)\tAcc 84.1797 (Avg-Acc 84.1797)\n",
            "Epoch: [125][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4886 (Avg-Loss 0.4592)\tAcc 83.5938 (Avg-Acc 83.9355)\n",
            "Epoch: [125][38/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4644 (Avg-Loss 0.4654)\tAcc 83.3984 (Avg-Acc 83.5587)\n",
            "Epoch: [125][57/78]\tTime 0.120 (Avg-Time 0.077)\t Loss 0.5248 (Avg-Loss 0.4717)\tAcc 83.3984 (Avg-Acc 83.4186)\n",
            "Epoch: [125][76/78]\tTime 0.057 (Avg-Time 0.086)\t Loss 0.5278 (Avg-Loss 0.4757)\tAcc 81.6406 (Avg-Acc 83.2183)\n",
            "Epoch: [125][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.6140 (Avg-Loss 0.4764)\tAcc 79.6875 (Avg-Acc 83.1950)\n",
            "EPOCH: 125 train Results: Acc 83.195 Loss: 0.4764\n",
            "Epoch: [125][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5758 (Avg-Loss 1.5758)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [125][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.7386 (Avg-Loss 1.6209)\tAcc 54.7794 (Avg-Acc 54.7400)\n",
            "EPOCH: 125 Validation Results: Acc 54.740 Loss: 1.6209\n",
            "Epoch: [126][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.4056 (Avg-Loss 0.4056)\tAcc 85.5469 (Avg-Acc 85.5469)\n",
            "Epoch: [126][19/78]\tTime 0.057 (Avg-Time 0.059)\t Loss 0.4963 (Avg-Loss 0.4631)\tAcc 82.8125 (Avg-Acc 83.8086)\n",
            "Epoch: [126][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4794 (Avg-Loss 0.4635)\tAcc 82.0312 (Avg-Acc 83.5337)\n",
            "Epoch: [126][57/78]\tTime 0.076 (Avg-Time 0.060)\t Loss 0.5311 (Avg-Loss 0.4663)\tAcc 82.2266 (Avg-Acc 83.5197)\n",
            "Epoch: [126][76/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4626 (Avg-Loss 0.4709)\tAcc 84.1797 (Avg-Acc 83.2995)\n",
            "Epoch: [126][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 0.6106 (Avg-Loss 0.4712)\tAcc 76.5625 (Avg-Acc 83.2975)\n",
            "EPOCH: 126 train Results: Acc 83.297 Loss: 0.4712\n",
            "Epoch: [126][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5734 (Avg-Loss 1.5734)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [126][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.8021 (Avg-Loss 1.6240)\tAcc 54.0441 (Avg-Acc 54.7300)\n",
            "EPOCH: 126 Validation Results: Acc 54.730 Loss: 1.6240\n",
            "Epoch: [127][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4812 (Avg-Loss 0.4812)\tAcc 82.6172 (Avg-Acc 82.6172)\n",
            "Epoch: [127][19/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.5370 (Avg-Loss 0.4773)\tAcc 82.6172 (Avg-Acc 83.3105)\n",
            "Epoch: [127][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4784 (Avg-Loss 0.4689)\tAcc 83.2031 (Avg-Acc 83.4235)\n",
            "Epoch: [127][57/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.4531 (Avg-Loss 0.4691)\tAcc 84.1797 (Avg-Acc 83.4456)\n",
            "Epoch: [127][76/78]\tTime 0.126 (Avg-Time 0.065)\t Loss 0.4825 (Avg-Loss 0.4701)\tAcc 83.2031 (Avg-Acc 83.3858)\n",
            "Epoch: [127][78/78]\tTime 0.223 (Avg-Time 0.068)\t Loss 0.9580 (Avg-Loss 0.4717)\tAcc 70.3125 (Avg-Acc 83.3175)\n",
            "EPOCH: 127 train Results: Acc 83.317 Loss: 0.4717\n",
            "Epoch: [127][0/19]\tTime 0.069 (Avg-Time 0.069)\t Loss 1.5975 (Avg-Loss 1.5975)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [127][19/19]\tTime 0.015 (Avg-Time 0.041)\t Loss 1.7734 (Avg-Loss 1.6332)\tAcc 54.0441 (Avg-Acc 54.7100)\n",
            "EPOCH: 127 Validation Results: Acc 54.710 Loss: 1.6332\n",
            "Epoch: [128][0/78]\tTime 0.235 (Avg-Time 0.235)\t Loss 0.4151 (Avg-Loss 0.4151)\tAcc 85.1562 (Avg-Acc 85.1562)\n",
            "Epoch: [128][19/78]\tTime 0.057 (Avg-Time 0.104)\t Loss 0.4646 (Avg-Loss 0.4578)\tAcc 82.6172 (Avg-Acc 83.7598)\n",
            "Epoch: [128][38/78]\tTime 0.063 (Avg-Time 0.083)\t Loss 0.4833 (Avg-Loss 0.4645)\tAcc 83.9844 (Avg-Acc 83.6138)\n",
            "Epoch: [128][57/78]\tTime 0.057 (Avg-Time 0.075)\t Loss 0.4947 (Avg-Loss 0.4688)\tAcc 82.4219 (Avg-Acc 83.2200)\n",
            "Epoch: [128][76/78]\tTime 0.057 (Avg-Time 0.072)\t Loss 0.4958 (Avg-Loss 0.4738)\tAcc 82.0312 (Avg-Acc 83.0941)\n",
            "Epoch: [128][78/78]\tTime 0.020 (Avg-Time 0.071)\t Loss 0.6883 (Avg-Loss 0.4739)\tAcc 79.6875 (Avg-Acc 83.0725)\n",
            "EPOCH: 128 train Results: Acc 83.073 Loss: 0.4739\n",
            "Epoch: [128][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5850 (Avg-Loss 1.5850)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [128][19/19]\tTime 0.017 (Avg-Time 0.014)\t Loss 1.7827 (Avg-Loss 1.6141)\tAcc 54.0441 (Avg-Acc 54.6800)\n",
            "EPOCH: 128 Validation Results: Acc 54.680 Loss: 1.6141\n",
            "Epoch: [129][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.4083 (Avg-Loss 0.4083)\tAcc 84.7656 (Avg-Acc 84.7656)\n",
            "Epoch: [129][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4440 (Avg-Loss 0.4636)\tAcc 84.9609 (Avg-Acc 84.1016)\n",
            "Epoch: [129][38/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4865 (Avg-Loss 0.4659)\tAcc 82.0312 (Avg-Acc 83.7139)\n",
            "Epoch: [129][57/78]\tTime 0.061 (Avg-Time 0.060)\t Loss 0.4681 (Avg-Loss 0.4696)\tAcc 84.5703 (Avg-Acc 83.5331)\n",
            "Epoch: [129][76/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4534 (Avg-Loss 0.4727)\tAcc 84.1797 (Avg-Acc 83.4137)\n",
            "Epoch: [129][78/78]\tTime 0.021 (Avg-Time 0.060)\t Loss 0.6565 (Avg-Loss 0.4725)\tAcc 79.6875 (Avg-Acc 83.4150)\n",
            "EPOCH: 129 train Results: Acc 83.415 Loss: 0.4725\n",
            "Epoch: [129][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5891 (Avg-Loss 1.5891)\tAcc 54.8828 (Avg-Acc 54.8828)\n",
            "Epoch: [129][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7579 (Avg-Loss 1.6069)\tAcc 54.7794 (Avg-Acc 54.4600)\n",
            "EPOCH: 129 Validation Results: Acc 54.460 Loss: 1.6069\n",
            "Epoch: [130][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4901 (Avg-Loss 0.4901)\tAcc 82.4219 (Avg-Acc 82.4219)\n",
            "Epoch: [130][19/78]\tTime 0.107 (Avg-Time 0.081)\t Loss 0.4799 (Avg-Loss 0.4583)\tAcc 83.0078 (Avg-Acc 83.4668)\n",
            "Epoch: [130][38/78]\tTime 0.056 (Avg-Time 0.111)\t Loss 0.4620 (Avg-Loss 0.4634)\tAcc 83.5938 (Avg-Acc 83.4635)\n",
            "Epoch: [130][57/78]\tTime 0.060 (Avg-Time 0.094)\t Loss 0.4733 (Avg-Loss 0.4662)\tAcc 84.9609 (Avg-Acc 83.4287)\n",
            "Epoch: [130][76/78]\tTime 0.063 (Avg-Time 0.086)\t Loss 0.4600 (Avg-Loss 0.4688)\tAcc 83.7891 (Avg-Acc 83.3401)\n",
            "Epoch: [130][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.6496 (Avg-Loss 0.4691)\tAcc 79.6875 (Avg-Acc 83.3100)\n",
            "EPOCH: 130 train Results: Acc 83.310 Loss: 0.4691\n",
            "Epoch: [130][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5964 (Avg-Loss 1.5964)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [130][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7714 (Avg-Loss 1.6294)\tAcc 54.7794 (Avg-Acc 54.3800)\n",
            "EPOCH: 130 Validation Results: Acc 54.380 Loss: 1.6294\n",
            "Epoch: [131][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4494 (Avg-Loss 0.4494)\tAcc 85.7422 (Avg-Acc 85.7422)\n",
            "Epoch: [131][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4691 (Avg-Loss 0.4567)\tAcc 82.6172 (Avg-Acc 83.7793)\n",
            "Epoch: [131][38/78]\tTime 0.056 (Avg-Time 0.060)\t Loss 0.4508 (Avg-Loss 0.4637)\tAcc 83.2031 (Avg-Acc 83.5787)\n",
            "Epoch: [131][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4407 (Avg-Loss 0.4674)\tAcc 83.7891 (Avg-Acc 83.4085)\n",
            "Epoch: [131][76/78]\tTime 0.056 (Avg-Time 0.060)\t Loss 0.5224 (Avg-Loss 0.4716)\tAcc 82.4219 (Avg-Acc 83.3426)\n",
            "Epoch: [131][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 0.7855 (Avg-Loss 0.4713)\tAcc 67.1875 (Avg-Acc 83.3450)\n",
            "EPOCH: 131 train Results: Acc 83.345 Loss: 0.4713\n",
            "Epoch: [131][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6205 (Avg-Loss 1.6205)\tAcc 54.4922 (Avg-Acc 54.4922)\n",
            "Epoch: [131][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.7457 (Avg-Loss 1.6233)\tAcc 55.1471 (Avg-Acc 54.3200)\n",
            "EPOCH: 131 Validation Results: Acc 54.320 Loss: 1.6233\n",
            "Epoch: [132][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4294 (Avg-Loss 0.4294)\tAcc 84.5703 (Avg-Acc 84.5703)\n",
            "Epoch: [132][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4788 (Avg-Loss 0.4582)\tAcc 83.2031 (Avg-Acc 83.4961)\n",
            "Epoch: [132][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5134 (Avg-Loss 0.4551)\tAcc 82.6172 (Avg-Acc 83.9093)\n",
            "Epoch: [132][57/78]\tTime 0.079 (Avg-Time 0.095)\t Loss 0.5299 (Avg-Loss 0.4587)\tAcc 83.0078 (Avg-Acc 83.9204)\n",
            "Epoch: [132][76/78]\tTime 0.056 (Avg-Time 0.086)\t Loss 0.4467 (Avg-Loss 0.4630)\tAcc 83.3984 (Avg-Acc 83.6622)\n",
            "Epoch: [132][78/78]\tTime 0.021 (Avg-Time 0.085)\t Loss 0.5884 (Avg-Loss 0.4643)\tAcc 82.8125 (Avg-Acc 83.6250)\n",
            "EPOCH: 132 train Results: Acc 83.625 Loss: 0.4643\n",
            "Epoch: [132][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.6189 (Avg-Loss 1.6189)\tAcc 54.2969 (Avg-Acc 54.2969)\n",
            "Epoch: [132][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7071 (Avg-Loss 1.6126)\tAcc 54.7794 (Avg-Acc 54.4800)\n",
            "EPOCH: 132 Validation Results: Acc 54.480 Loss: 1.6126\n",
            "Epoch: [133][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4614 (Avg-Loss 0.4614)\tAcc 83.9844 (Avg-Acc 83.9844)\n",
            "Epoch: [133][19/78]\tTime 0.058 (Avg-Time 0.062)\t Loss 0.4213 (Avg-Loss 0.4455)\tAcc 83.5938 (Avg-Acc 84.3848)\n",
            "Epoch: [133][38/78]\tTime 0.056 (Avg-Time 0.062)\t Loss 0.4241 (Avg-Loss 0.4507)\tAcc 85.5469 (Avg-Acc 84.1196)\n",
            "Epoch: [133][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4422 (Avg-Loss 0.4550)\tAcc 84.5703 (Avg-Acc 83.9339)\n",
            "Epoch: [133][76/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.5821 (Avg-Loss 0.4634)\tAcc 82.4219 (Avg-Acc 83.7231)\n",
            "Epoch: [133][78/78]\tTime 0.021 (Avg-Time 0.061)\t Loss 0.5859 (Avg-Loss 0.4631)\tAcc 78.1250 (Avg-Acc 83.7125)\n",
            "EPOCH: 133 train Results: Acc 83.713 Loss: 0.4631\n",
            "Epoch: [133][0/19]\tTime 0.016 (Avg-Time 0.016)\t Loss 1.5942 (Avg-Loss 1.5942)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [133][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7039 (Avg-Loss 1.6209)\tAcc 56.9853 (Avg-Acc 54.1300)\n",
            "EPOCH: 133 Validation Results: Acc 54.130 Loss: 1.6209\n",
            "Epoch: [134][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4241 (Avg-Loss 0.4241)\tAcc 84.5703 (Avg-Acc 84.5703)\n",
            "Epoch: [134][19/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.5536 (Avg-Loss 0.4558)\tAcc 79.8828 (Avg-Acc 83.6621)\n",
            "Epoch: [134][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5102 (Avg-Loss 0.4533)\tAcc 81.4453 (Avg-Acc 83.8141)\n",
            "Epoch: [134][57/78]\tTime 0.130 (Avg-Time 0.061)\t Loss 0.4960 (Avg-Loss 0.4585)\tAcc 82.2266 (Avg-Acc 83.5230)\n",
            "Epoch: [134][76/78]\tTime 0.129 (Avg-Time 0.084)\t Loss 0.4531 (Avg-Loss 0.4610)\tAcc 83.2031 (Avg-Acc 83.3934)\n",
            "Epoch: [134][78/78]\tTime 0.027 (Avg-Time 0.084)\t Loss 0.7196 (Avg-Loss 0.4618)\tAcc 73.4375 (Avg-Acc 83.3525)\n",
            "EPOCH: 134 train Results: Acc 83.353 Loss: 0.4618\n",
            "Epoch: [134][0/19]\tTime 0.019 (Avg-Time 0.019)\t Loss 1.5933 (Avg-Loss 1.5933)\tAcc 54.8828 (Avg-Acc 54.8828)\n",
            "Epoch: [134][19/19]\tTime 0.007 (Avg-Time 0.016)\t Loss 1.7506 (Avg-Loss 1.6254)\tAcc 52.5735 (Avg-Acc 53.9900)\n",
            "EPOCH: 134 Validation Results: Acc 53.990 Loss: 1.6254\n",
            "Epoch: [135][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4193 (Avg-Loss 0.4193)\tAcc 86.1328 (Avg-Acc 86.1328)\n",
            "Epoch: [135][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4367 (Avg-Loss 0.4467)\tAcc 84.1797 (Avg-Acc 84.3555)\n",
            "Epoch: [135][38/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4716 (Avg-Loss 0.4464)\tAcc 82.8125 (Avg-Acc 84.1396)\n",
            "Epoch: [135][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4824 (Avg-Loss 0.4585)\tAcc 81.4453 (Avg-Acc 83.6375)\n",
            "Epoch: [135][76/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4782 (Avg-Loss 0.4621)\tAcc 82.0312 (Avg-Acc 83.5506)\n",
            "Epoch: [135][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 0.8620 (Avg-Loss 0.4621)\tAcc 71.8750 (Avg-Acc 83.5800)\n",
            "EPOCH: 135 train Results: Acc 83.580 Loss: 0.4621\n",
            "Epoch: [135][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5935 (Avg-Loss 1.5935)\tAcc 55.2734 (Avg-Acc 55.2734)\n",
            "Epoch: [135][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7890 (Avg-Loss 1.6237)\tAcc 52.2059 (Avg-Acc 54.3600)\n",
            "EPOCH: 135 Validation Results: Acc 54.360 Loss: 1.6237\n",
            "Epoch: [136][0/78]\tTime 0.081 (Avg-Time 0.081)\t Loss 0.4317 (Avg-Loss 0.4317)\tAcc 85.5469 (Avg-Acc 85.5469)\n",
            "Epoch: [136][19/78]\tTime 0.060 (Avg-Time 0.061)\t Loss 0.5143 (Avg-Loss 0.4573)\tAcc 82.2266 (Avg-Acc 84.0137)\n",
            "Epoch: [136][38/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5047 (Avg-Loss 0.4580)\tAcc 82.2266 (Avg-Acc 83.8892)\n",
            "Epoch: [136][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4668 (Avg-Loss 0.4672)\tAcc 83.0078 (Avg-Acc 83.4422)\n",
            "Epoch: [136][76/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4394 (Avg-Loss 0.4715)\tAcc 85.3516 (Avg-Acc 83.3071)\n",
            "Epoch: [136][78/78]\tTime 0.023 (Avg-Time 0.060)\t Loss 0.8419 (Avg-Loss 0.4726)\tAcc 73.4375 (Avg-Acc 83.2525)\n",
            "EPOCH: 136 train Results: Acc 83.252 Loss: 0.4726\n",
            "Epoch: [136][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5769 (Avg-Loss 1.5769)\tAcc 53.9062 (Avg-Acc 53.9062)\n",
            "Epoch: [136][19/19]\tTime 0.015 (Avg-Time 0.020)\t Loss 1.7648 (Avg-Loss 1.6384)\tAcc 54.0441 (Avg-Acc 54.5300)\n",
            "EPOCH: 136 Validation Results: Acc 54.530 Loss: 1.6384\n",
            "Epoch: [137][0/78]\tTime 0.138 (Avg-Time 0.138)\t Loss 0.4155 (Avg-Loss 0.4155)\tAcc 85.1562 (Avg-Acc 85.1562)\n",
            "Epoch: [137][19/78]\tTime 0.111 (Avg-Time 0.144)\t Loss 0.4709 (Avg-Loss 0.4447)\tAcc 83.5938 (Avg-Acc 84.1504)\n",
            "Epoch: [137][38/78]\tTime 0.058 (Avg-Time 0.107)\t Loss 0.4361 (Avg-Loss 0.4524)\tAcc 82.8125 (Avg-Acc 83.8642)\n",
            "Epoch: [137][57/78]\tTime 0.064 (Avg-Time 0.092)\t Loss 0.4273 (Avg-Loss 0.4560)\tAcc 85.9375 (Avg-Acc 83.7251)\n",
            "Epoch: [137][76/78]\tTime 0.057 (Avg-Time 0.084)\t Loss 0.4785 (Avg-Loss 0.4625)\tAcc 83.0078 (Avg-Acc 83.5177)\n",
            "Epoch: [137][78/78]\tTime 0.020 (Avg-Time 0.083)\t Loss 0.6020 (Avg-Loss 0.4629)\tAcc 81.2500 (Avg-Acc 83.5025)\n",
            "EPOCH: 137 train Results: Acc 83.502 Loss: 0.4629\n",
            "Epoch: [137][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5874 (Avg-Loss 1.5874)\tAcc 54.1016 (Avg-Acc 54.1016)\n",
            "Epoch: [137][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7427 (Avg-Loss 1.6267)\tAcc 55.8824 (Avg-Acc 54.3500)\n",
            "EPOCH: 137 Validation Results: Acc 54.350 Loss: 1.6267\n",
            "Epoch: [138][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.3450 (Avg-Loss 0.3450)\tAcc 87.6953 (Avg-Acc 87.6953)\n",
            "Epoch: [138][19/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.4978 (Avg-Loss 0.4455)\tAcc 81.6406 (Avg-Acc 84.3066)\n",
            "Epoch: [138][38/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4774 (Avg-Loss 0.4509)\tAcc 84.3750 (Avg-Acc 84.0845)\n",
            "Epoch: [138][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5197 (Avg-Loss 0.4576)\tAcc 81.2500 (Avg-Acc 83.8396)\n",
            "Epoch: [138][76/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.5290 (Avg-Loss 0.4646)\tAcc 82.6172 (Avg-Acc 83.5075)\n",
            "Epoch: [138][78/78]\tTime 0.035 (Avg-Time 0.060)\t Loss 0.6606 (Avg-Loss 0.4648)\tAcc 70.3125 (Avg-Acc 83.4900)\n",
            "EPOCH: 138 train Results: Acc 83.490 Loss: 0.4648\n",
            "Epoch: [138][0/19]\tTime 0.023 (Avg-Time 0.023)\t Loss 1.6168 (Avg-Loss 1.6168)\tAcc 53.9062 (Avg-Acc 53.9062)\n",
            "Epoch: [138][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7851 (Avg-Loss 1.6391)\tAcc 55.5147 (Avg-Acc 54.3800)\n",
            "EPOCH: 138 Validation Results: Acc 54.380 Loss: 1.6391\n",
            "Epoch: [139][0/78]\tTime 0.064 (Avg-Time 0.064)\t Loss 0.3941 (Avg-Loss 0.3941)\tAcc 86.5234 (Avg-Acc 86.5234)\n",
            "Epoch: [139][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4700 (Avg-Loss 0.4538)\tAcc 82.6172 (Avg-Acc 83.6230)\n",
            "Epoch: [139][38/78]\tTime 0.118 (Avg-Time 0.100)\t Loss 0.4318 (Avg-Loss 0.4527)\tAcc 83.9844 (Avg-Acc 83.8141)\n",
            "Epoch: [139][57/78]\tTime 0.056 (Avg-Time 0.094)\t Loss 0.4023 (Avg-Loss 0.4537)\tAcc 85.1562 (Avg-Acc 83.7689)\n",
            "Epoch: [139][76/78]\tTime 0.059 (Avg-Time 0.086)\t Loss 0.4749 (Avg-Loss 0.4603)\tAcc 82.4219 (Avg-Acc 83.5735)\n",
            "Epoch: [139][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.6661 (Avg-Loss 0.4618)\tAcc 84.3750 (Avg-Acc 83.5075)\n",
            "EPOCH: 139 train Results: Acc 83.507 Loss: 0.4618\n",
            "Epoch: [139][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6142 (Avg-Loss 1.6142)\tAcc 54.1016 (Avg-Acc 54.1016)\n",
            "Epoch: [139][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7937 (Avg-Loss 1.6396)\tAcc 55.5147 (Avg-Acc 54.1800)\n",
            "EPOCH: 139 Validation Results: Acc 54.180 Loss: 1.6396\n",
            "Epoch: [140][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4350 (Avg-Loss 0.4350)\tAcc 84.7656 (Avg-Acc 84.7656)\n",
            "Epoch: [140][19/78]\tTime 0.059 (Avg-Time 0.062)\t Loss 0.4249 (Avg-Loss 0.4407)\tAcc 87.5000 (Avg-Acc 84.9316)\n",
            "Epoch: [140][38/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.4391 (Avg-Loss 0.4408)\tAcc 85.1562 (Avg-Acc 84.8458)\n",
            "Epoch: [140][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.5126 (Avg-Loss 0.4424)\tAcc 83.2031 (Avg-Acc 84.6915)\n",
            "Epoch: [140][76/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4741 (Avg-Loss 0.4525)\tAcc 84.3750 (Avg-Acc 84.2304)\n",
            "Epoch: [140][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.8769 (Avg-Loss 0.4533)\tAcc 70.3125 (Avg-Acc 84.2050)\n",
            "EPOCH: 140 train Results: Acc 84.205 Loss: 0.4533\n",
            "Epoch: [140][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5899 (Avg-Loss 1.5899)\tAcc 54.6875 (Avg-Acc 54.6875)\n",
            "Epoch: [140][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7865 (Avg-Loss 1.6359)\tAcc 55.1471 (Avg-Acc 54.5000)\n",
            "EPOCH: 140 Validation Results: Acc 54.500 Loss: 1.6359\n",
            "Epoch: [141][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4396 (Avg-Loss 0.4396)\tAcc 85.1562 (Avg-Acc 85.1562)\n",
            "Epoch: [141][19/78]\tTime 0.065 (Avg-Time 0.060)\t Loss 0.4422 (Avg-Loss 0.4450)\tAcc 84.1797 (Avg-Acc 83.8379)\n",
            "Epoch: [141][38/78]\tTime 0.075 (Avg-Time 0.061)\t Loss 0.4406 (Avg-Loss 0.4535)\tAcc 84.9609 (Avg-Acc 83.6338)\n",
            "Epoch: [141][57/78]\tTime 0.116 (Avg-Time 0.086)\t Loss 0.4249 (Avg-Loss 0.4522)\tAcc 85.1562 (Avg-Acc 83.7150)\n",
            "Epoch: [141][76/78]\tTime 0.080 (Avg-Time 0.086)\t Loss 0.4982 (Avg-Loss 0.4550)\tAcc 82.6172 (Avg-Acc 83.5811)\n",
            "Epoch: [141][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.6992 (Avg-Loss 0.4559)\tAcc 73.4375 (Avg-Acc 83.5500)\n",
            "EPOCH: 141 train Results: Acc 83.550 Loss: 0.4559\n",
            "Epoch: [141][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5746 (Avg-Loss 1.5746)\tAcc 54.8828 (Avg-Acc 54.8828)\n",
            "Epoch: [141][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.7544 (Avg-Loss 1.6246)\tAcc 55.5147 (Avg-Acc 54.6800)\n",
            "EPOCH: 141 Validation Results: Acc 54.680 Loss: 1.6246\n",
            "Epoch: [142][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.3928 (Avg-Loss 0.3928)\tAcc 85.9375 (Avg-Acc 85.9375)\n",
            "Epoch: [142][19/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4352 (Avg-Loss 0.4455)\tAcc 83.7891 (Avg-Acc 84.1016)\n",
            "Epoch: [142][38/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.4501 (Avg-Loss 0.4462)\tAcc 83.7891 (Avg-Acc 84.0445)\n",
            "Epoch: [142][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4931 (Avg-Loss 0.4498)\tAcc 82.2266 (Avg-Acc 83.9137)\n",
            "Epoch: [142][76/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4220 (Avg-Loss 0.4530)\tAcc 84.7656 (Avg-Acc 83.8778)\n",
            "Epoch: [142][78/78]\tTime 0.021 (Avg-Time 0.060)\t Loss 0.5997 (Avg-Loss 0.4533)\tAcc 85.9375 (Avg-Acc 83.9100)\n",
            "EPOCH: 142 train Results: Acc 83.910 Loss: 0.4533\n",
            "Epoch: [142][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6251 (Avg-Loss 1.6251)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [142][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.8100 (Avg-Loss 1.6620)\tAcc 54.4118 (Avg-Acc 54.2600)\n",
            "EPOCH: 142 Validation Results: Acc 54.260 Loss: 1.6620\n",
            "Epoch: [143][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.3746 (Avg-Loss 0.3746)\tAcc 85.3516 (Avg-Acc 85.3516)\n",
            "Epoch: [143][19/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.5049 (Avg-Loss 0.4401)\tAcc 83.5938 (Avg-Acc 83.9844)\n",
            "Epoch: [143][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4226 (Avg-Loss 0.4334)\tAcc 83.7891 (Avg-Acc 84.4301)\n",
            "Epoch: [143][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4814 (Avg-Loss 0.4431)\tAcc 83.5938 (Avg-Acc 84.2033)\n",
            "Epoch: [143][76/78]\tTime 0.127 (Avg-Time 0.080)\t Loss 0.4659 (Avg-Loss 0.4501)\tAcc 83.2031 (Avg-Acc 83.8956)\n",
            "Epoch: [143][78/78]\tTime 0.032 (Avg-Time 0.079)\t Loss 0.7456 (Avg-Loss 0.4520)\tAcc 78.1250 (Avg-Acc 83.8225)\n",
            "EPOCH: 143 train Results: Acc 83.823 Loss: 0.4520\n",
            "Epoch: [143][0/19]\tTime 0.024 (Avg-Time 0.024)\t Loss 1.6055 (Avg-Loss 1.6055)\tAcc 56.0547 (Avg-Acc 56.0547)\n",
            "Epoch: [143][19/19]\tTime 0.015 (Avg-Time 0.029)\t Loss 1.8005 (Avg-Loss 1.6462)\tAcc 55.8824 (Avg-Acc 54.4700)\n",
            "EPOCH: 143 Validation Results: Acc 54.470 Loss: 1.6462\n",
            "Epoch: [144][0/78]\tTime 0.108 (Avg-Time 0.108)\t Loss 0.4350 (Avg-Loss 0.4350)\tAcc 83.7891 (Avg-Acc 83.7891)\n",
            "Epoch: [144][19/78]\tTime 0.083 (Avg-Time 0.070)\t Loss 0.4557 (Avg-Loss 0.4419)\tAcc 84.1797 (Avg-Acc 84.1504)\n",
            "Epoch: [144][38/78]\tTime 0.060 (Avg-Time 0.065)\t Loss 0.4218 (Avg-Loss 0.4454)\tAcc 83.5938 (Avg-Acc 84.1046)\n",
            "Epoch: [144][57/78]\tTime 0.058 (Avg-Time 0.064)\t Loss 0.4526 (Avg-Loss 0.4532)\tAcc 85.1562 (Avg-Acc 83.8665)\n",
            "Epoch: [144][76/78]\tTime 0.058 (Avg-Time 0.063)\t Loss 0.4696 (Avg-Loss 0.4534)\tAcc 84.3750 (Avg-Acc 83.7916)\n",
            "Epoch: [144][78/78]\tTime 0.020 (Avg-Time 0.062)\t Loss 0.5123 (Avg-Loss 0.4534)\tAcc 78.1250 (Avg-Acc 83.7850)\n",
            "EPOCH: 144 train Results: Acc 83.785 Loss: 0.4534\n",
            "Epoch: [144][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5813 (Avg-Loss 1.5813)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [144][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7501 (Avg-Loss 1.6393)\tAcc 55.8824 (Avg-Acc 54.3300)\n",
            "EPOCH: 144 Validation Results: Acc 54.330 Loss: 1.6393\n",
            "Epoch: [145][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4427 (Avg-Loss 0.4427)\tAcc 83.3984 (Avg-Acc 83.3984)\n",
            "Epoch: [145][19/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.4637 (Avg-Loss 0.4317)\tAcc 82.8125 (Avg-Acc 84.4043)\n",
            "Epoch: [145][38/78]\tTime 0.060 (Avg-Time 0.061)\t Loss 0.5104 (Avg-Loss 0.4418)\tAcc 81.6406 (Avg-Acc 84.0445)\n",
            "Epoch: [145][57/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4652 (Avg-Loss 0.4507)\tAcc 83.9844 (Avg-Acc 83.8093)\n",
            "Epoch: [145][76/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4611 (Avg-Loss 0.4521)\tAcc 84.1797 (Avg-Acc 83.8170)\n",
            "Epoch: [145][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.6942 (Avg-Loss 0.4536)\tAcc 75.0000 (Avg-Acc 83.7775)\n",
            "EPOCH: 145 train Results: Acc 83.778 Loss: 0.4536\n",
            "Epoch: [145][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5929 (Avg-Loss 1.5929)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [145][19/19]\tTime 0.008 (Avg-Time 0.013)\t Loss 1.7458 (Avg-Loss 1.6459)\tAcc 54.7794 (Avg-Acc 54.6100)\n",
            "EPOCH: 145 Validation Results: Acc 54.610 Loss: 1.6459\n",
            "Epoch: [146][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4303 (Avg-Loss 0.4303)\tAcc 84.7656 (Avg-Acc 84.7656)\n",
            "Epoch: [146][19/78]\tTime 0.153 (Avg-Time 0.149)\t Loss 0.4242 (Avg-Loss 0.4283)\tAcc 84.7656 (Avg-Acc 84.6289)\n",
            "Epoch: [146][38/78]\tTime 0.059 (Avg-Time 0.111)\t Loss 0.4916 (Avg-Loss 0.4463)\tAcc 83.9844 (Avg-Acc 84.1246)\n",
            "Epoch: [146][57/78]\tTime 0.056 (Avg-Time 0.094)\t Loss 0.4289 (Avg-Loss 0.4496)\tAcc 85.3516 (Avg-Acc 84.0686)\n",
            "Epoch: [146][76/78]\tTime 0.056 (Avg-Time 0.085)\t Loss 0.5021 (Avg-Loss 0.4559)\tAcc 81.4453 (Avg-Acc 83.7789)\n",
            "Epoch: [146][78/78]\tTime 0.020 (Avg-Time 0.084)\t Loss 0.5777 (Avg-Loss 0.4556)\tAcc 85.9375 (Avg-Acc 83.8100)\n",
            "EPOCH: 146 train Results: Acc 83.810 Loss: 0.4556\n",
            "Epoch: [146][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6134 (Avg-Loss 1.6134)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [146][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7687 (Avg-Loss 1.6541)\tAcc 55.8824 (Avg-Acc 54.2500)\n",
            "EPOCH: 146 Validation Results: Acc 54.250 Loss: 1.6541\n",
            "Epoch: [147][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4180 (Avg-Loss 0.4180)\tAcc 84.1797 (Avg-Acc 84.1797)\n",
            "Epoch: [147][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.3769 (Avg-Loss 0.4332)\tAcc 86.3281 (Avg-Acc 84.3457)\n",
            "Epoch: [147][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4477 (Avg-Loss 0.4459)\tAcc 83.5938 (Avg-Acc 84.1797)\n",
            "Epoch: [147][57/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.5235 (Avg-Loss 0.4438)\tAcc 81.6406 (Avg-Acc 84.2942)\n",
            "Epoch: [147][76/78]\tTime 0.061 (Avg-Time 0.060)\t Loss 0.4682 (Avg-Loss 0.4494)\tAcc 83.3984 (Avg-Acc 84.0732)\n",
            "Epoch: [147][78/78]\tTime 0.020 (Avg-Time 0.059)\t Loss 0.8556 (Avg-Loss 0.4503)\tAcc 68.7500 (Avg-Acc 84.0225)\n",
            "EPOCH: 147 train Results: Acc 84.022 Loss: 0.4503\n",
            "Epoch: [147][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6202 (Avg-Loss 1.6202)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [147][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7492 (Avg-Loss 1.6477)\tAcc 56.2500 (Avg-Acc 54.6200)\n",
            "EPOCH: 147 Validation Results: Acc 54.620 Loss: 1.6477\n",
            "Epoch: [148][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4171 (Avg-Loss 0.4171)\tAcc 84.9609 (Avg-Acc 84.9609)\n",
            "Epoch: [148][19/78]\tTime 0.084 (Avg-Time 0.061)\t Loss 0.4208 (Avg-Loss 0.4390)\tAcc 85.1562 (Avg-Acc 84.2578)\n",
            "Epoch: [148][38/78]\tTime 0.131 (Avg-Time 0.089)\t Loss 0.4562 (Avg-Loss 0.4458)\tAcc 84.5703 (Avg-Acc 84.1046)\n",
            "Epoch: [148][57/78]\tTime 0.059 (Avg-Time 0.094)\t Loss 0.4517 (Avg-Loss 0.4515)\tAcc 83.5938 (Avg-Acc 83.8295)\n",
            "Epoch: [148][76/78]\tTime 0.058 (Avg-Time 0.086)\t Loss 0.4757 (Avg-Loss 0.4548)\tAcc 82.4219 (Avg-Acc 83.7104)\n",
            "Epoch: [148][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.5176 (Avg-Loss 0.4544)\tAcc 84.3750 (Avg-Acc 83.7325)\n",
            "EPOCH: 148 train Results: Acc 83.733 Loss: 0.4544\n",
            "Epoch: [148][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5952 (Avg-Loss 1.5952)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [148][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7707 (Avg-Loss 1.6402)\tAcc 55.1471 (Avg-Acc 54.3500)\n",
            "EPOCH: 148 Validation Results: Acc 54.350 Loss: 1.6402\n",
            "Epoch: [149][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.4464 (Avg-Loss 0.4464)\tAcc 85.3516 (Avg-Acc 85.3516)\n",
            "Epoch: [149][19/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.4237 (Avg-Loss 0.4285)\tAcc 83.5938 (Avg-Acc 84.7559)\n",
            "Epoch: [149][38/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.4449 (Avg-Loss 0.4402)\tAcc 85.1562 (Avg-Acc 84.3349)\n",
            "Epoch: [149][57/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4185 (Avg-Loss 0.4441)\tAcc 84.5703 (Avg-Acc 84.2470)\n",
            "Epoch: [149][76/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.4498 (Avg-Loss 0.4463)\tAcc 84.3750 (Avg-Acc 84.1695)\n",
            "Epoch: [149][78/78]\tTime 0.032 (Avg-Time 0.060)\t Loss 0.9957 (Avg-Loss 0.4477)\tAcc 62.5000 (Avg-Acc 84.1325)\n",
            "EPOCH: 149 train Results: Acc 84.132 Loss: 0.4477\n",
            "Epoch: [149][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6000 (Avg-Loss 1.6000)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [149][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7936 (Avg-Loss 1.6558)\tAcc 55.1471 (Avg-Acc 54.4800)\n",
            "EPOCH: 149 Validation Results: Acc 54.480 Loss: 1.6558\n",
            "Epoch: [150][0/78]\tTime 0.058 (Avg-Time 0.058)\t Loss 0.3967 (Avg-Loss 0.3967)\tAcc 87.1094 (Avg-Acc 87.1094)\n",
            "Epoch: [150][19/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.4944 (Avg-Loss 0.4738)\tAcc 83.3984 (Avg-Acc 83.4766)\n",
            "Epoch: [150][38/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4100 (Avg-Loss 0.4697)\tAcc 85.9375 (Avg-Acc 83.4585)\n",
            "Epoch: [150][57/78]\tTime 0.124 (Avg-Time 0.067)\t Loss 0.4329 (Avg-Loss 0.4660)\tAcc 84.9609 (Avg-Acc 83.7453)\n",
            "Epoch: [150][76/78]\tTime 0.058 (Avg-Time 0.086)\t Loss 0.4644 (Avg-Loss 0.4639)\tAcc 83.5938 (Avg-Acc 83.6546)\n",
            "Epoch: [150][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.8562 (Avg-Loss 0.4642)\tAcc 71.8750 (Avg-Acc 83.6550)\n",
            "EPOCH: 150 train Results: Acc 83.655 Loss: 0.4642\n",
            "Epoch: [150][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6215 (Avg-Loss 1.6215)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [150][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.8018 (Avg-Loss 1.6334)\tAcc 53.6765 (Avg-Acc 54.1000)\n",
            "EPOCH: 150 Validation Results: Acc 54.100 Loss: 1.6334\n",
            "Epoch: [151][0/78]\tTime 0.084 (Avg-Time 0.084)\t Loss 0.3987 (Avg-Loss 0.3987)\tAcc 85.5469 (Avg-Acc 85.5469)\n",
            "Epoch: [151][19/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4838 (Avg-Loss 0.4326)\tAcc 81.4453 (Avg-Acc 84.6484)\n",
            "Epoch: [151][38/78]\tTime 0.060 (Avg-Time 0.061)\t Loss 0.4352 (Avg-Loss 0.4433)\tAcc 83.7891 (Avg-Acc 84.0345)\n",
            "Epoch: [151][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4025 (Avg-Loss 0.4451)\tAcc 85.3516 (Avg-Acc 84.0854)\n",
            "Epoch: [151][76/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.5295 (Avg-Loss 0.4499)\tAcc 80.8594 (Avg-Acc 83.9742)\n",
            "Epoch: [151][78/78]\tTime 0.022 (Avg-Time 0.060)\t Loss 0.7447 (Avg-Loss 0.4504)\tAcc 76.5625 (Avg-Acc 83.9375)\n",
            "EPOCH: 151 train Results: Acc 83.938 Loss: 0.4504\n",
            "Epoch: [151][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5872 (Avg-Loss 1.5872)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [151][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7358 (Avg-Loss 1.6338)\tAcc 55.5147 (Avg-Acc 53.9900)\n",
            "EPOCH: 151 Validation Results: Acc 53.990 Loss: 1.6338\n",
            "Epoch: [152][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4380 (Avg-Loss 0.4380)\tAcc 83.0078 (Avg-Acc 83.0078)\n",
            "Epoch: [152][19/78]\tTime 0.061 (Avg-Time 0.060)\t Loss 0.3976 (Avg-Loss 0.4171)\tAcc 87.6953 (Avg-Acc 85.3223)\n",
            "Epoch: [152][38/78]\tTime 0.071 (Avg-Time 0.060)\t Loss 0.4950 (Avg-Loss 0.4275)\tAcc 83.9844 (Avg-Acc 84.9409)\n",
            "Epoch: [152][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4707 (Avg-Loss 0.4361)\tAcc 84.5703 (Avg-Acc 84.7993)\n",
            "Epoch: [152][76/78]\tTime 0.115 (Avg-Time 0.062)\t Loss 0.5203 (Avg-Loss 0.4460)\tAcc 81.8359 (Avg-Acc 84.4917)\n",
            "Epoch: [152][78/78]\tTime 0.109 (Avg-Time 0.065)\t Loss 0.7334 (Avg-Loss 0.4462)\tAcc 78.1250 (Avg-Acc 84.4875)\n",
            "EPOCH: 152 train Results: Acc 84.487 Loss: 0.4462\n",
            "Epoch: [152][0/19]\tTime 0.069 (Avg-Time 0.069)\t Loss 1.6593 (Avg-Loss 1.6593)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [152][19/19]\tTime 0.016 (Avg-Time 0.064)\t Loss 1.7858 (Avg-Loss 1.6671)\tAcc 54.4118 (Avg-Acc 54.1600)\n",
            "EPOCH: 152 Validation Results: Acc 54.160 Loss: 1.6671\n",
            "Epoch: [153][0/78]\tTime 0.119 (Avg-Time 0.119)\t Loss 0.4335 (Avg-Loss 0.4335)\tAcc 84.5703 (Avg-Acc 84.5703)\n",
            "Epoch: [153][19/78]\tTime 0.059 (Avg-Time 0.091)\t Loss 0.4142 (Avg-Loss 0.4437)\tAcc 85.5469 (Avg-Acc 84.1699)\n",
            "Epoch: [153][38/78]\tTime 0.057 (Avg-Time 0.076)\t Loss 0.3824 (Avg-Loss 0.4404)\tAcc 85.5469 (Avg-Acc 84.3199)\n",
            "Epoch: [153][57/78]\tTime 0.058 (Avg-Time 0.071)\t Loss 0.5063 (Avg-Loss 0.4433)\tAcc 82.2266 (Avg-Acc 84.2403)\n",
            "Epoch: [153][76/78]\tTime 0.061 (Avg-Time 0.069)\t Loss 0.3968 (Avg-Loss 0.4465)\tAcc 86.9141 (Avg-Acc 84.1873)\n",
            "Epoch: [153][78/78]\tTime 0.029 (Avg-Time 0.068)\t Loss 0.5703 (Avg-Loss 0.4469)\tAcc 79.6875 (Avg-Acc 84.1500)\n",
            "EPOCH: 153 train Results: Acc 84.150 Loss: 0.4469\n",
            "Epoch: [153][0/19]\tTime 0.024 (Avg-Time 0.024)\t Loss 1.6282 (Avg-Loss 1.6282)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [153][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.7885 (Avg-Loss 1.6524)\tAcc 53.6765 (Avg-Acc 54.3800)\n",
            "EPOCH: 153 Validation Results: Acc 54.380 Loss: 1.6524\n",
            "Epoch: [154][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.3964 (Avg-Loss 0.3964)\tAcc 85.9375 (Avg-Acc 85.9375)\n",
            "Epoch: [154][19/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.4340 (Avg-Loss 0.4295)\tAcc 83.3984 (Avg-Acc 84.6191)\n",
            "Epoch: [154][38/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4262 (Avg-Loss 0.4357)\tAcc 83.9844 (Avg-Acc 84.4251)\n",
            "Epoch: [154][57/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4404 (Avg-Loss 0.4405)\tAcc 83.9844 (Avg-Acc 84.2706)\n",
            "Epoch: [154][76/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4560 (Avg-Loss 0.4443)\tAcc 83.0078 (Avg-Acc 84.0833)\n",
            "Epoch: [154][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.6630 (Avg-Loss 0.4445)\tAcc 75.0000 (Avg-Acc 84.0875)\n",
            "EPOCH: 154 train Results: Acc 84.088 Loss: 0.4445\n",
            "Epoch: [154][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6307 (Avg-Loss 1.6307)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [154][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.7512 (Avg-Loss 1.6709)\tAcc 54.7794 (Avg-Acc 54.3900)\n",
            "EPOCH: 154 Validation Results: Acc 54.390 Loss: 1.6709\n",
            "Epoch: [155][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4745 (Avg-Loss 0.4745)\tAcc 79.8828 (Avg-Acc 79.8828)\n",
            "Epoch: [155][19/78]\tTime 0.340 (Avg-Time 0.102)\t Loss 0.4068 (Avg-Loss 0.4389)\tAcc 84.5703 (Avg-Acc 84.1504)\n",
            "Epoch: [155][38/78]\tTime 0.058 (Avg-Time 0.110)\t Loss 0.4768 (Avg-Loss 0.4357)\tAcc 83.9844 (Avg-Acc 84.3600)\n",
            "Epoch: [155][57/78]\tTime 0.058 (Avg-Time 0.094)\t Loss 0.5156 (Avg-Loss 0.4471)\tAcc 81.6406 (Avg-Acc 84.0854)\n",
            "Epoch: [155][76/78]\tTime 0.060 (Avg-Time 0.086)\t Loss 0.4761 (Avg-Loss 0.4484)\tAcc 84.7656 (Avg-Acc 84.0148)\n",
            "Epoch: [155][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.9535 (Avg-Loss 0.4499)\tAcc 70.3125 (Avg-Acc 83.9825)\n",
            "EPOCH: 155 train Results: Acc 83.983 Loss: 0.4499\n",
            "Epoch: [155][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5967 (Avg-Loss 1.5967)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [155][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7005 (Avg-Loss 1.6574)\tAcc 56.6176 (Avg-Acc 54.6800)\n",
            "EPOCH: 155 Validation Results: Acc 54.680 Loss: 1.6574\n",
            "Epoch: [156][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.3900 (Avg-Loss 0.3900)\tAcc 85.7422 (Avg-Acc 85.7422)\n",
            "Epoch: [156][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4659 (Avg-Loss 0.4312)\tAcc 86.3281 (Avg-Acc 85.0000)\n",
            "Epoch: [156][38/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4387 (Avg-Loss 0.4337)\tAcc 82.8125 (Avg-Acc 84.6955)\n",
            "Epoch: [156][57/78]\tTime 0.082 (Avg-Time 0.061)\t Loss 0.4330 (Avg-Loss 0.4387)\tAcc 85.9375 (Avg-Acc 84.5939)\n",
            "Epoch: [156][76/78]\tTime 0.060 (Avg-Time 0.061)\t Loss 0.4602 (Avg-Loss 0.4414)\tAcc 82.6172 (Avg-Acc 84.4333)\n",
            "Epoch: [156][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.6171 (Avg-Loss 0.4420)\tAcc 76.5625 (Avg-Acc 84.4050)\n",
            "EPOCH: 156 train Results: Acc 84.405 Loss: 0.4420\n",
            "Epoch: [156][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6166 (Avg-Loss 1.6166)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [156][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7066 (Avg-Loss 1.6390)\tAcc 55.1471 (Avg-Acc 54.5400)\n",
            "EPOCH: 156 Validation Results: Acc 54.540 Loss: 1.6390\n",
            "Epoch: [157][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.3883 (Avg-Loss 0.3883)\tAcc 85.7422 (Avg-Acc 85.7422)\n",
            "Epoch: [157][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4919 (Avg-Loss 0.4292)\tAcc 82.4219 (Avg-Acc 84.7559)\n",
            "Epoch: [157][38/78]\tTime 0.180 (Avg-Time 0.086)\t Loss 0.4551 (Avg-Loss 0.4398)\tAcc 85.3516 (Avg-Acc 84.4251)\n",
            "Epoch: [157][57/78]\tTime 0.059 (Avg-Time 0.094)\t Loss 0.4142 (Avg-Loss 0.4404)\tAcc 85.3516 (Avg-Acc 84.3885)\n",
            "Epoch: [157][76/78]\tTime 0.057 (Avg-Time 0.086)\t Loss 0.4595 (Avg-Loss 0.4445)\tAcc 83.7891 (Avg-Acc 84.2558)\n",
            "Epoch: [157][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.6861 (Avg-Loss 0.4453)\tAcc 76.5625 (Avg-Acc 84.2550)\n",
            "EPOCH: 157 train Results: Acc 84.255 Loss: 0.4453\n",
            "Epoch: [157][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6296 (Avg-Loss 1.6296)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [157][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.6832 (Avg-Loss 1.6460)\tAcc 55.8824 (Avg-Acc 54.2800)\n",
            "EPOCH: 157 Validation Results: Acc 54.280 Loss: 1.6460\n",
            "Epoch: [158][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.3875 (Avg-Loss 0.3875)\tAcc 85.1562 (Avg-Acc 85.1562)\n",
            "Epoch: [158][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4293 (Avg-Loss 0.4412)\tAcc 83.7891 (Avg-Acc 84.3164)\n",
            "Epoch: [158][38/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.3853 (Avg-Loss 0.4358)\tAcc 85.1562 (Avg-Acc 84.4802)\n",
            "Epoch: [158][57/78]\tTime 0.060 (Avg-Time 0.061)\t Loss 0.3607 (Avg-Loss 0.4369)\tAcc 86.7188 (Avg-Acc 84.3952)\n",
            "Epoch: [158][76/78]\tTime 0.064 (Avg-Time 0.061)\t Loss 0.4235 (Avg-Loss 0.4390)\tAcc 84.3750 (Avg-Acc 84.2735)\n",
            "Epoch: [158][78/78]\tTime 0.021 (Avg-Time 0.061)\t Loss 0.7219 (Avg-Loss 0.4396)\tAcc 73.4375 (Avg-Acc 84.2350)\n",
            "EPOCH: 158 train Results: Acc 84.235 Loss: 0.4396\n",
            "Epoch: [158][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6445 (Avg-Loss 1.6445)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [158][19/19]\tTime 0.020 (Avg-Time 0.014)\t Loss 1.7082 (Avg-Loss 1.6674)\tAcc 56.2500 (Avg-Acc 54.2500)\n",
            "EPOCH: 158 Validation Results: Acc 54.250 Loss: 1.6674\n",
            "Epoch: [159][0/78]\tTime 0.075 (Avg-Time 0.075)\t Loss 0.4307 (Avg-Loss 0.4307)\tAcc 84.9609 (Avg-Acc 84.9609)\n",
            "Epoch: [159][19/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4399 (Avg-Loss 0.4181)\tAcc 84.7656 (Avg-Acc 85.3613)\n",
            "Epoch: [159][38/78]\tTime 0.066 (Avg-Time 0.061)\t Loss 0.4743 (Avg-Loss 0.4216)\tAcc 82.0312 (Avg-Acc 85.0962)\n",
            "Epoch: [159][57/78]\tTime 0.233 (Avg-Time 0.077)\t Loss 0.4534 (Avg-Loss 0.4325)\tAcc 83.5938 (Avg-Acc 84.7016)\n",
            "Epoch: [159][76/78]\tTime 0.058 (Avg-Time 0.088)\t Loss 0.4779 (Avg-Loss 0.4371)\tAcc 83.0078 (Avg-Acc 84.5399)\n",
            "Epoch: [159][78/78]\tTime 0.020 (Avg-Time 0.086)\t Loss 0.6549 (Avg-Loss 0.4382)\tAcc 79.6875 (Avg-Acc 84.4900)\n",
            "EPOCH: 159 train Results: Acc 84.490 Loss: 0.4382\n",
            "Epoch: [159][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6528 (Avg-Loss 1.6528)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [159][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7162 (Avg-Loss 1.6608)\tAcc 54.0441 (Avg-Acc 54.0900)\n",
            "EPOCH: 159 Validation Results: Acc 54.090 Loss: 1.6608\n",
            "Epoch: [160][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.3763 (Avg-Loss 0.3763)\tAcc 87.5000 (Avg-Acc 87.5000)\n",
            "Epoch: [160][19/78]\tTime 0.067 (Avg-Time 0.061)\t Loss 0.4276 (Avg-Loss 0.4340)\tAcc 85.5469 (Avg-Acc 84.6973)\n",
            "Epoch: [160][38/78]\tTime 0.085 (Avg-Time 0.062)\t Loss 0.3566 (Avg-Loss 0.4306)\tAcc 87.5000 (Avg-Acc 84.7005)\n",
            "Epoch: [160][57/78]\tTime 0.058 (Avg-Time 0.062)\t Loss 0.4421 (Avg-Loss 0.4318)\tAcc 84.3750 (Avg-Acc 84.5703)\n",
            "Epoch: [160][76/78]\tTime 0.059 (Avg-Time 0.062)\t Loss 0.4570 (Avg-Loss 0.4355)\tAcc 81.8359 (Avg-Acc 84.5120)\n",
            "Epoch: [160][78/78]\tTime 0.020 (Avg-Time 0.061)\t Loss 0.6303 (Avg-Loss 0.4366)\tAcc 85.9375 (Avg-Acc 84.4600)\n",
            "EPOCH: 160 train Results: Acc 84.460 Loss: 0.4366\n",
            "Epoch: [160][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6537 (Avg-Loss 1.6537)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [160][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7621 (Avg-Loss 1.6753)\tAcc 54.7794 (Avg-Acc 54.4100)\n",
            "EPOCH: 160 Validation Results: Acc 54.410 Loss: 1.6753\n",
            "Epoch: [161][0/78]\tTime 0.064 (Avg-Time 0.064)\t Loss 0.4164 (Avg-Loss 0.4164)\tAcc 85.1562 (Avg-Acc 85.1562)\n",
            "Epoch: [161][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.3963 (Avg-Loss 0.4414)\tAcc 83.7891 (Avg-Acc 84.5020)\n",
            "Epoch: [161][38/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4239 (Avg-Loss 0.4378)\tAcc 85.1562 (Avg-Acc 84.7806)\n",
            "Epoch: [161][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4162 (Avg-Loss 0.4338)\tAcc 86.3281 (Avg-Acc 84.7320)\n",
            "Epoch: [161][76/78]\tTime 0.186 (Avg-Time 0.069)\t Loss 0.4899 (Avg-Loss 0.4401)\tAcc 83.7891 (Avg-Acc 84.6185)\n",
            "Epoch: [161][78/78]\tTime 0.042 (Avg-Time 0.071)\t Loss 0.7889 (Avg-Loss 0.4416)\tAcc 75.0000 (Avg-Acc 84.5575)\n",
            "EPOCH: 161 train Results: Acc 84.558 Loss: 0.4416\n",
            "Epoch: [161][0/19]\tTime 0.037 (Avg-Time 0.037)\t Loss 1.6514 (Avg-Loss 1.6514)\tAcc 55.0781 (Avg-Acc 55.0781)\n",
            "Epoch: [161][19/19]\tTime 0.012 (Avg-Time 0.043)\t Loss 1.7386 (Avg-Loss 1.6640)\tAcc 56.2500 (Avg-Acc 54.4900)\n",
            "EPOCH: 161 Validation Results: Acc 54.490 Loss: 1.6640\n",
            "Epoch: [162][0/78]\tTime 0.126 (Avg-Time 0.126)\t Loss 0.4510 (Avg-Loss 0.4510)\tAcc 83.5938 (Avg-Acc 83.5938)\n",
            "Epoch: [162][19/78]\tTime 0.059 (Avg-Time 0.088)\t Loss 0.4549 (Avg-Loss 0.4361)\tAcc 83.3984 (Avg-Acc 84.5020)\n",
            "Epoch: [162][38/78]\tTime 0.058 (Avg-Time 0.075)\t Loss 0.4516 (Avg-Loss 0.4347)\tAcc 83.0078 (Avg-Acc 84.3500)\n",
            "Epoch: [162][57/78]\tTime 0.058 (Avg-Time 0.070)\t Loss 0.5115 (Avg-Loss 0.4394)\tAcc 82.8125 (Avg-Acc 84.3312)\n",
            "Epoch: [162][76/78]\tTime 0.058 (Avg-Time 0.068)\t Loss 0.4863 (Avg-Loss 0.4407)\tAcc 83.7891 (Avg-Acc 84.3547)\n",
            "Epoch: [162][78/78]\tTime 0.021 (Avg-Time 0.067)\t Loss 0.7425 (Avg-Loss 0.4415)\tAcc 71.8750 (Avg-Acc 84.3100)\n",
            "EPOCH: 162 train Results: Acc 84.310 Loss: 0.4415\n",
            "Epoch: [162][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6584 (Avg-Loss 1.6584)\tAcc 55.2734 (Avg-Acc 55.2734)\n",
            "Epoch: [162][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.7883 (Avg-Loss 1.6752)\tAcc 54.7794 (Avg-Acc 54.5000)\n",
            "EPOCH: 162 Validation Results: Acc 54.500 Loss: 1.6752\n",
            "Epoch: [163][0/78]\tTime 0.065 (Avg-Time 0.065)\t Loss 0.3928 (Avg-Loss 0.3928)\tAcc 84.7656 (Avg-Acc 84.7656)\n",
            "Epoch: [163][19/78]\tTime 0.057 (Avg-Time 0.062)\t Loss 0.4275 (Avg-Loss 0.4271)\tAcc 84.3750 (Avg-Acc 84.7656)\n",
            "Epoch: [163][38/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4692 (Avg-Loss 0.4323)\tAcc 83.9844 (Avg-Acc 84.6254)\n",
            "Epoch: [163][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4721 (Avg-Loss 0.4300)\tAcc 83.2031 (Avg-Acc 84.6040)\n",
            "Epoch: [163][76/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4746 (Avg-Loss 0.4360)\tAcc 84.3750 (Avg-Acc 84.4663)\n",
            "Epoch: [163][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.9413 (Avg-Loss 0.4363)\tAcc 76.5625 (Avg-Acc 84.4825)\n",
            "EPOCH: 163 train Results: Acc 84.483 Loss: 0.4363\n",
            "Epoch: [163][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6504 (Avg-Loss 1.6504)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [163][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7447 (Avg-Loss 1.6625)\tAcc 54.4118 (Avg-Acc 54.9100)\n",
            "EPOCH: 163 Validation Results: Acc 54.910 Loss: 1.6625\n",
            "Epoch: [164][0/78]\tTime 0.070 (Avg-Time 0.070)\t Loss 0.4514 (Avg-Loss 0.4514)\tAcc 81.8359 (Avg-Acc 81.8359)\n",
            "Epoch: [164][19/78]\tTime 0.113 (Avg-Time 0.137)\t Loss 0.3972 (Avg-Loss 0.4349)\tAcc 85.7422 (Avg-Acc 84.6387)\n",
            "Epoch: [164][38/78]\tTime 0.082 (Avg-Time 0.111)\t Loss 0.4826 (Avg-Loss 0.4321)\tAcc 83.2031 (Avg-Acc 84.7656)\n",
            "Epoch: [164][57/78]\tTime 0.058 (Avg-Time 0.095)\t Loss 0.4400 (Avg-Loss 0.4385)\tAcc 83.9844 (Avg-Acc 84.4289)\n",
            "Epoch: [164][76/78]\tTime 0.058 (Avg-Time 0.086)\t Loss 0.4642 (Avg-Loss 0.4423)\tAcc 83.7891 (Avg-Acc 84.2964)\n",
            "Epoch: [164][78/78]\tTime 0.021 (Avg-Time 0.085)\t Loss 0.5898 (Avg-Loss 0.4422)\tAcc 79.6875 (Avg-Acc 84.2900)\n",
            "EPOCH: 164 train Results: Acc 84.290 Loss: 0.4422\n",
            "Epoch: [164][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6501 (Avg-Loss 1.6501)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [164][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7338 (Avg-Loss 1.6701)\tAcc 54.7794 (Avg-Acc 54.6600)\n",
            "EPOCH: 164 Validation Results: Acc 54.660 Loss: 1.6701\n",
            "Epoch: [165][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.3734 (Avg-Loss 0.3734)\tAcc 89.2578 (Avg-Acc 89.2578)\n",
            "Epoch: [165][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4979 (Avg-Loss 0.4283)\tAcc 83.3984 (Avg-Acc 85.0977)\n",
            "Epoch: [165][38/78]\tTime 0.062 (Avg-Time 0.061)\t Loss 0.3984 (Avg-Loss 0.4348)\tAcc 86.1328 (Avg-Acc 84.6454)\n",
            "Epoch: [165][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4096 (Avg-Loss 0.4338)\tAcc 85.5469 (Avg-Acc 84.7454)\n",
            "Epoch: [165][76/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4659 (Avg-Loss 0.4372)\tAcc 84.3750 (Avg-Acc 84.6540)\n",
            "Epoch: [165][78/78]\tTime 0.020 (Avg-Time 0.061)\t Loss 0.9604 (Avg-Loss 0.4391)\tAcc 68.7500 (Avg-Acc 84.5900)\n",
            "EPOCH: 165 train Results: Acc 84.590 Loss: 0.4391\n",
            "Epoch: [165][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.6307 (Avg-Loss 1.6307)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [165][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7195 (Avg-Loss 1.6539)\tAcc 53.3088 (Avg-Acc 54.4500)\n",
            "EPOCH: 165 Validation Results: Acc 54.450 Loss: 1.6539\n",
            "Epoch: [166][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4504 (Avg-Loss 0.4504)\tAcc 84.1797 (Avg-Acc 84.1797)\n",
            "Epoch: [166][19/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4335 (Avg-Loss 0.4250)\tAcc 84.5703 (Avg-Acc 84.8438)\n",
            "Epoch: [166][38/78]\tTime 0.191 (Avg-Time 0.086)\t Loss 0.4223 (Avg-Loss 0.4375)\tAcc 84.5703 (Avg-Acc 84.4201)\n",
            "Epoch: [166][57/78]\tTime 0.059 (Avg-Time 0.095)\t Loss 0.4701 (Avg-Loss 0.4417)\tAcc 83.3984 (Avg-Acc 84.3582)\n",
            "Epoch: [166][76/78]\tTime 0.057 (Avg-Time 0.086)\t Loss 0.3995 (Avg-Loss 0.4426)\tAcc 85.5469 (Avg-Acc 84.1822)\n",
            "Epoch: [166][78/78]\tTime 0.021 (Avg-Time 0.085)\t Loss 0.4916 (Avg-Loss 0.4439)\tAcc 76.5625 (Avg-Acc 84.1450)\n",
            "EPOCH: 166 train Results: Acc 84.145 Loss: 0.4439\n",
            "Epoch: [166][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6354 (Avg-Loss 1.6354)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [166][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.7058 (Avg-Loss 1.6655)\tAcc 54.4118 (Avg-Acc 54.5600)\n",
            "EPOCH: 166 Validation Results: Acc 54.560 Loss: 1.6655\n",
            "Epoch: [167][0/78]\tTime 0.063 (Avg-Time 0.063)\t Loss 0.4025 (Avg-Loss 0.4025)\tAcc 85.9375 (Avg-Acc 85.9375)\n",
            "Epoch: [167][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.3927 (Avg-Loss 0.4177)\tAcc 85.3516 (Avg-Acc 84.9414)\n",
            "Epoch: [167][38/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4298 (Avg-Loss 0.4184)\tAcc 85.3516 (Avg-Acc 85.0310)\n",
            "Epoch: [167][57/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4803 (Avg-Loss 0.4281)\tAcc 82.8125 (Avg-Acc 84.7724)\n",
            "Epoch: [167][76/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.5766 (Avg-Loss 0.4333)\tAcc 81.2500 (Avg-Acc 84.6515)\n",
            "Epoch: [167][78/78]\tTime 0.021 (Avg-Time 0.061)\t Loss 0.6155 (Avg-Loss 0.4335)\tAcc 81.2500 (Avg-Acc 84.6450)\n",
            "EPOCH: 167 train Results: Acc 84.645 Loss: 0.4335\n",
            "Epoch: [167][0/19]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.6581 (Avg-Loss 1.6581)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [167][19/19]\tTime 0.008 (Avg-Time 0.014)\t Loss 1.7521 (Avg-Loss 1.6696)\tAcc 53.6765 (Avg-Acc 54.8000)\n",
            "EPOCH: 167 Validation Results: Acc 54.800 Loss: 1.6696\n",
            "Epoch: [168][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4008 (Avg-Loss 0.4008)\tAcc 84.3750 (Avg-Acc 84.3750)\n",
            "Epoch: [168][19/78]\tTime 0.061 (Avg-Time 0.062)\t Loss 0.3745 (Avg-Loss 0.4056)\tAcc 87.1094 (Avg-Acc 85.3711)\n",
            "Epoch: [168][38/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4053 (Avg-Loss 0.4139)\tAcc 85.9375 (Avg-Acc 84.9659)\n",
            "Epoch: [168][57/78]\tTime 0.259 (Avg-Time 0.085)\t Loss 0.3947 (Avg-Loss 0.4197)\tAcc 86.3281 (Avg-Acc 84.8431)\n",
            "Epoch: [168][76/78]\tTime 0.058 (Avg-Time 0.088)\t Loss 0.4376 (Avg-Loss 0.4270)\tAcc 84.7656 (Avg-Acc 84.5500)\n",
            "Epoch: [168][78/78]\tTime 0.020 (Avg-Time 0.086)\t Loss 0.6408 (Avg-Loss 0.4283)\tAcc 79.6875 (Avg-Acc 84.5325)\n",
            "EPOCH: 168 train Results: Acc 84.532 Loss: 0.4283\n",
            "Epoch: [168][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6371 (Avg-Loss 1.6371)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [168][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7162 (Avg-Loss 1.6507)\tAcc 55.8824 (Avg-Acc 54.5800)\n",
            "EPOCH: 168 Validation Results: Acc 54.580 Loss: 1.6507\n",
            "Epoch: [169][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.3848 (Avg-Loss 0.3848)\tAcc 85.7422 (Avg-Acc 85.7422)\n",
            "Epoch: [169][19/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4314 (Avg-Loss 0.4250)\tAcc 83.7891 (Avg-Acc 85.0781)\n",
            "Epoch: [169][38/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.3881 (Avg-Loss 0.4170)\tAcc 85.3516 (Avg-Acc 85.2965)\n",
            "Epoch: [169][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4065 (Avg-Loss 0.4254)\tAcc 83.9844 (Avg-Acc 84.9811)\n",
            "Epoch: [169][76/78]\tTime 0.062 (Avg-Time 0.061)\t Loss 0.4583 (Avg-Loss 0.4297)\tAcc 85.1562 (Avg-Acc 84.8519)\n",
            "Epoch: [169][78/78]\tTime 0.020 (Avg-Time 0.061)\t Loss 0.5401 (Avg-Loss 0.4287)\tAcc 81.2500 (Avg-Acc 84.8950)\n",
            "EPOCH: 169 train Results: Acc 84.895 Loss: 0.4287\n",
            "Epoch: [169][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6458 (Avg-Loss 1.6458)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [169][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7080 (Avg-Loss 1.6792)\tAcc 56.6176 (Avg-Acc 54.8100)\n",
            "EPOCH: 169 Validation Results: Acc 54.810 Loss: 1.6792\n",
            "Epoch: [170][0/78]\tTime 0.063 (Avg-Time 0.063)\t Loss 0.3990 (Avg-Loss 0.3990)\tAcc 86.1328 (Avg-Acc 86.1328)\n",
            "Epoch: [170][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4137 (Avg-Loss 0.4295)\tAcc 83.9844 (Avg-Acc 85.0391)\n",
            "Epoch: [170][38/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4000 (Avg-Loss 0.4291)\tAcc 86.7188 (Avg-Acc 84.9058)\n",
            "Epoch: [170][57/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.4900 (Avg-Loss 0.4340)\tAcc 84.1797 (Avg-Acc 84.7959)\n",
            "Epoch: [170][76/78]\tTime 0.103 (Avg-Time 0.081)\t Loss 0.4582 (Avg-Loss 0.4353)\tAcc 84.1797 (Avg-Acc 84.6971)\n",
            "Epoch: [170][78/78]\tTime 0.058 (Avg-Time 0.082)\t Loss 0.3828 (Avg-Loss 0.4351)\tAcc 84.3750 (Avg-Acc 84.7025)\n",
            "EPOCH: 170 train Results: Acc 84.703 Loss: 0.4351\n",
            "Epoch: [170][0/19]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.6323 (Avg-Loss 1.6323)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [170][19/19]\tTime 0.016 (Avg-Time 0.031)\t Loss 1.7182 (Avg-Loss 1.6840)\tAcc 55.1471 (Avg-Acc 54.5400)\n",
            "EPOCH: 170 Validation Results: Acc 54.540 Loss: 1.6840\n",
            "Epoch: [171][0/78]\tTime 0.135 (Avg-Time 0.135)\t Loss 0.4001 (Avg-Loss 0.4001)\tAcc 86.7188 (Avg-Acc 86.7188)\n",
            "Epoch: [171][19/78]\tTime 0.060 (Avg-Time 0.066)\t Loss 0.4064 (Avg-Loss 0.4098)\tAcc 85.9375 (Avg-Acc 85.1758)\n",
            "Epoch: [171][38/78]\tTime 0.059 (Avg-Time 0.064)\t Loss 0.3735 (Avg-Loss 0.4137)\tAcc 86.9141 (Avg-Acc 85.1462)\n",
            "Epoch: [171][57/78]\tTime 0.060 (Avg-Time 0.063)\t Loss 0.3666 (Avg-Loss 0.4139)\tAcc 88.4766 (Avg-Acc 85.2404)\n",
            "Epoch: [171][76/78]\tTime 0.060 (Avg-Time 0.062)\t Loss 0.4653 (Avg-Loss 0.4238)\tAcc 82.8125 (Avg-Acc 84.8747)\n",
            "Epoch: [171][78/78]\tTime 0.020 (Avg-Time 0.062)\t Loss 0.5198 (Avg-Loss 0.4250)\tAcc 76.5625 (Avg-Acc 84.8200)\n",
            "EPOCH: 171 train Results: Acc 84.820 Loss: 0.4250\n",
            "Epoch: [171][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6288 (Avg-Loss 1.6288)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [171][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.7294 (Avg-Loss 1.6610)\tAcc 53.6765 (Avg-Acc 54.5000)\n",
            "EPOCH: 171 Validation Results: Acc 54.500 Loss: 1.6610\n",
            "Epoch: [172][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4769 (Avg-Loss 0.4769)\tAcc 82.4219 (Avg-Acc 82.4219)\n",
            "Epoch: [172][19/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.4314 (Avg-Loss 0.4247)\tAcc 84.1797 (Avg-Acc 84.8535)\n",
            "Epoch: [172][38/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4226 (Avg-Loss 0.4268)\tAcc 84.3750 (Avg-Acc 84.9058)\n",
            "Epoch: [172][57/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4331 (Avg-Loss 0.4341)\tAcc 84.7656 (Avg-Acc 84.6309)\n",
            "Epoch: [172][76/78]\tTime 0.062 (Avg-Time 0.061)\t Loss 0.4510 (Avg-Loss 0.4390)\tAcc 83.5938 (Avg-Acc 84.4739)\n",
            "Epoch: [172][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.4906 (Avg-Loss 0.4396)\tAcc 84.3750 (Avg-Acc 84.4425)\n",
            "EPOCH: 172 train Results: Acc 84.442 Loss: 0.4396\n",
            "Epoch: [172][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6060 (Avg-Loss 1.6060)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [172][19/19]\tTime 0.012 (Avg-Time 0.015)\t Loss 1.7134 (Avg-Loss 1.6577)\tAcc 53.6765 (Avg-Acc 54.5500)\n",
            "EPOCH: 172 Validation Results: Acc 54.550 Loss: 1.6577\n",
            "Epoch: [173][0/78]\tTime 0.115 (Avg-Time 0.115)\t Loss 0.4197 (Avg-Loss 0.4197)\tAcc 84.9609 (Avg-Acc 84.9609)\n",
            "Epoch: [173][19/78]\tTime 0.110 (Avg-Time 0.149)\t Loss 0.5060 (Avg-Loss 0.4084)\tAcc 82.6172 (Avg-Acc 85.4395)\n",
            "Epoch: [173][38/78]\tTime 0.061 (Avg-Time 0.111)\t Loss 0.4157 (Avg-Loss 0.4141)\tAcc 85.9375 (Avg-Acc 85.3766)\n",
            "Epoch: [173][57/78]\tTime 0.060 (Avg-Time 0.095)\t Loss 0.4405 (Avg-Loss 0.4221)\tAcc 83.9844 (Avg-Acc 84.9845)\n",
            "Epoch: [173][76/78]\tTime 0.058 (Avg-Time 0.086)\t Loss 0.3840 (Avg-Loss 0.4224)\tAcc 86.3281 (Avg-Acc 84.9153)\n",
            "Epoch: [173][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.5648 (Avg-Loss 0.4228)\tAcc 82.8125 (Avg-Acc 84.8975)\n",
            "EPOCH: 173 train Results: Acc 84.897 Loss: 0.4228\n",
            "Epoch: [173][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6239 (Avg-Loss 1.6239)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [173][19/19]\tTime 0.007 (Avg-Time 0.015)\t Loss 1.7032 (Avg-Loss 1.6483)\tAcc 54.0441 (Avg-Acc 54.2900)\n",
            "EPOCH: 173 Validation Results: Acc 54.290 Loss: 1.6483\n",
            "Epoch: [174][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4083 (Avg-Loss 0.4083)\tAcc 85.9375 (Avg-Acc 85.9375)\n",
            "Epoch: [174][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4426 (Avg-Loss 0.4168)\tAcc 83.7891 (Avg-Acc 84.8145)\n",
            "Epoch: [174][38/78]\tTime 0.080 (Avg-Time 0.062)\t Loss 0.4864 (Avg-Loss 0.4161)\tAcc 83.2031 (Avg-Acc 85.1362)\n",
            "Epoch: [174][57/78]\tTime 0.060 (Avg-Time 0.062)\t Loss 0.4638 (Avg-Loss 0.4245)\tAcc 83.0078 (Avg-Acc 84.9205)\n",
            "Epoch: [174][76/78]\tTime 0.061 (Avg-Time 0.062)\t Loss 0.4002 (Avg-Loss 0.4298)\tAcc 86.7188 (Avg-Acc 84.7225)\n",
            "Epoch: [174][78/78]\tTime 0.020 (Avg-Time 0.061)\t Loss 0.7400 (Avg-Loss 0.4305)\tAcc 73.4375 (Avg-Acc 84.6925)\n",
            "EPOCH: 174 train Results: Acc 84.692 Loss: 0.4305\n",
            "Epoch: [174][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6212 (Avg-Loss 1.6212)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [174][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7481 (Avg-Loss 1.6524)\tAcc 54.7794 (Avg-Acc 54.5500)\n",
            "EPOCH: 174 Validation Results: Acc 54.550 Loss: 1.6524\n",
            "Epoch: [175][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4205 (Avg-Loss 0.4205)\tAcc 87.1094 (Avg-Acc 87.1094)\n",
            "Epoch: [175][19/78]\tTime 0.068 (Avg-Time 0.062)\t Loss 0.5088 (Avg-Loss 0.4195)\tAcc 82.4219 (Avg-Acc 85.0977)\n",
            "Epoch: [175][38/78]\tTime 0.173 (Avg-Time 0.110)\t Loss 0.4190 (Avg-Loss 0.4297)\tAcc 83.2031 (Avg-Acc 84.7957)\n",
            "Epoch: [175][57/78]\tTime 0.057 (Avg-Time 0.095)\t Loss 0.4001 (Avg-Loss 0.4361)\tAcc 84.1797 (Avg-Acc 84.4659)\n",
            "Epoch: [175][76/78]\tTime 0.065 (Avg-Time 0.087)\t Loss 0.4458 (Avg-Loss 0.4383)\tAcc 82.4219 (Avg-Acc 84.4004)\n",
            "Epoch: [175][78/78]\tTime 0.039 (Avg-Time 0.086)\t Loss 0.6394 (Avg-Loss 0.4393)\tAcc 82.8125 (Avg-Acc 84.3700)\n",
            "EPOCH: 175 train Results: Acc 84.370 Loss: 0.4393\n",
            "Epoch: [175][0/19]\tTime 0.016 (Avg-Time 0.016)\t Loss 1.6237 (Avg-Loss 1.6237)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [175][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7088 (Avg-Loss 1.6501)\tAcc 53.6765 (Avg-Acc 54.3200)\n",
            "EPOCH: 175 Validation Results: Acc 54.320 Loss: 1.6501\n",
            "Epoch: [176][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4726 (Avg-Loss 0.4726)\tAcc 84.3750 (Avg-Acc 84.3750)\n",
            "Epoch: [176][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4096 (Avg-Loss 0.4189)\tAcc 86.5234 (Avg-Acc 85.7227)\n",
            "Epoch: [176][38/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.5112 (Avg-Loss 0.4221)\tAcc 81.4453 (Avg-Acc 85.4567)\n",
            "Epoch: [176][57/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.5318 (Avg-Loss 0.4247)\tAcc 83.2031 (Avg-Acc 85.2169)\n",
            "Epoch: [176][76/78]\tTime 0.063 (Avg-Time 0.061)\t Loss 0.4611 (Avg-Loss 0.4264)\tAcc 82.8125 (Avg-Acc 85.0700)\n",
            "Epoch: [176][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.3666 (Avg-Loss 0.4268)\tAcc 85.9375 (Avg-Acc 85.0625)\n",
            "EPOCH: 176 train Results: Acc 85.062 Loss: 0.4268\n",
            "Epoch: [176][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6461 (Avg-Loss 1.6461)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [176][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.7375 (Avg-Loss 1.6715)\tAcc 54.4118 (Avg-Acc 54.5800)\n",
            "EPOCH: 176 Validation Results: Acc 54.580 Loss: 1.6715\n",
            "Epoch: [177][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4777 (Avg-Loss 0.4777)\tAcc 83.7891 (Avg-Acc 83.7891)\n",
            "Epoch: [177][19/78]\tTime 0.060 (Avg-Time 0.061)\t Loss 0.3807 (Avg-Loss 0.4199)\tAcc 85.9375 (Avg-Acc 85.0586)\n",
            "Epoch: [177][38/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.3811 (Avg-Loss 0.4220)\tAcc 87.3047 (Avg-Acc 85.2464)\n",
            "Epoch: [177][57/78]\tTime 0.122 (Avg-Time 0.092)\t Loss 0.4694 (Avg-Loss 0.4201)\tAcc 81.6406 (Avg-Acc 85.3516)\n",
            "Epoch: [177][76/78]\tTime 0.057 (Avg-Time 0.087)\t Loss 0.3893 (Avg-Loss 0.4195)\tAcc 85.7422 (Avg-Acc 85.3363)\n",
            "Epoch: [177][78/78]\tTime 0.021 (Avg-Time 0.086)\t Loss 0.4972 (Avg-Loss 0.4193)\tAcc 82.8125 (Avg-Acc 85.3475)\n",
            "EPOCH: 177 train Results: Acc 85.347 Loss: 0.4193\n",
            "Epoch: [177][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6525 (Avg-Loss 1.6525)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [177][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7478 (Avg-Loss 1.6615)\tAcc 54.7794 (Avg-Acc 54.3100)\n",
            "EPOCH: 177 Validation Results: Acc 54.310 Loss: 1.6615\n",
            "Epoch: [178][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.4348 (Avg-Loss 0.4348)\tAcc 84.9609 (Avg-Acc 84.9609)\n",
            "Epoch: [178][19/78]\tTime 0.066 (Avg-Time 0.060)\t Loss 0.3595 (Avg-Loss 0.4039)\tAcc 86.7188 (Avg-Acc 85.5762)\n",
            "Epoch: [178][38/78]\tTime 0.060 (Avg-Time 0.061)\t Loss 0.4272 (Avg-Loss 0.4113)\tAcc 85.3516 (Avg-Acc 85.4918)\n",
            "Epoch: [178][57/78]\tTime 0.064 (Avg-Time 0.061)\t Loss 0.4613 (Avg-Loss 0.4147)\tAcc 83.9844 (Avg-Acc 85.3953)\n",
            "Epoch: [178][76/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.4811 (Avg-Loss 0.4231)\tAcc 82.0312 (Avg-Acc 85.1131)\n",
            "Epoch: [178][78/78]\tTime 0.020 (Avg-Time 0.061)\t Loss 0.6812 (Avg-Loss 0.4232)\tAcc 78.1250 (Avg-Acc 85.1075)\n",
            "EPOCH: 178 train Results: Acc 85.108 Loss: 0.4232\n",
            "Epoch: [178][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6206 (Avg-Loss 1.6206)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [178][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7183 (Avg-Loss 1.6446)\tAcc 53.6765 (Avg-Acc 54.3900)\n",
            "EPOCH: 178 Validation Results: Acc 54.390 Loss: 1.6446\n",
            "Epoch: [179][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4368 (Avg-Loss 0.4368)\tAcc 84.7656 (Avg-Acc 84.7656)\n",
            "Epoch: [179][19/78]\tTime 0.059 (Avg-Time 0.063)\t Loss 0.3695 (Avg-Loss 0.3949)\tAcc 86.7188 (Avg-Acc 85.9961)\n",
            "Epoch: [179][38/78]\tTime 0.073 (Avg-Time 0.063)\t Loss 0.4136 (Avg-Loss 0.4095)\tAcc 84.5703 (Avg-Acc 85.2965)\n",
            "Epoch: [179][57/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.3907 (Avg-Loss 0.4121)\tAcc 85.1562 (Avg-Acc 85.1226)\n",
            "Epoch: [179][76/78]\tTime 0.148 (Avg-Time 0.083)\t Loss 0.4368 (Avg-Loss 0.4168)\tAcc 83.5938 (Avg-Acc 84.9990)\n",
            "Epoch: [179][78/78]\tTime 0.045 (Avg-Time 0.083)\t Loss 0.5990 (Avg-Loss 0.4172)\tAcc 81.2500 (Avg-Acc 84.9775)\n",
            "EPOCH: 179 train Results: Acc 84.978 Loss: 0.4172\n",
            "Epoch: [179][0/19]\tTime 0.041 (Avg-Time 0.041)\t Loss 1.6379 (Avg-Loss 1.6379)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [179][19/19]\tTime 0.007 (Avg-Time 0.029)\t Loss 1.7837 (Avg-Loss 1.6748)\tAcc 55.1471 (Avg-Acc 54.4000)\n",
            "EPOCH: 179 Validation Results: Acc 54.400 Loss: 1.6748\n",
            "Epoch: [180][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4596 (Avg-Loss 0.4596)\tAcc 84.9609 (Avg-Acc 84.9609)\n",
            "Epoch: [180][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4227 (Avg-Loss 0.4193)\tAcc 85.1562 (Avg-Acc 85.2441)\n",
            "Epoch: [180][38/78]\tTime 0.062 (Avg-Time 0.061)\t Loss 0.4425 (Avg-Loss 0.4138)\tAcc 84.9609 (Avg-Acc 85.4517)\n",
            "Epoch: [180][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.3495 (Avg-Loss 0.4173)\tAcc 87.3047 (Avg-Acc 85.3011)\n",
            "Epoch: [180][76/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.4545 (Avg-Loss 0.4218)\tAcc 85.1562 (Avg-Acc 85.0497)\n",
            "Epoch: [180][78/78]\tTime 0.021 (Avg-Time 0.061)\t Loss 0.7606 (Avg-Loss 0.4227)\tAcc 71.8750 (Avg-Acc 85.0100)\n",
            "EPOCH: 180 train Results: Acc 85.010 Loss: 0.4227\n",
            "Epoch: [180][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6320 (Avg-Loss 1.6320)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [180][19/19]\tTime 0.008 (Avg-Time 0.013)\t Loss 1.7668 (Avg-Loss 1.6848)\tAcc 52.9412 (Avg-Acc 54.5800)\n",
            "EPOCH: 180 Validation Results: Acc 54.580 Loss: 1.6848\n",
            "Epoch: [181][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.3751 (Avg-Loss 0.3751)\tAcc 85.9375 (Avg-Acc 85.9375)\n",
            "Epoch: [181][19/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.4577 (Avg-Loss 0.4318)\tAcc 83.5938 (Avg-Acc 84.7754)\n",
            "Epoch: [181][38/78]\tTime 0.058 (Avg-Time 0.062)\t Loss 0.4030 (Avg-Loss 0.4283)\tAcc 85.5469 (Avg-Acc 84.9109)\n",
            "Epoch: [181][57/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.3865 (Avg-Loss 0.4324)\tAcc 86.1328 (Avg-Acc 84.6949)\n",
            "Epoch: [181][76/78]\tTime 0.059 (Avg-Time 0.062)\t Loss 0.4096 (Avg-Loss 0.4353)\tAcc 83.2031 (Avg-Acc 84.5576)\n",
            "Epoch: [181][78/78]\tTime 0.022 (Avg-Time 0.061)\t Loss 0.8395 (Avg-Loss 0.4368)\tAcc 78.1250 (Avg-Acc 84.5175)\n",
            "EPOCH: 181 train Results: Acc 84.517 Loss: 0.4368\n",
            "Epoch: [181][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6777 (Avg-Loss 1.6777)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [181][19/19]\tTime 0.010 (Avg-Time 0.028)\t Loss 1.7095 (Avg-Loss 1.6623)\tAcc 53.6765 (Avg-Acc 54.3200)\n",
            "EPOCH: 181 Validation Results: Acc 54.320 Loss: 1.6623\n",
            "Epoch: [182][0/78]\tTime 0.121 (Avg-Time 0.121)\t Loss 0.4314 (Avg-Loss 0.4314)\tAcc 84.7656 (Avg-Acc 84.7656)\n",
            "Epoch: [182][19/78]\tTime 0.058 (Avg-Time 0.147)\t Loss 0.3987 (Avg-Loss 0.4099)\tAcc 87.8906 (Avg-Acc 85.7129)\n",
            "Epoch: [182][38/78]\tTime 0.059 (Avg-Time 0.105)\t Loss 0.3851 (Avg-Loss 0.4199)\tAcc 87.6953 (Avg-Acc 85.3315)\n",
            "Epoch: [182][57/78]\tTime 0.058 (Avg-Time 0.091)\t Loss 0.5016 (Avg-Loss 0.4273)\tAcc 82.6172 (Avg-Acc 84.8700)\n",
            "Epoch: [182][76/78]\tTime 0.058 (Avg-Time 0.083)\t Loss 0.4131 (Avg-Loss 0.4316)\tAcc 84.9609 (Avg-Acc 84.6515)\n",
            "Epoch: [182][78/78]\tTime 0.021 (Avg-Time 0.082)\t Loss 0.4624 (Avg-Loss 0.4323)\tAcc 85.9375 (Avg-Acc 84.6350)\n",
            "EPOCH: 182 train Results: Acc 84.635 Loss: 0.4323\n",
            "Epoch: [182][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6908 (Avg-Loss 1.6908)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [182][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7224 (Avg-Loss 1.6726)\tAcc 52.5735 (Avg-Acc 54.0900)\n",
            "EPOCH: 182 Validation Results: Acc 54.090 Loss: 1.6726\n",
            "Epoch: [183][0/78]\tTime 0.078 (Avg-Time 0.078)\t Loss 0.4055 (Avg-Loss 0.4055)\tAcc 84.5703 (Avg-Acc 84.5703)\n",
            "Epoch: [183][19/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4661 (Avg-Loss 0.4073)\tAcc 84.5703 (Avg-Acc 85.4980)\n",
            "Epoch: [183][38/78]\tTime 0.060 (Avg-Time 0.062)\t Loss 0.4623 (Avg-Loss 0.4150)\tAcc 82.8125 (Avg-Acc 85.3616)\n",
            "Epoch: [183][57/78]\tTime 0.059 (Avg-Time 0.062)\t Loss 0.4152 (Avg-Loss 0.4212)\tAcc 85.9375 (Avg-Acc 85.0889)\n",
            "Epoch: [183][76/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.4618 (Avg-Loss 0.4225)\tAcc 83.3984 (Avg-Acc 84.9964)\n",
            "Epoch: [183][78/78]\tTime 0.020 (Avg-Time 0.061)\t Loss 0.5713 (Avg-Loss 0.4221)\tAcc 81.2500 (Avg-Acc 85.0150)\n",
            "EPOCH: 183 train Results: Acc 85.015 Loss: 0.4221\n",
            "Epoch: [183][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6981 (Avg-Loss 1.6981)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [183][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7091 (Avg-Loss 1.6957)\tAcc 55.1471 (Avg-Acc 54.6500)\n",
            "EPOCH: 183 Validation Results: Acc 54.650 Loss: 1.6957\n",
            "Epoch: [184][0/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.4065 (Avg-Loss 0.4065)\tAcc 87.3047 (Avg-Acc 87.3047)\n",
            "Epoch: [184][19/78]\tTime 0.105 (Avg-Time 0.086)\t Loss 0.4034 (Avg-Loss 0.4067)\tAcc 86.1328 (Avg-Acc 85.6641)\n",
            "Epoch: [184][38/78]\tTime 0.058 (Avg-Time 0.112)\t Loss 0.4278 (Avg-Loss 0.4225)\tAcc 83.5938 (Avg-Acc 85.0160)\n",
            "Epoch: [184][57/78]\tTime 0.086 (Avg-Time 0.096)\t Loss 0.4711 (Avg-Loss 0.4279)\tAcc 82.4219 (Avg-Acc 84.9744)\n",
            "Epoch: [184][76/78]\tTime 0.058 (Avg-Time 0.087)\t Loss 0.4735 (Avg-Loss 0.4293)\tAcc 83.9844 (Avg-Acc 84.9204)\n",
            "Epoch: [184][78/78]\tTime 0.020 (Avg-Time 0.086)\t Loss 0.6743 (Avg-Loss 0.4306)\tAcc 75.0000 (Avg-Acc 84.8725)\n",
            "EPOCH: 184 train Results: Acc 84.873 Loss: 0.4306\n",
            "Epoch: [184][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6434 (Avg-Loss 1.6434)\tAcc 54.6875 (Avg-Acc 54.6875)\n",
            "Epoch: [184][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7854 (Avg-Loss 1.6728)\tAcc 54.4118 (Avg-Acc 54.2500)\n",
            "EPOCH: 184 Validation Results: Acc 54.250 Loss: 1.6728\n",
            "Epoch: [185][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.3422 (Avg-Loss 0.3422)\tAcc 88.8672 (Avg-Acc 88.8672)\n",
            "Epoch: [185][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4347 (Avg-Loss 0.4153)\tAcc 84.3750 (Avg-Acc 85.1465)\n",
            "Epoch: [185][38/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.3958 (Avg-Loss 0.4137)\tAcc 86.3281 (Avg-Acc 85.3115)\n",
            "Epoch: [185][57/78]\tTime 0.060 (Avg-Time 0.061)\t Loss 0.3625 (Avg-Loss 0.4201)\tAcc 88.8672 (Avg-Acc 85.0788)\n",
            "Epoch: [185][76/78]\tTime 0.064 (Avg-Time 0.061)\t Loss 0.4021 (Avg-Loss 0.4207)\tAcc 84.7656 (Avg-Acc 85.0852)\n",
            "Epoch: [185][78/78]\tTime 0.021 (Avg-Time 0.061)\t Loss 0.5056 (Avg-Loss 0.4210)\tAcc 85.9375 (Avg-Acc 85.0725)\n",
            "EPOCH: 185 train Results: Acc 85.073 Loss: 0.4210\n",
            "Epoch: [185][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6582 (Avg-Loss 1.6582)\tAcc 55.0781 (Avg-Acc 55.0781)\n",
            "Epoch: [185][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7311 (Avg-Loss 1.6819)\tAcc 55.8824 (Avg-Acc 54.4700)\n",
            "EPOCH: 185 Validation Results: Acc 54.470 Loss: 1.6819\n",
            "Epoch: [186][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4835 (Avg-Loss 0.4835)\tAcc 82.8125 (Avg-Acc 82.8125)\n",
            "Epoch: [186][19/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.4044 (Avg-Loss 0.4086)\tAcc 85.7422 (Avg-Acc 85.2441)\n",
            "Epoch: [186][38/78]\tTime 0.134 (Avg-Time 0.073)\t Loss 0.4136 (Avg-Loss 0.4060)\tAcc 85.3516 (Avg-Acc 85.4667)\n",
            "Epoch: [186][57/78]\tTime 0.062 (Avg-Time 0.096)\t Loss 0.4222 (Avg-Loss 0.4143)\tAcc 85.1562 (Avg-Acc 85.2202)\n",
            "Epoch: [186][76/78]\tTime 0.058 (Avg-Time 0.088)\t Loss 0.3950 (Avg-Loss 0.4187)\tAcc 86.5234 (Avg-Acc 85.1030)\n",
            "Epoch: [186][78/78]\tTime 0.020 (Avg-Time 0.086)\t Loss 0.7076 (Avg-Loss 0.4191)\tAcc 79.6875 (Avg-Acc 85.0975)\n",
            "EPOCH: 186 train Results: Acc 85.097 Loss: 0.4191\n",
            "Epoch: [186][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6589 (Avg-Loss 1.6589)\tAcc 55.0781 (Avg-Acc 55.0781)\n",
            "Epoch: [186][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.7721 (Avg-Loss 1.6807)\tAcc 54.7794 (Avg-Acc 54.6000)\n",
            "EPOCH: 186 Validation Results: Acc 54.600 Loss: 1.6807\n",
            "Epoch: [187][0/78]\tTime 0.062 (Avg-Time 0.062)\t Loss 0.3481 (Avg-Loss 0.3481)\tAcc 89.0625 (Avg-Acc 89.0625)\n",
            "Epoch: [187][19/78]\tTime 0.062 (Avg-Time 0.061)\t Loss 0.4052 (Avg-Loss 0.4048)\tAcc 85.7422 (Avg-Acc 85.6836)\n",
            "Epoch: [187][38/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.4577 (Avg-Loss 0.4142)\tAcc 84.3750 (Avg-Acc 85.3616)\n",
            "Epoch: [187][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4157 (Avg-Loss 0.4215)\tAcc 85.5469 (Avg-Acc 85.1562)\n",
            "Epoch: [187][76/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4549 (Avg-Loss 0.4247)\tAcc 85.7422 (Avg-Acc 85.1562)\n",
            "Epoch: [187][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.7160 (Avg-Loss 0.4247)\tAcc 79.6875 (Avg-Acc 85.1500)\n",
            "EPOCH: 187 train Results: Acc 85.150 Loss: 0.4247\n",
            "Epoch: [187][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.7110 (Avg-Loss 1.7110)\tAcc 55.2734 (Avg-Acc 55.2734)\n",
            "Epoch: [187][19/19]\tTime 0.010 (Avg-Time 0.013)\t Loss 1.7539 (Avg-Loss 1.7112)\tAcc 54.4118 (Avg-Acc 54.8300)\n",
            "EPOCH: 187 Validation Results: Acc 54.830 Loss: 1.7112\n",
            "Epoch: [188][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4615 (Avg-Loss 0.4615)\tAcc 84.7656 (Avg-Acc 84.7656)\n",
            "Epoch: [188][19/78]\tTime 0.059 (Avg-Time 0.063)\t Loss 0.4412 (Avg-Loss 0.4207)\tAcc 84.5703 (Avg-Acc 85.5664)\n",
            "Epoch: [188][38/78]\tTime 0.059 (Avg-Time 0.062)\t Loss 0.3747 (Avg-Loss 0.4173)\tAcc 86.9141 (Avg-Acc 85.6120)\n",
            "Epoch: [188][57/78]\tTime 0.194 (Avg-Time 0.070)\t Loss 0.4001 (Avg-Loss 0.4189)\tAcc 86.9141 (Avg-Acc 85.4391)\n",
            "Epoch: [188][76/78]\tTime 0.060 (Avg-Time 0.088)\t Loss 0.4434 (Avg-Loss 0.4277)\tAcc 83.9844 (Avg-Acc 85.0497)\n",
            "Epoch: [188][78/78]\tTime 0.020 (Avg-Time 0.087)\t Loss 0.5743 (Avg-Loss 0.4275)\tAcc 75.0000 (Avg-Acc 85.0475)\n",
            "EPOCH: 188 train Results: Acc 85.047 Loss: 0.4275\n",
            "Epoch: [188][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6870 (Avg-Loss 1.6870)\tAcc 55.2734 (Avg-Acc 55.2734)\n",
            "Epoch: [188][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7900 (Avg-Loss 1.7066)\tAcc 52.9412 (Avg-Acc 54.4400)\n",
            "EPOCH: 188 Validation Results: Acc 54.440 Loss: 1.7066\n",
            "Epoch: [189][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.3915 (Avg-Loss 0.3915)\tAcc 86.3281 (Avg-Acc 86.3281)\n",
            "Epoch: [189][19/78]\tTime 0.060 (Avg-Time 0.064)\t Loss 0.4525 (Avg-Loss 0.4277)\tAcc 83.7891 (Avg-Acc 84.5898)\n",
            "Epoch: [189][38/78]\tTime 0.080 (Avg-Time 0.063)\t Loss 0.3253 (Avg-Loss 0.4235)\tAcc 89.4531 (Avg-Acc 85.1312)\n",
            "Epoch: [189][57/78]\tTime 0.058 (Avg-Time 0.062)\t Loss 0.4624 (Avg-Loss 0.4247)\tAcc 85.3516 (Avg-Acc 84.9879)\n",
            "Epoch: [189][76/78]\tTime 0.058 (Avg-Time 0.062)\t Loss 0.4257 (Avg-Loss 0.4297)\tAcc 85.7422 (Avg-Acc 84.7758)\n",
            "Epoch: [189][78/78]\tTime 0.021 (Avg-Time 0.061)\t Loss 0.6057 (Avg-Loss 0.4301)\tAcc 79.6875 (Avg-Acc 84.7675)\n",
            "EPOCH: 189 train Results: Acc 84.767 Loss: 0.4301\n",
            "Epoch: [189][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6690 (Avg-Loss 1.6690)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [189][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7031 (Avg-Loss 1.6646)\tAcc 53.6765 (Avg-Acc 54.3100)\n",
            "EPOCH: 189 Validation Results: Acc 54.310 Loss: 1.6646\n",
            "Epoch: [190][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4540 (Avg-Loss 0.4540)\tAcc 83.0078 (Avg-Acc 83.0078)\n",
            "Epoch: [190][19/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.4129 (Avg-Loss 0.4115)\tAcc 86.1328 (Avg-Acc 85.4492)\n",
            "Epoch: [190][38/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.3906 (Avg-Loss 0.4149)\tAcc 84.7656 (Avg-Acc 85.2013)\n",
            "Epoch: [190][57/78]\tTime 0.059 (Avg-Time 0.061)\t Loss 0.4474 (Avg-Loss 0.4152)\tAcc 83.5938 (Avg-Acc 85.2977)\n",
            "Epoch: [190][76/78]\tTime 0.134 (Avg-Time 0.074)\t Loss 0.3921 (Avg-Loss 0.4166)\tAcc 85.7422 (Avg-Acc 85.2044)\n",
            "Epoch: [190][78/78]\tTime 0.029 (Avg-Time 0.074)\t Loss 0.6864 (Avg-Loss 0.4166)\tAcc 71.8750 (Avg-Acc 85.2100)\n",
            "EPOCH: 190 train Results: Acc 85.210 Loss: 0.4166\n",
            "Epoch: [190][0/19]\tTime 0.032 (Avg-Time 0.032)\t Loss 1.6458 (Avg-Loss 1.6458)\tAcc 56.0547 (Avg-Acc 56.0547)\n",
            "Epoch: [190][19/19]\tTime 0.013 (Avg-Time 0.039)\t Loss 1.6707 (Avg-Loss 1.6561)\tAcc 55.1471 (Avg-Acc 54.2600)\n",
            "EPOCH: 190 Validation Results: Acc 54.260 Loss: 1.6561\n",
            "Epoch: [191][0/78]\tTime 0.126 (Avg-Time 0.126)\t Loss 0.4208 (Avg-Loss 0.4208)\tAcc 85.3516 (Avg-Acc 85.3516)\n",
            "Epoch: [191][19/78]\tTime 0.063 (Avg-Time 0.084)\t Loss 0.4320 (Avg-Loss 0.3989)\tAcc 84.1797 (Avg-Acc 85.7617)\n",
            "Epoch: [191][38/78]\tTime 0.058 (Avg-Time 0.073)\t Loss 0.4276 (Avg-Loss 0.4074)\tAcc 86.5234 (Avg-Acc 85.4167)\n",
            "Epoch: [191][57/78]\tTime 0.061 (Avg-Time 0.069)\t Loss 0.4704 (Avg-Loss 0.4152)\tAcc 83.9844 (Avg-Acc 85.2034)\n",
            "Epoch: [191][76/78]\tTime 0.058 (Avg-Time 0.067)\t Loss 0.4767 (Avg-Loss 0.4190)\tAcc 80.8594 (Avg-Acc 85.0928)\n",
            "Epoch: [191][78/78]\tTime 0.020 (Avg-Time 0.066)\t Loss 0.4544 (Avg-Loss 0.4195)\tAcc 82.8125 (Avg-Acc 85.0775)\n",
            "EPOCH: 191 train Results: Acc 85.078 Loss: 0.4195\n",
            "Epoch: [191][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.6666 (Avg-Loss 1.6666)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [191][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.7192 (Avg-Loss 1.6776)\tAcc 56.9853 (Avg-Acc 54.4800)\n",
            "EPOCH: 191 Validation Results: Acc 54.480 Loss: 1.6776\n",
            "Epoch: [192][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.3763 (Avg-Loss 0.3763)\tAcc 87.1094 (Avg-Acc 87.1094)\n",
            "Epoch: [192][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4469 (Avg-Loss 0.4118)\tAcc 84.1797 (Avg-Acc 85.1855)\n",
            "Epoch: [192][38/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.4997 (Avg-Loss 0.4172)\tAcc 83.5938 (Avg-Acc 84.9409)\n",
            "Epoch: [192][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.3661 (Avg-Loss 0.4196)\tAcc 86.9141 (Avg-Acc 84.8902)\n",
            "Epoch: [192][76/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4755 (Avg-Loss 0.4191)\tAcc 83.0078 (Avg-Acc 84.8848)\n",
            "Epoch: [192][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.4687 (Avg-Loss 0.4202)\tAcc 84.3750 (Avg-Acc 84.8625)\n",
            "EPOCH: 192 train Results: Acc 84.862 Loss: 0.4202\n",
            "Epoch: [192][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6788 (Avg-Loss 1.6788)\tAcc 56.0547 (Avg-Acc 56.0547)\n",
            "Epoch: [192][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7183 (Avg-Loss 1.6856)\tAcc 54.7794 (Avg-Acc 54.2300)\n",
            "EPOCH: 192 Validation Results: Acc 54.230 Loss: 1.6856\n",
            "Epoch: [193][0/78]\tTime 0.067 (Avg-Time 0.067)\t Loss 0.4145 (Avg-Loss 0.4145)\tAcc 85.3516 (Avg-Acc 85.3516)\n",
            "Epoch: [193][19/78]\tTime 0.116 (Avg-Time 0.133)\t Loss 0.4451 (Avg-Loss 0.4016)\tAcc 83.7891 (Avg-Acc 85.5957)\n",
            "Epoch: [193][38/78]\tTime 0.082 (Avg-Time 0.112)\t Loss 0.4253 (Avg-Loss 0.4057)\tAcc 85.5469 (Avg-Acc 85.5118)\n",
            "Epoch: [193][57/78]\tTime 0.057 (Avg-Time 0.095)\t Loss 0.4414 (Avg-Loss 0.4188)\tAcc 82.8125 (Avg-Acc 85.2505)\n",
            "Epoch: [193][76/78]\tTime 0.057 (Avg-Time 0.086)\t Loss 0.4068 (Avg-Loss 0.4192)\tAcc 85.9375 (Avg-Acc 85.2425)\n",
            "Epoch: [193][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.6436 (Avg-Loss 0.4193)\tAcc 75.0000 (Avg-Acc 85.2350)\n",
            "EPOCH: 193 train Results: Acc 85.235 Loss: 0.4193\n",
            "Epoch: [193][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6588 (Avg-Loss 1.6588)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [193][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7160 (Avg-Loss 1.6688)\tAcc 55.5147 (Avg-Acc 54.6600)\n",
            "EPOCH: 193 Validation Results: Acc 54.660 Loss: 1.6688\n",
            "Epoch: [194][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.3644 (Avg-Loss 0.3644)\tAcc 85.9375 (Avg-Acc 85.9375)\n",
            "Epoch: [194][19/78]\tTime 0.059 (Avg-Time 0.062)\t Loss 0.4103 (Avg-Loss 0.4157)\tAcc 84.9609 (Avg-Acc 85.2539)\n",
            "Epoch: [194][38/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.3806 (Avg-Loss 0.4234)\tAcc 85.9375 (Avg-Acc 85.0010)\n",
            "Epoch: [194][57/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.3987 (Avg-Loss 0.4175)\tAcc 85.3516 (Avg-Acc 85.3078)\n",
            "Epoch: [194][76/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.4540 (Avg-Loss 0.4206)\tAcc 83.2031 (Avg-Acc 85.2526)\n",
            "Epoch: [194][78/78]\tTime 0.021 (Avg-Time 0.061)\t Loss 0.6283 (Avg-Loss 0.4209)\tAcc 84.3750 (Avg-Acc 85.2625)\n",
            "EPOCH: 194 train Results: Acc 85.263 Loss: 0.4209\n",
            "Epoch: [194][0/19]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.6456 (Avg-Loss 1.6456)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [194][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7196 (Avg-Loss 1.6752)\tAcc 53.6765 (Avg-Acc 54.3900)\n",
            "EPOCH: 194 Validation Results: Acc 54.390 Loss: 1.6752\n",
            "Epoch: [195][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.3964 (Avg-Loss 0.3964)\tAcc 86.7188 (Avg-Acc 86.7188)\n",
            "Epoch: [195][19/78]\tTime 0.062 (Avg-Time 0.061)\t Loss 0.3537 (Avg-Loss 0.4073)\tAcc 86.7188 (Avg-Acc 85.2637)\n",
            "Epoch: [195][38/78]\tTime 0.134 (Avg-Time 0.096)\t Loss 0.3804 (Avg-Loss 0.4015)\tAcc 85.9375 (Avg-Acc 85.5469)\n",
            "Epoch: [195][57/78]\tTime 0.058 (Avg-Time 0.095)\t Loss 0.3849 (Avg-Loss 0.4041)\tAcc 85.9375 (Avg-Acc 85.4863)\n",
            "Epoch: [195][76/78]\tTime 0.057 (Avg-Time 0.087)\t Loss 0.4550 (Avg-Loss 0.4123)\tAcc 82.4219 (Avg-Acc 85.2323)\n",
            "Epoch: [195][78/78]\tTime 0.020 (Avg-Time 0.085)\t Loss 0.8187 (Avg-Loss 0.4136)\tAcc 71.8750 (Avg-Acc 85.1900)\n",
            "EPOCH: 195 train Results: Acc 85.190 Loss: 0.4136\n",
            "Epoch: [195][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.7059 (Avg-Loss 1.7059)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [195][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.7484 (Avg-Loss 1.7054)\tAcc 54.7794 (Avg-Acc 54.2600)\n",
            "EPOCH: 195 Validation Results: Acc 54.260 Loss: 1.7054\n",
            "Epoch: [196][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.3701 (Avg-Loss 0.3701)\tAcc 87.3047 (Avg-Acc 87.3047)\n",
            "Epoch: [196][19/78]\tTime 0.058 (Avg-Time 0.061)\t Loss 0.3988 (Avg-Loss 0.3985)\tAcc 84.9609 (Avg-Acc 85.8984)\n",
            "Epoch: [196][38/78]\tTime 0.060 (Avg-Time 0.061)\t Loss 0.5153 (Avg-Loss 0.4080)\tAcc 81.8359 (Avg-Acc 85.5769)\n",
            "Epoch: [196][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4463 (Avg-Loss 0.4123)\tAcc 82.6172 (Avg-Acc 85.4795)\n",
            "Epoch: [196][76/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4481 (Avg-Loss 0.4142)\tAcc 85.5469 (Avg-Acc 85.4429)\n",
            "Epoch: [196][78/78]\tTime 0.024 (Avg-Time 0.060)\t Loss 0.6091 (Avg-Loss 0.4146)\tAcc 78.1250 (Avg-Acc 85.4275)\n",
            "EPOCH: 196 train Results: Acc 85.427 Loss: 0.4146\n",
            "Epoch: [196][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6990 (Avg-Loss 1.6990)\tAcc 53.1250 (Avg-Acc 53.1250)\n",
            "Epoch: [196][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7005 (Avg-Loss 1.6970)\tAcc 55.1471 (Avg-Acc 54.0900)\n",
            "EPOCH: 196 Validation Results: Acc 54.090 Loss: 1.6970\n",
            "Epoch: [197][0/78]\tTime 0.064 (Avg-Time 0.064)\t Loss 0.3675 (Avg-Loss 0.3675)\tAcc 87.3047 (Avg-Acc 87.3047)\n",
            "Epoch: [197][19/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.4443 (Avg-Loss 0.3947)\tAcc 85.9375 (Avg-Acc 86.0742)\n",
            "Epoch: [197][38/78]\tTime 0.061 (Avg-Time 0.060)\t Loss 0.3864 (Avg-Loss 0.4022)\tAcc 85.5469 (Avg-Acc 86.0276)\n",
            "Epoch: [197][57/78]\tTime 0.126 (Avg-Time 0.078)\t Loss 0.4078 (Avg-Loss 0.4061)\tAcc 85.7422 (Avg-Acc 85.7658)\n",
            "Epoch: [197][76/78]\tTime 0.058 (Avg-Time 0.086)\t Loss 0.4452 (Avg-Loss 0.4098)\tAcc 83.2031 (Avg-Acc 85.6965)\n",
            "Epoch: [197][78/78]\tTime 0.020 (Avg-Time 0.084)\t Loss 0.6217 (Avg-Loss 0.4103)\tAcc 78.1250 (Avg-Acc 85.6775)\n",
            "EPOCH: 197 train Results: Acc 85.677 Loss: 0.4103\n",
            "Epoch: [197][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6609 (Avg-Loss 1.6609)\tAcc 55.2734 (Avg-Acc 55.2734)\n",
            "Epoch: [197][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7392 (Avg-Loss 1.6795)\tAcc 53.6765 (Avg-Acc 53.7400)\n",
            "EPOCH: 197 Validation Results: Acc 53.740 Loss: 1.6795\n",
            "Epoch: [198][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4156 (Avg-Loss 0.4156)\tAcc 85.3516 (Avg-Acc 85.3516)\n",
            "Epoch: [198][19/78]\tTime 0.058 (Avg-Time 0.060)\t Loss 0.4518 (Avg-Loss 0.3958)\tAcc 86.9141 (Avg-Acc 86.2207)\n",
            "Epoch: [198][38/78]\tTime 0.059 (Avg-Time 0.060)\t Loss 0.4529 (Avg-Loss 0.4044)\tAcc 82.0312 (Avg-Acc 85.7823)\n",
            "Epoch: [198][57/78]\tTime 0.057 (Avg-Time 0.060)\t Loss 0.4073 (Avg-Loss 0.4060)\tAcc 85.5469 (Avg-Acc 85.6782)\n",
            "Epoch: [198][76/78]\tTime 0.064 (Avg-Time 0.061)\t Loss 0.3858 (Avg-Loss 0.4106)\tAcc 85.3516 (Avg-Acc 85.5570)\n",
            "Epoch: [198][78/78]\tTime 0.020 (Avg-Time 0.060)\t Loss 0.8675 (Avg-Loss 0.4117)\tAcc 70.3125 (Avg-Acc 85.5200)\n",
            "EPOCH: 198 train Results: Acc 85.520 Loss: 0.4117\n",
            "Epoch: [198][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6505 (Avg-Loss 1.6505)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [198][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7549 (Avg-Loss 1.6880)\tAcc 53.6765 (Avg-Acc 54.1700)\n",
            "EPOCH: 198 Validation Results: Acc 54.170 Loss: 1.6880\n",
            "Epoch: [199][0/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.3691 (Avg-Loss 0.3691)\tAcc 87.8906 (Avg-Acc 87.8906)\n",
            "Epoch: [199][19/78]\tTime 0.060 (Avg-Time 0.060)\t Loss 0.4234 (Avg-Loss 0.4086)\tAcc 84.1797 (Avg-Acc 85.3516)\n",
            "Epoch: [199][38/78]\tTime 0.064 (Avg-Time 0.062)\t Loss 0.4323 (Avg-Loss 0.4113)\tAcc 84.7656 (Avg-Acc 85.3065)\n",
            "Epoch: [199][57/78]\tTime 0.057 (Avg-Time 0.061)\t Loss 0.3903 (Avg-Loss 0.4144)\tAcc 86.5234 (Avg-Acc 85.1428)\n",
            "Epoch: [199][76/78]\tTime 0.141 (Avg-Time 0.067)\t Loss 0.3858 (Avg-Loss 0.4178)\tAcc 85.9375 (Avg-Acc 85.0979)\n",
            "Epoch: [199][78/78]\tTime 0.029 (Avg-Time 0.069)\t Loss 0.4899 (Avg-Loss 0.4181)\tAcc 84.3750 (Avg-Acc 85.0825)\n",
            "EPOCH: 199 train Results: Acc 85.082 Loss: 0.4181\n",
            "Epoch: [199][0/19]\tTime 0.021 (Avg-Time 0.021)\t Loss 1.6625 (Avg-Loss 1.6625)\tAcc 55.2734 (Avg-Acc 55.2734)\n",
            "Epoch: [199][19/19]\tTime 0.067 (Avg-Time 0.047)\t Loss 1.7435 (Avg-Loss 1.6907)\tAcc 54.4118 (Avg-Acc 54.0500)\n",
            "EPOCH: 199 Validation Results: Acc 54.050 Loss: 1.6907\n",
            "Epoch: [200][0/78]\tTime 0.181 (Avg-Time 0.181)\t Loss 0.3717 (Avg-Loss 0.3717)\tAcc 86.9141 (Avg-Acc 86.9141)\n",
            "Epoch: [200][19/78]\tTime 0.058 (Avg-Time 0.094)\t Loss 0.3940 (Avg-Loss 0.3979)\tAcc 86.5234 (Avg-Acc 86.1426)\n",
            "Epoch: [200][38/78]\tTime 0.060 (Avg-Time 0.077)\t Loss 0.4208 (Avg-Loss 0.4055)\tAcc 87.3047 (Avg-Acc 85.7222)\n",
            "Epoch: [200][57/78]\tTime 0.057 (Avg-Time 0.072)\t Loss 0.3923 (Avg-Loss 0.4074)\tAcc 86.3281 (Avg-Acc 85.6748)\n",
            "Epoch: [200][76/78]\tTime 0.058 (Avg-Time 0.069)\t Loss 0.3965 (Avg-Loss 0.4164)\tAcc 86.3281 (Avg-Acc 85.3135)\n",
            "Epoch: [200][78/78]\tTime 0.020 (Avg-Time 0.068)\t Loss 0.7841 (Avg-Loss 0.4172)\tAcc 75.0000 (Avg-Acc 85.3100)\n",
            "EPOCH: 200 train Results: Acc 85.310 Loss: 0.4172\n",
            "Epoch: [200][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.6614 (Avg-Loss 1.6614)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [200][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.7390 (Avg-Loss 1.6872)\tAcc 56.9853 (Avg-Acc 54.3500)\n",
            "EPOCH: 200 Validation Results: Acc 54.350 Loss: 1.6872\n",
            "\n",
            "Test Accuracy: 54.11% (Best so far: 58.25%)\n",
            "\n",
            "\n",
            "=== Testing Configuration 31/864 ===\n",
            "Current Config: {'lr': 0.005, 'batch_size': 512, 'hidden_units': [512, 256], 'dropout_rates': [0.3, 0.3], 'pre-process': 'standardization', 'weight_decay': 0.001, 'optimizer': 'sgd'}\n",
            "Pre-process: standardization\n",
            "Pre-process: standardization\n",
            "Pre-process: standardization\n",
            "Epoch: [1][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 8.2336 (Avg-Loss 8.2336)\tAcc 11.3281 (Avg-Acc 11.3281)\n",
            "Epoch: [1][19/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 6.4575 (Avg-Loss 7.6326)\tAcc 14.0625 (Avg-Acc 11.3184)\n",
            "Epoch: [1][38/78]\tTime 0.050 (Avg-Time 0.051)\t Loss 5.8245 (Avg-Loss 6.8739)\tAcc 18.3594 (Avg-Acc 13.1661)\n",
            "Epoch: [1][57/78]\tTime 0.053 (Avg-Time 0.051)\t Loss 5.0064 (Avg-Loss 6.3332)\tAcc 19.3359 (Avg-Acc 15.1839)\n",
            "Epoch: [1][76/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 4.4533 (Avg-Loss 5.9292)\tAcc 24.8047 (Avg-Acc 16.9694)\n",
            "Epoch: [1][78/78]\tTime 0.009 (Avg-Time 0.051)\t Loss 4.4048 (Avg-Loss 5.9098)\tAcc 21.8750 (Avg-Acc 17.0625)\n",
            "EPOCH: 1 train Results: Acc 17.062 Loss: 5.9098\n",
            "Epoch: [1][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 2.4597 (Avg-Loss 2.4597)\tAcc 32.6172 (Avg-Acc 32.6172)\n",
            "Epoch: [1][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 2.5097 (Avg-Loss 2.5203)\tAcc 33.4559 (Avg-Acc 31.6600)\n",
            "EPOCH: 1 Validation Results: Acc 31.660 Loss: 2.5203\n",
            "Epoch: [2][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 4.2182 (Avg-Loss 4.2182)\tAcc 25.7812 (Avg-Acc 25.7812)\n",
            "Epoch: [2][19/78]\tTime 0.089 (Avg-Time 0.059)\t Loss 3.7621 (Avg-Loss 4.1734)\tAcc 27.7344 (Avg-Acc 24.5312)\n",
            "Epoch: [2][38/78]\tTime 0.048 (Avg-Time 0.105)\t Loss 4.0557 (Avg-Loss 4.0882)\tAcc 24.6094 (Avg-Acc 24.9048)\n",
            "Epoch: [2][57/78]\tTime 0.048 (Avg-Time 0.087)\t Loss 3.2992 (Avg-Loss 3.9550)\tAcc 29.4922 (Avg-Acc 25.4378)\n",
            "Epoch: [2][76/78]\tTime 0.048 (Avg-Time 0.078)\t Loss 3.2661 (Avg-Loss 3.8446)\tAcc 27.7344 (Avg-Acc 25.8751)\n",
            "Epoch: [2][78/78]\tTime 0.011 (Avg-Time 0.077)\t Loss 2.6472 (Avg-Loss 3.8389)\tAcc 37.5000 (Avg-Acc 25.9000)\n",
            "EPOCH: 2 train Results: Acc 25.900 Loss: 3.8389\n",
            "Epoch: [2][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 2.0512 (Avg-Loss 2.0512)\tAcc 40.0391 (Avg-Acc 40.0391)\n",
            "Epoch: [2][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 2.0870 (Avg-Loss 2.0880)\tAcc 36.7647 (Avg-Acc 36.9400)\n",
            "EPOCH: 2 Validation Results: Acc 36.940 Loss: 2.0880\n",
            "Epoch: [3][0/78]\tTime 0.053 (Avg-Time 0.053)\t Loss 3.5432 (Avg-Loss 3.5432)\tAcc 27.3438 (Avg-Acc 27.3438)\n",
            "Epoch: [3][19/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 3.1921 (Avg-Loss 3.2245)\tAcc 30.2734 (Avg-Acc 29.1211)\n",
            "Epoch: [3][38/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 2.9182 (Avg-Loss 3.1957)\tAcc 30.6641 (Avg-Acc 28.7660)\n",
            "Epoch: [3][57/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 2.9165 (Avg-Loss 3.1336)\tAcc 32.4219 (Avg-Acc 29.1117)\n",
            "Epoch: [3][76/78]\tTime 0.052 (Avg-Time 0.051)\t Loss 2.7483 (Avg-Loss 3.0712)\tAcc 28.9062 (Avg-Acc 29.3552)\n",
            "Epoch: [3][78/78]\tTime 0.009 (Avg-Time 0.051)\t Loss 2.5717 (Avg-Loss 3.0678)\tAcc 35.9375 (Avg-Acc 29.3725)\n",
            "EPOCH: 3 train Results: Acc 29.372 Loss: 3.0678\n",
            "Epoch: [3][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.8747 (Avg-Loss 1.8747)\tAcc 40.6250 (Avg-Acc 40.6250)\n",
            "Epoch: [3][19/19]\tTime 0.010 (Avg-Time 0.013)\t Loss 1.8815 (Avg-Loss 1.8862)\tAcc 39.7059 (Avg-Acc 39.0200)\n",
            "EPOCH: 3 Validation Results: Acc 39.020 Loss: 1.8862\n",
            "Epoch: [4][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 2.7103 (Avg-Loss 2.7103)\tAcc 32.4219 (Avg-Acc 32.4219)\n",
            "Epoch: [4][19/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 2.8143 (Avg-Loss 2.6838)\tAcc 28.7109 (Avg-Acc 31.8457)\n",
            "Epoch: [4][38/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 2.5230 (Avg-Loss 2.6623)\tAcc 33.0078 (Avg-Acc 31.7208)\n",
            "Epoch: [4][57/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 2.5163 (Avg-Loss 2.6204)\tAcc 32.6172 (Avg-Acc 31.8730)\n",
            "Epoch: [4][76/78]\tTime 0.143 (Avg-Time 0.064)\t Loss 2.4828 (Avg-Loss 2.5878)\tAcc 33.2031 (Avg-Acc 32.0160)\n",
            "Epoch: [4][78/78]\tTime 0.073 (Avg-Time 0.065)\t Loss 2.6058 (Avg-Loss 2.5868)\tAcc 29.6875 (Avg-Acc 32.0050)\n",
            "EPOCH: 4 train Results: Acc 32.005 Loss: 2.5868\n",
            "Epoch: [4][0/19]\tTime 0.089 (Avg-Time 0.089)\t Loss 1.7322 (Avg-Loss 1.7322)\tAcc 42.7734 (Avg-Acc 42.7734)\n",
            "Epoch: [4][19/19]\tTime 0.016 (Avg-Time 0.046)\t Loss 1.7730 (Avg-Loss 1.7628)\tAcc 41.5441 (Avg-Acc 40.4600)\n",
            "EPOCH: 4 Validation Results: Acc 40.460 Loss: 1.7628\n",
            "Epoch: [5][0/78]\tTime 0.106 (Avg-Time 0.106)\t Loss 2.4684 (Avg-Loss 2.4684)\tAcc 30.0781 (Avg-Acc 30.0781)\n",
            "Epoch: [5][19/78]\tTime 0.048 (Avg-Time 0.067)\t Loss 2.1935 (Avg-Loss 2.3489)\tAcc 35.7422 (Avg-Acc 33.7012)\n",
            "Epoch: [5][38/78]\tTime 0.048 (Avg-Time 0.059)\t Loss 2.2752 (Avg-Loss 2.3179)\tAcc 34.9609 (Avg-Acc 33.8942)\n",
            "Epoch: [5][57/78]\tTime 0.048 (Avg-Time 0.056)\t Loss 2.2521 (Avg-Loss 2.2882)\tAcc 32.4219 (Avg-Acc 34.1662)\n",
            "Epoch: [5][76/78]\tTime 0.051 (Avg-Time 0.055)\t Loss 2.2201 (Avg-Loss 2.2736)\tAcc 35.9375 (Avg-Acc 34.2456)\n",
            "Epoch: [5][78/78]\tTime 0.009 (Avg-Time 0.054)\t Loss 2.2269 (Avg-Loss 2.2722)\tAcc 29.6875 (Avg-Acc 34.2375)\n",
            "EPOCH: 5 train Results: Acc 34.237 Loss: 2.2722\n",
            "Epoch: [5][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.6374 (Avg-Loss 1.6374)\tAcc 44.3359 (Avg-Acc 44.3359)\n",
            "Epoch: [5][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.6708 (Avg-Loss 1.6768)\tAcc 43.0147 (Avg-Acc 42.0100)\n",
            "EPOCH: 5 Validation Results: Acc 42.010 Loss: 1.6768\n",
            "Epoch: [6][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 2.1207 (Avg-Loss 2.1207)\tAcc 33.2031 (Avg-Acc 33.2031)\n",
            "Epoch: [6][19/78]\tTime 0.052 (Avg-Time 0.051)\t Loss 2.0562 (Avg-Loss 2.1007)\tAcc 36.7188 (Avg-Acc 35.6152)\n",
            "Epoch: [6][38/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 2.0127 (Avg-Loss 2.0795)\tAcc 37.6953 (Avg-Acc 35.9075)\n",
            "Epoch: [6][57/78]\tTime 0.050 (Avg-Time 0.051)\t Loss 2.0513 (Avg-Loss 2.0594)\tAcc 36.1328 (Avg-Acc 36.0419)\n",
            "Epoch: [6][76/78]\tTime 0.050 (Avg-Time 0.051)\t Loss 1.9413 (Avg-Loss 2.0432)\tAcc 34.1797 (Avg-Acc 36.2114)\n",
            "Epoch: [6][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.8703 (Avg-Loss 2.0406)\tAcc 32.8125 (Avg-Acc 36.2700)\n",
            "EPOCH: 6 train Results: Acc 36.270 Loss: 2.0406\n",
            "Epoch: [6][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5867 (Avg-Loss 1.5867)\tAcc 45.5078 (Avg-Acc 45.5078)\n",
            "Epoch: [6][19/19]\tTime 0.008 (Avg-Time 0.013)\t Loss 1.6127 (Avg-Loss 1.6169)\tAcc 44.8529 (Avg-Acc 43.5800)\n",
            "EPOCH: 6 Validation Results: Acc 43.580 Loss: 1.6169\n",
            "Epoch: [7][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 1.9446 (Avg-Loss 1.9446)\tAcc 38.4766 (Avg-Acc 38.4766)\n",
            "Epoch: [7][19/78]\tTime 0.050 (Avg-Time 0.052)\t Loss 1.8674 (Avg-Loss 1.9099)\tAcc 41.7969 (Avg-Acc 37.6562)\n",
            "Epoch: [7][38/78]\tTime 0.093 (Avg-Time 0.055)\t Loss 1.8469 (Avg-Loss 1.8975)\tAcc 39.0625 (Avg-Acc 37.9457)\n",
            "Epoch: [7][57/78]\tTime 0.089 (Avg-Time 0.085)\t Loss 1.8260 (Avg-Loss 1.8883)\tAcc 41.9922 (Avg-Acc 37.9546)\n",
            "Epoch: [7][76/78]\tTime 0.050 (Avg-Time 0.078)\t Loss 1.8677 (Avg-Loss 1.8835)\tAcc 38.2812 (Avg-Acc 37.9109)\n",
            "Epoch: [7][78/78]\tTime 0.009 (Avg-Time 0.077)\t Loss 1.7652 (Avg-Loss 1.8832)\tAcc 37.5000 (Avg-Acc 37.9200)\n",
            "EPOCH: 7 train Results: Acc 37.920 Loss: 1.8832\n",
            "Epoch: [7][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5537 (Avg-Loss 1.5537)\tAcc 45.7031 (Avg-Acc 45.7031)\n",
            "Epoch: [7][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5769 (Avg-Loss 1.5787)\tAcc 45.5882 (Avg-Acc 44.7000)\n",
            "EPOCH: 7 Validation Results: Acc 44.700 Loss: 1.5787\n",
            "Epoch: [8][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 1.8501 (Avg-Loss 1.8501)\tAcc 36.3281 (Avg-Acc 36.3281)\n",
            "Epoch: [8][19/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.8211 (Avg-Loss 1.7944)\tAcc 42.5781 (Avg-Acc 39.8926)\n",
            "Epoch: [8][38/78]\tTime 0.050 (Avg-Time 0.052)\t Loss 1.8246 (Avg-Loss 1.7830)\tAcc 37.6953 (Avg-Acc 39.8387)\n",
            "Epoch: [8][57/78]\tTime 0.048 (Avg-Time 0.052)\t Loss 1.8680 (Avg-Loss 1.7715)\tAcc 40.0391 (Avg-Acc 40.2748)\n",
            "Epoch: [8][76/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.6902 (Avg-Loss 1.7644)\tAcc 41.4062 (Avg-Acc 40.2166)\n",
            "Epoch: [8][78/78]\tTime 0.010 (Avg-Time 0.051)\t Loss 1.9518 (Avg-Loss 1.7647)\tAcc 32.8125 (Avg-Acc 40.2000)\n",
            "EPOCH: 8 train Results: Acc 40.200 Loss: 1.7647\n",
            "Epoch: [8][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5175 (Avg-Loss 1.5175)\tAcc 45.8984 (Avg-Acc 45.8984)\n",
            "Epoch: [8][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5494 (Avg-Loss 1.5510)\tAcc 46.6912 (Avg-Acc 45.5900)\n",
            "EPOCH: 8 Validation Results: Acc 45.590 Loss: 1.5510\n",
            "Epoch: [9][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 1.6161 (Avg-Loss 1.6161)\tAcc 44.1406 (Avg-Acc 44.1406)\n",
            "Epoch: [9][19/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.5911 (Avg-Loss 1.6970)\tAcc 44.3359 (Avg-Acc 41.0645)\n",
            "Epoch: [9][38/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.7569 (Avg-Loss 1.6954)\tAcc 37.1094 (Avg-Acc 41.0907)\n",
            "Epoch: [9][57/78]\tTime 0.050 (Avg-Time 0.051)\t Loss 1.6482 (Avg-Loss 1.6807)\tAcc 42.3828 (Avg-Acc 41.4770)\n",
            "Epoch: [9][76/78]\tTime 0.053 (Avg-Time 0.051)\t Loss 1.6512 (Avg-Loss 1.6810)\tAcc 43.9453 (Avg-Acc 41.5584)\n",
            "Epoch: [9][78/78]\tTime 0.010 (Avg-Time 0.051)\t Loss 1.5351 (Avg-Loss 1.6793)\tAcc 48.4375 (Avg-Acc 41.6325)\n",
            "EPOCH: 9 train Results: Acc 41.633 Loss: 1.6793\n",
            "Epoch: [9][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.5045 (Avg-Loss 1.5045)\tAcc 47.8516 (Avg-Acc 47.8516)\n",
            "Epoch: [9][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5386 (Avg-Loss 1.5320)\tAcc 48.5294 (Avg-Acc 46.3000)\n",
            "EPOCH: 9 Validation Results: Acc 46.300 Loss: 1.5320\n",
            "Epoch: [10][0/78]\tTime 0.064 (Avg-Time 0.064)\t Loss 1.6129 (Avg-Loss 1.6129)\tAcc 42.3828 (Avg-Acc 42.3828)\n",
            "Epoch: [10][19/78]\tTime 0.107 (Avg-Time 0.121)\t Loss 1.5685 (Avg-Loss 1.6231)\tAcc 45.1172 (Avg-Acc 42.7734)\n",
            "Epoch: [10][38/78]\tTime 0.051 (Avg-Time 0.103)\t Loss 1.6156 (Avg-Loss 1.6242)\tAcc 44.1406 (Avg-Acc 42.7183)\n",
            "Epoch: [10][57/78]\tTime 0.052 (Avg-Time 0.086)\t Loss 1.6593 (Avg-Loss 1.6262)\tAcc 42.9688 (Avg-Acc 42.6791)\n",
            "Epoch: [10][76/78]\tTime 0.052 (Avg-Time 0.078)\t Loss 1.6148 (Avg-Loss 1.6163)\tAcc 41.9922 (Avg-Acc 42.9459)\n",
            "Epoch: [10][78/78]\tTime 0.009 (Avg-Time 0.077)\t Loss 1.6444 (Avg-Loss 1.6151)\tAcc 40.6250 (Avg-Acc 43.0075)\n",
            "EPOCH: 10 train Results: Acc 43.008 Loss: 1.6151\n",
            "Epoch: [10][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4891 (Avg-Loss 1.4891)\tAcc 47.2656 (Avg-Acc 47.2656)\n",
            "Epoch: [10][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.5164 (Avg-Loss 1.5125)\tAcc 50.7353 (Avg-Acc 47.0400)\n",
            "EPOCH: 10 Validation Results: Acc 47.040 Loss: 1.5125\n",
            "Epoch: [11][0/78]\tTime 0.056 (Avg-Time 0.056)\t Loss 1.5394 (Avg-Loss 1.5394)\tAcc 45.3125 (Avg-Acc 45.3125)\n",
            "Epoch: [11][19/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.5939 (Avg-Loss 1.5737)\tAcc 43.3594 (Avg-Acc 43.6523)\n",
            "Epoch: [11][38/78]\tTime 0.050 (Avg-Time 0.051)\t Loss 1.6106 (Avg-Loss 1.5664)\tAcc 42.7734 (Avg-Acc 44.1607)\n",
            "Epoch: [11][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.5040 (Avg-Loss 1.5628)\tAcc 46.2891 (Avg-Acc 44.3056)\n",
            "Epoch: [11][76/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.5412 (Avg-Loss 1.5590)\tAcc 44.5312 (Avg-Acc 44.5186)\n",
            "Epoch: [11][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.6614 (Avg-Loss 1.5589)\tAcc 39.0625 (Avg-Acc 44.5375)\n",
            "EPOCH: 11 train Results: Acc 44.538 Loss: 1.5589\n",
            "Epoch: [11][0/19]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4734 (Avg-Loss 1.4734)\tAcc 48.6328 (Avg-Acc 48.6328)\n",
            "Epoch: [11][19/19]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4976 (Avg-Loss 1.4973)\tAcc 49.6324 (Avg-Acc 47.8400)\n",
            "EPOCH: 11 Validation Results: Acc 47.840 Loss: 1.4973\n",
            "Epoch: [12][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 1.4416 (Avg-Loss 1.4416)\tAcc 48.2422 (Avg-Acc 48.2422)\n",
            "Epoch: [12][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.4933 (Avg-Loss 1.5094)\tAcc 50.5859 (Avg-Acc 46.2793)\n",
            "Epoch: [12][38/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.5290 (Avg-Loss 1.5124)\tAcc 46.2891 (Avg-Acc 46.2190)\n",
            "Epoch: [12][57/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.4363 (Avg-Loss 1.5096)\tAcc 50.3906 (Avg-Acc 46.1005)\n",
            "Epoch: [12][76/78]\tTime 0.090 (Avg-Time 0.073)\t Loss 1.5439 (Avg-Loss 1.5075)\tAcc 46.8750 (Avg-Acc 46.4489)\n",
            "Epoch: [12][78/78]\tTime 0.020 (Avg-Time 0.073)\t Loss 1.6481 (Avg-Loss 1.5070)\tAcc 35.9375 (Avg-Acc 46.4400)\n",
            "EPOCH: 12 train Results: Acc 46.440 Loss: 1.5070\n",
            "Epoch: [12][0/19]\tTime 0.027 (Avg-Time 0.027)\t Loss 1.4603 (Avg-Loss 1.4603)\tAcc 49.6094 (Avg-Acc 49.6094)\n",
            "Epoch: [12][19/19]\tTime 0.021 (Avg-Time 0.027)\t Loss 1.4855 (Avg-Loss 1.4833)\tAcc 51.8382 (Avg-Acc 48.4900)\n",
            "EPOCH: 12 Validation Results: Acc 48.490 Loss: 1.4833\n",
            "Epoch: [13][0/78]\tTime 0.073 (Avg-Time 0.073)\t Loss 1.5011 (Avg-Loss 1.5011)\tAcc 48.0469 (Avg-Acc 48.0469)\n",
            "Epoch: [13][19/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 1.4439 (Avg-Loss 1.4811)\tAcc 49.8047 (Avg-Acc 47.1289)\n",
            "Epoch: [13][38/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.4610 (Avg-Loss 1.4813)\tAcc 47.6562 (Avg-Acc 47.1054)\n",
            "Epoch: [13][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.4883 (Avg-Loss 1.4805)\tAcc 46.0938 (Avg-Acc 47.1377)\n",
            "Epoch: [13][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.5289 (Avg-Loss 1.4815)\tAcc 44.1406 (Avg-Acc 47.1667)\n",
            "Epoch: [13][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.4924 (Avg-Loss 1.4826)\tAcc 51.5625 (Avg-Acc 47.1225)\n",
            "EPOCH: 13 train Results: Acc 47.123 Loss: 1.4826\n",
            "Epoch: [13][0/19]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4465 (Avg-Loss 1.4465)\tAcc 50.9766 (Avg-Acc 50.9766)\n",
            "Epoch: [13][19/19]\tTime 0.010 (Avg-Time 0.013)\t Loss 1.4776 (Avg-Loss 1.4700)\tAcc 50.3676 (Avg-Acc 48.9600)\n",
            "EPOCH: 13 Validation Results: Acc 48.960 Loss: 1.4700\n",
            "Epoch: [14][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.4421 (Avg-Loss 1.4421)\tAcc 47.6562 (Avg-Acc 47.6562)\n",
            "Epoch: [14][19/78]\tTime 0.054 (Avg-Time 0.051)\t Loss 1.4224 (Avg-Loss 1.4431)\tAcc 49.0234 (Avg-Acc 48.7012)\n",
            "Epoch: [14][38/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.5391 (Avg-Loss 1.4559)\tAcc 45.1172 (Avg-Acc 48.2372)\n",
            "Epoch: [14][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.4462 (Avg-Loss 1.4589)\tAcc 47.8516 (Avg-Acc 48.0637)\n",
            "Epoch: [14][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.4037 (Avg-Loss 1.4514)\tAcc 50.0000 (Avg-Acc 48.2904)\n",
            "Epoch: [14][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.1990 (Avg-Loss 1.4514)\tAcc 62.5000 (Avg-Acc 48.3100)\n",
            "EPOCH: 14 train Results: Acc 48.310 Loss: 1.4514\n",
            "Epoch: [14][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.4356 (Avg-Loss 1.4356)\tAcc 50.9766 (Avg-Acc 50.9766)\n",
            "Epoch: [14][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.4576 (Avg-Loss 1.4558)\tAcc 50.3676 (Avg-Acc 49.2300)\n",
            "EPOCH: 14 Validation Results: Acc 49.230 Loss: 1.4558\n",
            "Epoch: [15][0/78]\tTime 0.053 (Avg-Time 0.053)\t Loss 1.4621 (Avg-Loss 1.4621)\tAcc 49.2188 (Avg-Acc 49.2188)\n",
            "Epoch: [15][19/78]\tTime 0.055 (Avg-Time 0.050)\t Loss 1.3331 (Avg-Loss 1.4256)\tAcc 51.7578 (Avg-Acc 49.4922)\n",
            "Epoch: [15][38/78]\tTime 0.135 (Avg-Time 0.060)\t Loss 1.3891 (Avg-Loss 1.4248)\tAcc 50.3906 (Avg-Acc 49.3940)\n",
            "Epoch: [15][57/78]\tTime 0.049 (Avg-Time 0.085)\t Loss 1.3531 (Avg-Loss 1.4229)\tAcc 52.5391 (Avg-Acc 49.5622)\n",
            "Epoch: [15][76/78]\tTime 0.049 (Avg-Time 0.077)\t Loss 1.4176 (Avg-Loss 1.4236)\tAcc 49.0234 (Avg-Acc 49.5510)\n",
            "Epoch: [15][78/78]\tTime 0.010 (Avg-Time 0.076)\t Loss 1.5622 (Avg-Loss 1.4237)\tAcc 43.7500 (Avg-Acc 49.5275)\n",
            "EPOCH: 15 train Results: Acc 49.528 Loss: 1.4237\n",
            "Epoch: [15][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4240 (Avg-Loss 1.4240)\tAcc 50.1953 (Avg-Acc 50.1953)\n",
            "Epoch: [15][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.4481 (Avg-Loss 1.4420)\tAcc 51.4706 (Avg-Acc 50.0900)\n",
            "EPOCH: 15 Validation Results: Acc 50.090 Loss: 1.4420\n",
            "Epoch: [16][0/78]\tTime 0.074 (Avg-Time 0.074)\t Loss 1.4844 (Avg-Loss 1.4844)\tAcc 44.5312 (Avg-Acc 44.5312)\n",
            "Epoch: [16][19/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.4736 (Avg-Loss 1.4098)\tAcc 48.0469 (Avg-Acc 49.7070)\n",
            "Epoch: [16][38/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.3753 (Avg-Loss 1.4063)\tAcc 50.7812 (Avg-Acc 49.8598)\n",
            "Epoch: [16][57/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.3515 (Avg-Loss 1.4029)\tAcc 53.9062 (Avg-Acc 50.2593)\n",
            "Epoch: [16][76/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.3853 (Avg-Loss 1.4034)\tAcc 52.9297 (Avg-Acc 50.2613)\n",
            "Epoch: [16][78/78]\tTime 0.010 (Avg-Time 0.051)\t Loss 1.3116 (Avg-Loss 1.4037)\tAcc 54.6875 (Avg-Acc 50.2850)\n",
            "EPOCH: 16 train Results: Acc 50.285 Loss: 1.4037\n",
            "Epoch: [16][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.4076 (Avg-Loss 1.4076)\tAcc 51.1719 (Avg-Acc 51.1719)\n",
            "Epoch: [16][19/19]\tTime 0.008 (Avg-Time 0.014)\t Loss 1.4282 (Avg-Loss 1.4264)\tAcc 50.0000 (Avg-Acc 50.5900)\n",
            "EPOCH: 16 Validation Results: Acc 50.590 Loss: 1.4264\n",
            "Epoch: [17][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 1.3609 (Avg-Loss 1.3609)\tAcc 51.9531 (Avg-Acc 51.9531)\n",
            "Epoch: [17][19/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.3924 (Avg-Loss 1.3808)\tAcc 52.9297 (Avg-Acc 51.4062)\n",
            "Epoch: [17][38/78]\tTime 0.056 (Avg-Time 0.051)\t Loss 1.4445 (Avg-Loss 1.3821)\tAcc 49.0234 (Avg-Acc 51.5224)\n",
            "Epoch: [17][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.3743 (Avg-Loss 1.3806)\tAcc 54.2969 (Avg-Acc 51.3571)\n",
            "Epoch: [17][76/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.3381 (Avg-Loss 1.3797)\tAcc 54.4922 (Avg-Acc 51.4636)\n",
            "Epoch: [17][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.3316 (Avg-Loss 1.3806)\tAcc 50.0000 (Avg-Acc 51.4300)\n",
            "EPOCH: 17 train Results: Acc 51.430 Loss: 1.3806\n",
            "Epoch: [17][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3895 (Avg-Loss 1.3895)\tAcc 51.9531 (Avg-Acc 51.9531)\n",
            "Epoch: [17][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.4121 (Avg-Loss 1.4127)\tAcc 50.7353 (Avg-Acc 50.8300)\n",
            "EPOCH: 17 Validation Results: Acc 50.830 Loss: 1.4127\n",
            "Epoch: [18][0/78]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.3830 (Avg-Loss 1.3830)\tAcc 51.5625 (Avg-Acc 51.5625)\n",
            "Epoch: [18][19/78]\tTime 0.118 (Avg-Time 0.141)\t Loss 1.4068 (Avg-Loss 1.3614)\tAcc 52.7344 (Avg-Acc 52.4805)\n",
            "Epoch: [18][38/78]\tTime 0.048 (Avg-Time 0.103)\t Loss 1.3751 (Avg-Loss 1.3641)\tAcc 51.1719 (Avg-Acc 51.9181)\n",
            "Epoch: [18][57/78]\tTime 0.051 (Avg-Time 0.086)\t Loss 1.3599 (Avg-Loss 1.3573)\tAcc 53.3203 (Avg-Acc 52.3067)\n",
            "Epoch: [18][76/78]\tTime 0.048 (Avg-Time 0.077)\t Loss 1.2861 (Avg-Loss 1.3548)\tAcc 56.6406 (Avg-Acc 52.3996)\n",
            "Epoch: [18][78/78]\tTime 0.009 (Avg-Time 0.076)\t Loss 1.2247 (Avg-Loss 1.3547)\tAcc 60.9375 (Avg-Acc 52.3975)\n",
            "EPOCH: 18 train Results: Acc 52.398 Loss: 1.3547\n",
            "Epoch: [18][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3724 (Avg-Loss 1.3724)\tAcc 52.3438 (Avg-Acc 52.3438)\n",
            "Epoch: [18][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.4041 (Avg-Loss 1.3965)\tAcc 50.3676 (Avg-Acc 51.4700)\n",
            "EPOCH: 18 Validation Results: Acc 51.470 Loss: 1.3965\n",
            "Epoch: [19][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.3470 (Avg-Loss 1.3470)\tAcc 53.3203 (Avg-Acc 53.3203)\n",
            "Epoch: [19][19/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.3221 (Avg-Loss 1.3384)\tAcc 57.6172 (Avg-Acc 52.9004)\n",
            "Epoch: [19][38/78]\tTime 0.053 (Avg-Time 0.050)\t Loss 1.3605 (Avg-Loss 1.3310)\tAcc 51.1719 (Avg-Acc 53.3203)\n",
            "Epoch: [19][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.3170 (Avg-Loss 1.3308)\tAcc 50.5859 (Avg-Acc 53.2395)\n",
            "Epoch: [19][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.2826 (Avg-Loss 1.3351)\tAcc 55.8594 (Avg-Acc 53.0438)\n",
            "Epoch: [19][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.3372 (Avg-Loss 1.3347)\tAcc 54.6875 (Avg-Acc 53.0775)\n",
            "EPOCH: 19 train Results: Acc 53.078 Loss: 1.3347\n",
            "Epoch: [19][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3605 (Avg-Loss 1.3605)\tAcc 52.5391 (Avg-Acc 52.5391)\n",
            "Epoch: [19][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3764 (Avg-Loss 1.3814)\tAcc 51.1029 (Avg-Acc 51.8900)\n",
            "EPOCH: 19 Validation Results: Acc 51.890 Loss: 1.3814\n",
            "Epoch: [20][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.2588 (Avg-Loss 1.2588)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [20][19/78]\tTime 0.048 (Avg-Time 0.052)\t Loss 1.3091 (Avg-Loss 1.2971)\tAcc 54.6875 (Avg-Acc 54.9414)\n",
            "Epoch: [20][38/78]\tTime 0.058 (Avg-Time 0.051)\t Loss 1.3324 (Avg-Loss 1.3072)\tAcc 52.9297 (Avg-Acc 54.2518)\n",
            "Epoch: [20][57/78]\tTime 0.100 (Avg-Time 0.053)\t Loss 1.2529 (Avg-Loss 1.3078)\tAcc 57.0312 (Avg-Acc 53.9635)\n",
            "Epoch: [20][76/78]\tTime 0.134 (Avg-Time 0.072)\t Loss 1.2652 (Avg-Loss 1.3082)\tAcc 57.8125 (Avg-Acc 53.9874)\n",
            "Epoch: [20][78/78]\tTime 0.018 (Avg-Time 0.072)\t Loss 1.3336 (Avg-Loss 1.3082)\tAcc 50.0000 (Avg-Acc 53.9575)\n",
            "EPOCH: 20 train Results: Acc 53.958 Loss: 1.3082\n",
            "Epoch: [20][0/19]\tTime 0.021 (Avg-Time 0.021)\t Loss 1.3466 (Avg-Loss 1.3466)\tAcc 52.5391 (Avg-Acc 52.5391)\n",
            "Epoch: [20][19/19]\tTime 0.007 (Avg-Time 0.029)\t Loss 1.3754 (Avg-Loss 1.3654)\tAcc 52.5735 (Avg-Acc 52.3800)\n",
            "EPOCH: 20 Validation Results: Acc 52.380 Loss: 1.3654\n",
            "Epoch: [21][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.2487 (Avg-Loss 1.2487)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [21][19/78]\tTime 0.054 (Avg-Time 0.049)\t Loss 1.2958 (Avg-Loss 1.2761)\tAcc 54.1016 (Avg-Acc 55.2832)\n",
            "Epoch: [21][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.2855 (Avg-Loss 1.2814)\tAcc 55.4688 (Avg-Acc 55.1983)\n",
            "Epoch: [21][57/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.4180 (Avg-Loss 1.2911)\tAcc 52.3438 (Avg-Acc 54.6336)\n",
            "Epoch: [21][76/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.3292 (Avg-Loss 1.2949)\tAcc 53.7109 (Avg-Acc 54.5531)\n",
            "Epoch: [21][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.2929 (Avg-Loss 1.2938)\tAcc 45.3125 (Avg-Acc 54.6100)\n",
            "EPOCH: 21 train Results: Acc 54.610 Loss: 1.2938\n",
            "Epoch: [21][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3278 (Avg-Loss 1.3278)\tAcc 53.3203 (Avg-Acc 53.3203)\n",
            "Epoch: [21][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3483 (Avg-Loss 1.3481)\tAcc 52.9412 (Avg-Acc 52.6000)\n",
            "EPOCH: 21 Validation Results: Acc 52.600 Loss: 1.3481\n",
            "Epoch: [22][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 1.2561 (Avg-Loss 1.2561)\tAcc 53.9062 (Avg-Acc 53.9062)\n",
            "Epoch: [22][19/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.2268 (Avg-Loss 1.2581)\tAcc 52.5391 (Avg-Acc 55.7129)\n",
            "Epoch: [22][38/78]\tTime 0.052 (Avg-Time 0.050)\t Loss 1.2794 (Avg-Loss 1.2658)\tAcc 54.4922 (Avg-Acc 55.7141)\n",
            "Epoch: [22][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.2567 (Avg-Loss 1.2695)\tAcc 55.2734 (Avg-Acc 55.3745)\n",
            "Epoch: [22][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.3008 (Avg-Loss 1.2722)\tAcc 54.1016 (Avg-Acc 55.1745)\n",
            "Epoch: [22][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.4285 (Avg-Loss 1.2725)\tAcc 46.8750 (Avg-Acc 55.1575)\n",
            "EPOCH: 22 train Results: Acc 55.157 Loss: 1.2725\n",
            "Epoch: [22][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3235 (Avg-Loss 1.3235)\tAcc 53.3203 (Avg-Acc 53.3203)\n",
            "Epoch: [22][19/19]\tTime 0.007 (Avg-Time 0.015)\t Loss 1.3355 (Avg-Loss 1.3381)\tAcc 54.4118 (Avg-Acc 53.1700)\n",
            "EPOCH: 22 Validation Results: Acc 53.170 Loss: 1.3381\n",
            "Epoch: [23][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.2305 (Avg-Loss 1.2305)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [23][19/78]\tTime 0.047 (Avg-Time 0.053)\t Loss 1.2128 (Avg-Loss 1.2408)\tAcc 56.0547 (Avg-Acc 56.6016)\n",
            "Epoch: [23][38/78]\tTime 0.135 (Avg-Time 0.064)\t Loss 1.3163 (Avg-Loss 1.2510)\tAcc 52.3438 (Avg-Acc 56.0998)\n",
            "Epoch: [23][57/78]\tTime 0.052 (Avg-Time 0.085)\t Loss 1.2813 (Avg-Loss 1.2532)\tAcc 60.1562 (Avg-Acc 56.2264)\n",
            "Epoch: [23][76/78]\tTime 0.050 (Avg-Time 0.077)\t Loss 1.2897 (Avg-Loss 1.2556)\tAcc 53.9062 (Avg-Acc 55.9862)\n",
            "Epoch: [23][78/78]\tTime 0.009 (Avg-Time 0.076)\t Loss 1.1688 (Avg-Loss 1.2552)\tAcc 62.5000 (Avg-Acc 56.0125)\n",
            "EPOCH: 23 train Results: Acc 56.013 Loss: 1.2552\n",
            "Epoch: [23][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.3092 (Avg-Loss 1.3092)\tAcc 53.7109 (Avg-Acc 53.7109)\n",
            "Epoch: [23][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3330 (Avg-Loss 1.3257)\tAcc 53.6765 (Avg-Acc 53.3100)\n",
            "EPOCH: 23 Validation Results: Acc 53.310 Loss: 1.3257\n",
            "Epoch: [24][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.2216 (Avg-Loss 1.2216)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [24][19/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.2242 (Avg-Loss 1.2156)\tAcc 57.8125 (Avg-Acc 57.6270)\n",
            "Epoch: [24][38/78]\tTime 0.073 (Avg-Time 0.051)\t Loss 1.2244 (Avg-Loss 1.2253)\tAcc 58.5938 (Avg-Acc 56.9511)\n",
            "Epoch: [24][57/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.2877 (Avg-Loss 1.2330)\tAcc 54.8828 (Avg-Acc 56.8763)\n",
            "Epoch: [24][76/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.2225 (Avg-Loss 1.2389)\tAcc 57.2266 (Avg-Acc 56.5316)\n",
            "Epoch: [24][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.3459 (Avg-Loss 1.2402)\tAcc 42.1875 (Avg-Acc 56.4825)\n",
            "EPOCH: 24 train Results: Acc 56.483 Loss: 1.2402\n",
            "Epoch: [24][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2788 (Avg-Loss 1.2788)\tAcc 55.2734 (Avg-Acc 55.2734)\n",
            "Epoch: [24][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3104 (Avg-Loss 1.3119)\tAcc 54.4118 (Avg-Acc 53.7800)\n",
            "EPOCH: 24 Validation Results: Acc 53.780 Loss: 1.3119\n",
            "Epoch: [25][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.1959 (Avg-Loss 1.1959)\tAcc 59.9609 (Avg-Acc 59.9609)\n",
            "Epoch: [25][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.2039 (Avg-Loss 1.2195)\tAcc 60.9375 (Avg-Acc 57.7441)\n",
            "Epoch: [25][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.2583 (Avg-Loss 1.2172)\tAcc 54.8828 (Avg-Acc 57.7274)\n",
            "Epoch: [25][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.2464 (Avg-Loss 1.2219)\tAcc 55.2734 (Avg-Acc 57.3209)\n",
            "Epoch: [25][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.2190 (Avg-Loss 1.2280)\tAcc 59.3750 (Avg-Acc 57.0439)\n",
            "Epoch: [25][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.3702 (Avg-Loss 1.2284)\tAcc 60.9375 (Avg-Acc 57.0300)\n",
            "EPOCH: 25 train Results: Acc 57.030 Loss: 1.2284\n",
            "Epoch: [25][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2867 (Avg-Loss 1.2867)\tAcc 53.7109 (Avg-Acc 53.7109)\n",
            "Epoch: [25][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3055 (Avg-Loss 1.3016)\tAcc 53.3088 (Avg-Acc 53.9900)\n",
            "EPOCH: 25 Validation Results: Acc 53.990 Loss: 1.3016\n",
            "Epoch: [26][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.1617 (Avg-Loss 1.1617)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [26][19/78]\tTime 0.087 (Avg-Time 0.126)\t Loss 1.1809 (Avg-Loss 1.1993)\tAcc 60.1562 (Avg-Acc 58.3496)\n",
            "Epoch: [26][38/78]\tTime 0.069 (Avg-Time 0.103)\t Loss 1.1572 (Avg-Loss 1.1975)\tAcc 60.1562 (Avg-Acc 58.2582)\n",
            "Epoch: [26][57/78]\tTime 0.048 (Avg-Time 0.085)\t Loss 1.2563 (Avg-Loss 1.2046)\tAcc 56.0547 (Avg-Acc 57.9371)\n",
            "Epoch: [26][76/78]\tTime 0.075 (Avg-Time 0.077)\t Loss 1.2181 (Avg-Loss 1.2092)\tAcc 57.4219 (Avg-Acc 57.5056)\n",
            "Epoch: [26][78/78]\tTime 0.009 (Avg-Time 0.076)\t Loss 1.2814 (Avg-Loss 1.2093)\tAcc 57.8125 (Avg-Acc 57.5225)\n",
            "EPOCH: 26 train Results: Acc 57.523 Loss: 1.2093\n",
            "Epoch: [26][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2679 (Avg-Loss 1.2679)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [26][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3014 (Avg-Loss 1.2905)\tAcc 54.0441 (Avg-Acc 54.7800)\n",
            "EPOCH: 26 Validation Results: Acc 54.780 Loss: 1.2905\n",
            "Epoch: [27][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.1221 (Avg-Loss 1.1221)\tAcc 62.5000 (Avg-Acc 62.5000)\n",
            "Epoch: [27][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1754 (Avg-Loss 1.1762)\tAcc 60.3516 (Avg-Acc 59.3945)\n",
            "Epoch: [27][38/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.2045 (Avg-Loss 1.1865)\tAcc 58.2031 (Avg-Acc 58.6639)\n",
            "Epoch: [27][57/78]\tTime 0.068 (Avg-Time 0.050)\t Loss 1.2091 (Avg-Loss 1.1923)\tAcc 57.2266 (Avg-Acc 58.2031)\n",
            "Epoch: [27][76/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.1426 (Avg-Loss 1.2018)\tAcc 59.3750 (Avg-Acc 57.7288)\n",
            "Epoch: [27][78/78]\tTime 0.012 (Avg-Time 0.050)\t Loss 1.1345 (Avg-Loss 1.2017)\tAcc 59.3750 (Avg-Acc 57.6600)\n",
            "EPOCH: 27 train Results: Acc 57.660 Loss: 1.2017\n",
            "Epoch: [27][0/19]\tTime 0.027 (Avg-Time 0.027)\t Loss 1.2643 (Avg-Loss 1.2643)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [27][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2888 (Avg-Loss 1.2832)\tAcc 54.7794 (Avg-Acc 54.9500)\n",
            "EPOCH: 27 Validation Results: Acc 54.950 Loss: 1.2832\n",
            "Epoch: [28][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.1613 (Avg-Loss 1.1613)\tAcc 60.1562 (Avg-Acc 60.1562)\n",
            "Epoch: [28][19/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.1812 (Avg-Loss 1.1640)\tAcc 57.2266 (Avg-Acc 59.4434)\n",
            "Epoch: [28][38/78]\tTime 0.052 (Avg-Time 0.051)\t Loss 1.2222 (Avg-Loss 1.1686)\tAcc 55.2734 (Avg-Acc 59.2097)\n",
            "Epoch: [28][57/78]\tTime 0.056 (Avg-Time 0.051)\t Loss 1.2476 (Avg-Loss 1.1783)\tAcc 55.8594 (Avg-Acc 58.6173)\n",
            "Epoch: [28][76/78]\tTime 0.095 (Avg-Time 0.073)\t Loss 1.2395 (Avg-Loss 1.1869)\tAcc 58.2031 (Avg-Acc 58.2158)\n",
            "Epoch: [28][78/78]\tTime 0.020 (Avg-Time 0.072)\t Loss 1.1928 (Avg-Loss 1.1873)\tAcc 59.3750 (Avg-Acc 58.1975)\n",
            "EPOCH: 28 train Results: Acc 58.197 Loss: 1.1873\n",
            "Epoch: [28][0/19]\tTime 0.036 (Avg-Time 0.036)\t Loss 1.2339 (Avg-Loss 1.2339)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [28][19/19]\tTime 0.013 (Avg-Time 0.026)\t Loss 1.2768 (Avg-Loss 1.2800)\tAcc 58.4559 (Avg-Acc 55.2000)\n",
            "EPOCH: 28 Validation Results: Acc 55.200 Loss: 1.2800\n",
            "Epoch: [29][0/78]\tTime 0.090 (Avg-Time 0.090)\t Loss 1.1378 (Avg-Loss 1.1378)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [29][19/78]\tTime 0.047 (Avg-Time 0.052)\t Loss 1.1569 (Avg-Loss 1.1490)\tAcc 58.3984 (Avg-Acc 59.6973)\n",
            "Epoch: [29][38/78]\tTime 0.048 (Avg-Time 0.052)\t Loss 1.1793 (Avg-Loss 1.1573)\tAcc 58.9844 (Avg-Acc 59.1446)\n",
            "Epoch: [29][57/78]\tTime 0.057 (Avg-Time 0.051)\t Loss 1.1283 (Avg-Loss 1.1718)\tAcc 60.5469 (Avg-Acc 58.6072)\n",
            "Epoch: [29][76/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.1975 (Avg-Loss 1.1807)\tAcc 58.2031 (Avg-Acc 58.2894)\n",
            "Epoch: [29][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.3023 (Avg-Loss 1.1817)\tAcc 46.8750 (Avg-Acc 58.2375)\n",
            "EPOCH: 29 train Results: Acc 58.237 Loss: 1.1817\n",
            "Epoch: [29][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2349 (Avg-Loss 1.2349)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [29][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2701 (Avg-Loss 1.2723)\tAcc 53.3088 (Avg-Acc 55.1400)\n",
            "EPOCH: 29 Validation Results: Acc 55.140 Loss: 1.2723\n",
            "Epoch: [30][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.1190 (Avg-Loss 1.1190)\tAcc 62.5000 (Avg-Acc 62.5000)\n",
            "Epoch: [30][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.1799 (Avg-Loss 1.1485)\tAcc 58.3984 (Avg-Acc 60.3125)\n",
            "Epoch: [30][38/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.2084 (Avg-Loss 1.1518)\tAcc 58.7891 (Avg-Acc 59.8558)\n",
            "Epoch: [30][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.1662 (Avg-Loss 1.1630)\tAcc 58.0078 (Avg-Acc 59.1797)\n",
            "Epoch: [30][76/78]\tTime 0.070 (Avg-Time 0.051)\t Loss 1.2507 (Avg-Loss 1.1714)\tAcc 57.2266 (Avg-Acc 58.8525)\n",
            "Epoch: [30][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.2303 (Avg-Loss 1.1721)\tAcc 57.8125 (Avg-Acc 58.8300)\n",
            "EPOCH: 30 train Results: Acc 58.830 Loss: 1.1721\n",
            "Epoch: [30][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2431 (Avg-Loss 1.2431)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [30][19/19]\tTime 0.009 (Avg-Time 0.013)\t Loss 1.2841 (Avg-Loss 1.2758)\tAcc 55.5147 (Avg-Acc 54.5500)\n",
            "EPOCH: 30 Validation Results: Acc 54.550 Loss: 1.2758\n",
            "Epoch: [31][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.1100 (Avg-Loss 1.1100)\tAcc 61.9141 (Avg-Acc 61.9141)\n",
            "Epoch: [31][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.1601 (Avg-Loss 1.1477)\tAcc 57.8125 (Avg-Acc 60.0195)\n",
            "Epoch: [31][38/78]\tTime 0.093 (Avg-Time 0.059)\t Loss 1.1991 (Avg-Loss 1.1528)\tAcc 59.9609 (Avg-Acc 59.6104)\n",
            "Epoch: [31][57/78]\tTime 0.086 (Avg-Time 0.085)\t Loss 1.2280 (Avg-Loss 1.1554)\tAcc 57.4219 (Avg-Acc 59.4423)\n",
            "Epoch: [31][76/78]\tTime 0.050 (Avg-Time 0.076)\t Loss 1.1216 (Avg-Loss 1.1603)\tAcc 62.3047 (Avg-Acc 59.1492)\n",
            "Epoch: [31][78/78]\tTime 0.020 (Avg-Time 0.075)\t Loss 1.2861 (Avg-Loss 1.1607)\tAcc 60.9375 (Avg-Acc 59.1400)\n",
            "EPOCH: 31 train Results: Acc 59.140 Loss: 1.1607\n",
            "Epoch: [31][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2130 (Avg-Loss 1.2130)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [31][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2664 (Avg-Loss 1.2661)\tAcc 56.6176 (Avg-Acc 55.2600)\n",
            "EPOCH: 31 Validation Results: Acc 55.260 Loss: 1.2661\n",
            "Epoch: [32][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 1.1386 (Avg-Loss 1.1386)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [32][19/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.1210 (Avg-Loss 1.1340)\tAcc 62.1094 (Avg-Acc 60.5762)\n",
            "Epoch: [32][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.1901 (Avg-Loss 1.1393)\tAcc 56.6406 (Avg-Acc 60.1112)\n",
            "Epoch: [32][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.1802 (Avg-Loss 1.1449)\tAcc 55.8594 (Avg-Acc 59.8936)\n",
            "Epoch: [32][76/78]\tTime 0.061 (Avg-Time 0.050)\t Loss 1.1350 (Avg-Loss 1.1518)\tAcc 57.6172 (Avg-Acc 59.5500)\n",
            "Epoch: [32][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.4150 (Avg-Loss 1.1516)\tAcc 56.2500 (Avg-Acc 59.5700)\n",
            "EPOCH: 32 train Results: Acc 59.570 Loss: 1.1516\n",
            "Epoch: [32][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2430 (Avg-Loss 1.2430)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [32][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2852 (Avg-Loss 1.2597)\tAcc 52.5735 (Avg-Acc 55.3000)\n",
            "EPOCH: 32 Validation Results: Acc 55.300 Loss: 1.2597\n",
            "Epoch: [33][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.1502 (Avg-Loss 1.1502)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [33][19/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 1.2193 (Avg-Loss 1.1269)\tAcc 55.8594 (Avg-Acc 60.0879)\n",
            "Epoch: [33][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1948 (Avg-Loss 1.1359)\tAcc 55.4688 (Avg-Acc 59.7105)\n",
            "Epoch: [33][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1972 (Avg-Loss 1.1425)\tAcc 59.3750 (Avg-Acc 59.6343)\n",
            "Epoch: [33][76/78]\tTime 0.069 (Avg-Time 0.050)\t Loss 1.1498 (Avg-Loss 1.1527)\tAcc 60.7422 (Avg-Acc 59.3496)\n",
            "Epoch: [33][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.1978 (Avg-Loss 1.1522)\tAcc 56.2500 (Avg-Acc 59.3475)\n",
            "EPOCH: 33 train Results: Acc 59.347 Loss: 1.1522\n",
            "Epoch: [33][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1993 (Avg-Loss 1.1993)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [33][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2585 (Avg-Loss 1.2591)\tAcc 54.4118 (Avg-Acc 55.8200)\n",
            "EPOCH: 33 Validation Results: Acc 55.820 Loss: 1.2591\n",
            "Epoch: [34][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.1130 (Avg-Loss 1.1130)\tAcc 61.9141 (Avg-Acc 61.9141)\n",
            "Epoch: [34][19/78]\tTime 0.149 (Avg-Time 0.124)\t Loss 1.1312 (Avg-Loss 1.1142)\tAcc 61.3281 (Avg-Acc 61.6309)\n",
            "Epoch: [34][38/78]\tTime 0.051 (Avg-Time 0.101)\t Loss 1.0581 (Avg-Loss 1.1185)\tAcc 63.4766 (Avg-Acc 61.0276)\n",
            "Epoch: [34][57/78]\tTime 0.050 (Avg-Time 0.084)\t Loss 1.1400 (Avg-Loss 1.1262)\tAcc 62.3047 (Avg-Acc 60.7119)\n",
            "Epoch: [34][76/78]\tTime 0.085 (Avg-Time 0.076)\t Loss 1.1876 (Avg-Loss 1.1392)\tAcc 58.2031 (Avg-Acc 59.9914)\n",
            "Epoch: [34][78/78]\tTime 0.016 (Avg-Time 0.075)\t Loss 1.1629 (Avg-Loss 1.1387)\tAcc 57.8125 (Avg-Acc 60.0200)\n",
            "EPOCH: 34 train Results: Acc 60.020 Loss: 1.1387\n",
            "Epoch: [34][0/19]\tTime 0.029 (Avg-Time 0.029)\t Loss 1.2212 (Avg-Loss 1.2212)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [34][19/19]\tTime 0.007 (Avg-Time 0.015)\t Loss 1.2574 (Avg-Loss 1.2515)\tAcc 56.9853 (Avg-Acc 55.9400)\n",
            "EPOCH: 34 Validation Results: Acc 55.940 Loss: 1.2515\n",
            "Epoch: [35][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.1377 (Avg-Loss 1.1377)\tAcc 63.2812 (Avg-Acc 63.2812)\n",
            "Epoch: [35][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.1723 (Avg-Loss 1.1131)\tAcc 59.5703 (Avg-Acc 60.9277)\n",
            "Epoch: [35][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.2023 (Avg-Loss 1.1212)\tAcc 58.9844 (Avg-Acc 60.6070)\n",
            "Epoch: [35][57/78]\tTime 0.052 (Avg-Time 0.050)\t Loss 1.1371 (Avg-Loss 1.1335)\tAcc 56.6406 (Avg-Acc 60.1057)\n",
            "Epoch: [35][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.2491 (Avg-Loss 1.1398)\tAcc 53.7109 (Avg-Acc 59.7834)\n",
            "Epoch: [35][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.1987 (Avg-Loss 1.1409)\tAcc 62.5000 (Avg-Acc 59.7550)\n",
            "EPOCH: 35 train Results: Acc 59.755 Loss: 1.1409\n",
            "Epoch: [35][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2284 (Avg-Loss 1.2284)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [35][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2449 (Avg-Loss 1.2570)\tAcc 56.2500 (Avg-Acc 55.7800)\n",
            "EPOCH: 35 Validation Results: Acc 55.780 Loss: 1.2570\n",
            "Epoch: [36][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0810 (Avg-Loss 1.0810)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [36][19/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.1226 (Avg-Loss 1.0938)\tAcc 58.2031 (Avg-Acc 61.4746)\n",
            "Epoch: [36][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.1115 (Avg-Loss 1.1104)\tAcc 57.4219 (Avg-Acc 60.9675)\n",
            "Epoch: [36][57/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 1.0869 (Avg-Loss 1.1204)\tAcc 60.7422 (Avg-Acc 60.5671)\n",
            "Epoch: [36][76/78]\tTime 0.096 (Avg-Time 0.070)\t Loss 1.1533 (Avg-Loss 1.1282)\tAcc 59.7656 (Avg-Acc 60.3161)\n",
            "Epoch: [36][78/78]\tTime 0.024 (Avg-Time 0.071)\t Loss 1.1250 (Avg-Loss 1.1289)\tAcc 59.3750 (Avg-Acc 60.2625)\n",
            "EPOCH: 36 train Results: Acc 60.263 Loss: 1.1289\n",
            "Epoch: [36][0/19]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.2326 (Avg-Loss 1.2326)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [36][19/19]\tTime 0.015 (Avg-Time 0.031)\t Loss 1.2419 (Avg-Loss 1.2538)\tAcc 56.9853 (Avg-Acc 55.8400)\n",
            "EPOCH: 36 Validation Results: Acc 55.840 Loss: 1.2538\n",
            "Epoch: [37][0/78]\tTime 0.097 (Avg-Time 0.097)\t Loss 1.0670 (Avg-Loss 1.0670)\tAcc 62.3047 (Avg-Acc 62.3047)\n",
            "Epoch: [37][19/78]\tTime 0.047 (Avg-Time 0.054)\t Loss 1.0802 (Avg-Loss 1.0947)\tAcc 61.5234 (Avg-Acc 61.7285)\n",
            "Epoch: [37][38/78]\tTime 0.051 (Avg-Time 0.053)\t Loss 1.0687 (Avg-Loss 1.1029)\tAcc 64.0625 (Avg-Acc 61.5084)\n",
            "Epoch: [37][57/78]\tTime 0.054 (Avg-Time 0.052)\t Loss 1.1694 (Avg-Loss 1.1144)\tAcc 60.9375 (Avg-Acc 61.0722)\n",
            "Epoch: [37][76/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.2021 (Avg-Loss 1.1223)\tAcc 59.5703 (Avg-Acc 60.8462)\n",
            "Epoch: [37][78/78]\tTime 0.010 (Avg-Time 0.051)\t Loss 0.9635 (Avg-Loss 1.1228)\tAcc 67.1875 (Avg-Acc 60.8100)\n",
            "EPOCH: 37 train Results: Acc 60.810 Loss: 1.1228\n",
            "Epoch: [37][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2258 (Avg-Loss 1.2258)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [37][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2587 (Avg-Loss 1.2550)\tAcc 55.5147 (Avg-Acc 55.9700)\n",
            "EPOCH: 37 Validation Results: Acc 55.970 Loss: 1.2550\n",
            "Epoch: [38][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0293 (Avg-Loss 1.0293)\tAcc 64.2578 (Avg-Acc 64.2578)\n",
            "Epoch: [38][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0728 (Avg-Loss 1.0673)\tAcc 62.3047 (Avg-Acc 62.8809)\n",
            "Epoch: [38][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.2377 (Avg-Loss 1.0927)\tAcc 58.5938 (Avg-Acc 61.7488)\n",
            "Epoch: [38][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.1563 (Avg-Loss 1.1028)\tAcc 58.0078 (Avg-Acc 61.2002)\n",
            "Epoch: [38][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1229 (Avg-Loss 1.1168)\tAcc 61.7188 (Avg-Acc 60.5697)\n",
            "Epoch: [38][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.1728 (Avg-Loss 1.1174)\tAcc 65.6250 (Avg-Acc 60.5325)\n",
            "EPOCH: 38 train Results: Acc 60.532 Loss: 1.1174\n",
            "Epoch: [38][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2302 (Avg-Loss 1.2302)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [38][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2736 (Avg-Loss 1.2463)\tAcc 54.7794 (Avg-Acc 56.1300)\n",
            "EPOCH: 38 Validation Results: Acc 56.130 Loss: 1.2463\n",
            "Epoch: [39][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0901 (Avg-Loss 1.0901)\tAcc 63.6719 (Avg-Acc 63.6719)\n",
            "Epoch: [39][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0692 (Avg-Loss 1.0736)\tAcc 64.0625 (Avg-Acc 62.4609)\n",
            "Epoch: [39][38/78]\tTime 0.096 (Avg-Time 0.055)\t Loss 1.1023 (Avg-Loss 1.0894)\tAcc 62.3047 (Avg-Acc 61.7889)\n",
            "Epoch: [39][57/78]\tTime 0.048 (Avg-Time 0.085)\t Loss 1.1261 (Avg-Loss 1.1028)\tAcc 59.7656 (Avg-Acc 61.1766)\n",
            "Epoch: [39][76/78]\tTime 0.049 (Avg-Time 0.076)\t Loss 1.2186 (Avg-Loss 1.1127)\tAcc 57.2266 (Avg-Acc 60.7599)\n",
            "Epoch: [39][78/78]\tTime 0.013 (Avg-Time 0.075)\t Loss 1.4305 (Avg-Loss 1.1137)\tAcc 50.0000 (Avg-Acc 60.7350)\n",
            "EPOCH: 39 train Results: Acc 60.735 Loss: 1.1137\n",
            "Epoch: [39][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2326 (Avg-Loss 1.2326)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [39][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2860 (Avg-Loss 1.2450)\tAcc 54.7794 (Avg-Acc 55.8800)\n",
            "EPOCH: 39 Validation Results: Acc 55.880 Loss: 1.2450\n",
            "Epoch: [40][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9635 (Avg-Loss 0.9635)\tAcc 64.6484 (Avg-Acc 64.6484)\n",
            "Epoch: [40][19/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.1847 (Avg-Loss 1.0934)\tAcc 59.5703 (Avg-Acc 61.6992)\n",
            "Epoch: [40][38/78]\tTime 0.054 (Avg-Time 0.051)\t Loss 1.0410 (Avg-Loss 1.1002)\tAcc 64.8438 (Avg-Acc 61.5885)\n",
            "Epoch: [40][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1605 (Avg-Loss 1.1057)\tAcc 58.7891 (Avg-Acc 61.4157)\n",
            "Epoch: [40][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0496 (Avg-Loss 1.1135)\tAcc 62.6953 (Avg-Acc 60.8817)\n",
            "Epoch: [40][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.3244 (Avg-Loss 1.1138)\tAcc 57.8125 (Avg-Acc 60.8675)\n",
            "EPOCH: 40 train Results: Acc 60.867 Loss: 1.1138\n",
            "Epoch: [40][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2162 (Avg-Loss 1.2162)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [40][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2574 (Avg-Loss 1.2420)\tAcc 54.7794 (Avg-Acc 56.2500)\n",
            "EPOCH: 40 Validation Results: Acc 56.250 Loss: 1.2420\n",
            "Epoch: [41][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.1043 (Avg-Loss 1.1043)\tAcc 60.9375 (Avg-Acc 60.9375)\n",
            "Epoch: [41][19/78]\tTime 0.048 (Avg-Time 0.053)\t Loss 1.1172 (Avg-Loss 1.0685)\tAcc 63.0859 (Avg-Acc 62.4902)\n",
            "Epoch: [41][38/78]\tTime 0.048 (Avg-Time 0.052)\t Loss 1.1275 (Avg-Loss 1.0853)\tAcc 62.8906 (Avg-Acc 62.0292)\n",
            "Epoch: [41][57/78]\tTime 0.049 (Avg-Time 0.052)\t Loss 1.0716 (Avg-Loss 1.0945)\tAcc 61.9141 (Avg-Acc 61.5908)\n",
            "Epoch: [41][76/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.2180 (Avg-Loss 1.1062)\tAcc 57.4219 (Avg-Acc 61.1201)\n",
            "Epoch: [41][78/78]\tTime 0.009 (Avg-Time 0.051)\t Loss 1.0772 (Avg-Loss 1.1068)\tAcc 64.0625 (Avg-Acc 61.1025)\n",
            "EPOCH: 41 train Results: Acc 61.102 Loss: 1.1068\n",
            "Epoch: [41][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2433 (Avg-Loss 1.2433)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [41][19/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.2660 (Avg-Loss 1.2489)\tAcc 55.1471 (Avg-Acc 55.4500)\n",
            "EPOCH: 41 Validation Results: Acc 55.450 Loss: 1.2489\n",
            "Epoch: [42][0/78]\tTime 0.101 (Avg-Time 0.101)\t Loss 1.0022 (Avg-Loss 1.0022)\tAcc 65.0391 (Avg-Acc 65.0391)\n",
            "Epoch: [42][19/78]\tTime 0.111 (Avg-Time 0.128)\t Loss 1.0503 (Avg-Loss 1.0778)\tAcc 63.0859 (Avg-Acc 62.1094)\n",
            "Epoch: [42][38/78]\tTime 0.048 (Avg-Time 0.103)\t Loss 1.1744 (Avg-Loss 1.0931)\tAcc 59.7656 (Avg-Acc 61.9892)\n",
            "Epoch: [42][57/78]\tTime 0.070 (Avg-Time 0.086)\t Loss 1.0942 (Avg-Loss 1.1012)\tAcc 60.7422 (Avg-Acc 61.6312)\n",
            "Epoch: [42][76/78]\tTime 0.050 (Avg-Time 0.077)\t Loss 1.0365 (Avg-Loss 1.1097)\tAcc 63.0859 (Avg-Acc 61.3712)\n",
            "Epoch: [42][78/78]\tTime 0.023 (Avg-Time 0.076)\t Loss 1.1121 (Avg-Loss 1.1102)\tAcc 62.5000 (Avg-Acc 61.3425)\n",
            "EPOCH: 42 train Results: Acc 61.343 Loss: 1.1102\n",
            "Epoch: [42][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2243 (Avg-Loss 1.2243)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [42][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3075 (Avg-Loss 1.2389)\tAcc 54.0441 (Avg-Acc 56.6000)\n",
            "EPOCH: 42 Validation Results: Acc 56.600 Loss: 1.2389\n",
            "Epoch: [43][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0880 (Avg-Loss 1.0880)\tAcc 62.5000 (Avg-Acc 62.5000)\n",
            "Epoch: [43][19/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.0140 (Avg-Loss 1.0774)\tAcc 65.4297 (Avg-Acc 62.9688)\n",
            "Epoch: [43][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0447 (Avg-Loss 1.0767)\tAcc 64.6484 (Avg-Acc 62.8956)\n",
            "Epoch: [43][57/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.1325 (Avg-Loss 1.0924)\tAcc 60.7422 (Avg-Acc 62.0656)\n",
            "Epoch: [43][76/78]\tTime 0.063 (Avg-Time 0.050)\t Loss 1.1173 (Avg-Loss 1.0965)\tAcc 62.3047 (Avg-Acc 61.8278)\n",
            "Epoch: [43][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.1783 (Avg-Loss 1.0968)\tAcc 60.9375 (Avg-Acc 61.8100)\n",
            "EPOCH: 43 train Results: Acc 61.810 Loss: 1.0968\n",
            "Epoch: [43][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2179 (Avg-Loss 1.2179)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [43][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2704 (Avg-Loss 1.2450)\tAcc 54.7794 (Avg-Acc 56.5600)\n",
            "EPOCH: 43 Validation Results: Acc 56.560 Loss: 1.2450\n",
            "Epoch: [44][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0564 (Avg-Loss 1.0564)\tAcc 62.6953 (Avg-Acc 62.6953)\n",
            "Epoch: [44][19/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.0187 (Avg-Loss 1.0675)\tAcc 65.4297 (Avg-Acc 62.7344)\n",
            "Epoch: [44][38/78]\tTime 0.050 (Avg-Time 0.051)\t Loss 1.1017 (Avg-Loss 1.0774)\tAcc 61.5234 (Avg-Acc 62.3698)\n",
            "Epoch: [44][57/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 1.1452 (Avg-Loss 1.0837)\tAcc 60.5469 (Avg-Acc 61.9006)\n",
            "Epoch: [44][76/78]\tTime 0.104 (Avg-Time 0.075)\t Loss 1.1007 (Avg-Loss 1.0915)\tAcc 62.3047 (Avg-Acc 61.6908)\n",
            "Epoch: [44][78/78]\tTime 0.028 (Avg-Time 0.075)\t Loss 1.1519 (Avg-Loss 1.0917)\tAcc 59.3750 (Avg-Acc 61.6825)\n",
            "EPOCH: 44 train Results: Acc 61.682 Loss: 1.0917\n",
            "Epoch: [44][0/19]\tTime 0.026 (Avg-Time 0.026)\t Loss 1.2258 (Avg-Loss 1.2258)\tAcc 56.0547 (Avg-Acc 56.0547)\n",
            "Epoch: [44][19/19]\tTime 0.007 (Avg-Time 0.017)\t Loss 1.2934 (Avg-Loss 1.2484)\tAcc 52.2059 (Avg-Acc 55.8400)\n",
            "EPOCH: 44 Validation Results: Acc 55.840 Loss: 1.2484\n",
            "Epoch: [45][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0543 (Avg-Loss 1.0543)\tAcc 61.9141 (Avg-Acc 61.9141)\n",
            "Epoch: [45][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0075 (Avg-Loss 1.0621)\tAcc 66.0156 (Avg-Acc 63.3887)\n",
            "Epoch: [45][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.1199 (Avg-Loss 1.0762)\tAcc 59.7656 (Avg-Acc 62.6552)\n",
            "Epoch: [45][57/78]\tTime 0.069 (Avg-Time 0.050)\t Loss 1.1012 (Avg-Loss 1.0865)\tAcc 61.3281 (Avg-Acc 62.0589)\n",
            "Epoch: [45][76/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.1296 (Avg-Loss 1.0960)\tAcc 60.1562 (Avg-Acc 61.5234)\n",
            "Epoch: [45][78/78]\tTime 0.010 (Avg-Time 0.050)\t Loss 1.2565 (Avg-Loss 1.0967)\tAcc 60.9375 (Avg-Acc 61.4850)\n",
            "EPOCH: 45 train Results: Acc 61.485 Loss: 1.0967\n",
            "Epoch: [45][0/19]\tTime 0.038 (Avg-Time 0.038)\t Loss 1.2381 (Avg-Loss 1.2381)\tAcc 55.0781 (Avg-Acc 55.0781)\n",
            "Epoch: [45][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2623 (Avg-Loss 1.2384)\tAcc 56.9853 (Avg-Acc 56.3300)\n",
            "EPOCH: 45 Validation Results: Acc 56.330 Loss: 1.2384\n",
            "Epoch: [46][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.1309 (Avg-Loss 1.1309)\tAcc 60.5469 (Avg-Acc 60.5469)\n",
            "Epoch: [46][19/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.0355 (Avg-Loss 1.0605)\tAcc 66.0156 (Avg-Acc 63.1152)\n",
            "Epoch: [46][38/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 1.1062 (Avg-Loss 1.0667)\tAcc 60.5469 (Avg-Acc 62.8856)\n",
            "Epoch: [46][57/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.0889 (Avg-Loss 1.0784)\tAcc 62.3047 (Avg-Acc 62.3720)\n",
            "Epoch: [46][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1034 (Avg-Loss 1.0904)\tAcc 60.1562 (Avg-Acc 61.8557)\n",
            "Epoch: [46][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.2829 (Avg-Loss 1.0914)\tAcc 53.1250 (Avg-Acc 61.8225)\n",
            "EPOCH: 46 train Results: Acc 61.822 Loss: 1.0914\n",
            "Epoch: [46][0/19]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2086 (Avg-Loss 1.2086)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [46][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2465 (Avg-Loss 1.2329)\tAcc 55.1471 (Avg-Acc 56.5600)\n",
            "EPOCH: 46 Validation Results: Acc 56.560 Loss: 1.2329\n",
            "Epoch: [47][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 1.0057 (Avg-Loss 1.0057)\tAcc 67.7734 (Avg-Acc 67.7734)\n",
            "Epoch: [47][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0456 (Avg-Loss 1.0512)\tAcc 65.0391 (Avg-Acc 63.8477)\n",
            "Epoch: [47][38/78]\tTime 0.210 (Avg-Time 0.080)\t Loss 1.0879 (Avg-Loss 1.0624)\tAcc 61.1328 (Avg-Acc 63.3614)\n",
            "Epoch: [47][57/78]\tTime 0.067 (Avg-Time 0.086)\t Loss 1.1115 (Avg-Loss 1.0785)\tAcc 61.1328 (Avg-Acc 62.7829)\n",
            "Epoch: [47][76/78]\tTime 0.047 (Avg-Time 0.077)\t Loss 1.1795 (Avg-Loss 1.0884)\tAcc 59.3750 (Avg-Acc 62.3427)\n",
            "Epoch: [47][78/78]\tTime 0.015 (Avg-Time 0.076)\t Loss 1.1520 (Avg-Loss 1.0884)\tAcc 62.5000 (Avg-Acc 62.3525)\n",
            "EPOCH: 47 train Results: Acc 62.352 Loss: 1.0884\n",
            "Epoch: [47][0/19]\tTime 0.019 (Avg-Time 0.019)\t Loss 1.2030 (Avg-Loss 1.2030)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [47][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2748 (Avg-Loss 1.2369)\tAcc 56.6176 (Avg-Acc 56.9400)\n",
            "EPOCH: 47 Validation Results: Acc 56.940 Loss: 1.2369\n",
            "Epoch: [48][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0400 (Avg-Loss 1.0400)\tAcc 66.0156 (Avg-Acc 66.0156)\n",
            "Epoch: [48][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0376 (Avg-Loss 1.0500)\tAcc 63.8672 (Avg-Acc 63.2422)\n",
            "Epoch: [48][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0670 (Avg-Loss 1.0617)\tAcc 60.3516 (Avg-Acc 62.7254)\n",
            "Epoch: [48][57/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.1284 (Avg-Loss 1.0772)\tAcc 60.9375 (Avg-Acc 62.1801)\n",
            "Epoch: [48][76/78]\tTime 0.062 (Avg-Time 0.051)\t Loss 1.1021 (Avg-Loss 1.0892)\tAcc 61.1328 (Avg-Acc 61.8608)\n",
            "Epoch: [48][78/78]\tTime 0.009 (Avg-Time 0.051)\t Loss 1.0487 (Avg-Loss 1.0895)\tAcc 64.0625 (Avg-Acc 61.8350)\n",
            "EPOCH: 48 train Results: Acc 61.835 Loss: 1.0895\n",
            "Epoch: [48][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2074 (Avg-Loss 1.2074)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [48][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2626 (Avg-Loss 1.2344)\tAcc 54.0441 (Avg-Acc 56.0500)\n",
            "EPOCH: 48 Validation Results: Acc 56.050 Loss: 1.2344\n",
            "Epoch: [49][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0570 (Avg-Loss 1.0570)\tAcc 66.0156 (Avg-Acc 66.0156)\n",
            "Epoch: [49][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0420 (Avg-Loss 1.0484)\tAcc 62.3047 (Avg-Acc 63.5254)\n",
            "Epoch: [49][38/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 1.0610 (Avg-Loss 1.0649)\tAcc 60.3516 (Avg-Acc 62.6903)\n",
            "Epoch: [49][57/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 1.0541 (Avg-Loss 1.0760)\tAcc 61.3281 (Avg-Acc 62.2811)\n",
            "Epoch: [49][76/78]\tTime 0.072 (Avg-Time 0.050)\t Loss 1.1272 (Avg-Loss 1.0840)\tAcc 59.3750 (Avg-Acc 61.9090)\n",
            "Epoch: [49][78/78]\tTime 0.011 (Avg-Time 0.050)\t Loss 1.0747 (Avg-Loss 1.0845)\tAcc 62.5000 (Avg-Acc 61.8750)\n",
            "EPOCH: 49 train Results: Acc 61.875 Loss: 1.0845\n",
            "Epoch: [49][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1939 (Avg-Loss 1.1939)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [49][19/19]\tTime 0.014 (Avg-Time 0.016)\t Loss 1.2604 (Avg-Loss 1.2346)\tAcc 51.4706 (Avg-Acc 56.4800)\n",
            "EPOCH: 49 Validation Results: Acc 56.480 Loss: 1.2346\n",
            "Epoch: [50][0/78]\tTime 0.113 (Avg-Time 0.113)\t Loss 1.0255 (Avg-Loss 1.0255)\tAcc 63.2812 (Avg-Acc 63.2812)\n",
            "Epoch: [50][19/78]\tTime 0.136 (Avg-Time 0.131)\t Loss 1.0889 (Avg-Loss 1.0565)\tAcc 61.3281 (Avg-Acc 62.9004)\n",
            "Epoch: [50][38/78]\tTime 0.048 (Avg-Time 0.100)\t Loss 1.0649 (Avg-Loss 1.0605)\tAcc 62.8906 (Avg-Acc 63.2161)\n",
            "Epoch: [50][57/78]\tTime 0.073 (Avg-Time 0.084)\t Loss 1.1683 (Avg-Loss 1.0721)\tAcc 59.9609 (Avg-Acc 62.5606)\n",
            "Epoch: [50][76/78]\tTime 0.049 (Avg-Time 0.076)\t Loss 1.1231 (Avg-Loss 1.0816)\tAcc 60.5469 (Avg-Acc 62.1956)\n",
            "Epoch: [50][78/78]\tTime 0.022 (Avg-Time 0.075)\t Loss 1.3955 (Avg-Loss 1.0835)\tAcc 43.7500 (Avg-Acc 62.1475)\n",
            "EPOCH: 50 train Results: Acc 62.148 Loss: 1.0835\n",
            "Epoch: [50][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2043 (Avg-Loss 1.2043)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [50][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2622 (Avg-Loss 1.2285)\tAcc 55.1471 (Avg-Acc 56.0500)\n",
            "EPOCH: 50 Validation Results: Acc 56.050 Loss: 1.2285\n",
            "Epoch: [51][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9546 (Avg-Loss 0.9546)\tAcc 66.7969 (Avg-Acc 66.7969)\n",
            "Epoch: [51][19/78]\tTime 0.047 (Avg-Time 0.052)\t Loss 1.1104 (Avg-Loss 1.0467)\tAcc 62.3047 (Avg-Acc 63.4277)\n",
            "Epoch: [51][38/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.0789 (Avg-Loss 1.0636)\tAcc 59.9609 (Avg-Acc 62.5851)\n",
            "Epoch: [51][57/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.1271 (Avg-Loss 1.0704)\tAcc 62.8906 (Avg-Acc 62.5034)\n",
            "Epoch: [51][76/78]\tTime 0.071 (Avg-Time 0.050)\t Loss 1.1093 (Avg-Loss 1.0809)\tAcc 60.7422 (Avg-Acc 61.8735)\n",
            "Epoch: [51][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 0.9596 (Avg-Loss 1.0814)\tAcc 67.1875 (Avg-Acc 61.8675)\n",
            "EPOCH: 51 train Results: Acc 61.867 Loss: 1.0814\n",
            "Epoch: [51][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2118 (Avg-Loss 1.2118)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [51][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2730 (Avg-Loss 1.2319)\tAcc 55.1471 (Avg-Acc 56.5100)\n",
            "EPOCH: 51 Validation Results: Acc 56.510 Loss: 1.2319\n",
            "Epoch: [52][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0048 (Avg-Loss 1.0048)\tAcc 64.6484 (Avg-Acc 64.6484)\n",
            "Epoch: [52][19/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 1.0193 (Avg-Loss 1.0466)\tAcc 62.1094 (Avg-Acc 63.7012)\n",
            "Epoch: [52][38/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.1365 (Avg-Loss 1.0535)\tAcc 61.9141 (Avg-Acc 63.3864)\n",
            "Epoch: [52][57/78]\tTime 0.085 (Avg-Time 0.051)\t Loss 1.0934 (Avg-Loss 1.0611)\tAcc 62.1094 (Avg-Acc 62.9916)\n",
            "Epoch: [52][76/78]\tTime 0.100 (Avg-Time 0.071)\t Loss 1.0397 (Avg-Loss 1.0730)\tAcc 62.1094 (Avg-Acc 62.6623)\n",
            "Epoch: [52][78/78]\tTime 0.019 (Avg-Time 0.071)\t Loss 1.4604 (Avg-Loss 1.0736)\tAcc 46.8750 (Avg-Acc 62.6075)\n",
            "EPOCH: 52 train Results: Acc 62.608 Loss: 1.0736\n",
            "Epoch: [52][0/19]\tTime 0.028 (Avg-Time 0.028)\t Loss 1.2066 (Avg-Loss 1.2066)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [52][19/19]\tTime 0.013 (Avg-Time 0.032)\t Loss 1.2983 (Avg-Loss 1.2382)\tAcc 54.0441 (Avg-Acc 56.5800)\n",
            "EPOCH: 52 Validation Results: Acc 56.580 Loss: 1.2382\n",
            "Epoch: [53][0/78]\tTime 0.093 (Avg-Time 0.093)\t Loss 0.9343 (Avg-Loss 0.9343)\tAcc 68.1641 (Avg-Acc 68.1641)\n",
            "Epoch: [53][19/78]\tTime 0.048 (Avg-Time 0.053)\t Loss 1.0620 (Avg-Loss 1.0292)\tAcc 63.8672 (Avg-Acc 64.4141)\n",
            "Epoch: [53][38/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.0487 (Avg-Loss 1.0446)\tAcc 60.5469 (Avg-Acc 63.7470)\n",
            "Epoch: [53][57/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.0885 (Avg-Loss 1.0647)\tAcc 63.0859 (Avg-Acc 63.0624)\n",
            "Epoch: [53][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0439 (Avg-Loss 1.0750)\tAcc 62.1094 (Avg-Acc 62.5330)\n",
            "Epoch: [53][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.1749 (Avg-Loss 1.0747)\tAcc 53.1250 (Avg-Acc 62.5225)\n",
            "EPOCH: 53 train Results: Acc 62.523 Loss: 1.0747\n",
            "Epoch: [53][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2246 (Avg-Loss 1.2246)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [53][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2824 (Avg-Loss 1.2340)\tAcc 54.0441 (Avg-Acc 56.7900)\n",
            "EPOCH: 53 Validation Results: Acc 56.790 Loss: 1.2340\n",
            "Epoch: [54][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9827 (Avg-Loss 0.9827)\tAcc 67.3828 (Avg-Acc 67.3828)\n",
            "Epoch: [54][19/78]\tTime 0.052 (Avg-Time 0.050)\t Loss 1.0939 (Avg-Loss 1.0510)\tAcc 64.2578 (Avg-Acc 63.4375)\n",
            "Epoch: [54][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0791 (Avg-Loss 1.0580)\tAcc 62.8906 (Avg-Acc 63.3263)\n",
            "Epoch: [54][57/78]\tTime 0.056 (Avg-Time 0.050)\t Loss 1.0915 (Avg-Loss 1.0714)\tAcc 60.9375 (Avg-Acc 62.6751)\n",
            "Epoch: [54][76/78]\tTime 0.060 (Avg-Time 0.050)\t Loss 1.0798 (Avg-Loss 1.0792)\tAcc 60.7422 (Avg-Acc 62.3478)\n",
            "Epoch: [54][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.0808 (Avg-Loss 1.0800)\tAcc 62.5000 (Avg-Acc 62.2850)\n",
            "EPOCH: 54 train Results: Acc 62.285 Loss: 1.0800\n",
            "Epoch: [54][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2001 (Avg-Loss 1.2001)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [54][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2892 (Avg-Loss 1.2369)\tAcc 54.4118 (Avg-Acc 56.2100)\n",
            "EPOCH: 54 Validation Results: Acc 56.210 Loss: 1.2369\n",
            "Epoch: [55][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9904 (Avg-Loss 0.9904)\tAcc 66.9922 (Avg-Acc 66.9922)\n",
            "Epoch: [55][19/78]\tTime 0.050 (Avg-Time 0.051)\t Loss 1.0349 (Avg-Loss 1.0491)\tAcc 64.4531 (Avg-Acc 63.3105)\n",
            "Epoch: [55][38/78]\tTime 0.179 (Avg-Time 0.060)\t Loss 1.1306 (Avg-Loss 1.0588)\tAcc 60.5469 (Avg-Acc 62.9958)\n",
            "Epoch: [55][57/78]\tTime 0.061 (Avg-Time 0.086)\t Loss 1.1044 (Avg-Loss 1.0659)\tAcc 62.5000 (Avg-Acc 62.7290)\n",
            "Epoch: [55][76/78]\tTime 0.048 (Avg-Time 0.077)\t Loss 1.0671 (Avg-Loss 1.0719)\tAcc 62.8906 (Avg-Acc 62.4467)\n",
            "Epoch: [55][78/78]\tTime 0.011 (Avg-Time 0.076)\t Loss 1.1250 (Avg-Loss 1.0722)\tAcc 56.2500 (Avg-Acc 62.4500)\n",
            "EPOCH: 55 train Results: Acc 62.450 Loss: 1.0722\n",
            "Epoch: [55][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2084 (Avg-Loss 1.2084)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [55][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2738 (Avg-Loss 1.2343)\tAcc 56.2500 (Avg-Acc 56.4900)\n",
            "EPOCH: 55 Validation Results: Acc 56.490 Loss: 1.2343\n",
            "Epoch: [56][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0004 (Avg-Loss 1.0004)\tAcc 66.2109 (Avg-Acc 66.2109)\n",
            "Epoch: [56][19/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 1.0665 (Avg-Loss 1.0486)\tAcc 62.3047 (Avg-Acc 63.7891)\n",
            "Epoch: [56][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0080 (Avg-Loss 1.0495)\tAcc 64.2578 (Avg-Acc 63.7320)\n",
            "Epoch: [56][57/78]\tTime 0.062 (Avg-Time 0.050)\t Loss 1.0809 (Avg-Loss 1.0610)\tAcc 61.1328 (Avg-Acc 63.2678)\n",
            "Epoch: [56][76/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0352 (Avg-Loss 1.0687)\tAcc 65.0391 (Avg-Acc 62.7004)\n",
            "Epoch: [56][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.0094 (Avg-Loss 1.0696)\tAcc 64.0625 (Avg-Acc 62.6625)\n",
            "EPOCH: 56 train Results: Acc 62.663 Loss: 1.0696\n",
            "Epoch: [56][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1942 (Avg-Loss 1.1942)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [56][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2543 (Avg-Loss 1.2323)\tAcc 55.1471 (Avg-Acc 56.5400)\n",
            "EPOCH: 56 Validation Results: Acc 56.540 Loss: 1.2323\n",
            "Epoch: [57][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.9785 (Avg-Loss 0.9785)\tAcc 65.0391 (Avg-Acc 65.0391)\n",
            "Epoch: [57][19/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0927 (Avg-Loss 1.0201)\tAcc 60.5469 (Avg-Acc 64.5703)\n",
            "Epoch: [57][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0371 (Avg-Loss 1.0386)\tAcc 66.0156 (Avg-Acc 64.0775)\n",
            "Epoch: [57][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0491 (Avg-Loss 1.0588)\tAcc 64.0625 (Avg-Acc 63.0523)\n",
            "Epoch: [57][76/78]\tTime 0.067 (Avg-Time 0.050)\t Loss 1.0051 (Avg-Loss 1.0656)\tAcc 64.8438 (Avg-Acc 62.7663)\n",
            "Epoch: [57][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.1108 (Avg-Loss 1.0656)\tAcc 56.2500 (Avg-Acc 62.7375)\n",
            "EPOCH: 57 train Results: Acc 62.737 Loss: 1.0656\n",
            "Epoch: [57][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1941 (Avg-Loss 1.1941)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [57][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2653 (Avg-Loss 1.2352)\tAcc 54.4118 (Avg-Acc 56.2600)\n",
            "EPOCH: 57 Validation Results: Acc 56.260 Loss: 1.2352\n",
            "Epoch: [58][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9903 (Avg-Loss 0.9903)\tAcc 66.0156 (Avg-Acc 66.0156)\n",
            "Epoch: [58][19/78]\tTime 0.113 (Avg-Time 0.121)\t Loss 1.0487 (Avg-Loss 1.0297)\tAcc 62.1094 (Avg-Acc 64.0039)\n",
            "Epoch: [58][38/78]\tTime 0.047 (Avg-Time 0.102)\t Loss 1.0207 (Avg-Loss 1.0439)\tAcc 64.8438 (Avg-Acc 63.2863)\n",
            "Epoch: [58][57/78]\tTime 0.071 (Avg-Time 0.085)\t Loss 1.1067 (Avg-Loss 1.0563)\tAcc 62.1094 (Avg-Acc 62.9176)\n",
            "Epoch: [58][76/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 1.1466 (Avg-Loss 1.0654)\tAcc 57.6172 (Avg-Acc 62.5609)\n",
            "Epoch: [58][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.1875 (Avg-Loss 1.0660)\tAcc 57.8125 (Avg-Acc 62.5500)\n",
            "EPOCH: 58 train Results: Acc 62.550 Loss: 1.0660\n",
            "Epoch: [58][0/19]\tTime 0.016 (Avg-Time 0.016)\t Loss 1.1955 (Avg-Loss 1.1955)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [58][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2611 (Avg-Loss 1.2267)\tAcc 55.5147 (Avg-Acc 57.0800)\n",
            "EPOCH: 58 Validation Results: Acc 57.080 Loss: 1.2267\n",
            "Epoch: [59][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9655 (Avg-Loss 0.9655)\tAcc 65.8203 (Avg-Acc 65.8203)\n",
            "Epoch: [59][19/78]\tTime 0.048 (Avg-Time 0.052)\t Loss 1.0631 (Avg-Loss 1.0404)\tAcc 62.8906 (Avg-Acc 63.3301)\n",
            "Epoch: [59][38/78]\tTime 0.048 (Avg-Time 0.052)\t Loss 1.0198 (Avg-Loss 1.0481)\tAcc 67.7734 (Avg-Acc 63.3614)\n",
            "Epoch: [59][57/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.1498 (Avg-Loss 1.0570)\tAcc 58.3984 (Avg-Acc 63.0422)\n",
            "Epoch: [59][76/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.0493 (Avg-Loss 1.0633)\tAcc 65.0391 (Avg-Acc 62.8830)\n",
            "Epoch: [59][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.0256 (Avg-Loss 1.0637)\tAcc 65.6250 (Avg-Acc 62.8750)\n",
            "EPOCH: 59 train Results: Acc 62.875 Loss: 1.0637\n",
            "Epoch: [59][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1738 (Avg-Loss 1.1738)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [59][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3324 (Avg-Loss 1.2325)\tAcc 52.9412 (Avg-Acc 56.5600)\n",
            "EPOCH: 59 Validation Results: Acc 56.560 Loss: 1.2325\n",
            "Epoch: [60][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0257 (Avg-Loss 1.0257)\tAcc 64.2578 (Avg-Acc 64.2578)\n",
            "Epoch: [60][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0168 (Avg-Loss 1.0394)\tAcc 63.8672 (Avg-Acc 63.5645)\n",
            "Epoch: [60][38/78]\tTime 0.052 (Avg-Time 0.050)\t Loss 1.0742 (Avg-Loss 1.0440)\tAcc 62.5000 (Avg-Acc 63.4916)\n",
            "Epoch: [60][57/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 1.0560 (Avg-Loss 1.0518)\tAcc 61.7188 (Avg-Acc 63.1567)\n",
            "Epoch: [60][76/78]\tTime 0.090 (Avg-Time 0.069)\t Loss 1.0160 (Avg-Loss 1.0583)\tAcc 63.0859 (Avg-Acc 62.9642)\n",
            "Epoch: [60][78/78]\tTime 0.022 (Avg-Time 0.069)\t Loss 1.3363 (Avg-Loss 1.0596)\tAcc 54.6875 (Avg-Acc 62.9050)\n",
            "EPOCH: 60 train Results: Acc 62.905 Loss: 1.0596\n",
            "Epoch: [60][0/19]\tTime 0.025 (Avg-Time 0.025)\t Loss 1.1917 (Avg-Loss 1.1917)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [60][19/19]\tTime 0.012 (Avg-Time 0.030)\t Loss 1.2783 (Avg-Loss 1.2252)\tAcc 54.7794 (Avg-Acc 56.5700)\n",
            "EPOCH: 60 Validation Results: Acc 56.570 Loss: 1.2252\n",
            "Epoch: [61][0/78]\tTime 0.135 (Avg-Time 0.135)\t Loss 0.9266 (Avg-Loss 0.9266)\tAcc 70.3125 (Avg-Acc 70.3125)\n",
            "Epoch: [61][19/78]\tTime 0.047 (Avg-Time 0.060)\t Loss 1.0414 (Avg-Loss 1.0273)\tAcc 65.8203 (Avg-Acc 64.5312)\n",
            "Epoch: [61][38/78]\tTime 0.053 (Avg-Time 0.055)\t Loss 1.0234 (Avg-Loss 1.0388)\tAcc 63.2812 (Avg-Acc 64.0024)\n",
            "Epoch: [61][57/78]\tTime 0.062 (Avg-Time 0.054)\t Loss 1.0665 (Avg-Loss 1.0569)\tAcc 60.3516 (Avg-Acc 63.1903)\n",
            "Epoch: [61][76/78]\tTime 0.048 (Avg-Time 0.053)\t Loss 1.1082 (Avg-Loss 1.0630)\tAcc 62.5000 (Avg-Acc 63.0149)\n",
            "Epoch: [61][78/78]\tTime 0.011 (Avg-Time 0.052)\t Loss 1.1333 (Avg-Loss 1.0638)\tAcc 60.9375 (Avg-Acc 62.9875)\n",
            "EPOCH: 61 train Results: Acc 62.987 Loss: 1.0638\n",
            "Epoch: [61][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2034 (Avg-Loss 1.2034)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [61][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3015 (Avg-Loss 1.2437)\tAcc 52.2059 (Avg-Acc 55.9200)\n",
            "EPOCH: 61 Validation Results: Acc 55.920 Loss: 1.2437\n",
            "Epoch: [62][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0093 (Avg-Loss 1.0093)\tAcc 66.6016 (Avg-Acc 66.6016)\n",
            "Epoch: [62][19/78]\tTime 0.053 (Avg-Time 0.051)\t Loss 1.0950 (Avg-Loss 1.0563)\tAcc 61.9141 (Avg-Acc 63.4570)\n",
            "Epoch: [62][38/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 1.0572 (Avg-Loss 1.0542)\tAcc 65.2344 (Avg-Acc 63.5968)\n",
            "Epoch: [62][57/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.0518 (Avg-Loss 1.0614)\tAcc 64.2578 (Avg-Acc 63.2341)\n",
            "Epoch: [62][76/78]\tTime 0.067 (Avg-Time 0.051)\t Loss 1.1190 (Avg-Loss 1.0653)\tAcc 60.5469 (Avg-Acc 63.1012)\n",
            "Epoch: [62][78/78]\tTime 0.009 (Avg-Time 0.051)\t Loss 0.9533 (Avg-Loss 1.0652)\tAcc 65.6250 (Avg-Acc 63.0975)\n",
            "EPOCH: 62 train Results: Acc 63.097 Loss: 1.0652\n",
            "Epoch: [62][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2075 (Avg-Loss 1.2075)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [62][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2784 (Avg-Loss 1.2349)\tAcc 54.7794 (Avg-Acc 56.6500)\n",
            "EPOCH: 62 Validation Results: Acc 56.650 Loss: 1.2349\n",
            "Epoch: [63][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0189 (Avg-Loss 1.0189)\tAcc 64.6484 (Avg-Acc 64.6484)\n",
            "Epoch: [63][19/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.1144 (Avg-Loss 1.0446)\tAcc 59.5703 (Avg-Acc 63.4668)\n",
            "Epoch: [63][38/78]\tTime 0.097 (Avg-Time 0.054)\t Loss 1.0277 (Avg-Loss 1.0336)\tAcc 63.6719 (Avg-Acc 63.9974)\n",
            "Epoch: [63][57/78]\tTime 0.081 (Avg-Time 0.084)\t Loss 1.0063 (Avg-Loss 1.0455)\tAcc 66.6016 (Avg-Acc 63.3520)\n",
            "Epoch: [63][76/78]\tTime 0.050 (Avg-Time 0.076)\t Loss 1.0599 (Avg-Loss 1.0579)\tAcc 62.3047 (Avg-Acc 62.9743)\n",
            "Epoch: [63][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.1987 (Avg-Loss 1.0585)\tAcc 64.0625 (Avg-Acc 62.9475)\n",
            "EPOCH: 63 train Results: Acc 62.947 Loss: 1.0585\n",
            "Epoch: [63][0/19]\tTime 0.019 (Avg-Time 0.019)\t Loss 1.1865 (Avg-Loss 1.1865)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [63][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2819 (Avg-Loss 1.2376)\tAcc 52.9412 (Avg-Acc 56.0500)\n",
            "EPOCH: 63 Validation Results: Acc 56.050 Loss: 1.2376\n",
            "Epoch: [64][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9460 (Avg-Loss 0.9460)\tAcc 68.9453 (Avg-Acc 68.9453)\n",
            "Epoch: [64][19/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 1.0541 (Avg-Loss 1.0289)\tAcc 62.8906 (Avg-Acc 64.5605)\n",
            "Epoch: [64][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0857 (Avg-Loss 1.0472)\tAcc 63.8672 (Avg-Acc 63.7971)\n",
            "Epoch: [64][57/78]\tTime 0.058 (Avg-Time 0.051)\t Loss 1.1158 (Avg-Loss 1.0516)\tAcc 61.9141 (Avg-Acc 63.6012)\n",
            "Epoch: [64][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0788 (Avg-Loss 1.0606)\tAcc 62.1094 (Avg-Acc 63.2635)\n",
            "Epoch: [64][78/78]\tTime 0.010 (Avg-Time 0.050)\t Loss 1.0652 (Avg-Loss 1.0608)\tAcc 65.6250 (Avg-Acc 63.2650)\n",
            "EPOCH: 64 train Results: Acc 63.265 Loss: 1.0608\n",
            "Epoch: [64][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1784 (Avg-Loss 1.1784)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [64][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2611 (Avg-Loss 1.2284)\tAcc 54.4118 (Avg-Acc 56.6200)\n",
            "EPOCH: 64 Validation Results: Acc 56.620 Loss: 1.2284\n",
            "Epoch: [65][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0106 (Avg-Loss 1.0106)\tAcc 66.2109 (Avg-Acc 66.2109)\n",
            "Epoch: [65][19/78]\tTime 0.052 (Avg-Time 0.050)\t Loss 1.0612 (Avg-Loss 1.0223)\tAcc 63.8672 (Avg-Acc 65.2734)\n",
            "Epoch: [65][38/78]\tTime 0.053 (Avg-Time 0.052)\t Loss 1.0237 (Avg-Loss 1.0321)\tAcc 64.2578 (Avg-Acc 64.4081)\n",
            "Epoch: [65][57/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.1106 (Avg-Loss 1.0443)\tAcc 57.6172 (Avg-Acc 63.7392)\n",
            "Epoch: [65][76/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.1081 (Avg-Loss 1.0502)\tAcc 62.3047 (Avg-Acc 63.4055)\n",
            "Epoch: [65][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.2205 (Avg-Loss 1.0507)\tAcc 59.3750 (Avg-Acc 63.3925)\n",
            "EPOCH: 65 train Results: Acc 63.392 Loss: 1.0507\n",
            "Epoch: [65][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2025 (Avg-Loss 1.2025)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [65][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2916 (Avg-Loss 1.2282)\tAcc 51.4706 (Avg-Acc 56.4700)\n",
            "EPOCH: 65 Validation Results: Acc 56.470 Loss: 1.2282\n",
            "Epoch: [66][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0639 (Avg-Loss 1.0639)\tAcc 64.4531 (Avg-Acc 64.4531)\n",
            "Epoch: [66][19/78]\tTime 0.095 (Avg-Time 0.123)\t Loss 1.1045 (Avg-Loss 1.0356)\tAcc 63.6719 (Avg-Acc 64.7559)\n",
            "Epoch: [66][38/78]\tTime 0.046 (Avg-Time 0.102)\t Loss 1.1035 (Avg-Loss 1.0451)\tAcc 61.1328 (Avg-Acc 64.1627)\n",
            "Epoch: [66][57/78]\tTime 0.054 (Avg-Time 0.085)\t Loss 1.0722 (Avg-Loss 1.0471)\tAcc 62.5000 (Avg-Acc 63.8369)\n",
            "Epoch: [66][76/78]\tTime 0.048 (Avg-Time 0.076)\t Loss 1.1078 (Avg-Loss 1.0553)\tAcc 61.7188 (Avg-Acc 63.4766)\n",
            "Epoch: [66][78/78]\tTime 0.021 (Avg-Time 0.075)\t Loss 1.1383 (Avg-Loss 1.0559)\tAcc 59.3750 (Avg-Acc 63.5050)\n",
            "EPOCH: 66 train Results: Acc 63.505 Loss: 1.0559\n",
            "Epoch: [66][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2017 (Avg-Loss 1.2017)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [66][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2862 (Avg-Loss 1.2359)\tAcc 52.2059 (Avg-Acc 56.7700)\n",
            "EPOCH: 66 Validation Results: Acc 56.770 Loss: 1.2359\n",
            "Epoch: [67][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.9456 (Avg-Loss 0.9456)\tAcc 66.4062 (Avg-Acc 66.4062)\n",
            "Epoch: [67][19/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9388 (Avg-Loss 1.0224)\tAcc 66.2109 (Avg-Acc 64.0332)\n",
            "Epoch: [67][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9974 (Avg-Loss 1.0320)\tAcc 61.9141 (Avg-Acc 63.6168)\n",
            "Epoch: [67][57/78]\tTime 0.062 (Avg-Time 0.050)\t Loss 1.0051 (Avg-Loss 1.0455)\tAcc 64.0625 (Avg-Acc 63.2543)\n",
            "Epoch: [67][76/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 1.1626 (Avg-Loss 1.0544)\tAcc 60.3516 (Avg-Acc 62.9743)\n",
            "Epoch: [67][78/78]\tTime 0.018 (Avg-Time 0.049)\t Loss 1.1594 (Avg-Loss 1.0547)\tAcc 59.3750 (Avg-Acc 62.9650)\n",
            "EPOCH: 67 train Results: Acc 62.965 Loss: 1.0547\n",
            "Epoch: [67][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.1802 (Avg-Loss 1.1802)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [67][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2624 (Avg-Loss 1.2224)\tAcc 55.8824 (Avg-Acc 56.9000)\n",
            "EPOCH: 67 Validation Results: Acc 56.900 Loss: 1.2224\n",
            "Epoch: [68][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9590 (Avg-Loss 0.9590)\tAcc 66.0156 (Avg-Acc 66.0156)\n",
            "Epoch: [68][19/78]\tTime 0.054 (Avg-Time 0.050)\t Loss 0.9819 (Avg-Loss 1.0158)\tAcc 64.8438 (Avg-Acc 64.6387)\n",
            "Epoch: [68][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0870 (Avg-Loss 1.0367)\tAcc 62.8906 (Avg-Acc 63.7770)\n",
            "Epoch: [68][57/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 1.0734 (Avg-Loss 1.0444)\tAcc 62.6953 (Avg-Acc 63.4294)\n",
            "Epoch: [68][76/78]\tTime 0.153 (Avg-Time 0.069)\t Loss 1.0878 (Avg-Loss 1.0504)\tAcc 63.6719 (Avg-Acc 63.3142)\n",
            "Epoch: [68][78/78]\tTime 0.021 (Avg-Time 0.068)\t Loss 1.1379 (Avg-Loss 1.0514)\tAcc 59.3750 (Avg-Acc 63.2500)\n",
            "EPOCH: 68 train Results: Acc 63.250 Loss: 1.0514\n",
            "Epoch: [68][0/19]\tTime 0.031 (Avg-Time 0.031)\t Loss 1.1845 (Avg-Loss 1.1845)\tAcc 60.1562 (Avg-Acc 60.1562)\n",
            "Epoch: [68][19/19]\tTime 0.019 (Avg-Time 0.031)\t Loss 1.2704 (Avg-Loss 1.2328)\tAcc 52.9412 (Avg-Acc 56.7300)\n",
            "EPOCH: 68 Validation Results: Acc 56.730 Loss: 1.2328\n",
            "Epoch: [69][0/78]\tTime 0.113 (Avg-Time 0.113)\t Loss 0.9951 (Avg-Loss 0.9951)\tAcc 66.4062 (Avg-Acc 66.4062)\n",
            "Epoch: [69][19/78]\tTime 0.049 (Avg-Time 0.060)\t Loss 1.0340 (Avg-Loss 1.0406)\tAcc 64.0625 (Avg-Acc 63.6816)\n",
            "Epoch: [69][38/78]\tTime 0.073 (Avg-Time 0.055)\t Loss 1.0695 (Avg-Loss 1.0379)\tAcc 60.5469 (Avg-Acc 63.4465)\n",
            "Epoch: [69][57/78]\tTime 0.048 (Avg-Time 0.053)\t Loss 1.0608 (Avg-Loss 1.0444)\tAcc 62.1094 (Avg-Acc 63.1971)\n",
            "Epoch: [69][76/78]\tTime 0.050 (Avg-Time 0.052)\t Loss 1.1133 (Avg-Loss 1.0562)\tAcc 61.9141 (Avg-Acc 62.8982)\n",
            "Epoch: [69][78/78]\tTime 0.009 (Avg-Time 0.052)\t Loss 0.9889 (Avg-Loss 1.0557)\tAcc 67.1875 (Avg-Acc 62.9150)\n",
            "EPOCH: 69 train Results: Acc 62.915 Loss: 1.0557\n",
            "Epoch: [69][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1860 (Avg-Loss 1.1860)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [69][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2826 (Avg-Loss 1.2269)\tAcc 52.9412 (Avg-Acc 56.9600)\n",
            "EPOCH: 69 Validation Results: Acc 56.960 Loss: 1.2269\n",
            "Epoch: [70][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9909 (Avg-Loss 0.9909)\tAcc 64.2578 (Avg-Acc 64.2578)\n",
            "Epoch: [70][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0721 (Avg-Loss 1.0206)\tAcc 62.6953 (Avg-Acc 64.5215)\n",
            "Epoch: [70][38/78]\tTime 0.055 (Avg-Time 0.051)\t Loss 1.0799 (Avg-Loss 1.0264)\tAcc 62.1094 (Avg-Acc 64.3630)\n",
            "Epoch: [70][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0783 (Avg-Loss 1.0370)\tAcc 63.0859 (Avg-Acc 64.1602)\n",
            "Epoch: [70][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0193 (Avg-Loss 1.0463)\tAcc 66.2109 (Avg-Acc 63.7911)\n",
            "Epoch: [70][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 0.9852 (Avg-Loss 1.0471)\tAcc 65.6250 (Avg-Acc 63.7550)\n",
            "EPOCH: 70 train Results: Acc 63.755 Loss: 1.0471\n",
            "Epoch: [70][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1874 (Avg-Loss 1.1874)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [70][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2793 (Avg-Loss 1.2266)\tAcc 50.0000 (Avg-Acc 56.8000)\n",
            "EPOCH: 70 Validation Results: Acc 56.800 Loss: 1.2266\n",
            "Epoch: [71][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9228 (Avg-Loss 0.9228)\tAcc 68.9453 (Avg-Acc 68.9453)\n",
            "Epoch: [71][19/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0329 (Avg-Loss 1.0153)\tAcc 65.6250 (Avg-Acc 64.7363)\n",
            "Epoch: [71][38/78]\tTime 0.090 (Avg-Time 0.055)\t Loss 1.0205 (Avg-Loss 1.0176)\tAcc 65.6250 (Avg-Acc 64.5182)\n",
            "Epoch: [71][57/78]\tTime 0.048 (Avg-Time 0.085)\t Loss 0.9961 (Avg-Loss 1.0276)\tAcc 64.6484 (Avg-Acc 64.1400)\n",
            "Epoch: [71][76/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 1.0688 (Avg-Loss 1.0446)\tAcc 64.0625 (Avg-Acc 63.4715)\n",
            "Epoch: [71][78/78]\tTime 0.010 (Avg-Time 0.075)\t Loss 1.0812 (Avg-Loss 1.0453)\tAcc 53.1250 (Avg-Acc 63.4375)\n",
            "EPOCH: 71 train Results: Acc 63.438 Loss: 1.0453\n",
            "Epoch: [71][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2053 (Avg-Loss 1.2053)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [71][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2941 (Avg-Loss 1.2207)\tAcc 52.5735 (Avg-Acc 56.9900)\n",
            "EPOCH: 71 Validation Results: Acc 56.990 Loss: 1.2207\n",
            "Epoch: [72][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.8661 (Avg-Loss 0.8661)\tAcc 71.4844 (Avg-Acc 71.4844)\n",
            "Epoch: [72][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0529 (Avg-Loss 1.0164)\tAcc 64.4531 (Avg-Acc 64.8242)\n",
            "Epoch: [72][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9917 (Avg-Loss 1.0275)\tAcc 66.6016 (Avg-Acc 64.4181)\n",
            "Epoch: [72][57/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0942 (Avg-Loss 1.0398)\tAcc 62.1094 (Avg-Acc 63.6954)\n",
            "Epoch: [72][76/78]\tTime 0.052 (Avg-Time 0.050)\t Loss 1.1282 (Avg-Loss 1.0499)\tAcc 58.0078 (Avg-Acc 63.2483)\n",
            "Epoch: [72][78/78]\tTime 0.010 (Avg-Time 0.050)\t Loss 0.9368 (Avg-Loss 1.0512)\tAcc 68.7500 (Avg-Acc 63.2200)\n",
            "EPOCH: 72 train Results: Acc 63.220 Loss: 1.0512\n",
            "Epoch: [72][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1780 (Avg-Loss 1.1780)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [72][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2964 (Avg-Loss 1.2274)\tAcc 52.5735 (Avg-Acc 56.9800)\n",
            "EPOCH: 72 Validation Results: Acc 56.980 Loss: 1.2274\n",
            "Epoch: [73][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0060 (Avg-Loss 1.0060)\tAcc 64.0625 (Avg-Acc 64.0625)\n",
            "Epoch: [73][19/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.0578 (Avg-Loss 1.0073)\tAcc 63.2812 (Avg-Acc 64.8535)\n",
            "Epoch: [73][38/78]\tTime 0.055 (Avg-Time 0.050)\t Loss 1.0794 (Avg-Loss 1.0195)\tAcc 60.7422 (Avg-Acc 64.4481)\n",
            "Epoch: [73][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0563 (Avg-Loss 1.0301)\tAcc 64.4531 (Avg-Acc 64.0120)\n",
            "Epoch: [73][76/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0921 (Avg-Loss 1.0428)\tAcc 60.9375 (Avg-Acc 63.4943)\n",
            "Epoch: [73][78/78]\tTime 0.014 (Avg-Time 0.050)\t Loss 0.9895 (Avg-Loss 1.0436)\tAcc 68.7500 (Avg-Acc 63.4825)\n",
            "EPOCH: 73 train Results: Acc 63.483 Loss: 1.0436\n",
            "Epoch: [73][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1874 (Avg-Loss 1.1874)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [73][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3214 (Avg-Loss 1.2278)\tAcc 50.3676 (Avg-Acc 56.6700)\n",
            "EPOCH: 73 Validation Results: Acc 56.670 Loss: 1.2278\n",
            "Epoch: [74][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0260 (Avg-Loss 1.0260)\tAcc 63.6719 (Avg-Acc 63.6719)\n",
            "Epoch: [74][19/78]\tTime 0.255 (Avg-Time 0.115)\t Loss 1.0250 (Avg-Loss 1.0174)\tAcc 61.5234 (Avg-Acc 64.2578)\n",
            "Epoch: [74][38/78]\tTime 0.048 (Avg-Time 0.102)\t Loss 0.9854 (Avg-Loss 1.0248)\tAcc 67.5781 (Avg-Acc 64.1927)\n",
            "Epoch: [74][57/78]\tTime 0.047 (Avg-Time 0.085)\t Loss 1.0759 (Avg-Loss 1.0384)\tAcc 62.5000 (Avg-Acc 63.7493)\n",
            "Epoch: [74][76/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 1.0846 (Avg-Loss 1.0471)\tAcc 60.3516 (Avg-Acc 63.3294)\n",
            "Epoch: [74][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.2075 (Avg-Loss 1.0478)\tAcc 53.1250 (Avg-Acc 63.3200)\n",
            "EPOCH: 74 train Results: Acc 63.320 Loss: 1.0478\n",
            "Epoch: [74][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1931 (Avg-Loss 1.1931)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [74][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2875 (Avg-Loss 1.2247)\tAcc 51.4706 (Avg-Acc 56.7800)\n",
            "EPOCH: 74 Validation Results: Acc 56.780 Loss: 1.2247\n",
            "Epoch: [75][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9244 (Avg-Loss 0.9244)\tAcc 66.9922 (Avg-Acc 66.9922)\n",
            "Epoch: [75][19/78]\tTime 0.065 (Avg-Time 0.050)\t Loss 0.9944 (Avg-Loss 1.0157)\tAcc 67.9688 (Avg-Acc 64.2871)\n",
            "Epoch: [75][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1040 (Avg-Loss 1.0249)\tAcc 56.6406 (Avg-Acc 64.2278)\n",
            "Epoch: [75][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0240 (Avg-Loss 1.0330)\tAcc 66.7969 (Avg-Acc 63.9716)\n",
            "Epoch: [75][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1150 (Avg-Loss 1.0469)\tAcc 60.7422 (Avg-Acc 63.4055)\n",
            "Epoch: [75][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.2545 (Avg-Loss 1.0489)\tAcc 59.3750 (Avg-Acc 63.3850)\n",
            "EPOCH: 75 train Results: Acc 63.385 Loss: 1.0489\n",
            "Epoch: [75][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1751 (Avg-Loss 1.1751)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [75][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2411 (Avg-Loss 1.2285)\tAcc 51.8382 (Avg-Acc 56.8200)\n",
            "EPOCH: 75 Validation Results: Acc 56.820 Loss: 1.2285\n",
            "Epoch: [76][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0321 (Avg-Loss 1.0321)\tAcc 66.4062 (Avg-Acc 66.4062)\n",
            "Epoch: [76][19/78]\tTime 0.071 (Avg-Time 0.050)\t Loss 1.0502 (Avg-Loss 1.0290)\tAcc 64.2578 (Avg-Acc 64.6973)\n",
            "Epoch: [76][38/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0443 (Avg-Loss 1.0330)\tAcc 61.1328 (Avg-Acc 64.2829)\n",
            "Epoch: [76][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0697 (Avg-Loss 1.0407)\tAcc 61.9141 (Avg-Acc 63.7460)\n",
            "Epoch: [76][76/78]\tTime 0.078 (Avg-Time 0.069)\t Loss 1.0092 (Avg-Loss 1.0479)\tAcc 64.8438 (Avg-Acc 63.4106)\n",
            "Epoch: [76][78/78]\tTime 0.016 (Avg-Time 0.069)\t Loss 1.2830 (Avg-Loss 1.0492)\tAcc 57.8125 (Avg-Acc 63.3400)\n",
            "EPOCH: 76 train Results: Acc 63.340 Loss: 1.0492\n",
            "Epoch: [76][0/19]\tTime 0.023 (Avg-Time 0.023)\t Loss 1.2067 (Avg-Loss 1.2067)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [76][19/19]\tTime 0.015 (Avg-Time 0.029)\t Loss 1.2622 (Avg-Loss 1.2258)\tAcc 54.0441 (Avg-Acc 56.8200)\n",
            "EPOCH: 76 Validation Results: Acc 56.820 Loss: 1.2258\n",
            "Epoch: [77][0/78]\tTime 0.130 (Avg-Time 0.130)\t Loss 1.0298 (Avg-Loss 1.0298)\tAcc 64.2578 (Avg-Acc 64.2578)\n",
            "Epoch: [77][19/78]\tTime 0.047 (Avg-Time 0.060)\t Loss 0.9942 (Avg-Loss 1.0251)\tAcc 66.0156 (Avg-Acc 64.1406)\n",
            "Epoch: [77][38/78]\tTime 0.048 (Avg-Time 0.057)\t Loss 1.0372 (Avg-Loss 1.0300)\tAcc 63.2812 (Avg-Acc 63.7270)\n",
            "Epoch: [77][57/78]\tTime 0.048 (Avg-Time 0.055)\t Loss 1.0703 (Avg-Loss 1.0410)\tAcc 61.3281 (Avg-Acc 63.5506)\n",
            "Epoch: [77][76/78]\tTime 0.048 (Avg-Time 0.054)\t Loss 1.0755 (Avg-Loss 1.0516)\tAcc 64.0625 (Avg-Acc 63.0986)\n",
            "Epoch: [77][78/78]\tTime 0.010 (Avg-Time 0.053)\t Loss 0.9729 (Avg-Loss 1.0509)\tAcc 71.8750 (Avg-Acc 63.1100)\n",
            "EPOCH: 77 train Results: Acc 63.110 Loss: 1.0509\n",
            "Epoch: [77][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1958 (Avg-Loss 1.1958)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [77][19/19]\tTime 0.007 (Avg-Time 0.015)\t Loss 1.2825 (Avg-Loss 1.2292)\tAcc 52.5735 (Avg-Acc 56.5400)\n",
            "EPOCH: 77 Validation Results: Acc 56.540 Loss: 1.2292\n",
            "Epoch: [78][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9584 (Avg-Loss 0.9584)\tAcc 66.7969 (Avg-Acc 66.7969)\n",
            "Epoch: [78][19/78]\tTime 0.060 (Avg-Time 0.050)\t Loss 1.0352 (Avg-Loss 1.0153)\tAcc 64.6484 (Avg-Acc 64.8242)\n",
            "Epoch: [78][38/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0552 (Avg-Loss 1.0224)\tAcc 62.3047 (Avg-Acc 64.5082)\n",
            "Epoch: [78][57/78]\tTime 0.053 (Avg-Time 0.050)\t Loss 1.0376 (Avg-Loss 1.0291)\tAcc 63.6719 (Avg-Acc 64.0423)\n",
            "Epoch: [78][76/78]\tTime 0.052 (Avg-Time 0.050)\t Loss 1.0439 (Avg-Loss 1.0421)\tAcc 65.2344 (Avg-Acc 63.5856)\n",
            "Epoch: [78][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.1150 (Avg-Loss 1.0424)\tAcc 65.6250 (Avg-Acc 63.5750)\n",
            "EPOCH: 78 train Results: Acc 63.575 Loss: 1.0424\n",
            "Epoch: [78][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2061 (Avg-Loss 1.2061)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [78][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2847 (Avg-Loss 1.2269)\tAcc 54.4118 (Avg-Acc 56.3200)\n",
            "EPOCH: 78 Validation Results: Acc 56.320 Loss: 1.2269\n",
            "Epoch: [79][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9399 (Avg-Loss 0.9399)\tAcc 68.1641 (Avg-Acc 68.1641)\n",
            "Epoch: [79][19/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 0.9752 (Avg-Loss 1.0192)\tAcc 65.4297 (Avg-Acc 65.1172)\n",
            "Epoch: [79][38/78]\tTime 0.120 (Avg-Time 0.056)\t Loss 1.0285 (Avg-Loss 1.0229)\tAcc 62.1094 (Avg-Acc 64.4181)\n",
            "Epoch: [79][57/78]\tTime 0.083 (Avg-Time 0.082)\t Loss 1.0284 (Avg-Loss 1.0349)\tAcc 61.9141 (Avg-Acc 64.1063)\n",
            "Epoch: [79][76/78]\tTime 0.050 (Avg-Time 0.077)\t Loss 1.0625 (Avg-Loss 1.0423)\tAcc 62.8906 (Avg-Acc 63.8215)\n",
            "Epoch: [79][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 0.9536 (Avg-Loss 1.0437)\tAcc 65.6250 (Avg-Acc 63.7775)\n",
            "EPOCH: 79 train Results: Acc 63.778 Loss: 1.0437\n",
            "Epoch: [79][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1912 (Avg-Loss 1.1912)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [79][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2502 (Avg-Loss 1.2302)\tAcc 53.6765 (Avg-Acc 56.3200)\n",
            "EPOCH: 79 Validation Results: Acc 56.320 Loss: 1.2302\n",
            "Epoch: [80][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9595 (Avg-Loss 0.9595)\tAcc 67.3828 (Avg-Acc 67.3828)\n",
            "Epoch: [80][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9735 (Avg-Loss 0.9969)\tAcc 65.2344 (Avg-Acc 65.3613)\n",
            "Epoch: [80][38/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 1.0484 (Avg-Loss 1.0098)\tAcc 66.7969 (Avg-Acc 64.9038)\n",
            "Epoch: [80][57/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.1021 (Avg-Loss 1.0251)\tAcc 60.5469 (Avg-Acc 64.3050)\n",
            "Epoch: [80][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1167 (Avg-Loss 1.0408)\tAcc 61.3281 (Avg-Acc 63.6161)\n",
            "Epoch: [80][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0866 (Avg-Loss 1.0412)\tAcc 62.5000 (Avg-Acc 63.6050)\n",
            "EPOCH: 80 train Results: Acc 63.605 Loss: 1.0412\n",
            "Epoch: [80][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2046 (Avg-Loss 1.2046)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [80][19/19]\tTime 0.022 (Avg-Time 0.014)\t Loss 1.3008 (Avg-Loss 1.2341)\tAcc 50.7353 (Avg-Acc 56.5100)\n",
            "EPOCH: 80 Validation Results: Acc 56.510 Loss: 1.2341\n",
            "Epoch: [81][0/78]\tTime 0.054 (Avg-Time 0.054)\t Loss 1.0776 (Avg-Loss 1.0776)\tAcc 62.8906 (Avg-Acc 62.8906)\n",
            "Epoch: [81][19/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 0.9978 (Avg-Loss 1.0152)\tAcc 65.8203 (Avg-Acc 64.5703)\n",
            "Epoch: [81][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0338 (Avg-Loss 1.0176)\tAcc 63.8672 (Avg-Acc 64.4030)\n",
            "Epoch: [81][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0885 (Avg-Loss 1.0302)\tAcc 63.2812 (Avg-Acc 63.8436)\n",
            "Epoch: [81][76/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 1.1392 (Avg-Loss 1.0415)\tAcc 60.1562 (Avg-Acc 63.4334)\n",
            "Epoch: [81][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.7825 (Avg-Loss 1.0429)\tAcc 75.0000 (Avg-Acc 63.3800)\n",
            "EPOCH: 81 train Results: Acc 63.380 Loss: 1.0429\n",
            "Epoch: [81][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1676 (Avg-Loss 1.1676)\tAcc 60.7422 (Avg-Acc 60.7422)\n",
            "Epoch: [81][19/19]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2717 (Avg-Loss 1.2274)\tAcc 52.5735 (Avg-Acc 56.9600)\n",
            "EPOCH: 81 Validation Results: Acc 56.960 Loss: 1.2274\n",
            "Epoch: [82][0/78]\tTime 0.054 (Avg-Time 0.054)\t Loss 0.9439 (Avg-Loss 0.9439)\tAcc 66.9922 (Avg-Acc 66.9922)\n",
            "Epoch: [82][19/78]\tTime 0.173 (Avg-Time 0.080)\t Loss 1.0373 (Avg-Loss 1.0050)\tAcc 63.4766 (Avg-Acc 64.9512)\n",
            "Epoch: [82][38/78]\tTime 0.050 (Avg-Time 0.102)\t Loss 1.0246 (Avg-Loss 1.0172)\tAcc 62.8906 (Avg-Acc 64.5483)\n",
            "Epoch: [82][57/78]\tTime 0.051 (Avg-Time 0.085)\t Loss 1.1422 (Avg-Loss 1.0264)\tAcc 60.5469 (Avg-Acc 64.3588)\n",
            "Epoch: [82][76/78]\tTime 0.047 (Avg-Time 0.077)\t Loss 1.0857 (Avg-Loss 1.0360)\tAcc 60.1562 (Avg-Acc 63.9407)\n",
            "Epoch: [82][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.0790 (Avg-Loss 1.0362)\tAcc 59.3750 (Avg-Acc 63.9400)\n",
            "EPOCH: 82 train Results: Acc 63.940 Loss: 1.0362\n",
            "Epoch: [82][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1911 (Avg-Loss 1.1911)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [82][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2531 (Avg-Loss 1.2234)\tAcc 54.4118 (Avg-Acc 56.8000)\n",
            "EPOCH: 82 Validation Results: Acc 56.800 Loss: 1.2234\n",
            "Epoch: [83][0/78]\tTime 0.053 (Avg-Time 0.053)\t Loss 0.9270 (Avg-Loss 0.9270)\tAcc 66.9922 (Avg-Acc 66.9922)\n",
            "Epoch: [83][19/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0480 (Avg-Loss 1.0202)\tAcc 63.4766 (Avg-Acc 64.1504)\n",
            "Epoch: [83][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0497 (Avg-Loss 1.0213)\tAcc 62.3047 (Avg-Acc 64.1827)\n",
            "Epoch: [83][57/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.0468 (Avg-Loss 1.0246)\tAcc 64.2578 (Avg-Acc 64.2039)\n",
            "Epoch: [83][76/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.1027 (Avg-Loss 1.0381)\tAcc 58.5938 (Avg-Acc 63.5653)\n",
            "Epoch: [83][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.0423 (Avg-Loss 1.0385)\tAcc 67.1875 (Avg-Acc 63.5625)\n",
            "EPOCH: 83 train Results: Acc 63.562 Loss: 1.0385\n",
            "Epoch: [83][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2116 (Avg-Loss 1.2116)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [83][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2736 (Avg-Loss 1.2360)\tAcc 55.5147 (Avg-Acc 56.4300)\n",
            "EPOCH: 83 Validation Results: Acc 56.430 Loss: 1.2360\n",
            "Epoch: [84][0/78]\tTime 0.057 (Avg-Time 0.057)\t Loss 0.8823 (Avg-Loss 0.8823)\tAcc 68.9453 (Avg-Acc 68.9453)\n",
            "Epoch: [84][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0079 (Avg-Loss 0.9930)\tAcc 65.6250 (Avg-Acc 65.8203)\n",
            "Epoch: [84][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1104 (Avg-Loss 1.0102)\tAcc 62.3047 (Avg-Acc 65.0841)\n",
            "Epoch: [84][57/78]\tTime 0.054 (Avg-Time 0.050)\t Loss 1.1013 (Avg-Loss 1.0254)\tAcc 59.3750 (Avg-Acc 64.3218)\n",
            "Epoch: [84][76/78]\tTime 0.293 (Avg-Time 0.058)\t Loss 1.1636 (Avg-Loss 1.0390)\tAcc 59.1797 (Avg-Acc 63.7784)\n",
            "Epoch: [84][78/78]\tTime 0.055 (Avg-Time 0.061)\t Loss 1.1212 (Avg-Loss 1.0391)\tAcc 64.0625 (Avg-Acc 63.7675)\n",
            "EPOCH: 84 train Results: Acc 63.767 Loss: 1.0391\n",
            "Epoch: [84][0/19]\tTime 0.029 (Avg-Time 0.029)\t Loss 1.1947 (Avg-Loss 1.1947)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [84][19/19]\tTime 0.018 (Avg-Time 0.046)\t Loss 1.2399 (Avg-Loss 1.2235)\tAcc 55.5147 (Avg-Acc 56.3200)\n",
            "EPOCH: 84 Validation Results: Acc 56.320 Loss: 1.2235\n",
            "Epoch: [85][0/78]\tTime 0.102 (Avg-Time 0.102)\t Loss 0.9266 (Avg-Loss 0.9266)\tAcc 68.7500 (Avg-Acc 68.7500)\n",
            "Epoch: [85][19/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 0.9851 (Avg-Loss 1.0067)\tAcc 65.2344 (Avg-Acc 65.2539)\n",
            "Epoch: [85][38/78]\tTime 0.048 (Avg-Time 0.063)\t Loss 1.0184 (Avg-Loss 1.0152)\tAcc 65.6250 (Avg-Acc 64.6735)\n",
            "Epoch: [85][57/78]\tTime 0.048 (Avg-Time 0.059)\t Loss 1.0680 (Avg-Loss 1.0263)\tAcc 64.0625 (Avg-Acc 64.1837)\n",
            "Epoch: [85][76/78]\tTime 0.048 (Avg-Time 0.057)\t Loss 1.0227 (Avg-Loss 1.0350)\tAcc 64.0625 (Avg-Acc 63.7505)\n",
            "Epoch: [85][78/78]\tTime 0.009 (Avg-Time 0.056)\t Loss 1.2615 (Avg-Loss 1.0359)\tAcc 54.6875 (Avg-Acc 63.7125)\n",
            "EPOCH: 85 train Results: Acc 63.712 Loss: 1.0359\n",
            "Epoch: [85][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1962 (Avg-Loss 1.1962)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [85][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2717 (Avg-Loss 1.2231)\tAcc 52.9412 (Avg-Acc 56.5200)\n",
            "EPOCH: 85 Validation Results: Acc 56.520 Loss: 1.2231\n",
            "Epoch: [86][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9776 (Avg-Loss 0.9776)\tAcc 66.6016 (Avg-Acc 66.6016)\n",
            "Epoch: [86][19/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0198 (Avg-Loss 1.0095)\tAcc 64.8438 (Avg-Acc 65.2344)\n",
            "Epoch: [86][38/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.9826 (Avg-Loss 1.0224)\tAcc 66.0156 (Avg-Acc 64.6635)\n",
            "Epoch: [86][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1061 (Avg-Loss 1.0302)\tAcc 60.3516 (Avg-Acc 64.2881)\n",
            "Epoch: [86][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1018 (Avg-Loss 1.0399)\tAcc 60.3516 (Avg-Acc 63.8342)\n",
            "Epoch: [86][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.0279 (Avg-Loss 1.0404)\tAcc 60.9375 (Avg-Acc 63.7950)\n",
            "EPOCH: 86 train Results: Acc 63.795 Loss: 1.0404\n",
            "Epoch: [86][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2336 (Avg-Loss 1.2336)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [86][19/19]\tTime 0.008 (Avg-Time 0.013)\t Loss 1.2735 (Avg-Loss 1.2248)\tAcc 52.9412 (Avg-Acc 56.9100)\n",
            "EPOCH: 86 Validation Results: Acc 56.910 Loss: 1.2248\n",
            "Epoch: [87][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.9839 (Avg-Loss 0.9839)\tAcc 65.6250 (Avg-Acc 65.6250)\n",
            "Epoch: [87][19/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 0.9843 (Avg-Loss 1.0033)\tAcc 65.0391 (Avg-Acc 65.1367)\n",
            "Epoch: [87][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0207 (Avg-Loss 1.0147)\tAcc 64.0625 (Avg-Acc 64.5583)\n",
            "Epoch: [87][57/78]\tTime 0.105 (Avg-Time 0.078)\t Loss 1.0544 (Avg-Loss 1.0273)\tAcc 62.6953 (Avg-Acc 64.1029)\n",
            "Epoch: [87][76/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 1.0517 (Avg-Loss 1.0364)\tAcc 64.2578 (Avg-Acc 63.7784)\n",
            "Epoch: [87][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.1083 (Avg-Loss 1.0371)\tAcc 65.6250 (Avg-Acc 63.7525)\n",
            "EPOCH: 87 train Results: Acc 63.752 Loss: 1.0371\n",
            "Epoch: [87][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1804 (Avg-Loss 1.1804)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [87][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2760 (Avg-Loss 1.2191)\tAcc 54.4118 (Avg-Acc 57.4300)\n",
            "EPOCH: 87 Validation Results: Acc 57.430 Loss: 1.2191\n",
            "Epoch: [88][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9200 (Avg-Loss 0.9200)\tAcc 68.5547 (Avg-Acc 68.5547)\n",
            "Epoch: [88][19/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0201 (Avg-Loss 0.9972)\tAcc 61.5234 (Avg-Acc 64.9609)\n",
            "Epoch: [88][38/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.0395 (Avg-Loss 1.0106)\tAcc 63.4766 (Avg-Acc 64.6835)\n",
            "Epoch: [88][57/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0438 (Avg-Loss 1.0215)\tAcc 64.0625 (Avg-Acc 64.3959)\n",
            "Epoch: [88][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0406 (Avg-Loss 1.0330)\tAcc 63.2812 (Avg-Acc 64.0371)\n",
            "Epoch: [88][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.3164 (Avg-Loss 1.0344)\tAcc 48.4375 (Avg-Acc 63.9750)\n",
            "EPOCH: 88 train Results: Acc 63.975 Loss: 1.0344\n",
            "Epoch: [88][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1900 (Avg-Loss 1.1900)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [88][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2569 (Avg-Loss 1.2340)\tAcc 58.0882 (Avg-Acc 56.2700)\n",
            "EPOCH: 88 Validation Results: Acc 56.270 Loss: 1.2340\n",
            "Epoch: [89][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9256 (Avg-Loss 0.9256)\tAcc 67.1875 (Avg-Acc 67.1875)\n",
            "Epoch: [89][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0156 (Avg-Loss 1.0139)\tAcc 64.6484 (Avg-Acc 64.1992)\n",
            "Epoch: [89][38/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 1.0463 (Avg-Loss 1.0232)\tAcc 61.9141 (Avg-Acc 64.0976)\n",
            "Epoch: [89][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1087 (Avg-Loss 1.0278)\tAcc 61.7188 (Avg-Acc 63.9244)\n",
            "Epoch: [89][76/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.0454 (Avg-Loss 1.0409)\tAcc 62.8906 (Avg-Acc 63.5070)\n",
            "Epoch: [89][78/78]\tTime 0.015 (Avg-Time 0.050)\t Loss 1.1541 (Avg-Loss 1.0412)\tAcc 53.1250 (Avg-Acc 63.5125)\n",
            "EPOCH: 89 train Results: Acc 63.513 Loss: 1.0412\n",
            "Epoch: [89][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2031 (Avg-Loss 1.2031)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [89][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2944 (Avg-Loss 1.2249)\tAcc 52.9412 (Avg-Acc 56.7700)\n",
            "EPOCH: 89 Validation Results: Acc 56.770 Loss: 1.2249\n",
            "Epoch: [90][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0323 (Avg-Loss 1.0323)\tAcc 64.4531 (Avg-Acc 64.4531)\n",
            "Epoch: [90][19/78]\tTime 0.235 (Avg-Time 0.079)\t Loss 0.9689 (Avg-Loss 1.0252)\tAcc 64.4531 (Avg-Acc 64.2285)\n",
            "Epoch: [90][38/78]\tTime 0.048 (Avg-Time 0.102)\t Loss 1.0566 (Avg-Loss 1.0198)\tAcc 62.1094 (Avg-Acc 64.5683)\n",
            "Epoch: [90][57/78]\tTime 0.050 (Avg-Time 0.085)\t Loss 1.0423 (Avg-Loss 1.0260)\tAcc 65.2344 (Avg-Acc 64.2578)\n",
            "Epoch: [90][76/78]\tTime 0.048 (Avg-Time 0.076)\t Loss 1.0743 (Avg-Loss 1.0348)\tAcc 61.9141 (Avg-Acc 63.9331)\n",
            "Epoch: [90][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.0455 (Avg-Loss 1.0349)\tAcc 65.6250 (Avg-Acc 63.9575)\n",
            "EPOCH: 90 train Results: Acc 63.958 Loss: 1.0349\n",
            "Epoch: [90][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1764 (Avg-Loss 1.1764)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [90][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3017 (Avg-Loss 1.2217)\tAcc 55.1471 (Avg-Acc 57.0200)\n",
            "EPOCH: 90 Validation Results: Acc 57.020 Loss: 1.2217\n",
            "Epoch: [91][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.9258 (Avg-Loss 0.9258)\tAcc 65.2344 (Avg-Acc 65.2344)\n",
            "Epoch: [91][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9499 (Avg-Loss 0.9952)\tAcc 67.3828 (Avg-Acc 65.1855)\n",
            "Epoch: [91][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0524 (Avg-Loss 1.0077)\tAcc 64.4531 (Avg-Acc 64.9089)\n",
            "Epoch: [91][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0458 (Avg-Loss 1.0236)\tAcc 65.8203 (Avg-Acc 64.5440)\n",
            "Epoch: [91][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0354 (Avg-Loss 1.0328)\tAcc 64.4531 (Avg-Acc 64.1766)\n",
            "Epoch: [91][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1070 (Avg-Loss 1.0327)\tAcc 68.7500 (Avg-Acc 64.1950)\n",
            "EPOCH: 91 train Results: Acc 64.195 Loss: 1.0327\n",
            "Epoch: [91][0/19]\tTime 0.011 (Avg-Time 0.011)\t Loss 1.1831 (Avg-Loss 1.1831)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [91][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2982 (Avg-Loss 1.2304)\tAcc 54.4118 (Avg-Acc 56.4600)\n",
            "EPOCH: 91 Validation Results: Acc 56.460 Loss: 1.2304\n",
            "Epoch: [92][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9241 (Avg-Loss 0.9241)\tAcc 67.1875 (Avg-Acc 67.1875)\n",
            "Epoch: [92][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9509 (Avg-Loss 0.9999)\tAcc 65.2344 (Avg-Acc 65.1074)\n",
            "Epoch: [92][38/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 0.9746 (Avg-Loss 1.0178)\tAcc 67.1875 (Avg-Acc 64.4932)\n",
            "Epoch: [92][57/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.1128 (Avg-Loss 1.0239)\tAcc 61.9141 (Avg-Acc 64.3790)\n",
            "Epoch: [92][76/78]\tTime 0.087 (Avg-Time 0.062)\t Loss 1.1020 (Avg-Loss 1.0344)\tAcc 60.3516 (Avg-Acc 63.8748)\n",
            "Epoch: [92][78/78]\tTime 0.182 (Avg-Time 0.066)\t Loss 1.0677 (Avg-Loss 1.0344)\tAcc 57.8125 (Avg-Acc 63.8575)\n",
            "EPOCH: 92 train Results: Acc 63.858 Loss: 1.0344\n",
            "Epoch: [92][0/19]\tTime 0.070 (Avg-Time 0.070)\t Loss 1.1621 (Avg-Loss 1.1621)\tAcc 59.9609 (Avg-Acc 59.9609)\n",
            "Epoch: [92][19/19]\tTime 0.021 (Avg-Time 0.032)\t Loss 1.2791 (Avg-Loss 1.2216)\tAcc 54.0441 (Avg-Acc 56.8600)\n",
            "EPOCH: 92 Validation Results: Acc 56.860 Loss: 1.2216\n",
            "Epoch: [93][0/78]\tTime 0.135 (Avg-Time 0.135)\t Loss 0.9291 (Avg-Loss 0.9291)\tAcc 68.5547 (Avg-Acc 68.5547)\n",
            "Epoch: [93][19/78]\tTime 0.049 (Avg-Time 0.069)\t Loss 1.0444 (Avg-Loss 0.9818)\tAcc 65.2344 (Avg-Acc 66.6211)\n",
            "Epoch: [93][38/78]\tTime 0.050 (Avg-Time 0.060)\t Loss 1.0612 (Avg-Loss 1.0016)\tAcc 62.5000 (Avg-Acc 65.3145)\n",
            "Epoch: [93][57/78]\tTime 0.049 (Avg-Time 0.057)\t Loss 1.0178 (Avg-Loss 1.0114)\tAcc 66.4062 (Avg-Acc 64.9650)\n",
            "Epoch: [93][76/78]\tTime 0.048 (Avg-Time 0.055)\t Loss 1.0886 (Avg-Loss 1.0299)\tAcc 61.7188 (Avg-Acc 64.1766)\n",
            "Epoch: [93][78/78]\tTime 0.009 (Avg-Time 0.054)\t Loss 1.0219 (Avg-Loss 1.0307)\tAcc 60.9375 (Avg-Acc 64.1350)\n",
            "EPOCH: 93 train Results: Acc 64.135 Loss: 1.0307\n",
            "Epoch: [93][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1786 (Avg-Loss 1.1786)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [93][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3078 (Avg-Loss 1.2276)\tAcc 51.4706 (Avg-Acc 56.7400)\n",
            "EPOCH: 93 Validation Results: Acc 56.740 Loss: 1.2276\n",
            "Epoch: [94][0/78]\tTime 0.053 (Avg-Time 0.053)\t Loss 0.9202 (Avg-Loss 0.9202)\tAcc 67.5781 (Avg-Acc 67.5781)\n",
            "Epoch: [94][19/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9571 (Avg-Loss 0.9840)\tAcc 69.9219 (Avg-Acc 65.9277)\n",
            "Epoch: [94][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9551 (Avg-Loss 0.9894)\tAcc 67.7734 (Avg-Acc 65.8804)\n",
            "Epoch: [94][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9688 (Avg-Loss 1.0048)\tAcc 65.8203 (Avg-Acc 65.2007)\n",
            "Epoch: [94][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1563 (Avg-Loss 1.0242)\tAcc 59.1797 (Avg-Acc 64.6434)\n",
            "Epoch: [94][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.2419 (Avg-Loss 1.0255)\tAcc 56.2500 (Avg-Acc 64.5900)\n",
            "EPOCH: 94 train Results: Acc 64.590 Loss: 1.0255\n",
            "Epoch: [94][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1971 (Avg-Loss 1.1971)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [94][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2774 (Avg-Loss 1.2296)\tAcc 54.0441 (Avg-Acc 56.5500)\n",
            "EPOCH: 94 Validation Results: Acc 56.550 Loss: 1.2296\n",
            "Epoch: [95][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.8975 (Avg-Loss 0.8975)\tAcc 70.7031 (Avg-Acc 70.7031)\n",
            "Epoch: [95][19/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.0878 (Avg-Loss 1.0063)\tAcc 61.9141 (Avg-Acc 64.7754)\n",
            "Epoch: [95][38/78]\tTime 0.083 (Avg-Time 0.054)\t Loss 1.0166 (Avg-Loss 1.0157)\tAcc 64.2578 (Avg-Acc 64.5483)\n",
            "Epoch: [95][57/78]\tTime 0.084 (Avg-Time 0.086)\t Loss 0.9889 (Avg-Loss 1.0233)\tAcc 64.6484 (Avg-Acc 64.3016)\n",
            "Epoch: [95][76/78]\tTime 0.047 (Avg-Time 0.078)\t Loss 1.0474 (Avg-Loss 1.0312)\tAcc 62.6953 (Avg-Acc 63.9788)\n",
            "Epoch: [95][78/78]\tTime 0.009 (Avg-Time 0.077)\t Loss 1.1318 (Avg-Loss 1.0329)\tAcc 67.1875 (Avg-Acc 63.9150)\n",
            "EPOCH: 95 train Results: Acc 63.915 Loss: 1.0329\n",
            "Epoch: [95][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2111 (Avg-Loss 1.2111)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [95][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3174 (Avg-Loss 1.2307)\tAcc 51.4706 (Avg-Acc 56.3600)\n",
            "EPOCH: 95 Validation Results: Acc 56.360 Loss: 1.2307\n",
            "Epoch: [96][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9740 (Avg-Loss 0.9740)\tAcc 66.4062 (Avg-Acc 66.4062)\n",
            "Epoch: [96][19/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.0044 (Avg-Loss 0.9951)\tAcc 65.6250 (Avg-Acc 65.3223)\n",
            "Epoch: [96][38/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.0206 (Avg-Loss 1.0062)\tAcc 64.2578 (Avg-Acc 64.8087)\n",
            "Epoch: [96][57/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.1748 (Avg-Loss 1.0207)\tAcc 56.2500 (Avg-Acc 64.3689)\n",
            "Epoch: [96][76/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.0553 (Avg-Loss 1.0297)\tAcc 61.9141 (Avg-Acc 63.9230)\n",
            "Epoch: [96][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.1966 (Avg-Loss 1.0306)\tAcc 51.5625 (Avg-Acc 63.8500)\n",
            "EPOCH: 96 train Results: Acc 63.850 Loss: 1.0306\n",
            "Epoch: [96][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2078 (Avg-Loss 1.2078)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [96][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2792 (Avg-Loss 1.2327)\tAcc 54.4118 (Avg-Acc 56.5200)\n",
            "EPOCH: 96 Validation Results: Acc 56.520 Loss: 1.2327\n",
            "Epoch: [97][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9548 (Avg-Loss 0.9548)\tAcc 68.9453 (Avg-Acc 68.9453)\n",
            "Epoch: [97][19/78]\tTime 0.049 (Avg-Time 0.054)\t Loss 1.0154 (Avg-Loss 1.0006)\tAcc 65.0391 (Avg-Acc 65.0098)\n",
            "Epoch: [97][38/78]\tTime 0.051 (Avg-Time 0.053)\t Loss 1.0190 (Avg-Loss 1.0212)\tAcc 60.9375 (Avg-Acc 64.1827)\n",
            "Epoch: [97][57/78]\tTime 0.048 (Avg-Time 0.052)\t Loss 1.0142 (Avg-Loss 1.0283)\tAcc 66.2109 (Avg-Acc 63.8504)\n",
            "Epoch: [97][76/78]\tTime 0.048 (Avg-Time 0.052)\t Loss 1.1136 (Avg-Loss 1.0355)\tAcc 59.7656 (Avg-Acc 63.6693)\n",
            "Epoch: [97][78/78]\tTime 0.009 (Avg-Time 0.051)\t Loss 1.0101 (Avg-Loss 1.0349)\tAcc 64.0625 (Avg-Acc 63.6700)\n",
            "EPOCH: 97 train Results: Acc 63.670 Loss: 1.0349\n",
            "Epoch: [97][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1890 (Avg-Loss 1.1890)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [97][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3138 (Avg-Loss 1.2302)\tAcc 51.4706 (Avg-Acc 56.8100)\n",
            "EPOCH: 97 Validation Results: Acc 56.810 Loss: 1.2302\n",
            "Epoch: [98][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9895 (Avg-Loss 0.9895)\tAcc 64.0625 (Avg-Acc 64.0625)\n",
            "Epoch: [98][19/78]\tTime 0.154 (Avg-Time 0.136)\t Loss 0.9231 (Avg-Loss 0.9923)\tAcc 68.1641 (Avg-Acc 65.9570)\n",
            "Epoch: [98][38/78]\tTime 0.049 (Avg-Time 0.102)\t Loss 1.0108 (Avg-Loss 1.0033)\tAcc 63.4766 (Avg-Acc 65.3145)\n",
            "Epoch: [98][57/78]\tTime 0.047 (Avg-Time 0.085)\t Loss 1.0654 (Avg-Loss 1.0155)\tAcc 64.4531 (Avg-Acc 64.7461)\n",
            "Epoch: [98][76/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 1.0392 (Avg-Loss 1.0249)\tAcc 62.5000 (Avg-Acc 64.4024)\n",
            "Epoch: [98][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 0.9210 (Avg-Loss 1.0265)\tAcc 60.9375 (Avg-Acc 64.3075)\n",
            "EPOCH: 98 train Results: Acc 64.308 Loss: 1.0265\n",
            "Epoch: [98][0/19]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.1873 (Avg-Loss 1.1873)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [98][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2670 (Avg-Loss 1.2282)\tAcc 56.6176 (Avg-Acc 56.9100)\n",
            "EPOCH: 98 Validation Results: Acc 56.910 Loss: 1.2282\n",
            "Epoch: [99][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9732 (Avg-Loss 0.9732)\tAcc 65.6250 (Avg-Acc 65.6250)\n",
            "Epoch: [99][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9570 (Avg-Loss 0.9824)\tAcc 68.3594 (Avg-Acc 66.2402)\n",
            "Epoch: [99][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0369 (Avg-Loss 1.0035)\tAcc 64.8438 (Avg-Acc 64.9990)\n",
            "Epoch: [99][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0564 (Avg-Loss 1.0163)\tAcc 64.6484 (Avg-Acc 64.6215)\n",
            "Epoch: [99][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1399 (Avg-Loss 1.0286)\tAcc 59.7656 (Avg-Acc 63.9889)\n",
            "Epoch: [99][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.0105 (Avg-Loss 1.0290)\tAcc 68.7500 (Avg-Acc 63.9800)\n",
            "EPOCH: 99 train Results: Acc 63.980 Loss: 1.0290\n",
            "Epoch: [99][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1654 (Avg-Loss 1.1654)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [99][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2761 (Avg-Loss 1.2234)\tAcc 52.5735 (Avg-Acc 56.8400)\n",
            "EPOCH: 99 Validation Results: Acc 56.840 Loss: 1.2234\n",
            "Epoch: [100][0/78]\tTime 0.053 (Avg-Time 0.053)\t Loss 0.9886 (Avg-Loss 0.9886)\tAcc 65.0391 (Avg-Acc 65.0391)\n",
            "Epoch: [100][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9738 (Avg-Loss 0.9947)\tAcc 65.6250 (Avg-Acc 65.6738)\n",
            "Epoch: [100][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0447 (Avg-Loss 1.0019)\tAcc 60.9375 (Avg-Acc 65.2143)\n",
            "Epoch: [100][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9892 (Avg-Loss 1.0141)\tAcc 63.6719 (Avg-Acc 64.7495)\n",
            "Epoch: [100][76/78]\tTime 0.105 (Avg-Time 0.070)\t Loss 1.1035 (Avg-Loss 1.0236)\tAcc 61.3281 (Avg-Acc 64.4582)\n",
            "Epoch: [100][78/78]\tTime 0.022 (Avg-Time 0.070)\t Loss 1.1592 (Avg-Loss 1.0244)\tAcc 56.2500 (Avg-Acc 64.3975)\n",
            "EPOCH: 100 train Results: Acc 64.397 Loss: 1.0244\n",
            "Epoch: [100][0/19]\tTime 0.026 (Avg-Time 0.026)\t Loss 1.1736 (Avg-Loss 1.1736)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [100][19/19]\tTime 0.022 (Avg-Time 0.026)\t Loss 1.2913 (Avg-Loss 1.2268)\tAcc 51.8382 (Avg-Acc 56.5400)\n",
            "EPOCH: 100 Validation Results: Acc 56.540 Loss: 1.2268\n",
            "Epoch: [101][0/78]\tTime 0.166 (Avg-Time 0.166)\t Loss 0.9680 (Avg-Loss 0.9680)\tAcc 65.0391 (Avg-Acc 65.0391)\n",
            "Epoch: [101][19/78]\tTime 0.048 (Avg-Time 0.057)\t Loss 0.9571 (Avg-Loss 0.9874)\tAcc 67.5781 (Avg-Acc 65.5566)\n",
            "Epoch: [101][38/78]\tTime 0.049 (Avg-Time 0.053)\t Loss 1.0460 (Avg-Loss 0.9942)\tAcc 64.0625 (Avg-Acc 65.4247)\n",
            "Epoch: [101][57/78]\tTime 0.051 (Avg-Time 0.053)\t Loss 1.0740 (Avg-Loss 1.0063)\tAcc 62.6953 (Avg-Acc 64.8202)\n",
            "Epoch: [101][76/78]\tTime 0.048 (Avg-Time 0.052)\t Loss 1.0640 (Avg-Loss 1.0241)\tAcc 61.1328 (Avg-Acc 64.1234)\n",
            "Epoch: [101][78/78]\tTime 0.009 (Avg-Time 0.052)\t Loss 1.0000 (Avg-Loss 1.0252)\tAcc 64.0625 (Avg-Acc 64.0775)\n",
            "EPOCH: 101 train Results: Acc 64.078 Loss: 1.0252\n",
            "Epoch: [101][0/19]\tTime 0.011 (Avg-Time 0.011)\t Loss 1.1521 (Avg-Loss 1.1521)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [101][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2797 (Avg-Loss 1.2282)\tAcc 53.6765 (Avg-Acc 56.8800)\n",
            "EPOCH: 101 Validation Results: Acc 56.880 Loss: 1.2282\n",
            "Epoch: [102][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9920 (Avg-Loss 0.9920)\tAcc 66.2109 (Avg-Acc 66.2109)\n",
            "Epoch: [102][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9971 (Avg-Loss 0.9932)\tAcc 63.6719 (Avg-Acc 65.0684)\n",
            "Epoch: [102][38/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0847 (Avg-Loss 1.0005)\tAcc 61.9141 (Avg-Acc 64.8838)\n",
            "Epoch: [102][57/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0759 (Avg-Loss 1.0104)\tAcc 60.9375 (Avg-Acc 64.7057)\n",
            "Epoch: [102][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0664 (Avg-Loss 1.0234)\tAcc 62.5000 (Avg-Acc 64.1893)\n",
            "Epoch: [102][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.2482 (Avg-Loss 1.0248)\tAcc 53.1250 (Avg-Acc 64.1500)\n",
            "EPOCH: 102 train Results: Acc 64.150 Loss: 1.0248\n",
            "Epoch: [102][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1805 (Avg-Loss 1.1805)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [102][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3172 (Avg-Loss 1.2293)\tAcc 51.1029 (Avg-Acc 56.8000)\n",
            "EPOCH: 102 Validation Results: Acc 56.800 Loss: 1.2293\n",
            "Epoch: [103][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.9617 (Avg-Loss 0.9617)\tAcc 66.9922 (Avg-Acc 66.9922)\n",
            "Epoch: [103][19/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.0500 (Avg-Loss 1.0011)\tAcc 61.7188 (Avg-Acc 64.7754)\n",
            "Epoch: [103][38/78]\tTime 0.205 (Avg-Time 0.061)\t Loss 1.0337 (Avg-Loss 1.0094)\tAcc 63.6719 (Avg-Acc 64.8788)\n",
            "Epoch: [103][57/78]\tTime 0.047 (Avg-Time 0.084)\t Loss 1.0012 (Avg-Loss 1.0123)\tAcc 64.2578 (Avg-Acc 64.7091)\n",
            "Epoch: [103][76/78]\tTime 0.048 (Avg-Time 0.076)\t Loss 1.0819 (Avg-Loss 1.0262)\tAcc 63.0859 (Avg-Acc 64.0219)\n",
            "Epoch: [103][78/78]\tTime 0.010 (Avg-Time 0.075)\t Loss 1.2365 (Avg-Loss 1.0267)\tAcc 54.6875 (Avg-Acc 63.9850)\n",
            "EPOCH: 103 train Results: Acc 63.985 Loss: 1.0267\n",
            "Epoch: [103][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1865 (Avg-Loss 1.1865)\tAcc 60.1562 (Avg-Acc 60.1562)\n",
            "Epoch: [103][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3037 (Avg-Loss 1.2234)\tAcc 54.0441 (Avg-Acc 57.0900)\n",
            "EPOCH: 103 Validation Results: Acc 57.090 Loss: 1.2234\n",
            "Epoch: [104][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9393 (Avg-Loss 0.9393)\tAcc 66.7969 (Avg-Acc 66.7969)\n",
            "Epoch: [104][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0420 (Avg-Loss 0.9971)\tAcc 64.6484 (Avg-Acc 65.3125)\n",
            "Epoch: [104][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0403 (Avg-Loss 1.0076)\tAcc 63.0859 (Avg-Acc 65.0691)\n",
            "Epoch: [104][57/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0836 (Avg-Loss 1.0161)\tAcc 62.8906 (Avg-Acc 64.7730)\n",
            "Epoch: [104][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1431 (Avg-Loss 1.0271)\tAcc 61.5234 (Avg-Acc 64.2705)\n",
            "Epoch: [104][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.0815 (Avg-Loss 1.0282)\tAcc 67.1875 (Avg-Acc 64.2250)\n",
            "EPOCH: 104 train Results: Acc 64.225 Loss: 1.0282\n",
            "Epoch: [104][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1864 (Avg-Loss 1.1864)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [104][19/19]\tTime 0.006 (Avg-Time 0.013)\t Loss 1.3049 (Avg-Loss 1.2173)\tAcc 52.9412 (Avg-Acc 56.9100)\n",
            "EPOCH: 104 Validation Results: Acc 56.910 Loss: 1.2173\n",
            "Epoch: [105][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9590 (Avg-Loss 0.9590)\tAcc 68.3594 (Avg-Acc 68.3594)\n",
            "Epoch: [105][19/78]\tTime 0.056 (Avg-Time 0.051)\t Loss 0.9933 (Avg-Loss 0.9899)\tAcc 65.4297 (Avg-Acc 66.3086)\n",
            "Epoch: [105][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9494 (Avg-Loss 0.9989)\tAcc 68.1641 (Avg-Acc 65.8353)\n",
            "Epoch: [105][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0965 (Avg-Loss 1.0072)\tAcc 63.2812 (Avg-Acc 65.3051)\n",
            "Epoch: [105][76/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 1.0095 (Avg-Loss 1.0214)\tAcc 62.3047 (Avg-Acc 64.5800)\n",
            "Epoch: [105][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0645 (Avg-Loss 1.0227)\tAcc 59.3750 (Avg-Acc 64.5375)\n",
            "EPOCH: 105 train Results: Acc 64.537 Loss: 1.0227\n",
            "Epoch: [105][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1962 (Avg-Loss 1.1962)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [105][19/19]\tTime 0.006 (Avg-Time 0.013)\t Loss 1.3121 (Avg-Loss 1.2280)\tAcc 52.2059 (Avg-Acc 56.2500)\n",
            "EPOCH: 105 Validation Results: Acc 56.250 Loss: 1.2280\n",
            "Epoch: [106][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9305 (Avg-Loss 0.9305)\tAcc 69.3359 (Avg-Acc 69.3359)\n",
            "Epoch: [106][19/78]\tTime 0.190 (Avg-Time 0.132)\t Loss 1.0622 (Avg-Loss 0.9926)\tAcc 63.0859 (Avg-Acc 65.4590)\n",
            "Epoch: [106][38/78]\tTime 0.047 (Avg-Time 0.105)\t Loss 1.0517 (Avg-Loss 1.0042)\tAcc 63.4766 (Avg-Acc 65.0341)\n",
            "Epoch: [106][57/78]\tTime 0.046 (Avg-Time 0.086)\t Loss 1.0527 (Avg-Loss 1.0142)\tAcc 64.4531 (Avg-Acc 64.6114)\n",
            "Epoch: [106][76/78]\tTime 0.048 (Avg-Time 0.077)\t Loss 1.0665 (Avg-Loss 1.0244)\tAcc 63.0859 (Avg-Acc 64.2147)\n",
            "Epoch: [106][78/78]\tTime 0.009 (Avg-Time 0.076)\t Loss 1.0460 (Avg-Loss 1.0249)\tAcc 65.6250 (Avg-Acc 64.2050)\n",
            "EPOCH: 106 train Results: Acc 64.205 Loss: 1.0249\n",
            "Epoch: [106][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2090 (Avg-Loss 1.2090)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [106][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2890 (Avg-Loss 1.2237)\tAcc 52.9412 (Avg-Acc 56.6700)\n",
            "EPOCH: 106 Validation Results: Acc 56.670 Loss: 1.2237\n",
            "Epoch: [107][0/78]\tTime 0.059 (Avg-Time 0.059)\t Loss 0.9905 (Avg-Loss 0.9905)\tAcc 64.6484 (Avg-Acc 64.6484)\n",
            "Epoch: [107][19/78]\tTime 0.052 (Avg-Time 0.051)\t Loss 1.0154 (Avg-Loss 0.9824)\tAcc 65.2344 (Avg-Acc 66.0840)\n",
            "Epoch: [107][38/78]\tTime 0.046 (Avg-Time 0.051)\t Loss 1.0567 (Avg-Loss 0.9972)\tAcc 64.8438 (Avg-Acc 65.5799)\n",
            "Epoch: [107][57/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0307 (Avg-Loss 1.0106)\tAcc 64.6484 (Avg-Acc 65.0593)\n",
            "Epoch: [107][76/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 1.0720 (Avg-Loss 1.0236)\tAcc 62.6953 (Avg-Acc 64.4988)\n",
            "Epoch: [107][78/78]\tTime 0.010 (Avg-Time 0.050)\t Loss 1.1139 (Avg-Loss 1.0250)\tAcc 56.2500 (Avg-Acc 64.4475)\n",
            "EPOCH: 107 train Results: Acc 64.448 Loss: 1.0250\n",
            "Epoch: [107][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1919 (Avg-Loss 1.1919)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [107][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2685 (Avg-Loss 1.2226)\tAcc 55.1471 (Avg-Acc 57.1000)\n",
            "EPOCH: 107 Validation Results: Acc 57.100 Loss: 1.2226\n",
            "Epoch: [108][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0028 (Avg-Loss 1.0028)\tAcc 63.8672 (Avg-Acc 63.8672)\n",
            "Epoch: [108][19/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 1.0018 (Avg-Loss 1.0013)\tAcc 65.0391 (Avg-Acc 65.4590)\n",
            "Epoch: [108][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9658 (Avg-Loss 1.0042)\tAcc 67.5781 (Avg-Acc 65.3446)\n",
            "Epoch: [108][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9730 (Avg-Loss 1.0100)\tAcc 65.0391 (Avg-Acc 64.9650)\n",
            "Epoch: [108][76/78]\tTime 0.163 (Avg-Time 0.072)\t Loss 1.1636 (Avg-Loss 1.0246)\tAcc 57.0312 (Avg-Acc 64.3263)\n",
            "Epoch: [108][78/78]\tTime 0.029 (Avg-Time 0.072)\t Loss 1.1492 (Avg-Loss 1.0248)\tAcc 54.6875 (Avg-Acc 64.3075)\n",
            "EPOCH: 108 train Results: Acc 64.308 Loss: 1.0248\n",
            "Epoch: [108][0/19]\tTime 0.018 (Avg-Time 0.018)\t Loss 1.2270 (Avg-Loss 1.2270)\tAcc 52.7344 (Avg-Acc 52.7344)\n",
            "Epoch: [108][19/19]\tTime 0.019 (Avg-Time 0.024)\t Loss 1.2273 (Avg-Loss 1.2338)\tAcc 58.4559 (Avg-Acc 56.4500)\n",
            "EPOCH: 108 Validation Results: Acc 56.450 Loss: 1.2338\n",
            "Epoch: [109][0/78]\tTime 0.106 (Avg-Time 0.106)\t Loss 0.9904 (Avg-Loss 0.9904)\tAcc 67.7734 (Avg-Acc 67.7734)\n",
            "Epoch: [109][19/78]\tTime 0.048 (Avg-Time 0.053)\t Loss 0.9155 (Avg-Loss 0.9922)\tAcc 68.5547 (Avg-Acc 65.9766)\n",
            "Epoch: [109][38/78]\tTime 0.047 (Avg-Time 0.052)\t Loss 0.8975 (Avg-Loss 0.9896)\tAcc 70.5078 (Avg-Acc 65.8003)\n",
            "Epoch: [109][57/78]\tTime 0.047 (Avg-Time 0.052)\t Loss 1.0016 (Avg-Loss 1.0086)\tAcc 66.7969 (Avg-Acc 65.1536)\n",
            "Epoch: [109][76/78]\tTime 0.052 (Avg-Time 0.051)\t Loss 1.0825 (Avg-Loss 1.0226)\tAcc 62.5000 (Avg-Acc 64.5089)\n",
            "Epoch: [109][78/78]\tTime 0.009 (Avg-Time 0.051)\t Loss 0.8913 (Avg-Loss 1.0239)\tAcc 70.3125 (Avg-Acc 64.4625)\n",
            "EPOCH: 109 train Results: Acc 64.463 Loss: 1.0239\n",
            "Epoch: [109][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1948 (Avg-Loss 1.1948)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [109][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2872 (Avg-Loss 1.2292)\tAcc 55.5147 (Avg-Acc 56.6300)\n",
            "EPOCH: 109 Validation Results: Acc 56.630 Loss: 1.2292\n",
            "Epoch: [110][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9754 (Avg-Loss 0.9754)\tAcc 67.9688 (Avg-Acc 67.9688)\n",
            "Epoch: [110][19/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.0190 (Avg-Loss 0.9818)\tAcc 62.3047 (Avg-Acc 65.6543)\n",
            "Epoch: [110][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0012 (Avg-Loss 1.0011)\tAcc 66.7969 (Avg-Acc 64.8988)\n",
            "Epoch: [110][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9897 (Avg-Loss 1.0055)\tAcc 64.2578 (Avg-Acc 64.7865)\n",
            "Epoch: [110][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0880 (Avg-Loss 1.0191)\tAcc 63.0859 (Avg-Acc 64.3263)\n",
            "Epoch: [110][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.9733 (Avg-Loss 1.0194)\tAcc 68.7500 (Avg-Acc 64.3225)\n",
            "EPOCH: 110 train Results: Acc 64.323 Loss: 1.0194\n",
            "Epoch: [110][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2010 (Avg-Loss 1.2010)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [110][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2845 (Avg-Loss 1.2312)\tAcc 54.4118 (Avg-Acc 56.5400)\n",
            "EPOCH: 110 Validation Results: Acc 56.540 Loss: 1.2312\n",
            "Epoch: [111][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.9437 (Avg-Loss 0.9437)\tAcc 70.5078 (Avg-Acc 70.5078)\n",
            "Epoch: [111][19/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.9851 (Avg-Loss 0.9790)\tAcc 64.2578 (Avg-Acc 66.1523)\n",
            "Epoch: [111][38/78]\tTime 0.085 (Avg-Time 0.053)\t Loss 1.0218 (Avg-Loss 0.9913)\tAcc 63.4766 (Avg-Acc 65.3496)\n",
            "Epoch: [111][57/78]\tTime 0.091 (Avg-Time 0.081)\t Loss 1.0284 (Avg-Loss 1.0051)\tAcc 65.2344 (Avg-Acc 64.9178)\n",
            "Epoch: [111][76/78]\tTime 0.047 (Avg-Time 0.076)\t Loss 1.1343 (Avg-Loss 1.0186)\tAcc 59.5703 (Avg-Acc 64.5419)\n",
            "Epoch: [111][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.0437 (Avg-Loss 1.0193)\tAcc 62.5000 (Avg-Acc 64.5000)\n",
            "EPOCH: 111 train Results: Acc 64.500 Loss: 1.0193\n",
            "Epoch: [111][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2323 (Avg-Loss 1.2323)\tAcc 55.2734 (Avg-Acc 55.2734)\n",
            "Epoch: [111][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2923 (Avg-Loss 1.2399)\tAcc 53.3088 (Avg-Acc 56.4300)\n",
            "EPOCH: 111 Validation Results: Acc 56.430 Loss: 1.2399\n",
            "Epoch: [112][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9263 (Avg-Loss 0.9263)\tAcc 65.4297 (Avg-Acc 65.4297)\n",
            "Epoch: [112][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9852 (Avg-Loss 1.0021)\tAcc 64.4531 (Avg-Acc 64.8145)\n",
            "Epoch: [112][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0533 (Avg-Loss 1.0045)\tAcc 62.5000 (Avg-Acc 64.9038)\n",
            "Epoch: [112][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0120 (Avg-Loss 1.0120)\tAcc 65.6250 (Avg-Acc 64.6922)\n",
            "Epoch: [112][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0688 (Avg-Loss 1.0177)\tAcc 61.9141 (Avg-Acc 64.4481)\n",
            "Epoch: [112][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.0319 (Avg-Loss 1.0183)\tAcc 68.7500 (Avg-Acc 64.4300)\n",
            "EPOCH: 112 train Results: Acc 64.430 Loss: 1.0183\n",
            "Epoch: [112][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2118 (Avg-Loss 1.2118)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [112][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2848 (Avg-Loss 1.2306)\tAcc 52.5735 (Avg-Acc 56.5100)\n",
            "EPOCH: 112 Validation Results: Acc 56.510 Loss: 1.2306\n",
            "Epoch: [113][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.9393 (Avg-Loss 0.9393)\tAcc 68.3594 (Avg-Acc 68.3594)\n",
            "Epoch: [113][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9014 (Avg-Loss 0.9793)\tAcc 67.3828 (Avg-Acc 65.9082)\n",
            "Epoch: [113][38/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9997 (Avg-Loss 0.9999)\tAcc 66.7969 (Avg-Acc 65.2394)\n",
            "Epoch: [113][57/78]\tTime 0.064 (Avg-Time 0.049)\t Loss 1.1399 (Avg-Loss 1.0098)\tAcc 60.7422 (Avg-Acc 64.7326)\n",
            "Epoch: [113][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0978 (Avg-Loss 1.0186)\tAcc 60.7422 (Avg-Acc 64.3643)\n",
            "Epoch: [113][78/78]\tTime 0.009 (Avg-Time 0.048)\t Loss 0.7189 (Avg-Loss 1.0178)\tAcc 75.0000 (Avg-Acc 64.3900)\n",
            "EPOCH: 113 train Results: Acc 64.390 Loss: 1.0178\n",
            "Epoch: [113][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2243 (Avg-Loss 1.2243)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [113][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2511 (Avg-Loss 1.2347)\tAcc 55.1471 (Avg-Acc 56.1300)\n",
            "EPOCH: 113 Validation Results: Acc 56.130 Loss: 1.2347\n",
            "Epoch: [114][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.9492 (Avg-Loss 0.9492)\tAcc 66.9922 (Avg-Acc 66.9922)\n",
            "Epoch: [114][19/78]\tTime 0.293 (Avg-Time 0.089)\t Loss 0.9210 (Avg-Loss 0.9772)\tAcc 69.5312 (Avg-Acc 66.1230)\n",
            "Epoch: [114][38/78]\tTime 0.050 (Avg-Time 0.105)\t Loss 1.0035 (Avg-Loss 0.9872)\tAcc 66.9922 (Avg-Acc 65.7051)\n",
            "Epoch: [114][57/78]\tTime 0.046 (Avg-Time 0.086)\t Loss 1.0135 (Avg-Loss 0.9996)\tAcc 64.2578 (Avg-Acc 65.3118)\n",
            "Epoch: [114][76/78]\tTime 0.047 (Avg-Time 0.077)\t Loss 1.0624 (Avg-Loss 1.0125)\tAcc 63.8672 (Avg-Acc 64.7398)\n",
            "Epoch: [114][78/78]\tTime 0.009 (Avg-Time 0.076)\t Loss 1.0949 (Avg-Loss 1.0135)\tAcc 59.3750 (Avg-Acc 64.6725)\n",
            "EPOCH: 114 train Results: Acc 64.672 Loss: 1.0135\n",
            "Epoch: [114][0/19]\tTime 0.011 (Avg-Time 0.011)\t Loss 1.1911 (Avg-Loss 1.1911)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [114][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2229 (Avg-Loss 1.2213)\tAcc 56.6176 (Avg-Acc 57.1700)\n",
            "EPOCH: 114 Validation Results: Acc 57.170 Loss: 1.2213\n",
            "Epoch: [115][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9028 (Avg-Loss 0.9028)\tAcc 69.9219 (Avg-Acc 69.9219)\n",
            "Epoch: [115][19/78]\tTime 0.045 (Avg-Time 0.051)\t Loss 0.9840 (Avg-Loss 0.9861)\tAcc 68.1641 (Avg-Acc 65.9766)\n",
            "Epoch: [115][38/78]\tTime 0.065 (Avg-Time 0.050)\t Loss 1.0916 (Avg-Loss 0.9972)\tAcc 60.9375 (Avg-Acc 65.5449)\n",
            "Epoch: [115][57/78]\tTime 0.054 (Avg-Time 0.049)\t Loss 0.9669 (Avg-Loss 1.0071)\tAcc 68.9453 (Avg-Acc 65.0929)\n",
            "Epoch: [115][76/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0504 (Avg-Loss 1.0186)\tAcc 63.8672 (Avg-Acc 64.6180)\n",
            "Epoch: [115][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.2945 (Avg-Loss 1.0192)\tAcc 54.6875 (Avg-Acc 64.5975)\n",
            "EPOCH: 115 train Results: Acc 64.597 Loss: 1.0192\n",
            "Epoch: [115][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2134 (Avg-Loss 1.2134)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [115][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2990 (Avg-Loss 1.2296)\tAcc 54.7794 (Avg-Acc 56.3400)\n",
            "EPOCH: 115 Validation Results: Acc 56.340 Loss: 1.2296\n",
            "Epoch: [116][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9961 (Avg-Loss 0.9961)\tAcc 65.8203 (Avg-Acc 65.8203)\n",
            "Epoch: [116][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9796 (Avg-Loss 0.9679)\tAcc 66.7969 (Avg-Acc 66.7676)\n",
            "Epoch: [116][38/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 1.0386 (Avg-Loss 0.9903)\tAcc 63.8672 (Avg-Acc 65.4748)\n",
            "Epoch: [116][57/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 1.0692 (Avg-Loss 1.0060)\tAcc 62.5000 (Avg-Acc 64.8808)\n",
            "Epoch: [116][76/78]\tTime 0.097 (Avg-Time 0.054)\t Loss 1.0588 (Avg-Loss 1.0177)\tAcc 64.2578 (Avg-Acc 64.4861)\n",
            "Epoch: [116][78/78]\tTime 0.114 (Avg-Time 0.055)\t Loss 1.2042 (Avg-Loss 1.0189)\tAcc 57.8125 (Avg-Acc 64.4400)\n",
            "EPOCH: 116 train Results: Acc 64.440 Loss: 1.0189\n",
            "Epoch: [116][0/19]\tTime 0.082 (Avg-Time 0.082)\t Loss 1.1999 (Avg-Loss 1.1999)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [116][19/19]\tTime 0.017 (Avg-Time 0.063)\t Loss 1.2987 (Avg-Loss 1.2205)\tAcc 54.0441 (Avg-Acc 56.7000)\n",
            "EPOCH: 116 Validation Results: Acc 56.700 Loss: 1.2205\n",
            "Epoch: [117][0/78]\tTime 0.106 (Avg-Time 0.106)\t Loss 0.9468 (Avg-Loss 0.9468)\tAcc 66.7969 (Avg-Acc 66.7969)\n",
            "Epoch: [117][19/78]\tTime 0.073 (Avg-Time 0.079)\t Loss 1.0440 (Avg-Loss 0.9819)\tAcc 63.4766 (Avg-Acc 65.9180)\n",
            "Epoch: [117][38/78]\tTime 0.047 (Avg-Time 0.064)\t Loss 1.0459 (Avg-Loss 1.0021)\tAcc 64.4531 (Avg-Acc 65.1492)\n",
            "Epoch: [117][57/78]\tTime 0.047 (Avg-Time 0.059)\t Loss 1.0123 (Avg-Loss 1.0133)\tAcc 62.8906 (Avg-Acc 64.6148)\n",
            "Epoch: [117][76/78]\tTime 0.047 (Avg-Time 0.057)\t Loss 1.0097 (Avg-Loss 1.0228)\tAcc 65.0391 (Avg-Acc 64.1766)\n",
            "Epoch: [117][78/78]\tTime 0.009 (Avg-Time 0.057)\t Loss 0.9505 (Avg-Loss 1.0236)\tAcc 64.0625 (Avg-Acc 64.1050)\n",
            "EPOCH: 117 train Results: Acc 64.105 Loss: 1.0236\n",
            "Epoch: [117][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1959 (Avg-Loss 1.1959)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [117][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2857 (Avg-Loss 1.2254)\tAcc 54.7794 (Avg-Acc 56.8600)\n",
            "EPOCH: 117 Validation Results: Acc 56.860 Loss: 1.2254\n",
            "Epoch: [118][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9913 (Avg-Loss 0.9913)\tAcc 66.9922 (Avg-Acc 66.9922)\n",
            "Epoch: [118][19/78]\tTime 0.070 (Avg-Time 0.050)\t Loss 0.9657 (Avg-Loss 0.9691)\tAcc 66.2109 (Avg-Acc 66.8652)\n",
            "Epoch: [118][38/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0772 (Avg-Loss 0.9819)\tAcc 63.4766 (Avg-Acc 66.2260)\n",
            "Epoch: [118][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9899 (Avg-Loss 0.9997)\tAcc 67.7734 (Avg-Acc 65.4802)\n",
            "Epoch: [118][76/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0500 (Avg-Loss 1.0074)\tAcc 64.0625 (Avg-Acc 65.0771)\n",
            "Epoch: [118][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0519 (Avg-Loss 1.0085)\tAcc 62.5000 (Avg-Acc 65.0400)\n",
            "EPOCH: 118 train Results: Acc 65.040 Loss: 1.0085\n",
            "Epoch: [118][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2052 (Avg-Loss 1.2052)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [118][19/19]\tTime 0.007 (Avg-Time 0.015)\t Loss 1.2967 (Avg-Loss 1.2374)\tAcc 55.8824 (Avg-Acc 56.5500)\n",
            "EPOCH: 118 Validation Results: Acc 56.550 Loss: 1.2374\n",
            "Epoch: [119][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9260 (Avg-Loss 0.9260)\tAcc 66.2109 (Avg-Acc 66.2109)\n",
            "Epoch: [119][19/78]\tTime 0.047 (Avg-Time 0.048)\t Loss 0.9971 (Avg-Loss 0.9852)\tAcc 63.0859 (Avg-Acc 65.5762)\n",
            "Epoch: [119][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0114 (Avg-Loss 0.9946)\tAcc 65.0391 (Avg-Acc 65.2945)\n",
            "Epoch: [119][57/78]\tTime 0.123 (Avg-Time 0.078)\t Loss 1.1068 (Avg-Loss 1.0044)\tAcc 62.3047 (Avg-Acc 64.9683)\n",
            "Epoch: [119][76/78]\tTime 0.050 (Avg-Time 0.076)\t Loss 1.1776 (Avg-Loss 1.0146)\tAcc 59.3750 (Avg-Acc 64.6839)\n",
            "Epoch: [119][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.0925 (Avg-Loss 1.0155)\tAcc 64.0625 (Avg-Acc 64.6350)\n",
            "EPOCH: 119 train Results: Acc 64.635 Loss: 1.0155\n",
            "Epoch: [119][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1616 (Avg-Loss 1.1616)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [119][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2968 (Avg-Loss 1.2223)\tAcc 52.2059 (Avg-Acc 56.5700)\n",
            "EPOCH: 119 Validation Results: Acc 56.570 Loss: 1.2223\n",
            "Epoch: [120][0/78]\tTime 0.061 (Avg-Time 0.061)\t Loss 0.8918 (Avg-Loss 0.8918)\tAcc 71.4844 (Avg-Acc 71.4844)\n",
            "Epoch: [120][19/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9476 (Avg-Loss 0.9649)\tAcc 68.7500 (Avg-Acc 67.0117)\n",
            "Epoch: [120][38/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9558 (Avg-Loss 0.9893)\tAcc 67.9688 (Avg-Acc 65.8103)\n",
            "Epoch: [120][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0555 (Avg-Loss 1.0009)\tAcc 61.9141 (Avg-Acc 65.2680)\n",
            "Epoch: [120][76/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0728 (Avg-Loss 1.0091)\tAcc 62.8906 (Avg-Acc 64.9325)\n",
            "Epoch: [120][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.2155 (Avg-Loss 1.0102)\tAcc 57.8125 (Avg-Acc 64.8900)\n",
            "EPOCH: 120 train Results: Acc 64.890 Loss: 1.0102\n",
            "Epoch: [120][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1741 (Avg-Loss 1.1741)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [120][19/19]\tTime 0.009 (Avg-Time 0.012)\t Loss 1.2941 (Avg-Loss 1.2180)\tAcc 51.1029 (Avg-Acc 56.8600)\n",
            "EPOCH: 120 Validation Results: Acc 56.860 Loss: 1.2180\n",
            "Epoch: [121][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9165 (Avg-Loss 0.9165)\tAcc 70.1172 (Avg-Acc 70.1172)\n",
            "Epoch: [121][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0041 (Avg-Loss 0.9758)\tAcc 61.7188 (Avg-Acc 66.0742)\n",
            "Epoch: [121][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0520 (Avg-Loss 0.9910)\tAcc 64.0625 (Avg-Acc 65.5749)\n",
            "Epoch: [121][57/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.1188 (Avg-Loss 1.0041)\tAcc 62.5000 (Avg-Acc 65.0694)\n",
            "Epoch: [121][76/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9995 (Avg-Loss 1.0153)\tAcc 65.8203 (Avg-Acc 64.5444)\n",
            "Epoch: [121][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1687 (Avg-Loss 1.0167)\tAcc 57.8125 (Avg-Acc 64.5075)\n",
            "EPOCH: 121 train Results: Acc 64.507 Loss: 1.0167\n",
            "Epoch: [121][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1888 (Avg-Loss 1.1888)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [121][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2687 (Avg-Loss 1.2241)\tAcc 57.3529 (Avg-Acc 56.6900)\n",
            "EPOCH: 121 Validation Results: Acc 56.690 Loss: 1.2241\n",
            "Epoch: [122][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8852 (Avg-Loss 0.8852)\tAcc 72.0703 (Avg-Acc 72.0703)\n",
            "Epoch: [122][19/78]\tTime 0.107 (Avg-Time 0.060)\t Loss 0.9368 (Avg-Loss 0.9662)\tAcc 68.1641 (Avg-Acc 66.5527)\n",
            "Epoch: [122][38/78]\tTime 0.105 (Avg-Time 0.100)\t Loss 0.9926 (Avg-Loss 0.9797)\tAcc 65.8203 (Avg-Acc 65.7953)\n",
            "Epoch: [122][57/78]\tTime 0.047 (Avg-Time 0.086)\t Loss 1.0500 (Avg-Loss 0.9966)\tAcc 63.6719 (Avg-Acc 65.1300)\n",
            "Epoch: [122][76/78]\tTime 0.046 (Avg-Time 0.076)\t Loss 1.0929 (Avg-Loss 1.0088)\tAcc 62.3047 (Avg-Acc 64.6865)\n",
            "Epoch: [122][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.0178 (Avg-Loss 1.0090)\tAcc 67.1875 (Avg-Acc 64.6525)\n",
            "EPOCH: 122 train Results: Acc 64.653 Loss: 1.0090\n",
            "Epoch: [122][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1863 (Avg-Loss 1.1863)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [122][19/19]\tTime 0.006 (Avg-Time 0.013)\t Loss 1.2934 (Avg-Loss 1.2318)\tAcc 55.1471 (Avg-Acc 56.2600)\n",
            "EPOCH: 122 Validation Results: Acc 56.260 Loss: 1.2318\n",
            "Epoch: [123][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.8877 (Avg-Loss 0.8877)\tAcc 69.3359 (Avg-Acc 69.3359)\n",
            "Epoch: [123][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 1.0693 (Avg-Loss 0.9774)\tAcc 60.1562 (Avg-Acc 65.5664)\n",
            "Epoch: [123][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0773 (Avg-Loss 0.9955)\tAcc 58.2031 (Avg-Acc 65.0992)\n",
            "Epoch: [123][57/78]\tTime 0.051 (Avg-Time 0.049)\t Loss 1.0450 (Avg-Loss 0.9998)\tAcc 61.7188 (Avg-Acc 64.9414)\n",
            "Epoch: [123][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.1248 (Avg-Loss 1.0165)\tAcc 60.3516 (Avg-Acc 64.3872)\n",
            "Epoch: [123][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1180 (Avg-Loss 1.0175)\tAcc 62.5000 (Avg-Acc 64.3325)\n",
            "EPOCH: 123 train Results: Acc 64.332 Loss: 1.0175\n",
            "Epoch: [123][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1946 (Avg-Loss 1.1946)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [123][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2863 (Avg-Loss 1.2276)\tAcc 52.9412 (Avg-Acc 56.2800)\n",
            "EPOCH: 123 Validation Results: Acc 56.280 Loss: 1.2276\n",
            "Epoch: [124][0/78]\tTime 0.065 (Avg-Time 0.065)\t Loss 0.9587 (Avg-Loss 0.9587)\tAcc 68.5547 (Avg-Acc 68.5547)\n",
            "Epoch: [124][19/78]\tTime 0.048 (Avg-Time 0.051)\t Loss 0.9649 (Avg-Loss 0.9777)\tAcc 68.3594 (Avg-Acc 66.2109)\n",
            "Epoch: [124][38/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0330 (Avg-Loss 0.9916)\tAcc 63.8672 (Avg-Acc 65.4347)\n",
            "Epoch: [124][57/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 1.0053 (Avg-Loss 1.0029)\tAcc 64.0625 (Avg-Acc 65.0323)\n",
            "Epoch: [124][76/78]\tTime 0.106 (Avg-Time 0.051)\t Loss 0.9805 (Avg-Loss 1.0120)\tAcc 65.6250 (Avg-Acc 64.6383)\n",
            "Epoch: [124][78/78]\tTime 0.022 (Avg-Time 0.051)\t Loss 1.1177 (Avg-Loss 1.0125)\tAcc 65.6250 (Avg-Acc 64.6400)\n",
            "EPOCH: 124 train Results: Acc 64.640 Loss: 1.0125\n",
            "Epoch: [124][0/19]\tTime 0.025 (Avg-Time 0.025)\t Loss 1.2033 (Avg-Loss 1.2033)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [124][19/19]\tTime 0.023 (Avg-Time 0.028)\t Loss 1.3006 (Avg-Loss 1.2308)\tAcc 55.5147 (Avg-Acc 56.8200)\n",
            "EPOCH: 124 Validation Results: Acc 56.820 Loss: 1.2308\n",
            "Epoch: [125][0/78]\tTime 0.234 (Avg-Time 0.234)\t Loss 0.9465 (Avg-Loss 0.9465)\tAcc 67.1875 (Avg-Acc 67.1875)\n",
            "Epoch: [125][19/78]\tTime 0.049 (Avg-Time 0.132)\t Loss 1.0404 (Avg-Loss 0.9868)\tAcc 64.0625 (Avg-Acc 66.0254)\n",
            "Epoch: [125][38/78]\tTime 0.048 (Avg-Time 0.092)\t Loss 1.0530 (Avg-Loss 0.9908)\tAcc 62.8906 (Avg-Acc 65.8103)\n",
            "Epoch: [125][57/78]\tTime 0.048 (Avg-Time 0.078)\t Loss 1.0186 (Avg-Loss 1.0027)\tAcc 62.6953 (Avg-Acc 65.1165)\n",
            "Epoch: [125][76/78]\tTime 0.048 (Avg-Time 0.071)\t Loss 1.1016 (Avg-Loss 1.0127)\tAcc 60.1562 (Avg-Acc 64.6738)\n",
            "Epoch: [125][78/78]\tTime 0.009 (Avg-Time 0.070)\t Loss 1.2025 (Avg-Loss 1.0144)\tAcc 50.0000 (Avg-Acc 64.5900)\n",
            "EPOCH: 125 train Results: Acc 64.590 Loss: 1.0144\n",
            "Epoch: [125][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1968 (Avg-Loss 1.1968)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [125][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2743 (Avg-Loss 1.2291)\tAcc 54.4118 (Avg-Acc 56.4400)\n",
            "EPOCH: 125 Validation Results: Acc 56.440 Loss: 1.2291\n",
            "Epoch: [126][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.9939 (Avg-Loss 0.9939)\tAcc 66.4062 (Avg-Acc 66.4062)\n",
            "Epoch: [126][19/78]\tTime 0.050 (Avg-Time 0.052)\t Loss 0.9803 (Avg-Loss 0.9963)\tAcc 66.2109 (Avg-Acc 65.0977)\n",
            "Epoch: [126][38/78]\tTime 0.048 (Avg-Time 0.052)\t Loss 0.9881 (Avg-Loss 0.9966)\tAcc 64.4531 (Avg-Acc 65.0441)\n",
            "Epoch: [126][57/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.0944 (Avg-Loss 1.0054)\tAcc 61.5234 (Avg-Acc 64.9818)\n",
            "Epoch: [126][76/78]\tTime 0.049 (Avg-Time 0.051)\t Loss 1.1228 (Avg-Loss 1.0154)\tAcc 59.1797 (Avg-Acc 64.5749)\n",
            "Epoch: [126][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.0156 (Avg-Loss 1.0150)\tAcc 59.3750 (Avg-Acc 64.5975)\n",
            "EPOCH: 126 train Results: Acc 64.597 Loss: 1.0150\n",
            "Epoch: [126][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2053 (Avg-Loss 1.2053)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [126][19/19]\tTime 0.006 (Avg-Time 0.013)\t Loss 1.2787 (Avg-Loss 1.2337)\tAcc 54.0441 (Avg-Acc 56.9500)\n",
            "EPOCH: 126 Validation Results: Acc 56.950 Loss: 1.2337\n",
            "Epoch: [127][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9380 (Avg-Loss 0.9380)\tAcc 68.9453 (Avg-Acc 68.9453)\n",
            "Epoch: [127][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 1.0313 (Avg-Loss 0.9850)\tAcc 61.1328 (Avg-Acc 65.4688)\n",
            "Epoch: [127][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0094 (Avg-Loss 0.9968)\tAcc 64.2578 (Avg-Acc 65.2895)\n",
            "Epoch: [127][57/78]\tTime 0.086 (Avg-Time 0.057)\t Loss 1.1171 (Avg-Loss 1.0053)\tAcc 61.9141 (Avg-Acc 65.1098)\n",
            "Epoch: [127][76/78]\tTime 0.093 (Avg-Time 0.075)\t Loss 1.0999 (Avg-Loss 1.0175)\tAcc 61.3281 (Avg-Acc 64.6434)\n",
            "Epoch: [127][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 0.9982 (Avg-Loss 1.0178)\tAcc 64.0625 (Avg-Acc 64.6225)\n",
            "EPOCH: 127 train Results: Acc 64.623 Loss: 1.0178\n",
            "Epoch: [127][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1842 (Avg-Loss 1.1842)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [127][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2893 (Avg-Loss 1.2230)\tAcc 54.4118 (Avg-Acc 56.7300)\n",
            "EPOCH: 127 Validation Results: Acc 56.730 Loss: 1.2230\n",
            "Epoch: [128][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9615 (Avg-Loss 0.9615)\tAcc 68.9453 (Avg-Acc 68.9453)\n",
            "Epoch: [128][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9435 (Avg-Loss 0.9692)\tAcc 67.1875 (Avg-Acc 66.8066)\n",
            "Epoch: [128][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0431 (Avg-Loss 0.9744)\tAcc 66.2109 (Avg-Acc 66.5565)\n",
            "Epoch: [128][57/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.9988 (Avg-Loss 0.9906)\tAcc 65.8203 (Avg-Acc 65.8910)\n",
            "Epoch: [128][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0436 (Avg-Loss 1.0030)\tAcc 64.8438 (Avg-Acc 65.2394)\n",
            "Epoch: [128][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0910 (Avg-Loss 1.0034)\tAcc 62.5000 (Avg-Acc 65.2350)\n",
            "EPOCH: 128 train Results: Acc 65.235 Loss: 1.0034\n",
            "Epoch: [128][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2006 (Avg-Loss 1.2006)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [128][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2844 (Avg-Loss 1.2180)\tAcc 54.0441 (Avg-Acc 57.2000)\n",
            "EPOCH: 128 Validation Results: Acc 57.200 Loss: 1.2180\n",
            "Epoch: [129][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.8793 (Avg-Loss 0.8793)\tAcc 69.3359 (Avg-Acc 69.3359)\n",
            "Epoch: [129][19/78]\tTime 0.052 (Avg-Time 0.049)\t Loss 1.0346 (Avg-Loss 0.9612)\tAcc 62.5000 (Avg-Acc 66.9434)\n",
            "Epoch: [129][38/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 1.0103 (Avg-Loss 0.9754)\tAcc 62.3047 (Avg-Acc 65.9405)\n",
            "Epoch: [129][57/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0126 (Avg-Loss 0.9949)\tAcc 63.2812 (Avg-Acc 65.0323)\n",
            "Epoch: [129][76/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 1.0784 (Avg-Loss 1.0081)\tAcc 61.7188 (Avg-Acc 64.5622)\n",
            "Epoch: [129][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1623 (Avg-Loss 1.0088)\tAcc 56.2500 (Avg-Acc 64.5425)\n",
            "EPOCH: 129 train Results: Acc 64.543 Loss: 1.0088\n",
            "Epoch: [129][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1982 (Avg-Loss 1.1982)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [129][19/19]\tTime 0.006 (Avg-Time 0.013)\t Loss 1.3007 (Avg-Loss 1.2238)\tAcc 53.6765 (Avg-Acc 57.0800)\n",
            "EPOCH: 129 Validation Results: Acc 57.080 Loss: 1.2238\n",
            "Epoch: [130][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9485 (Avg-Loss 0.9485)\tAcc 67.7734 (Avg-Acc 67.7734)\n",
            "Epoch: [130][19/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0111 (Avg-Loss 0.9881)\tAcc 64.4531 (Avg-Acc 65.0586)\n",
            "Epoch: [130][38/78]\tTime 0.156 (Avg-Time 0.077)\t Loss 1.0199 (Avg-Loss 0.9996)\tAcc 65.4297 (Avg-Acc 65.2594)\n",
            "Epoch: [130][57/78]\tTime 0.047 (Avg-Time 0.086)\t Loss 1.0031 (Avg-Loss 1.0047)\tAcc 65.2344 (Avg-Acc 65.0727)\n",
            "Epoch: [130][76/78]\tTime 0.055 (Avg-Time 0.077)\t Loss 1.0519 (Avg-Loss 1.0095)\tAcc 61.7188 (Avg-Acc 64.6687)\n",
            "Epoch: [130][78/78]\tTime 0.009 (Avg-Time 0.076)\t Loss 1.1776 (Avg-Loss 1.0106)\tAcc 54.6875 (Avg-Acc 64.6225)\n",
            "EPOCH: 130 train Results: Acc 64.623 Loss: 1.0106\n",
            "Epoch: [130][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1925 (Avg-Loss 1.1925)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [130][19/19]\tTime 0.008 (Avg-Time 0.013)\t Loss 1.2942 (Avg-Loss 1.2244)\tAcc 51.1029 (Avg-Acc 56.9900)\n",
            "EPOCH: 130 Validation Results: Acc 56.990 Loss: 1.2244\n",
            "Epoch: [131][0/78]\tTime 0.053 (Avg-Time 0.053)\t Loss 0.8800 (Avg-Loss 0.8800)\tAcc 70.5078 (Avg-Acc 70.5078)\n",
            "Epoch: [131][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9830 (Avg-Loss 0.9779)\tAcc 65.0391 (Avg-Acc 65.8496)\n",
            "Epoch: [131][38/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 1.0168 (Avg-Loss 0.9893)\tAcc 62.8906 (Avg-Acc 65.1292)\n",
            "Epoch: [131][57/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.9626 (Avg-Loss 1.0041)\tAcc 66.7969 (Avg-Acc 64.7562)\n",
            "Epoch: [131][76/78]\tTime 0.070 (Avg-Time 0.050)\t Loss 0.9599 (Avg-Loss 1.0124)\tAcc 68.9453 (Avg-Acc 64.4988)\n",
            "Epoch: [131][78/78]\tTime 0.012 (Avg-Time 0.049)\t Loss 1.1116 (Avg-Loss 1.0124)\tAcc 60.9375 (Avg-Acc 64.5100)\n",
            "EPOCH: 131 train Results: Acc 64.510 Loss: 1.0124\n",
            "Epoch: [131][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2016 (Avg-Loss 1.2016)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [131][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2942 (Avg-Loss 1.2250)\tAcc 54.4118 (Avg-Acc 57.0800)\n",
            "EPOCH: 131 Validation Results: Acc 57.080 Loss: 1.2250\n",
            "Epoch: [132][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9432 (Avg-Loss 0.9432)\tAcc 66.9922 (Avg-Acc 66.9922)\n",
            "Epoch: [132][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9059 (Avg-Loss 0.9535)\tAcc 70.1172 (Avg-Acc 67.1875)\n",
            "Epoch: [132][38/78]\tTime 0.046 (Avg-Time 0.051)\t Loss 0.9909 (Avg-Loss 0.9784)\tAcc 62.6953 (Avg-Acc 66.0106)\n",
            "Epoch: [132][57/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.0331 (Avg-Loss 0.9889)\tAcc 64.0625 (Avg-Acc 65.5172)\n",
            "Epoch: [132][76/78]\tTime 0.071 (Avg-Time 0.050)\t Loss 1.0164 (Avg-Loss 1.0043)\tAcc 64.8438 (Avg-Acc 64.9122)\n",
            "Epoch: [132][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.0322 (Avg-Loss 1.0058)\tAcc 68.7500 (Avg-Acc 64.8500)\n",
            "EPOCH: 132 train Results: Acc 64.850 Loss: 1.0058\n",
            "Epoch: [132][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1923 (Avg-Loss 1.1923)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [132][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.3129 (Avg-Loss 1.2263)\tAcc 51.1029 (Avg-Acc 56.9500)\n",
            "EPOCH: 132 Validation Results: Acc 56.950 Loss: 1.2263\n",
            "Epoch: [133][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9338 (Avg-Loss 0.9338)\tAcc 67.7734 (Avg-Acc 67.7734)\n",
            "Epoch: [133][19/78]\tTime 0.080 (Avg-Time 0.126)\t Loss 0.9996 (Avg-Loss 0.9624)\tAcc 64.6484 (Avg-Acc 66.7773)\n",
            "Epoch: [133][38/78]\tTime 0.047 (Avg-Time 0.104)\t Loss 0.9836 (Avg-Loss 0.9823)\tAcc 66.2109 (Avg-Acc 65.8704)\n",
            "Epoch: [133][57/78]\tTime 0.049 (Avg-Time 0.086)\t Loss 1.0242 (Avg-Loss 0.9964)\tAcc 62.1094 (Avg-Acc 65.2916)\n",
            "Epoch: [133][76/78]\tTime 0.047 (Avg-Time 0.077)\t Loss 1.0535 (Avg-Loss 1.0086)\tAcc 62.3047 (Avg-Acc 64.6839)\n",
            "Epoch: [133][78/78]\tTime 0.009 (Avg-Time 0.076)\t Loss 0.9506 (Avg-Loss 1.0086)\tAcc 67.1875 (Avg-Acc 64.6775)\n",
            "EPOCH: 133 train Results: Acc 64.677 Loss: 1.0086\n",
            "Epoch: [133][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2008 (Avg-Loss 1.2008)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [133][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2706 (Avg-Loss 1.2238)\tAcc 51.4706 (Avg-Acc 56.4700)\n",
            "EPOCH: 133 Validation Results: Acc 56.470 Loss: 1.2238\n",
            "Epoch: [134][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9182 (Avg-Loss 0.9182)\tAcc 67.5781 (Avg-Acc 67.5781)\n",
            "Epoch: [134][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0131 (Avg-Loss 0.9698)\tAcc 65.4297 (Avg-Acc 66.2109)\n",
            "Epoch: [134][38/78]\tTime 0.047 (Avg-Time 0.051)\t Loss 1.0540 (Avg-Loss 0.9899)\tAcc 63.2812 (Avg-Acc 65.4748)\n",
            "Epoch: [134][57/78]\tTime 0.046 (Avg-Time 0.051)\t Loss 1.0117 (Avg-Loss 0.9943)\tAcc 63.8672 (Avg-Acc 65.2782)\n",
            "Epoch: [134][76/78]\tTime 0.067 (Avg-Time 0.051)\t Loss 0.9714 (Avg-Loss 1.0057)\tAcc 66.4062 (Avg-Acc 64.9300)\n",
            "Epoch: [134][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 0.8756 (Avg-Loss 1.0069)\tAcc 67.1875 (Avg-Acc 64.8725)\n",
            "EPOCH: 134 train Results: Acc 64.873 Loss: 1.0069\n",
            "Epoch: [134][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1905 (Avg-Loss 1.1905)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [134][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2709 (Avg-Loss 1.2235)\tAcc 52.9412 (Avg-Acc 56.9800)\n",
            "EPOCH: 134 Validation Results: Acc 56.980 Loss: 1.2235\n",
            "Epoch: [135][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.0188 (Avg-Loss 1.0188)\tAcc 63.4766 (Avg-Acc 63.4766)\n",
            "Epoch: [135][19/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9415 (Avg-Loss 0.9665)\tAcc 67.7734 (Avg-Acc 66.1133)\n",
            "Epoch: [135][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0364 (Avg-Loss 0.9872)\tAcc 63.8672 (Avg-Acc 65.3295)\n",
            "Epoch: [135][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0639 (Avg-Loss 0.9935)\tAcc 62.8906 (Avg-Acc 65.2243)\n",
            "Epoch: [135][76/78]\tTime 0.300 (Avg-Time 0.067)\t Loss 1.0638 (Avg-Loss 1.0043)\tAcc 62.5000 (Avg-Acc 64.7499)\n",
            "Epoch: [135][78/78]\tTime 0.013 (Avg-Time 0.067)\t Loss 1.1508 (Avg-Loss 1.0045)\tAcc 57.8125 (Avg-Acc 64.7475)\n",
            "EPOCH: 135 train Results: Acc 64.748 Loss: 1.0045\n",
            "Epoch: [135][0/19]\tTime 0.021 (Avg-Time 0.021)\t Loss 1.1860 (Avg-Loss 1.1860)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [135][19/19]\tTime 0.037 (Avg-Time 0.027)\t Loss 1.2473 (Avg-Loss 1.2271)\tAcc 57.3529 (Avg-Acc 56.4300)\n",
            "EPOCH: 135 Validation Results: Acc 56.430 Loss: 1.2271\n",
            "Epoch: [136][0/78]\tTime 0.107 (Avg-Time 0.107)\t Loss 1.0366 (Avg-Loss 1.0366)\tAcc 64.6484 (Avg-Acc 64.6484)\n",
            "Epoch: [136][19/78]\tTime 0.047 (Avg-Time 0.070)\t Loss 0.9825 (Avg-Loss 0.9772)\tAcc 67.7734 (Avg-Acc 66.1133)\n",
            "Epoch: [136][38/78]\tTime 0.050 (Avg-Time 0.060)\t Loss 1.0126 (Avg-Loss 0.9869)\tAcc 66.2109 (Avg-Acc 66.0657)\n",
            "Epoch: [136][57/78]\tTime 0.051 (Avg-Time 0.056)\t Loss 1.0726 (Avg-Loss 0.9994)\tAcc 61.9141 (Avg-Acc 65.4802)\n",
            "Epoch: [136][76/78]\tTime 0.047 (Avg-Time 0.055)\t Loss 1.0593 (Avg-Loss 1.0108)\tAcc 64.8438 (Avg-Acc 65.0162)\n",
            "Epoch: [136][78/78]\tTime 0.009 (Avg-Time 0.054)\t Loss 1.0338 (Avg-Loss 1.0103)\tAcc 62.5000 (Avg-Acc 65.0450)\n",
            "EPOCH: 136 train Results: Acc 65.045 Loss: 1.0103\n",
            "Epoch: [136][0/19]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1869 (Avg-Loss 1.1869)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [136][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2722 (Avg-Loss 1.2299)\tAcc 54.4118 (Avg-Acc 56.6000)\n",
            "EPOCH: 136 Validation Results: Acc 56.600 Loss: 1.2299\n",
            "Epoch: [137][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 1.0157 (Avg-Loss 1.0157)\tAcc 64.2578 (Avg-Acc 64.2578)\n",
            "Epoch: [137][19/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.8410 (Avg-Loss 0.9661)\tAcc 70.7031 (Avg-Acc 66.4551)\n",
            "Epoch: [137][38/78]\tTime 0.067 (Avg-Time 0.050)\t Loss 0.9748 (Avg-Loss 0.9801)\tAcc 67.3828 (Avg-Acc 65.8854)\n",
            "Epoch: [137][57/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 1.0376 (Avg-Loss 0.9953)\tAcc 61.7188 (Avg-Acc 65.0929)\n",
            "Epoch: [137][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0671 (Avg-Loss 1.0048)\tAcc 60.1562 (Avg-Acc 64.5926)\n",
            "Epoch: [137][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1740 (Avg-Loss 1.0053)\tAcc 53.1250 (Avg-Acc 64.5175)\n",
            "EPOCH: 137 train Results: Acc 64.517 Loss: 1.0053\n",
            "Epoch: [137][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1826 (Avg-Loss 1.1826)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [137][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2860 (Avg-Loss 1.2256)\tAcc 52.2059 (Avg-Acc 56.5900)\n",
            "EPOCH: 137 Validation Results: Acc 56.590 Loss: 1.2256\n",
            "Epoch: [138][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9097 (Avg-Loss 0.9097)\tAcc 69.9219 (Avg-Acc 69.9219)\n",
            "Epoch: [138][19/78]\tTime 0.055 (Avg-Time 0.049)\t Loss 0.9366 (Avg-Loss 0.9660)\tAcc 69.5312 (Avg-Acc 66.8945)\n",
            "Epoch: [138][38/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9704 (Avg-Loss 0.9725)\tAcc 66.9922 (Avg-Acc 66.2861)\n",
            "Epoch: [138][57/78]\tTime 0.191 (Avg-Time 0.081)\t Loss 0.9714 (Avg-Loss 0.9914)\tAcc 66.0156 (Avg-Acc 65.4667)\n",
            "Epoch: [138][76/78]\tTime 0.048 (Avg-Time 0.077)\t Loss 1.0062 (Avg-Loss 1.0079)\tAcc 64.6484 (Avg-Acc 64.8742)\n",
            "Epoch: [138][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.0279 (Avg-Loss 1.0079)\tAcc 54.6875 (Avg-Acc 64.8750)\n",
            "EPOCH: 138 train Results: Acc 64.875 Loss: 1.0079\n",
            "Epoch: [138][0/19]\tTime 0.011 (Avg-Time 0.011)\t Loss 1.1835 (Avg-Loss 1.1835)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [138][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3207 (Avg-Loss 1.2242)\tAcc 50.7353 (Avg-Acc 56.5700)\n",
            "EPOCH: 138 Validation Results: Acc 56.570 Loss: 1.2242\n",
            "Epoch: [139][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9150 (Avg-Loss 0.9150)\tAcc 68.3594 (Avg-Acc 68.3594)\n",
            "Epoch: [139][19/78]\tTime 0.046 (Avg-Time 0.048)\t Loss 1.0424 (Avg-Loss 0.9685)\tAcc 61.5234 (Avg-Acc 66.0742)\n",
            "Epoch: [139][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0855 (Avg-Loss 0.9859)\tAcc 65.0391 (Avg-Acc 65.5198)\n",
            "Epoch: [139][57/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 1.0614 (Avg-Loss 0.9903)\tAcc 62.1094 (Avg-Acc 65.2613)\n",
            "Epoch: [139][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0187 (Avg-Loss 1.0058)\tAcc 65.4297 (Avg-Acc 64.8818)\n",
            "Epoch: [139][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0419 (Avg-Loss 1.0071)\tAcc 64.0625 (Avg-Acc 64.8200)\n",
            "EPOCH: 139 train Results: Acc 64.820 Loss: 1.0071\n",
            "Epoch: [139][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1962 (Avg-Loss 1.1962)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [139][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3180 (Avg-Loss 1.2304)\tAcc 52.2059 (Avg-Acc 56.6300)\n",
            "EPOCH: 139 Validation Results: Acc 56.630 Loss: 1.2304\n",
            "Epoch: [140][0/78]\tTime 0.069 (Avg-Time 0.069)\t Loss 0.9378 (Avg-Loss 0.9378)\tAcc 67.1875 (Avg-Acc 67.1875)\n",
            "Epoch: [140][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0193 (Avg-Loss 0.9551)\tAcc 65.0391 (Avg-Acc 67.1387)\n",
            "Epoch: [140][38/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.9613 (Avg-Loss 0.9761)\tAcc 66.9922 (Avg-Acc 66.0507)\n",
            "Epoch: [140][57/78]\tTime 0.052 (Avg-Time 0.050)\t Loss 0.9582 (Avg-Loss 0.9878)\tAcc 67.7734 (Avg-Acc 65.5913)\n",
            "Epoch: [140][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0447 (Avg-Loss 0.9995)\tAcc 62.5000 (Avg-Acc 65.1836)\n",
            "Epoch: [140][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.9035 (Avg-Loss 1.0006)\tAcc 62.5000 (Avg-Acc 65.1375)\n",
            "EPOCH: 140 train Results: Acc 65.138 Loss: 1.0006\n",
            "Epoch: [140][0/19]\tTime 0.011 (Avg-Time 0.011)\t Loss 1.2019 (Avg-Loss 1.2019)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [140][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2636 (Avg-Loss 1.2220)\tAcc 51.8382 (Avg-Acc 57.0400)\n",
            "EPOCH: 140 Validation Results: Acc 57.040 Loss: 1.2220\n",
            "Epoch: [141][0/78]\tTime 0.076 (Avg-Time 0.076)\t Loss 1.0012 (Avg-Loss 1.0012)\tAcc 66.2109 (Avg-Acc 66.2109)\n",
            "Epoch: [141][19/78]\tTime 0.102 (Avg-Time 0.061)\t Loss 0.9925 (Avg-Loss 0.9633)\tAcc 64.4531 (Avg-Acc 66.6504)\n",
            "Epoch: [141][38/78]\tTime 0.046 (Avg-Time 0.102)\t Loss 0.9714 (Avg-Loss 0.9677)\tAcc 66.9922 (Avg-Acc 66.4213)\n",
            "Epoch: [141][57/78]\tTime 0.047 (Avg-Time 0.084)\t Loss 1.0126 (Avg-Loss 0.9886)\tAcc 64.4531 (Avg-Acc 65.3825)\n",
            "Epoch: [141][76/78]\tTime 0.048 (Avg-Time 0.076)\t Loss 1.0398 (Avg-Loss 1.0006)\tAcc 63.0859 (Avg-Acc 64.9959)\n",
            "Epoch: [141][78/78]\tTime 0.009 (Avg-Time 0.075)\t Loss 1.0902 (Avg-Loss 1.0021)\tAcc 64.0625 (Avg-Acc 64.9475)\n",
            "EPOCH: 141 train Results: Acc 64.948 Loss: 1.0021\n",
            "Epoch: [141][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.1849 (Avg-Loss 1.1849)\tAcc 56.0547 (Avg-Acc 56.0547)\n",
            "Epoch: [141][19/19]\tTime 0.006 (Avg-Time 0.013)\t Loss 1.2645 (Avg-Loss 1.2289)\tAcc 56.9853 (Avg-Acc 56.7100)\n",
            "EPOCH: 141 Validation Results: Acc 56.710 Loss: 1.2289\n",
            "Epoch: [142][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9614 (Avg-Loss 0.9614)\tAcc 66.6016 (Avg-Acc 66.6016)\n",
            "Epoch: [142][19/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.9854 (Avg-Loss 0.9739)\tAcc 68.3594 (Avg-Acc 66.4551)\n",
            "Epoch: [142][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0192 (Avg-Loss 0.9834)\tAcc 66.7969 (Avg-Acc 66.1158)\n",
            "Epoch: [142][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.1232 (Avg-Loss 0.9969)\tAcc 59.5703 (Avg-Acc 65.4937)\n",
            "Epoch: [142][76/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 1.0432 (Avg-Loss 1.0030)\tAcc 64.8438 (Avg-Acc 65.2471)\n",
            "Epoch: [142][78/78]\tTime 0.011 (Avg-Time 0.049)\t Loss 1.1819 (Avg-Loss 1.0042)\tAcc 59.3750 (Avg-Acc 65.1750)\n",
            "EPOCH: 142 train Results: Acc 65.175 Loss: 1.0042\n",
            "Epoch: [142][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1686 (Avg-Loss 1.1686)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [142][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2884 (Avg-Loss 1.2202)\tAcc 55.1471 (Avg-Acc 56.7100)\n",
            "EPOCH: 142 Validation Results: Acc 56.710 Loss: 1.2202\n",
            "Epoch: [143][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.8720 (Avg-Loss 0.8720)\tAcc 73.6328 (Avg-Acc 73.6328)\n",
            "Epoch: [143][19/78]\tTime 0.054 (Avg-Time 0.050)\t Loss 0.9666 (Avg-Loss 0.9652)\tAcc 65.6250 (Avg-Acc 66.7676)\n",
            "Epoch: [143][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9650 (Avg-Loss 0.9728)\tAcc 65.8203 (Avg-Acc 66.3812)\n",
            "Epoch: [143][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0008 (Avg-Loss 0.9854)\tAcc 65.6250 (Avg-Acc 65.7563)\n",
            "Epoch: [143][76/78]\tTime 0.117 (Avg-Time 0.052)\t Loss 1.0583 (Avg-Loss 1.0008)\tAcc 63.4766 (Avg-Acc 65.2242)\n",
            "Epoch: [143][78/78]\tTime 0.016 (Avg-Time 0.053)\t Loss 1.4048 (Avg-Loss 1.0009)\tAcc 53.1250 (Avg-Acc 65.2050)\n",
            "EPOCH: 143 train Results: Acc 65.205 Loss: 1.0009\n",
            "Epoch: [143][0/19]\tTime 0.023 (Avg-Time 0.023)\t Loss 1.2067 (Avg-Loss 1.2067)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [143][19/19]\tTime 0.014 (Avg-Time 0.045)\t Loss 1.2735 (Avg-Loss 1.2253)\tAcc 52.2059 (Avg-Acc 57.0300)\n",
            "EPOCH: 143 Validation Results: Acc 57.030 Loss: 1.2253\n",
            "Epoch: [144][0/78]\tTime 0.122 (Avg-Time 0.122)\t Loss 0.9301 (Avg-Loss 0.9301)\tAcc 66.6016 (Avg-Acc 66.6016)\n",
            "Epoch: [144][19/78]\tTime 0.049 (Avg-Time 0.104)\t Loss 1.0021 (Avg-Loss 0.9790)\tAcc 66.6016 (Avg-Acc 66.1328)\n",
            "Epoch: [144][38/78]\tTime 0.054 (Avg-Time 0.079)\t Loss 1.0466 (Avg-Loss 0.9873)\tAcc 66.6016 (Avg-Acc 65.6100)\n",
            "Epoch: [144][57/78]\tTime 0.047 (Avg-Time 0.069)\t Loss 1.0387 (Avg-Loss 0.9976)\tAcc 63.8672 (Avg-Acc 65.0593)\n",
            "Epoch: [144][76/78]\tTime 0.047 (Avg-Time 0.064)\t Loss 1.1324 (Avg-Loss 1.0089)\tAcc 59.1797 (Avg-Acc 64.4734)\n",
            "Epoch: [144][78/78]\tTime 0.009 (Avg-Time 0.063)\t Loss 1.0864 (Avg-Loss 1.0089)\tAcc 56.2500 (Avg-Acc 64.4650)\n",
            "EPOCH: 144 train Results: Acc 64.465 Loss: 1.0089\n",
            "Epoch: [144][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2205 (Avg-Loss 1.2205)\tAcc 56.0547 (Avg-Acc 56.0547)\n",
            "Epoch: [144][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2776 (Avg-Loss 1.2237)\tAcc 50.7353 (Avg-Acc 56.7200)\n",
            "EPOCH: 144 Validation Results: Acc 56.720 Loss: 1.2237\n",
            "Epoch: [145][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9080 (Avg-Loss 0.9080)\tAcc 68.7500 (Avg-Acc 68.7500)\n",
            "Epoch: [145][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9293 (Avg-Loss 0.9664)\tAcc 66.9922 (Avg-Acc 66.8555)\n",
            "Epoch: [145][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9816 (Avg-Loss 0.9770)\tAcc 66.9922 (Avg-Acc 66.1358)\n",
            "Epoch: [145][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0349 (Avg-Loss 0.9910)\tAcc 65.0391 (Avg-Acc 65.6250)\n",
            "Epoch: [145][76/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0194 (Avg-Loss 1.0029)\tAcc 63.6719 (Avg-Acc 65.1760)\n",
            "Epoch: [145][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.1398 (Avg-Loss 1.0032)\tAcc 57.8125 (Avg-Acc 65.1450)\n",
            "EPOCH: 145 train Results: Acc 65.145 Loss: 1.0032\n",
            "Epoch: [145][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1705 (Avg-Loss 1.1705)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [145][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2615 (Avg-Loss 1.2109)\tAcc 54.0441 (Avg-Acc 57.4200)\n",
            "EPOCH: 145 Validation Results: Acc 57.420 Loss: 1.2109\n",
            "Epoch: [146][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.8509 (Avg-Loss 0.8509)\tAcc 71.4844 (Avg-Acc 71.4844)\n",
            "Epoch: [146][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9328 (Avg-Loss 0.9770)\tAcc 68.5547 (Avg-Acc 65.9277)\n",
            "Epoch: [146][38/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 0.9511 (Avg-Loss 0.9742)\tAcc 65.6250 (Avg-Acc 66.1859)\n",
            "Epoch: [146][57/78]\tTime 0.272 (Avg-Time 0.065)\t Loss 1.1026 (Avg-Loss 0.9872)\tAcc 61.1328 (Avg-Acc 65.5812)\n",
            "Epoch: [146][76/78]\tTime 0.046 (Avg-Time 0.076)\t Loss 0.9931 (Avg-Loss 0.9968)\tAcc 64.4531 (Avg-Acc 65.0949)\n",
            "Epoch: [146][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 1.0102 (Avg-Loss 0.9986)\tAcc 67.1875 (Avg-Acc 65.0075)\n",
            "EPOCH: 146 train Results: Acc 65.007 Loss: 0.9986\n",
            "Epoch: [146][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2052 (Avg-Loss 1.2052)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [146][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2457 (Avg-Loss 1.2293)\tAcc 55.1471 (Avg-Acc 56.7400)\n",
            "EPOCH: 146 Validation Results: Acc 56.740 Loss: 1.2293\n",
            "Epoch: [147][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9499 (Avg-Loss 0.9499)\tAcc 66.7969 (Avg-Acc 66.7969)\n",
            "Epoch: [147][19/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9326 (Avg-Loss 0.9636)\tAcc 67.5781 (Avg-Acc 66.1035)\n",
            "Epoch: [147][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0072 (Avg-Loss 0.9745)\tAcc 64.8438 (Avg-Acc 65.8253)\n",
            "Epoch: [147][57/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 1.1024 (Avg-Loss 0.9896)\tAcc 60.5469 (Avg-Acc 65.3691)\n",
            "Epoch: [147][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0201 (Avg-Loss 0.9993)\tAcc 63.2812 (Avg-Acc 64.8843)\n",
            "Epoch: [147][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.2232 (Avg-Loss 0.9996)\tAcc 60.9375 (Avg-Acc 64.8975)\n",
            "EPOCH: 147 train Results: Acc 64.897 Loss: 0.9996\n",
            "Epoch: [147][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1898 (Avg-Loss 1.1898)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [147][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.3030 (Avg-Loss 1.2310)\tAcc 51.1029 (Avg-Acc 56.6200)\n",
            "EPOCH: 147 Validation Results: Acc 56.620 Loss: 1.2310\n",
            "Epoch: [148][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9687 (Avg-Loss 0.9687)\tAcc 66.7969 (Avg-Acc 66.7969)\n",
            "Epoch: [148][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0399 (Avg-Loss 0.9713)\tAcc 64.6484 (Avg-Acc 66.6895)\n",
            "Epoch: [148][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9918 (Avg-Loss 0.9756)\tAcc 64.2578 (Avg-Acc 66.4613)\n",
            "Epoch: [148][57/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 1.0392 (Avg-Loss 0.9875)\tAcc 64.8438 (Avg-Acc 66.0190)\n",
            "Epoch: [148][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0619 (Avg-Loss 0.9990)\tAcc 59.1797 (Avg-Acc 65.4170)\n",
            "Epoch: [148][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.9421 (Avg-Loss 1.0001)\tAcc 65.6250 (Avg-Acc 65.3750)\n",
            "EPOCH: 148 train Results: Acc 65.375 Loss: 1.0001\n",
            "Epoch: [148][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1847 (Avg-Loss 1.1847)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [148][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2254 (Avg-Loss 1.2175)\tAcc 58.8235 (Avg-Acc 57.1600)\n",
            "EPOCH: 148 Validation Results: Acc 57.160 Loss: 1.2175\n",
            "Epoch: [149][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.8462 (Avg-Loss 0.8462)\tAcc 72.8516 (Avg-Acc 72.8516)\n",
            "Epoch: [149][19/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9582 (Avg-Loss 0.9590)\tAcc 65.4297 (Avg-Acc 66.4355)\n",
            "Epoch: [149][38/78]\tTime 0.087 (Avg-Time 0.089)\t Loss 0.9104 (Avg-Loss 0.9783)\tAcc 69.5312 (Avg-Acc 65.5499)\n",
            "Epoch: [149][57/78]\tTime 0.048 (Avg-Time 0.086)\t Loss 0.9763 (Avg-Loss 0.9878)\tAcc 65.8203 (Avg-Acc 65.1603)\n",
            "Epoch: [149][76/78]\tTime 0.047 (Avg-Time 0.077)\t Loss 1.0724 (Avg-Loss 0.9993)\tAcc 62.6953 (Avg-Acc 64.7727)\n",
            "Epoch: [149][78/78]\tTime 0.009 (Avg-Time 0.076)\t Loss 1.0791 (Avg-Loss 0.9993)\tAcc 65.6250 (Avg-Acc 64.8075)\n",
            "EPOCH: 149 train Results: Acc 64.808 Loss: 0.9993\n",
            "Epoch: [149][0/19]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2070 (Avg-Loss 1.2070)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [149][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2574 (Avg-Loss 1.2256)\tAcc 54.0441 (Avg-Acc 57.1400)\n",
            "EPOCH: 149 Validation Results: Acc 57.140 Loss: 1.2256\n",
            "Epoch: [150][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.8500 (Avg-Loss 0.8500)\tAcc 71.0938 (Avg-Acc 71.0938)\n",
            "Epoch: [150][19/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 0.9893 (Avg-Loss 0.9658)\tAcc 66.4062 (Avg-Acc 66.8652)\n",
            "Epoch: [150][38/78]\tTime 0.051 (Avg-Time 0.050)\t Loss 0.9872 (Avg-Loss 0.9775)\tAcc 66.0156 (Avg-Acc 66.3662)\n",
            "Epoch: [150][57/78]\tTime 0.084 (Avg-Time 0.050)\t Loss 1.0330 (Avg-Loss 0.9916)\tAcc 65.4297 (Avg-Acc 65.8203)\n",
            "Epoch: [150][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0372 (Avg-Loss 1.0016)\tAcc 61.1328 (Avg-Acc 65.1608)\n",
            "Epoch: [150][78/78]\tTime 0.009 (Avg-Time 0.050)\t Loss 1.1853 (Avg-Loss 1.0024)\tAcc 57.8125 (Avg-Acc 65.1150)\n",
            "EPOCH: 150 train Results: Acc 65.115 Loss: 1.0024\n",
            "Epoch: [150][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1636 (Avg-Loss 1.1636)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [150][19/19]\tTime 0.007 (Avg-Time 0.013)\t Loss 1.2583 (Avg-Loss 1.2225)\tAcc 53.3088 (Avg-Acc 57.4100)\n",
            "EPOCH: 150 Validation Results: Acc 57.410 Loss: 1.2225\n",
            "Epoch: [151][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9095 (Avg-Loss 0.9095)\tAcc 66.4062 (Avg-Acc 66.4062)\n",
            "Epoch: [151][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 1.1058 (Avg-Loss 0.9876)\tAcc 61.7188 (Avg-Acc 65.4004)\n",
            "Epoch: [151][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0777 (Avg-Loss 0.9900)\tAcc 61.3281 (Avg-Acc 65.3045)\n",
            "Epoch: [151][57/78]\tTime 0.046 (Avg-Time 0.050)\t Loss 1.0214 (Avg-Loss 1.0000)\tAcc 64.2578 (Avg-Acc 64.9515)\n",
            "Epoch: [151][76/78]\tTime 0.051 (Avg-Time 0.049)\t Loss 1.0139 (Avg-Loss 1.0056)\tAcc 64.2578 (Avg-Acc 64.6839)\n",
            "Epoch: [151][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1306 (Avg-Loss 1.0061)\tAcc 62.5000 (Avg-Acc 64.6750)\n",
            "EPOCH: 151 train Results: Acc 64.675 Loss: 1.0061\n",
            "Epoch: [151][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1832 (Avg-Loss 1.1832)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [151][19/19]\tTime 0.013 (Avg-Time 0.017)\t Loss 1.2797 (Avg-Loss 1.2241)\tAcc 53.6765 (Avg-Acc 57.0400)\n",
            "EPOCH: 151 Validation Results: Acc 57.040 Loss: 1.2241\n",
            "Epoch: [152][0/78]\tTime 0.101 (Avg-Time 0.101)\t Loss 0.8376 (Avg-Loss 0.8376)\tAcc 72.0703 (Avg-Acc 72.0703)\n",
            "Epoch: [152][19/78]\tTime 0.103 (Avg-Time 0.141)\t Loss 1.0070 (Avg-Loss 0.9765)\tAcc 63.4766 (Avg-Acc 66.0449)\n",
            "Epoch: [152][38/78]\tTime 0.047 (Avg-Time 0.099)\t Loss 0.9495 (Avg-Loss 0.9762)\tAcc 66.4062 (Avg-Acc 66.1158)\n",
            "Epoch: [152][57/78]\tTime 0.054 (Avg-Time 0.083)\t Loss 1.0320 (Avg-Loss 0.9850)\tAcc 62.1094 (Avg-Acc 65.7395)\n",
            "Epoch: [152][76/78]\tTime 0.046 (Avg-Time 0.074)\t Loss 1.0100 (Avg-Loss 0.9943)\tAcc 66.0156 (Avg-Acc 65.1786)\n",
            "Epoch: [152][78/78]\tTime 0.009 (Avg-Time 0.074)\t Loss 1.1126 (Avg-Loss 0.9955)\tAcc 54.6875 (Avg-Acc 65.1250)\n",
            "EPOCH: 152 train Results: Acc 65.125 Loss: 0.9955\n",
            "Epoch: [152][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.2351 (Avg-Loss 1.2351)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [152][19/19]\tTime 0.007 (Avg-Time 0.012)\t Loss 1.2683 (Avg-Loss 1.2293)\tAcc 56.2500 (Avg-Acc 56.9500)\n",
            "EPOCH: 152 Validation Results: Acc 56.950 Loss: 1.2293\n",
            "Epoch: [153][0/78]\tTime 0.050 (Avg-Time 0.050)\t Loss 0.9397 (Avg-Loss 0.9397)\tAcc 66.7969 (Avg-Acc 66.7969)\n",
            "Epoch: [153][19/78]\tTime 0.049 (Avg-Time 0.050)\t Loss 0.9913 (Avg-Loss 0.9504)\tAcc 66.4062 (Avg-Acc 67.0605)\n",
            "Epoch: [153][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 0.9875 (Avg-Loss 0.9693)\tAcc 66.4062 (Avg-Acc 66.5264)\n",
            "Epoch: [153][57/78]\tTime 0.071 (Avg-Time 0.050)\t Loss 1.0739 (Avg-Loss 0.9866)\tAcc 60.3516 (Avg-Acc 65.8843)\n",
            "Epoch: [153][76/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0813 (Avg-Loss 0.9974)\tAcc 60.7422 (Avg-Acc 65.2750)\n",
            "Epoch: [153][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.0160 (Avg-Loss 0.9984)\tAcc 67.1875 (Avg-Acc 65.2200)\n",
            "EPOCH: 153 train Results: Acc 65.220 Loss: 0.9984\n",
            "Epoch: [153][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1682 (Avg-Loss 1.1682)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [153][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.3054 (Avg-Loss 1.2234)\tAcc 55.5147 (Avg-Acc 56.5400)\n",
            "EPOCH: 153 Validation Results: Acc 56.540 Loss: 1.2234\n",
            "Epoch: [154][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9535 (Avg-Loss 0.9535)\tAcc 66.9922 (Avg-Acc 66.9922)\n",
            "Epoch: [154][19/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 1.0409 (Avg-Loss 0.9503)\tAcc 63.6719 (Avg-Acc 67.4121)\n",
            "Epoch: [154][38/78]\tTime 0.048 (Avg-Time 0.050)\t Loss 1.0348 (Avg-Loss 0.9679)\tAcc 64.0625 (Avg-Acc 66.4513)\n",
            "Epoch: [154][57/78]\tTime 0.114 (Avg-Time 0.050)\t Loss 1.1062 (Avg-Loss 0.9854)\tAcc 59.7656 (Avg-Acc 65.7294)\n",
            "Epoch: [154][76/78]\tTime 0.083 (Avg-Time 0.072)\t Loss 0.9767 (Avg-Loss 0.9971)\tAcc 65.6250 (Avg-Acc 65.2724)\n",
            "Epoch: [154][78/78]\tTime 0.019 (Avg-Time 0.071)\t Loss 1.3606 (Avg-Loss 0.9982)\tAcc 53.1250 (Avg-Acc 65.2025)\n",
            "EPOCH: 154 train Results: Acc 65.203 Loss: 0.9982\n",
            "Epoch: [154][0/19]\tTime 0.027 (Avg-Time 0.027)\t Loss 1.1638 (Avg-Loss 1.1638)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [154][19/19]\tTime 0.007 (Avg-Time 0.025)\t Loss 1.2928 (Avg-Loss 1.2228)\tAcc 52.5735 (Avg-Acc 57.0700)\n",
            "EPOCH: 154 Validation Results: Acc 57.070 Loss: 1.2228\n",
            "Epoch: [155][0/78]\tTime 0.052 (Avg-Time 0.052)\t Loss 0.8672 (Avg-Loss 0.8672)\tAcc 70.3125 (Avg-Acc 70.3125)\n",
            "Epoch: [155][19/78]\tTime 0.055 (Avg-Time 0.049)\t Loss 1.0436 (Avg-Loss 0.9585)\tAcc 60.9375 (Avg-Acc 66.5332)\n",
            "Epoch: [155][38/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9440 (Avg-Loss 0.9697)\tAcc 66.7969 (Avg-Acc 66.2009)\n",
            "Epoch: [155][57/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9846 (Avg-Loss 0.9835)\tAcc 67.3828 (Avg-Acc 65.7227)\n",
            "Epoch: [155][76/78]\tTime 0.047 (Avg-Time 0.050)\t Loss 0.9887 (Avg-Loss 0.9971)\tAcc 63.6719 (Avg-Acc 65.1481)\n",
            "Epoch: [155][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 0.9119 (Avg-Loss 0.9972)\tAcc 67.1875 (Avg-Acc 65.1400)\n",
            "EPOCH: 155 train Results: Acc 65.140 Loss: 0.9972\n",
            "Epoch: [155][0/19]\tTime 0.013 (Avg-Time 0.013)\t Loss 1.1961 (Avg-Loss 1.1961)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [155][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2643 (Avg-Loss 1.2293)\tAcc 54.4118 (Avg-Acc 57.1300)\n",
            "EPOCH: 155 Validation Results: Acc 57.130 Loss: 1.2293\n",
            "Epoch: [156][0/78]\tTime 0.049 (Avg-Time 0.049)\t Loss 0.9102 (Avg-Loss 0.9102)\tAcc 68.5547 (Avg-Acc 68.5547)\n",
            "Epoch: [156][19/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 0.9666 (Avg-Loss 0.9486)\tAcc 65.6250 (Avg-Acc 67.1973)\n",
            "Epoch: [156][38/78]\tTime 0.050 (Avg-Time 0.049)\t Loss 1.0033 (Avg-Loss 0.9644)\tAcc 66.4062 (Avg-Acc 66.6016)\n",
            "Epoch: [156][57/78]\tTime 0.048 (Avg-Time 0.049)\t Loss 1.0000 (Avg-Loss 0.9785)\tAcc 65.6250 (Avg-Acc 66.1604)\n",
            "Epoch: [156][76/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 1.1131 (Avg-Loss 0.9888)\tAcc 63.2812 (Avg-Acc 65.6833)\n",
            "Epoch: [156][78/78]\tTime 0.009 (Avg-Time 0.049)\t Loss 1.1157 (Avg-Loss 0.9892)\tAcc 60.9375 (Avg-Acc 65.6800)\n",
            "EPOCH: 156 train Results: Acc 65.680 Loss: 0.9892\n",
            "Epoch: [156][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1995 (Avg-Loss 1.1995)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [156][19/19]\tTime 0.007 (Avg-Time 0.016)\t Loss 1.2423 (Avg-Loss 1.2326)\tAcc 55.8824 (Avg-Acc 56.6300)\n",
            "EPOCH: 156 Validation Results: Acc 56.630 Loss: 1.2326\n",
            "Epoch: [157][0/78]\tTime 0.048 (Avg-Time 0.048)\t Loss 0.9744 (Avg-Loss 0.9744)\tAcc 67.1875 (Avg-Acc 67.1875)\n",
            "Epoch: [157][19/78]\tTime 0.074 (Avg-Time 0.051)\t Loss 1.0228 (Avg-Loss 0.9702)\tAcc 65.6250 (Avg-Acc 66.1133)\n",
            "Epoch: [157][38/78]\tTime 0.115 (Avg-Time 0.058)\t Loss 0.9557 (Avg-Loss 0.9713)\tAcc 66.7969 (Avg-Acc 66.0707)\n",
            "Epoch: [157][57/78]\tTime 0.109 (Avg-Time 0.085)\t Loss 1.0439 (Avg-Loss 0.9824)\tAcc 66.2109 (Avg-Acc 65.7126)\n",
            "Epoch: [157][76/78]\tTime 0.049 (Avg-Time 0.077)\t Loss 1.0828 (Avg-Loss 0.9950)\tAcc 62.1094 (Avg-Acc 65.2268)\n",
            "Epoch: [157][78/78]\tTime 0.010 (Avg-Time 0.076)\t Loss 1.2639 (Avg-Loss 0.9963)\tAcc 56.2500 (Avg-Acc 65.1875)\n",
            "EPOCH: 157 train Results: Acc 65.188 Loss: 0.9963\n",
            "Epoch: [157][0/19]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1820 (Avg-Loss 1.1820)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [157][19/19]\tTime 0.007 (Avg-Time 0.014)\t Loss 1.2532 (Avg-Loss 1.2261)\tAcc 57.7206 (Avg-Acc 57.1500)\n",
            "EPOCH: 157 Validation Results: Acc 57.150 Loss: 1.2261\n",
            "Epoch: [158][0/78]\tTime 0.054 (Avg-Time 0.054)\t Loss 0.9621 (Avg-Loss 0.9621)\tAcc 65.6250 (Avg-Acc 65.6250)\n",
            "Epoch: [158][19/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 0.9818 (Avg-Loss 0.9601)\tAcc 65.0391 (Avg-Acc 67.0605)\n",
            "Epoch: [158][38/78]\tTime 0.047 (Avg-Time 0.049)\t Loss 1.0179 (Avg-Loss 0.9723)\tAcc 64.0625 (Avg-Acc 66.3261)\n",
            "Epoch: [158][57/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 1.0478 (Avg-Loss 0.9842)\tAcc 62.8906 (Avg-Acc 65.9820)\n",
            "Epoch: [158][76/78]\tTime 0.046 (Avg-Time 0.049)\t Loss 0.9488 (Avg-Loss 0.9943)\tAcc 68.1641 (Avg-Acc 65.3916)\n",
            "Epoch: [158][78/78]\tTime 0.010 (Avg-Time 0.049)\t Loss 1.0626 (Avg-Loss 0.9944)\tAcc 54.6875 (Avg-Acc 65.3875)\n",
            "EPOCH: 158 train Results: Acc 65.388 Loss: 0.9944\n",
            "Epoch: [158][0/19]\tTime 0.012 (Avg-Time 0.012)\t Loss 1.1580 (Avg-Loss 1.1580)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [158][19/19]\tTime 0.014 (Avg-Time 0.015)\t Loss 1.2787 (Avg-Loss 1.2333)\tAcc 56.6176 (Avg-Acc 56.7600)\n",
            "EPOCH: 158 Validation Results: Acc 56.760 Loss: 1.2333\n",
            "Epoch: [159][0/78]\tTime 0.051 (Avg-Time 0.051)\t Loss 0.9674 (Avg-Loss 0.9674)\tAcc 64.4531 (Avg-Acc 64.4531)\n",
            "Epoch: [159][19/78]\tTime 0.047 (Avg-Time 0.048)\t Loss 0.8828 (Avg-Loss 0.9500)\tAcc 68.7500 (Avg-Acc 66.5332)\n",
            "Epoch: [159][38/78]\tTime 0.051 (Avg-Time 0.049)\t Loss 0.9621 (Avg-Loss 0.9702)\tAcc 66.6016 (Avg-Acc 66.0106)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import time\n",
        "import seaborn as sns\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools\n",
        "def timer(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        print('Start time: ', time.ctime())\n",
        "        start_time = time.time()  # start time\n",
        "        result = func(*args, **kwargs)  # run\n",
        "        end_time = time.time()  # end time\n",
        "        print('End time: ', time.ctime())\n",
        "        print(f\"{func.__name__} executed in {(end_time - start_time):.4f} seconds\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "def pre_processing(X, mode=None):\n",
        "    if mode == 'min-max':\n",
        "        print('Pre-process: min-max normalization')\n",
        "        min_each_feature = np.min(X, axis=0)\n",
        "        max_each_feature = np.max(X, axis=0)\n",
        "        scale = max_each_feature - min_each_feature\n",
        "        scale[scale == 0] = 1   # To avoid divided by 0\n",
        "        scaled_train = (X - min_each_feature) / scale\n",
        "        return scaled_train\n",
        "\n",
        "    elif mode == 'standardization':\n",
        "        print('Pre-process: standardization')\n",
        "        std_each_feature = np.std(X, axis=0)\n",
        "        mean_each_feature = np.mean(X, axis=0)\n",
        "        std_each_feature[std_each_feature == 0] = 1     # To avoid divided by 0\n",
        "        norm_train = (X - mean_each_feature) / std_each_feature\n",
        "        return norm_train\n",
        "\n",
        "    else:\n",
        "        print('No pre-process')\n",
        "    return X\n",
        "\n",
        "def accuracy(y_hat, y):\n",
        "    preds = y_hat.argmax(axis=1, keepdims=True)\n",
        "    return np.mean(preds == y) * 100\n",
        "\n",
        "def calculate_gain(nonlinearity, param=None):\n",
        "    gains = {\n",
        "        'sigmoid': 1.0,\n",
        "        'tanh': 5.0 / 3,\n",
        "        'relu': math.sqrt(2.0),\n",
        "        'selu': 3.0 / 4\n",
        "    }\n",
        "\n",
        "    if nonlinearity in gains:\n",
        "        return gains[nonlinearity]\n",
        "\n",
        "    if nonlinearity == 'leaky_relu':\n",
        "        negative_slope = param if isinstance(param, (int, float)) and not isinstance(param, bool) else 0.01\n",
        "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
        "\n",
        "    raise ValueError(f\"Unsupported nonlinearity: {nonlinearity}\")\n",
        "\n",
        "def calculate_fan(array):\n",
        "    if array.ndim < 2:\n",
        "        raise ValueError(\"Fan in and fan out require at least 2D tensors\")\n",
        "\n",
        "    fan_in = array.shape[1] * np.prod(array.shape[2:]) if array.ndim > 2 else array.shape[1]\n",
        "    fan_out = array.shape[0] * np.prod(array.shape[2:]) if array.ndim > 2 else array.shape[0]\n",
        "\n",
        "    return fan_in, fan_out\n",
        "\n",
        "def get_correct_fan(array, mode):\n",
        "    mode = mode.lower()\n",
        "    if mode not in {'fan_in', 'fan_out'}:\n",
        "        raise ValueError(\"Mode must be 'fan_in' or 'fan_out'\")\n",
        "\n",
        "    fan_in, fan_out = calculate_fan(array)\n",
        "    return fan_in if mode == 'fan_in' else fan_out\n",
        "\n",
        "def kaiming_normal(array: np.ndarray, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'relu'):\n",
        "    fan = get_correct_fan(array, mode)\n",
        "    gain = calculate_gain(nonlinearity, a)\n",
        "    std = gain / math.sqrt(fan)\n",
        "    return np.random.normal(0, std, array.shape)\n",
        "\n",
        "class Layer(object):\n",
        "    def __init__(self, name, requires_grad=False):\n",
        "        self.name = name\n",
        "        self.requires_grad = requires_grad\n",
        "        self.train = True  # Added train mode flag\n",
        "\n",
        "    def _forward(self, *args):\n",
        "        pass\n",
        "\n",
        "    def _backward(self, *args):\n",
        "        pass\n",
        "\n",
        "    def _fit(self, mode='train'):\n",
        "        self.train = mode == 'train'\n",
        "\n",
        "class ReLU(Layer):\n",
        "    def __init__(self, name, requires_grad=False):\n",
        "        super().__init__(name, requires_grad)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        self.x = x\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def _backward(self, gradient_output):\n",
        "        gradient_output[self.x <= 0] = 0\n",
        "        return gradient_output\n",
        "\n",
        "class FCLayer(Layer):\n",
        "    def __init__(self, name: str, n_in: int, n_out: int, skip_decay=False) -> None:\n",
        "        super().__init__(name, requires_grad=True)\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        W = kaiming_normal(np.array([0] * n_in * n_out).reshape(n_in, n_out), a=math.sqrt(5))\n",
        "        self.W = W\n",
        "        self.b = np.zeros(self.n_out)\n",
        "        self.W_grad = None\n",
        "        self.b_grad = None\n",
        "        self.skip_decay = skip_decay\n",
        "\n",
        "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.x = x\n",
        "        temp = x @ self.W + self.b\n",
        "        return temp\n",
        "\n",
        "    def _backward(self, delta: np.ndarray) -> np.ndarray:\n",
        "        batch_size = delta.shape[0]\n",
        "        self.W_grad = self.x.T @ delta / batch_size\n",
        "        self.b_grad = delta.sum(axis=0) / batch_size\n",
        "        return delta @ self.W.T\n",
        "\n",
        "class Softmax(Layer):\n",
        "    def __init__(self, name, requires_grad=False):\n",
        "        super().__init__(name, requires_grad)\n",
        "\n",
        "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        x_exp = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return x_exp/x_exp.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def _backward(self, gradient_output: np.ndarray) -> np.ndarray:\n",
        "        return gradient_output\n",
        "\n",
        "class CrossEntropy(object):\n",
        "    def __init__(self):\n",
        "        self.softmax = Softmax('softmax')\n",
        "\n",
        "    def __call__(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        self.batch_size = x.shape[0]\n",
        "        self.class_num = x.shape[1]\n",
        "        y_hat = self.softmax._forward(x)\n",
        "        y = self.one_hot_encoding(y)\n",
        "        self.grad = y_hat - y\n",
        "        loss = -1 * (y * np.log(y_hat + 1e-8)).sum() / self.batch_size\n",
        "        return loss\n",
        "\n",
        "    def one_hot_encoding(self, x):\n",
        "        one_hot_encoded = np.zeros((self.batch_size, self.class_num))\n",
        "        one_hot_encoded[np.arange(x.shape[0]), x.flatten()] = 1\n",
        "        return one_hot_encoded\n",
        "\n",
        "class BatchNormalization(Layer):\n",
        "    def __init__(self, name, feature_num, skip_decay=True, epsilon=1e-5, requires_grad=True):\n",
        "        super().__init__(name)\n",
        "        self.epsilon = epsilon\n",
        "        self.requires_grad = requires_grad\n",
        "        self.skip_decay = skip_decay\n",
        "        self.gamma = np.ones(feature_num)\n",
        "        self.beta = np.zeros(feature_num)\n",
        "        self.gamma_grad = None\n",
        "        self.beta_grad = None\n",
        "        self.ema = np.zeros(feature_num)\n",
        "        self.emv = np.zeros(feature_num)\n",
        "\n",
        "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        if self.train:\n",
        "            batch_mean = x.mean(axis=0)\n",
        "            batch_variance = x.var(axis=0)\n",
        "            batch_std = np.sqrt(batch_variance + self.epsilon)\n",
        "            momentum = 0.9\n",
        "            self.ema = momentum * self.ema + (1 - momentum) * batch_mean\n",
        "            self.emv = momentum * self.emv + (1 - momentum) * batch_variance\n",
        "        else:\n",
        "            batch_mean = self.ema\n",
        "            batch_std = np.sqrt(self.emv + self.epsilon)\n",
        "        self.norm = (x - batch_mean) / batch_std\n",
        "        self.gamma_norm = self.gamma / batch_std\n",
        "        return self.gamma * self.norm + self.beta\n",
        "\n",
        "    def _backward(self, gradient_output: np.ndarray) -> np.ndarray:\n",
        "        batch_size = gradient_output.shape[0]\n",
        "        self.gamma_grad = (gradient_output * self.norm).sum(axis=0) / batch_size\n",
        "        self.beta_grad = gradient_output.sum(axis=0) / batch_size\n",
        "        dLdx = self.gamma_norm * (gradient_output - self.norm * self.gamma_grad - self.beta_grad)\n",
        "        return dLdx\n",
        "\n",
        "class Dropout(Layer):\n",
        "    def __init__(self, name, drop_rate=0.5, requires_grad=False):\n",
        "        super().__init__(name, requires_grad)\n",
        "        self.drop_rate = drop_rate\n",
        "        self.fix_value = 1 / (1 - self.drop_rate)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        if self.train:\n",
        "            self.mask = np.random.uniform(0, 1, x.shape) > self.drop_rate\n",
        "            return x * self.mask * self.fix_value\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def _backward(self, grad_output):\n",
        "        if self.train:\n",
        "            return grad_output * self.mask\n",
        "        else:\n",
        "            return grad_output\n",
        "\n",
        "class MLP():\n",
        "    def __init__(self, hidden_units=[256, 128], dropout_rates=[0.3, 0.3]):\n",
        "        self.layers = []\n",
        "        # Input layer\n",
        "        self.layers.append(FCLayer('fc1', n_in=128, n_out=hidden_units[0]))\n",
        "        self.layers.append(BatchNormalization(\"batchnorm1\", feature_num=hidden_units[0]))\n",
        "        self.layers.append(Dropout('dropout1', drop_rate=dropout_rates[0]))\n",
        "        self.layers.append(ReLU('relu1'))\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(hidden_units)):\n",
        "            self.layers.append(FCLayer(f'fc{i+1}', n_in=hidden_units[i-1], n_out=hidden_units[i]))\n",
        "            self.layers.append(BatchNormalization(f\"batchnorm{i+1}\", feature_num=hidden_units[i]))\n",
        "            self.layers.append(Dropout(f'dropout{i+1}', drop_rate=dropout_rates[i]))\n",
        "            self.layers.append(ReLU(f'relu{i+1}'))\n",
        "\n",
        "        # Output layer\n",
        "        self.layers.append(FCLayer(f'fc{len(hidden_units)+1}', n_in=hidden_units[-1], n_out=10))\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.parameters = []\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, \"W\"):\n",
        "                self.parameters.append([layer.W, layer.W_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"b\"):\n",
        "                self.parameters.append([layer.b, layer.b_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"gamma\"):\n",
        "                self.parameters.append([layer.gamma, layer.gamma_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"beta\"):\n",
        "                self.parameters.append([layer.beta, layer.beta_grad, layer.skip_decay])\n",
        "\n",
        "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        for layer in self.layers:\n",
        "            x = layer._forward(x)\n",
        "        return x\n",
        "\n",
        "    def _backward(self, gradient_output: np.ndarray) -> np.ndarray:\n",
        "        for layer in self.layers[::-1]:\n",
        "            gradient_output = layer._backward(gradient_output)\n",
        "        return gradient_output\n",
        "\n",
        "    def _fit(self, mode='train'):\n",
        "        for layer in self.layers:\n",
        "            layer._fit(mode)\n",
        "\n",
        "    def _predict(self, x: np.ndarray) -> np.ndarray:\n",
        "        self._fit('eval')\n",
        "        y_hat = self._forward(x)\n",
        "        return y_hat\n",
        "\n",
        "class AdamW(object):\n",
        "    def __init__(self, model, lr=1e-3, decoupled_weight_decay=0, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.decoupled_weight_decay = decoupled_weight_decay\n",
        "        self.epsilon = epsilon\n",
        "        self.t = 0\n",
        "        self.m = [np.zeros(p[0].shape) for p in self.get_parameters()]\n",
        "        self.v = [np.zeros(p[0].shape) for p in self.get_parameters()]\n",
        "\n",
        "    def get_parameters(self):\n",
        "        parameters = []\n",
        "        for layer in self.model.layers:\n",
        "            if hasattr(layer, \"W\"):\n",
        "                parameters.append([layer.W, layer.W_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"b\"):\n",
        "                parameters.append([layer.b, layer.b_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"gamma\"):\n",
        "                parameters.append([layer.gamma, layer.gamma_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"beta\"):\n",
        "                parameters.append([layer.beta, layer.beta_grad, layer.skip_decay])\n",
        "        return parameters\n",
        "\n",
        "    def step(self):\n",
        "        parameters = self.get_parameters()\n",
        "        for i, (param_list, m, v) in enumerate(zip(parameters, self.m, self.v)):\n",
        "            param, param_grad, skip_decay = param_list\n",
        "            self.t += 1\n",
        "            m = self.beta1 * m + (1 - self.beta1) * param_grad\n",
        "            v = self.beta2 * v + (1 - self.beta2) * np.power(param_grad, 2)\n",
        "            self.m[i] = m\n",
        "            self.v[i] = v\n",
        "            m_hat = m / (1 - np.power(self.beta1, self.t))\n",
        "            v_hat = v / (1 - np.power(self.beta2, self.t))\n",
        "\n",
        "            update = self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "            if not skip_decay:\n",
        "                param -= update\n",
        "                param *= (1 - self.lr * self.decoupled_weight_decay)\n",
        "            else:\n",
        "                param -= update\n",
        "\n",
        "class SGDMomentum:\n",
        "    def __init__(self, model, lr=0.01, momentum=0.9, weight_decay=0.0001):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.weight_decay = weight_decay\n",
        "        self.v = [np.zeros(param[0].shape) for param in self.model.parameters]\n",
        "\n",
        "    def get_parameters(self):\n",
        "        parameters = []\n",
        "        for layer in self.model.layers:\n",
        "            if hasattr(layer, \"W\"):\n",
        "                parameters.append([layer.W, layer.W_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"b\"):\n",
        "                parameters.append([layer.b, layer.b_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"gamma\"):\n",
        "                parameters.append([layer.gamma, layer.gamma_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"beta\"):\n",
        "                parameters.append([layer.beta, layer.beta_grad, layer.skip_decay])\n",
        "        return parameters\n",
        "\n",
        "    def step(self):\n",
        "        self.parameters = self.get_parameters()\n",
        "        for i, (v, param_list) in enumerate(zip(self.v, self.parameters)):\n",
        "            param, param_grad, skip_decay = param_list\n",
        "            if param_grad is not None:\n",
        "                if not skip_decay:\n",
        "                    param -= self.weight_decay * param\n",
        "                v[:] = self.momentum * v + self.lr * param_grad\n",
        "                self.v[i] = v\n",
        "                param -= v\n",
        "\n",
        "class AverageMeterics(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, config, model=None, train_loader=None, valid_loader=None):\n",
        "        self.config = config\n",
        "        self.epochs = self.config['epoch']\n",
        "        self.lr = self.config['lr']\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.valid_loader = valid_loader\n",
        "        self.print_freq = self.config['print_freq']\n",
        "        self.train_accuracy = []\n",
        "        self.valid_accuracy = []\n",
        "        self.train_loss = []\n",
        "        self.valid_loss = []\n",
        "        self.criterion = CrossEntropy()\n",
        "\n",
        "        if self.config['optimizer'] == 'sgd':\n",
        "            self.optimizer = SGDMomentum(self.model, self.lr, self.config['momentum'],\n",
        "                                       self.config['weight_decay'])\n",
        "        elif self.config['optimizer'] == 'adamw':\n",
        "            self.optimizer = AdamW(self.model, self.lr, self.config['weight_decay'])\n",
        "\n",
        "    @timer\n",
        "    def train(self):\n",
        "        best_accuracy = 0\n",
        "        for epoch in range(self.epochs):\n",
        "            print('current lr {:.5e}'.format(self.optimizer.lr))\n",
        "            self.train_per_epoch(epoch)\n",
        "            acc1 = self.validate(epoch)\n",
        "            best_accuracy = max(acc1, best_accuracy)\n",
        "            output_best = f'Best Accuracy: {best_accuracy:.4f}\\n'\n",
        "            print(output_best)\n",
        "\n",
        "    def train_per_epoch(self, epoch):\n",
        "        batch_time = AverageMeterics()\n",
        "        losses = AverageMeterics()\n",
        "        best_acc = AverageMeterics()\n",
        "        self.model._fit()\n",
        "        end_time = time.time()\n",
        "\n",
        "        for i, (X, y) in enumerate(self.train_loader):\n",
        "            y_hat = self.model._forward(X)\n",
        "            loss = self.criterion(y_hat, y)\n",
        "\n",
        "            self.model._backward(self.criterion.grad)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            acc = accuracy(y_hat, y)\n",
        "            losses.update(loss, X.shape[0])\n",
        "            best_acc.update(acc, X.shape[0])\n",
        "\n",
        "            batch_time.update(time.time() - end_time)\n",
        "            end_time = time.time()\n",
        "\n",
        "            if (i % self.print_freq == 0) or (i == len(self.train_loader)-1):\n",
        "                print(f'Epoch: [{epoch + 1}][{i}/{len(self.train_loader) - 1}]\\tTime {batch_time.val:.3f} (Avg-Time {batch_time.avg:.3f})\\t '\n",
        "                      f'Loss {losses.val:.4f} (Avg-Loss {losses.avg:.4f})\\t'\n",
        "                      f'Acc {best_acc.val:.4f} (Avg-Acc {best_acc.avg:.4f})')\n",
        "\n",
        "        print(f'EPOCH: {epoch+1} train Results: Acc {best_acc.avg:.3f} Loss: {losses.avg:.4f}')\n",
        "        self.train_loss.append(losses.avg)\n",
        "        self.train_accuracy.append(best_acc.avg)\n",
        "\n",
        "    def validate(self, epoch):\n",
        "        batch_time = AverageMeterics()\n",
        "        losses = AverageMeterics()\n",
        "        best_acc = AverageMeterics()\n",
        "        self.model._fit(mode='eval')\n",
        "        end = time.time()\n",
        "\n",
        "        for i, (X, y) in enumerate(self.valid_loader):\n",
        "            y_hat = self.model._forward(X)\n",
        "            loss = self.criterion(y_hat, y)\n",
        "            acc = accuracy(y_hat, y)\n",
        "            losses.update(loss, X.shape[0])\n",
        "            best_acc.update(acc, X.shape[0])\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if (i % self.print_freq == 0) or (i == len(self.valid_loader) - 1):\n",
        "                print(f'Epoch: [{epoch + 1}][{i}/{len(self.valid_loader) - 1}]\\tTime {batch_time.val:.3f} (Avg-Time {batch_time.avg:.3f})\\t '\n",
        "                      f'Loss {losses.val:.4f} (Avg-Loss {losses.avg:.4f})\\t'\n",
        "                      f'Acc {best_acc.val:.4f} (Avg-Acc {best_acc.avg:.4f})')\n",
        "\n",
        "        print(f'EPOCH: {epoch+1} Validation Results: Acc {best_acc.avg:.3f} Loss: {losses.avg:.4f}')\n",
        "        self.valid_loss.append(losses.avg)\n",
        "        self.valid_accuracy.append(best_acc.avg)\n",
        "        return best_acc.avg\n",
        "\n",
        "class Dataloader(object):\n",
        "    def __init__(self, X, y, batch_size, shuffle=True, seed=None):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.index = np.arange(X.shape[0])\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            if self.seed is not None:\n",
        "                np.random.seed(self.seed)\n",
        "            np.random.shuffle(self.index)\n",
        "        self.n = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.n >= len(self.index):\n",
        "            raise StopIteration\n",
        "        index = self.index[self.n:self.n + self.batch_size]\n",
        "        batch_X = self.X[index]\n",
        "        batch_y = self.y[index]\n",
        "        self.n += self.batch_size\n",
        "        return batch_X, batch_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.index) + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "def hyperparameter_tuning():\n",
        "    # Define hyperparameter grid for exhaustive search\n",
        "    param_grid = {\n",
        "        'lr': [0.005, 0.01, 0.02],\n",
        "        'batch_size': [512, 1024, 2048],\n",
        "        'hidden_units': [\n",
        "            [256, 128],  # Original best\n",
        "            [512, 256],  # Larger network\n",
        "            [256, 256],  # Equal size layers\n",
        "            [128, 64]    # Smaller network\n",
        "        ],\n",
        "        'dropout_rates': [\n",
        "            [0.3, 0.3],  # Original\n",
        "            [0.2, 0.2],  # Less dropout\n",
        "            [0.4, 0.4]   # More dropout\n",
        "        ],\n",
        "        'pre-process': [None, 'standardization'], #'min-max'\n",
        "        'weight_decay': [5e-4, 1e-3],\n",
        "        'optimizer': ['sgd', 'adamw']\n",
        "    }\n",
        "\n",
        "    # Generate all combinations\n",
        "    keys, values = zip(*param_grid.items())\n",
        "    configs = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    # Limit to reasonable number of combinations (optional)\n",
        "    #configs = configs[:20]  # Test first 20 combinations for demo\n",
        "\n",
        "    best_acc = 0\n",
        "    best_config = {}\n",
        "    results = []\n",
        "\n",
        "    # Load data\n",
        "    dir_path = ''\n",
        "    train_data = np.load(dir_path + 'train_data.npy')\n",
        "    train_label = np.load(dir_path + 'train_label.npy')\n",
        "    test_X = np.load(dir_path + 'test_data.npy')\n",
        "    test_label = np.load(dir_path + 'test_label.npy')\n",
        "\n",
        "    # Split validation set\n",
        "    train_X, val_X, train_y, val_y = train_test_split(train_data, train_label,\n",
        "                                                     test_size=0.2, random_state=5329)\n",
        "\n",
        "    for i, cfg in enumerate(configs):\n",
        "        print(f\"\\n\\n=== Testing Configuration {i+1}/{len(configs)} ===\")\n",
        "        print(\"Current Config:\", cfg)\n",
        "\n",
        "        # Set default values for missing params\n",
        "        cfg.setdefault('epoch', 200)  # Reduced epochs for faster grid search\n",
        "        cfg.setdefault('momentum', 0.9)\n",
        "        cfg.setdefault('seed', 0)\n",
        "\n",
        "        # Data preprocessing\n",
        "        train_X_processed = pre_processing(train_X, cfg['pre-process'])\n",
        "        val_X_processed = pre_processing(val_X, cfg['pre-process'])\n",
        "        test_X_processed = pre_processing(test_X, cfg['pre-process'])\n",
        "\n",
        "        # Create dataloaders\n",
        "        train_dataloader = Dataloader(train_X_processed, train_y, cfg['batch_size'], shuffle=True)\n",
        "        val_dataloader = Dataloader(val_X_processed, val_y, cfg['batch_size'], shuffle=False)\n",
        "        test_dataloader = Dataloader(test_X_processed, test_label, cfg['batch_size'], shuffle=False)\n",
        "\n",
        "        # Training config\n",
        "        current_config = {\n",
        "            'lr': cfg['lr'],\n",
        "            'batch_size': cfg['batch_size'],\n",
        "            'momentum': cfg['momentum'],\n",
        "            'weight_decay': cfg['weight_decay'],\n",
        "            'seed': cfg['seed'],\n",
        "            'epoch': cfg['epoch'],\n",
        "            'optimizer': cfg['optimizer'],\n",
        "            'scheduler': None,\n",
        "            'pre-process': cfg['pre-process'],\n",
        "            'print_freq': max(1, 50000 // cfg['batch_size'] // 5)\n",
        "        }\n",
        "\n",
        "        # Initialize model\n",
        "        model = MLP(hidden_units=cfg['hidden_units'],\n",
        "                   dropout_rates=cfg['dropout_rates'])\n",
        "\n",
        "        # Train with reduced verbosity\n",
        "        trainer = Trainer(current_config, model, train_dataloader, val_dataloader)\n",
        "\n",
        "        # Simple training without detailed prints\n",
        "        best_val_acc = 0\n",
        "        for epoch in range(current_config['epoch']):\n",
        "            trainer.train_per_epoch(epoch)\n",
        "            val_acc = trainer.validate(epoch)\n",
        "            best_val_acc = max(val_acc, best_val_acc)\n",
        "\n",
        "        # Final test evaluation\n",
        "        model._fit(mode='eval')\n",
        "        y_hat = model._predict(test_dataloader.X)\n",
        "        test_acc = accuracy(y_hat, test_dataloader.y)\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            'config': cfg,\n",
        "            'val_accuracy': best_val_acc,\n",
        "            'test_accuracy': test_acc\n",
        "        })\n",
        "\n",
        "        # Update best config\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            best_config = cfg.copy()\n",
        "            best_config['test_accuracy'] = test_acc\n",
        "\n",
        "        print(f\"\\nTest Accuracy: {test_acc:.2f}% (Best so far: {best_acc:.2f}%)\")\n",
        "\n",
        "    # Results analysis\n",
        "    print(\"\\n\\n=== Grid Search Results ===\")\n",
        "    print(f\"Best Accuracy: {best_acc:.2f}%\")\n",
        "    print(\"Best Configuration:\")\n",
        "    for k, v in best_config.items():\n",
        "        print(f\"{k:>15}: {v}\")\n",
        "\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    print(results_df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    hyperparameter_tuning()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import time\n",
        "import seaborn as sns\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def timer(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        print('Start time: ', time.ctime())\n",
        "        start_time = time.time()  # start time\n",
        "        result = func(*args, **kwargs)  # run\n",
        "        end_time = time.time()  # end time\n",
        "        print('End time: ', time.ctime())\n",
        "        print(f\"{func.__name__} executed in {(end_time - start_time):.4f} seconds\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "def pre_processing(X, mode=None):\n",
        "    if mode == 'min-max':\n",
        "        print('Pre-process: min-max normalization')\n",
        "        min_each_feature = np.min(X, axis=0)\n",
        "        max_each_feature = np.max(X, axis=0)\n",
        "        scale = max_each_feature - min_each_feature\n",
        "        scale[scale == 0] = 1   # To avoid divided by 0\n",
        "        scaled_train = (X - min_each_feature) / scale\n",
        "        return scaled_train\n",
        "\n",
        "    elif mode == 'standardization':\n",
        "        print('Pre-process: standardization')\n",
        "        std_each_feature = np.std(X, axis=0)\n",
        "        mean_each_feature = np.mean(X, axis=0)\n",
        "        std_each_feature[std_each_feature == 0] = 1     # To avoid divided by 0\n",
        "        norm_train = (X - mean_each_feature) / std_each_feature\n",
        "        return norm_train\n",
        "\n",
        "    else:\n",
        "        print('No pre-process')\n",
        "    return X\n",
        "\n",
        "def accuracy(y_hat, y):\n",
        "    preds = y_hat.argmax(axis=1, keepdims=True)\n",
        "    return np.mean(preds == y) * 100\n",
        "\n",
        "def calculate_gain(nonlinearity, param=None):\n",
        "    gains = {\n",
        "        'sigmoid': 1.0,\n",
        "        'tanh': 5.0 / 3,\n",
        "        'relu': math.sqrt(2.0),\n",
        "        'selu': 3.0 / 4\n",
        "    }\n",
        "\n",
        "    if nonlinearity in gains:\n",
        "        return gains[nonlinearity]\n",
        "\n",
        "    if nonlinearity == 'leaky_relu':\n",
        "        negative_slope = param if isinstance(param, (int, float)) and not isinstance(param, bool) else 0.01\n",
        "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
        "\n",
        "    raise ValueError(f\"Unsupported nonlinearity: {nonlinearity}\")\n",
        "\n",
        "def calculate_fan(array):\n",
        "    if array.ndim < 2:\n",
        "        raise ValueError(\"Fan in and fan out require at least 2D tensors\")\n",
        "\n",
        "    fan_in = array.shape[1] * np.prod(array.shape[2:]) if array.ndim > 2 else array.shape[1]\n",
        "    fan_out = array.shape[0] * np.prod(array.shape[2:]) if array.ndim > 2 else array.shape[0]\n",
        "\n",
        "    return fan_in, fan_out\n",
        "\n",
        "def get_correct_fan(array, mode):\n",
        "    mode = mode.lower()\n",
        "    if mode not in {'fan_in', 'fan_out'}:\n",
        "        raise ValueError(\"Mode must be 'fan_in' or 'fan_out'\")\n",
        "\n",
        "    fan_in, fan_out = calculate_fan(array)\n",
        "    return fan_in if mode == 'fan_in' else fan_out\n",
        "\n",
        "def kaiming_normal(array: np.ndarray, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'relu'):\n",
        "    fan = get_correct_fan(array, mode)\n",
        "    gain = calculate_gain(nonlinearity, a)\n",
        "    std = gain / math.sqrt(fan)\n",
        "    return np.random.normal(0, std, array.shape)\n",
        "\n",
        "class Layer(object):\n",
        "    def __init__(self, name, requires_grad=False):\n",
        "        self.name = name\n",
        "        self.requires_grad = requires_grad\n",
        "        self.train = True  # Added train mode flag\n",
        "\n",
        "    def _forward(self, *args):\n",
        "        pass\n",
        "\n",
        "    def _backward(self, *args):\n",
        "        pass\n",
        "\n",
        "    def _fit(self, mode='train'):\n",
        "        self.train = mode == 'train'\n",
        "\n",
        "class ReLU(Layer):\n",
        "    def __init__(self, name, requires_grad=False):\n",
        "        super().__init__(name, requires_grad)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        self.x = x\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def _backward(self, gradient_output):\n",
        "        gradient_output[self.x <= 0] = 0\n",
        "        return gradient_output\n",
        "\n",
        "class FCLayer(Layer):\n",
        "    def __init__(self, name: str, n_in: int, n_out: int, skip_decay=False) -> None:\n",
        "        super().__init__(name, requires_grad=True)\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        W = kaiming_normal(np.array([0] * n_in * n_out).reshape(n_in, n_out), a=math.sqrt(5))\n",
        "        self.W = W\n",
        "        self.b = np.zeros(self.n_out)\n",
        "        self.W_grad = None\n",
        "        self.b_grad = None\n",
        "        self.skip_decay = skip_decay\n",
        "\n",
        "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.x = x\n",
        "        temp = x @ self.W + self.b\n",
        "        return temp\n",
        "\n",
        "    def _backward(self, delta: np.ndarray) -> np.ndarray:\n",
        "        batch_size = delta.shape[0]\n",
        "        self.W_grad = self.x.T @ delta / batch_size\n",
        "        self.b_grad = delta.sum(axis=0) / batch_size\n",
        "        return delta @ self.W.T\n",
        "\n",
        "class Softmax(Layer):\n",
        "    def __init__(self, name, requires_grad=False):\n",
        "        super().__init__(name, requires_grad)\n",
        "\n",
        "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        x_exp = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return x_exp/x_exp.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def _backward(self, gradient_output: np.ndarray) -> np.ndarray:\n",
        "        return gradient_output\n",
        "\n",
        "class CrossEntropy(object):\n",
        "    def __init__(self):\n",
        "        self.softmax = Softmax('softmax')\n",
        "\n",
        "    def __call__(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        self.batch_size = x.shape[0]\n",
        "        self.class_num = x.shape[1]\n",
        "        y_hat = self.softmax._forward(x)\n",
        "        y = self.one_hot_encoding(y)\n",
        "        self.grad = y_hat - y\n",
        "        loss = -1 * (y * np.log(y_hat + 1e-8)).sum() / self.batch_size\n",
        "        return loss\n",
        "\n",
        "    def one_hot_encoding(self, x):\n",
        "        one_hot_encoded = np.zeros((self.batch_size, self.class_num))\n",
        "        one_hot_encoded[np.arange(x.shape[0]), x.flatten()] = 1\n",
        "        return one_hot_encoded\n",
        "\n",
        "class BatchNormalization(Layer):\n",
        "    def __init__(self, name, feature_num, skip_decay=True, epsilon=1e-5, requires_grad=True):\n",
        "        super().__init__(name)\n",
        "        self.epsilon = epsilon\n",
        "        self.requires_grad = requires_grad\n",
        "        self.skip_decay = skip_decay\n",
        "        self.gamma = np.ones(feature_num)\n",
        "        self.beta = np.zeros(feature_num)\n",
        "        self.gamma_grad = None\n",
        "        self.beta_grad = None\n",
        "        self.ema = np.zeros(feature_num)\n",
        "        self.emv = np.zeros(feature_num)\n",
        "\n",
        "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        if self.train:\n",
        "            batch_mean = x.mean(axis=0)\n",
        "            batch_variance = x.var(axis=0)\n",
        "            batch_std = np.sqrt(batch_variance + self.epsilon)\n",
        "            momentum = 0.9\n",
        "            self.ema = momentum * self.ema + (1 - momentum) * batch_mean\n",
        "            self.emv = momentum * self.emv + (1 - momentum) * batch_variance\n",
        "        else:\n",
        "            batch_mean = self.ema\n",
        "            batch_std = np.sqrt(self.emv + self.epsilon)\n",
        "        self.norm = (x - batch_mean) / batch_std\n",
        "        self.gamma_norm = self.gamma / batch_std\n",
        "        return self.gamma * self.norm + self.beta\n",
        "\n",
        "    def _backward(self, gradient_output: np.ndarray) -> np.ndarray:\n",
        "        batch_size = gradient_output.shape[0]\n",
        "        self.gamma_grad = (gradient_output * self.norm).sum(axis=0) / batch_size\n",
        "        self.beta_grad = gradient_output.sum(axis=0) / batch_size\n",
        "        dLdx = self.gamma_norm * (gradient_output - self.norm * self.gamma_grad - self.beta_grad)\n",
        "        return dLdx\n",
        "\n",
        "class Dropout(Layer):\n",
        "    def __init__(self, name, drop_rate=0.5, requires_grad=False):\n",
        "        super().__init__(name, requires_grad)\n",
        "        self.drop_rate = drop_rate\n",
        "        self.fix_value = 1 / (1 - self.drop_rate)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        if self.train:\n",
        "            self.mask = np.random.uniform(0, 1, x.shape) > self.drop_rate\n",
        "            return x * self.mask * self.fix_value\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def _backward(self, grad_output):\n",
        "        if self.train:\n",
        "            return grad_output * self.mask\n",
        "        else:\n",
        "            return grad_output\n",
        "\n",
        "class MLP():\n",
        "    def __init__(self, hidden_units=[256, 128], dropout_rates=[0.3, 0.3]):\n",
        "        self.layers = []\n",
        "        # Input layer\n",
        "        self.layers.append(FCLayer('fc1', n_in=128, n_out=hidden_units[0]))\n",
        "        self.layers.append(BatchNormalization(\"batchnorm1\", feature_num=hidden_units[0]))\n",
        "        self.layers.append(Dropout('dropout1', drop_rate=dropout_rates[0]))\n",
        "        self.layers.append(ReLU('relu1'))\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(hidden_units)):\n",
        "            self.layers.append(FCLayer(f'fc{i+1}', n_in=hidden_units[i-1], n_out=hidden_units[i]))\n",
        "            self.layers.append(BatchNormalization(f\"batchnorm{i+1}\", feature_num=hidden_units[i]))\n",
        "            self.layers.append(Dropout(f'dropout{i+1}', drop_rate=dropout_rates[i]))\n",
        "            self.layers.append(ReLU(f'relu{i+1}'))\n",
        "\n",
        "        # Output layer\n",
        "        self.layers.append(FCLayer(f'fc{len(hidden_units)+1}', n_in=hidden_units[-1], n_out=10))\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.parameters = []\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, \"W\"):\n",
        "                self.parameters.append([layer.W, layer.W_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"b\"):\n",
        "                self.parameters.append([layer.b, layer.b_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"gamma\"):\n",
        "                self.parameters.append([layer.gamma, layer.gamma_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"beta\"):\n",
        "                self.parameters.append([layer.beta, layer.beta_grad, layer.skip_decay])\n",
        "\n",
        "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        for layer in self.layers:\n",
        "            x = layer._forward(x)\n",
        "        return x\n",
        "\n",
        "    def _backward(self, gradient_output: np.ndarray) -> np.ndarray:\n",
        "        for layer in self.layers[::-1]:\n",
        "            gradient_output = layer._backward(gradient_output)\n",
        "        return gradient_output\n",
        "\n",
        "    def _fit(self, mode='train'):\n",
        "        for layer in self.layers:\n",
        "            layer._fit(mode)\n",
        "\n",
        "    def _predict(self, x: np.ndarray) -> np.ndarray:\n",
        "        self._fit('eval')\n",
        "        y_hat = self._forward(x)\n",
        "        return y_hat\n",
        "\n",
        "class AdamW(object):\n",
        "    def __init__(self, model, lr=1e-3, decoupled_weight_decay=0, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.decoupled_weight_decay = decoupled_weight_decay\n",
        "        self.epsilon = epsilon\n",
        "        self.t = 0\n",
        "        self.m = [np.zeros(p[0].shape) for p in self.get_parameters()]\n",
        "        self.v = [np.zeros(p[0].shape) for p in self.get_parameters()]\n",
        "\n",
        "    def get_parameters(self):\n",
        "        parameters = []\n",
        "        for layer in self.model.layers:\n",
        "            if hasattr(layer, \"W\"):\n",
        "                parameters.append([layer.W, layer.W_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"b\"):\n",
        "                parameters.append([layer.b, layer.b_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"gamma\"):\n",
        "                parameters.append([layer.gamma, layer.gamma_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"beta\"):\n",
        "                parameters.append([layer.beta, layer.beta_grad, layer.skip_decay])\n",
        "        return parameters\n",
        "\n",
        "    def step(self):\n",
        "        parameters = self.get_parameters()\n",
        "        for i, (param_list, m, v) in enumerate(zip(parameters, self.m, self.v)):\n",
        "            param, param_grad, skip_decay = param_list\n",
        "            self.t += 1\n",
        "            m = self.beta1 * m + (1 - self.beta1) * param_grad\n",
        "            v = self.beta2 * v + (1 - self.beta2) * np.power(param_grad, 2)\n",
        "            self.m[i] = m\n",
        "            self.v[i] = v\n",
        "            m_hat = m / (1 - np.power(self.beta1, self.t))\n",
        "            v_hat = v / (1 - np.power(self.beta2, self.t))\n",
        "\n",
        "            update = self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "            if not skip_decay:\n",
        "                param -= update\n",
        "                param *= (1 - self.lr * self.decoupled_weight_decay)\n",
        "            else:\n",
        "                param -= update\n",
        "\n",
        "class SGDMomentum:\n",
        "    def __init__(self, model, lr=0.01, momentum=0.9, weight_decay=0.0001):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.weight_decay = weight_decay\n",
        "        self.v = [np.zeros(param[0].shape) for param in self.model.parameters]\n",
        "\n",
        "    def get_parameters(self):\n",
        "        parameters = []\n",
        "        for layer in self.model.layers:\n",
        "            if hasattr(layer, \"W\"):\n",
        "                parameters.append([layer.W, layer.W_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"b\"):\n",
        "                parameters.append([layer.b, layer.b_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"gamma\"):\n",
        "                parameters.append([layer.gamma, layer.gamma_grad, layer.skip_decay])\n",
        "            if hasattr(layer, \"beta\"):\n",
        "                parameters.append([layer.beta, layer.beta_grad, layer.skip_decay])\n",
        "        return parameters\n",
        "\n",
        "    def step(self):\n",
        "        self.parameters = self.get_parameters()\n",
        "        for i, (v, param_list) in enumerate(zip(self.v, self.parameters)):\n",
        "            param, param_grad, skip_decay = param_list\n",
        "            if param_grad is not None:\n",
        "                if not skip_decay:\n",
        "                    param -= self.weight_decay * param\n",
        "                v[:] = self.momentum * v + self.lr * param_grad\n",
        "                self.v[i] = v\n",
        "                param -= v\n",
        "\n",
        "class AverageMeterics(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, config, model=None, train_loader=None, valid_loader=None):\n",
        "        self.config = config\n",
        "        self.epochs = self.config['epoch']\n",
        "        self.lr = self.config['lr']\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.valid_loader = valid_loader\n",
        "        self.print_freq = self.config['print_freq']\n",
        "        self.train_accuracy = []\n",
        "        self.valid_accuracy = []\n",
        "        self.train_loss = []\n",
        "        self.valid_loss = []\n",
        "        self.criterion = CrossEntropy()\n",
        "\n",
        "        if self.config['optimizer'] == 'sgd':\n",
        "            self.optimizer = SGDMomentum(self.model, self.lr, self.config['momentum'],\n",
        "                                       self.config['weight_decay'])\n",
        "        elif self.config['optimizer'] == 'adamw':\n",
        "            self.optimizer = AdamW(self.model, self.lr, self.config['weight_decay'])\n",
        "\n",
        "    @timer\n",
        "    def train(self):\n",
        "        best_accuracy = 0\n",
        "        for epoch in range(self.epochs):\n",
        "            print('current lr {:.5e}'.format(self.optimizer.lr))\n",
        "            self.train_per_epoch(epoch)\n",
        "            acc1 = self.validate(epoch)\n",
        "            best_accuracy = max(acc1, best_accuracy)\n",
        "            output_best = f'Best Accuracy: {best_accuracy:.4f}\\n'\n",
        "            print(output_best)\n",
        "\n",
        "    def train_per_epoch(self, epoch):\n",
        "        batch_time = AverageMeterics()\n",
        "        losses = AverageMeterics()\n",
        "        best_acc = AverageMeterics()\n",
        "        self.model._fit()\n",
        "        end_time = time.time()\n",
        "\n",
        "        for i, (X, y) in enumerate(self.train_loader):\n",
        "            y_hat = self.model._forward(X)\n",
        "            loss = self.criterion(y_hat, y)\n",
        "\n",
        "            self.model._backward(self.criterion.grad)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            acc = accuracy(y_hat, y)\n",
        "            losses.update(loss, X.shape[0])\n",
        "            best_acc.update(acc, X.shape[0])\n",
        "\n",
        "            batch_time.update(time.time() - end_time)\n",
        "            end_time = time.time()\n",
        "\n",
        "            if (i % self.print_freq == 0) or (i == len(self.train_loader)-1):\n",
        "                print(f'Epoch: [{epoch + 1}][{i}/{len(self.train_loader) - 1}]\\tTime {batch_time.val:.3f} (Avg-Time {batch_time.avg:.3f})\\t '\n",
        "                      f'Loss {losses.val:.4f} (Avg-Loss {losses.avg:.4f})\\t'\n",
        "                      f'Acc {best_acc.val:.4f} (Avg-Acc {best_acc.avg:.4f})')\n",
        "\n",
        "        print(f'EPOCH: {epoch+1} train Results: Acc {best_acc.avg:.3f} Loss: {losses.avg:.4f}')\n",
        "        self.train_loss.append(losses.avg)\n",
        "        self.train_accuracy.append(best_acc.avg)\n",
        "\n",
        "    def validate(self, epoch):\n",
        "        batch_time = AverageMeterics()\n",
        "        losses = AverageMeterics()\n",
        "        best_acc = AverageMeterics()\n",
        "        self.model._fit(mode='eval')\n",
        "        end = time.time()\n",
        "\n",
        "        for i, (X, y) in enumerate(self.valid_loader):\n",
        "            y_hat = self.model._forward(X)\n",
        "            loss = self.criterion(y_hat, y)\n",
        "            acc = accuracy(y_hat, y)\n",
        "            losses.update(loss, X.shape[0])\n",
        "            best_acc.update(acc, X.shape[0])\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if (i % self.print_freq == 0) or (i == len(self.valid_loader) - 1):\n",
        "                print(f'Epoch: [{epoch + 1}][{i}/{len(self.valid_loader) - 1}]\\tTime {batch_time.val:.3f} (Avg-Time {batch_time.avg:.3f})\\t '\n",
        "                      f'Loss {losses.val:.4f} (Avg-Loss {losses.avg:.4f})\\t'\n",
        "                      f'Acc {best_acc.val:.4f} (Avg-Acc {best_acc.avg:.4f})')\n",
        "\n",
        "        print(f'EPOCH: {epoch+1} Validation Results: Acc {best_acc.avg:.3f} Loss: {losses.avg:.4f}')\n",
        "        self.valid_loss.append(losses.avg)\n",
        "        self.valid_accuracy.append(best_acc.avg)\n",
        "        return best_acc.avg\n",
        "\n",
        "class Dataloader(object):\n",
        "    def __init__(self, X, y, batch_size, shuffle=True, seed=None):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.index = np.arange(X.shape[0])\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            if self.seed is not None:\n",
        "                np.random.seed(self.seed)\n",
        "            np.random.shuffle(self.index)\n",
        "        self.n = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.n >= len(self.index):\n",
        "            raise StopIteration\n",
        "        index = self.index[self.n:self.n + self.batch_size]\n",
        "        batch_X = self.X[index]\n",
        "        batch_y = self.y[index]\n",
        "        self.n += self.batch_size\n",
        "        return batch_X, batch_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.index) + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "def hyperparameter_tuning():\n",
        "    # Define hyperparameter search space\n",
        "    configs = [\n",
        "        {\n",
        "            'lr': 0.005,\n",
        "            'batch_size': 512,\n",
        "            'hidden_units': [256, 128],\n",
        "            'dropout_rates': [0.3, 0.3],\n",
        "            'pre-process': None,\n",
        "            'epoch': 200,\n",
        "            'weight_decay': 5e-4,\n",
        "            'momentum': 0.9,\n",
        "            'optimizer': 'sgd'\n",
        "        },\n",
        "        {\n",
        "            'lr': 0.01,\n",
        "            'batch_size': 1024,\n",
        "            'hidden_units': [256, 128],\n",
        "            'dropout_rates': [0.25, 0.25],\n",
        "            'pre-process': 'standardization',\n",
        "            'epoch': 200,\n",
        "            'weight_decay': 5e-4,\n",
        "            'momentum': 0.9,\n",
        "            'optimizer': 'sgd'\n",
        "        },\n",
        "        {\n",
        "            'lr': 0.02,\n",
        "            'batch_size': 2048,\n",
        "            'hidden_units': [512, 256],\n",
        "            'dropout_rates': [0.4, 0.4],\n",
        "            'pre-process': None,\n",
        "            'epoch': 200,\n",
        "            'weight_decay': 5e-4,\n",
        "            'momentum': 0.9,\n",
        "            'optimizer': 'sgd'\n",
        "        },\n",
        "        {\n",
        "            'lr': 0.01,\n",
        "            'batch_size': 1024,\n",
        "            'hidden_units': [256, 256],\n",
        "            'dropout_rates': [0.3, 0.3],\n",
        "            'pre-process': None,\n",
        "            'epoch': 200,\n",
        "            'weight_decay': 5e-4,\n",
        "            'momentum': 0.9,\n",
        "            'optimizer': 'sgd'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    best_acc = 0\n",
        "    best_config = {}\n",
        "\n",
        "    # Load data\n",
        "    dir_path = ''\n",
        "    train_data = np.load(dir_path + 'train_data.npy')\n",
        "    train_label = np.load(dir_path + 'train_label.npy')\n",
        "    test_X = np.load(dir_path + 'test_data.npy')\n",
        "    test_label = np.load(dir_path + 'test_label.npy')\n",
        "\n",
        "    # Split validation set\n",
        "    train_X, val_X, train_y, val_y = train_test_split(train_data, train_label,\n",
        "                                                     test_size=0.2, random_state=5329)\n",
        "\n",
        "    for cfg in configs:\n",
        "        print(\"\\n\\n=== Testing New Configuration ===\")\n",
        "        print(f\"Config: {cfg}\")\n",
        "\n",
        "        # Data preprocessing\n",
        "        train_X_processed = pre_processing(train_X, cfg['pre-process'])\n",
        "        val_X_processed = pre_processing(val_X, cfg['pre-process'])\n",
        "        test_X_processed = pre_processing(test_X, cfg['pre-process'])\n",
        "\n",
        "        # Create dataloaders\n",
        "        train_dataloader = Dataloader(train_X_processed, train_y, cfg['batch_size'], shuffle=True)\n",
        "        val_dataloader = Dataloader(val_X_processed, val_y, cfg['batch_size'], shuffle=False)\n",
        "        test_dataloader = Dataloader(test_X_processed, test_label, cfg['batch_size'], shuffle=False)\n",
        "\n",
        "        # Update config\n",
        "        current_config = {\n",
        "            'lr': cfg['lr'],\n",
        "            'batch_size': cfg['batch_size'],\n",
        "            'momentum': cfg['momentum'],\n",
        "            'weight_decay': cfg['weight_decay'],\n",
        "            'seed': 0,\n",
        "            'epoch': cfg['epoch'],\n",
        "            'optimizer': cfg['optimizer'],\n",
        "            'scheduler': None,\n",
        "            'pre-process': cfg['pre-process'],\n",
        "            'print_freq': 50000 // cfg['batch_size'] // 5\n",
        "        }\n",
        "\n",
        "        # Initialize model with current config\n",
        "        model = MLP(hidden_units=cfg['hidden_units'],\n",
        "                   dropout_rates=cfg['dropout_rates'])\n",
        "\n",
        "        # Train\n",
        "        trainer = Trainer(current_config, model, train_dataloader, val_dataloader)\n",
        "        trainer.train()\n",
        "\n",
        "        # Evaluate on test set\n",
        "        model._fit(mode='eval')\n",
        "        y_hat = model._predict(test_dataloader.X)\n",
        "        test_acc = accuracy(y_hat, test_dataloader.y)\n",
        "\n",
        "        print(f\"\\nTest Accuracy for Config: {test_acc:.2f}%\")\n",
        "\n",
        "        # Update best configuration\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            best_config = cfg\n",
        "\n",
        "    print(\"\\n\\n=== Tuning Results ===\")\n",
        "    print(f\"Best Accuracy: {best_acc:.2f}%\")\n",
        "    print(\"Best Configuration:\")\n",
        "    print(best_config)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    hyperparameter_tuning()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQCjYTdaDHUJ",
        "outputId": "b4e9aada-38f0-4b4e-ca4e-0edd1a324b65"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: [18][0/19]\tTime 0.226 (Avg-Time 0.226)\t Loss 1.7518 (Avg-Loss 1.7518)\tAcc 37.7930 (Avg-Acc 37.7930)\n",
            "Epoch: [18][4/19]\tTime 0.185 (Avg-Time 0.197)\t Loss 1.7319 (Avg-Loss 1.7695)\tAcc 38.9648 (Avg-Acc 37.8711)\n",
            "Epoch: [18][8/19]\tTime 0.183 (Avg-Time 0.196)\t Loss 1.8245 (Avg-Loss 1.7747)\tAcc 37.2559 (Avg-Acc 38.0263)\n",
            "Epoch: [18][12/19]\tTime 0.403 (Avg-Time 0.222)\t Loss 1.7969 (Avg-Loss 1.7744)\tAcc 37.3047 (Avg-Acc 37.7441)\n",
            "Epoch: [18][16/19]\tTime 0.395 (Avg-Time 0.288)\t Loss 1.7201 (Avg-Loss 1.7698)\tAcc 40.4785 (Avg-Acc 38.0256)\n",
            "Epoch: [18][19/19]\tTime 0.099 (Avg-Time 0.284)\t Loss 1.8160 (Avg-Loss 1.7684)\tAcc 35.3860 (Avg-Acc 38.0350)\n",
            "EPOCH: 18 train Results: Acc 38.035 Loss: 1.7684\n",
            "Epoch: [18][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.6059 (Avg-Loss 1.6059)\tAcc 45.1172 (Avg-Acc 45.1172)\n",
            "Epoch: [18][4/4]\tTime 0.053 (Avg-Time 0.049)\t Loss 1.5981 (Avg-Loss 1.5900)\tAcc 44.3584 (Avg-Acc 44.6800)\n",
            "EPOCH: 18 Validation Results: Acc 44.680 Loss: 1.5900\n",
            "Best Accuracy: 44.6800\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [19][0/19]\tTime 0.184 (Avg-Time 0.184)\t Loss 1.7863 (Avg-Loss 1.7863)\tAcc 37.9883 (Avg-Acc 37.9883)\n",
            "Epoch: [19][4/19]\tTime 0.184 (Avg-Time 0.191)\t Loss 1.7690 (Avg-Loss 1.7564)\tAcc 38.4766 (Avg-Acc 38.8477)\n",
            "Epoch: [19][8/19]\tTime 0.198 (Avg-Time 0.192)\t Loss 1.7048 (Avg-Loss 1.7541)\tAcc 39.2578 (Avg-Acc 38.4711)\n",
            "Epoch: [19][12/19]\tTime 0.189 (Avg-Time 0.191)\t Loss 1.7325 (Avg-Loss 1.7481)\tAcc 37.9883 (Avg-Acc 38.4127)\n",
            "Epoch: [19][16/19]\tTime 0.185 (Avg-Time 0.192)\t Loss 1.7458 (Avg-Loss 1.7510)\tAcc 38.4766 (Avg-Acc 38.1980)\n",
            "Epoch: [19][19/19]\tTime 0.125 (Avg-Time 0.189)\t Loss 1.8106 (Avg-Loss 1.7483)\tAcc 36.3051 (Avg-Acc 38.3450)\n",
            "EPOCH: 19 train Results: Acc 38.345 Loss: 1.7483\n",
            "Epoch: [19][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.6018 (Avg-Loss 1.6018)\tAcc 44.9707 (Avg-Acc 44.9707)\n",
            "Epoch: [19][4/4]\tTime 0.050 (Avg-Time 0.048)\t Loss 1.5950 (Avg-Loss 1.5861)\tAcc 44.7456 (Avg-Acc 44.9500)\n",
            "EPOCH: 19 Validation Results: Acc 44.950 Loss: 1.5861\n",
            "Best Accuracy: 44.9500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [20][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.7159 (Avg-Loss 1.7159)\tAcc 38.3301 (Avg-Acc 38.3301)\n",
            "Epoch: [20][4/19]\tTime 0.185 (Avg-Time 0.193)\t Loss 1.7413 (Avg-Loss 1.7236)\tAcc 39.0137 (Avg-Acc 39.2188)\n",
            "Epoch: [20][8/19]\tTime 0.192 (Avg-Time 0.191)\t Loss 1.7686 (Avg-Loss 1.7355)\tAcc 38.5742 (Avg-Acc 38.9703)\n",
            "Epoch: [20][12/19]\tTime 0.187 (Avg-Time 0.193)\t Loss 1.7511 (Avg-Loss 1.7343)\tAcc 38.0371 (Avg-Acc 39.0325)\n",
            "Epoch: [20][16/19]\tTime 0.183 (Avg-Time 0.193)\t Loss 1.7096 (Avg-Loss 1.7318)\tAcc 40.1855 (Avg-Acc 39.1286)\n",
            "Epoch: [20][19/19]\tTime 0.097 (Avg-Time 0.188)\t Loss 1.7581 (Avg-Loss 1.7312)\tAcc 38.2353 (Avg-Acc 39.0975)\n",
            "EPOCH: 20 train Results: Acc 39.097 Loss: 1.7312\n",
            "Epoch: [20][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.5970 (Avg-Loss 1.5970)\tAcc 45.3125 (Avg-Acc 45.3125)\n",
            "Epoch: [20][4/4]\tTime 0.046 (Avg-Time 0.052)\t Loss 1.5921 (Avg-Loss 1.5823)\tAcc 45.4093 (Avg-Acc 45.1300)\n",
            "EPOCH: 20 Validation Results: Acc 45.130 Loss: 1.5823\n",
            "Best Accuracy: 45.1300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [21][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.7664 (Avg-Loss 1.7664)\tAcc 38.0859 (Avg-Acc 38.0859)\n",
            "Epoch: [21][4/19]\tTime 0.198 (Avg-Time 0.194)\t Loss 1.6782 (Avg-Loss 1.7142)\tAcc 39.5996 (Avg-Acc 39.5117)\n",
            "Epoch: [21][8/19]\tTime 0.318 (Avg-Time 0.213)\t Loss 1.6469 (Avg-Loss 1.7084)\tAcc 41.0156 (Avg-Acc 39.7624)\n",
            "Epoch: [21][12/19]\tTime 0.330 (Avg-Time 0.301)\t Loss 1.7294 (Avg-Loss 1.7156)\tAcc 39.3066 (Avg-Acc 39.5921)\n",
            "Epoch: [21][16/19]\tTime 0.208 (Avg-Time 0.301)\t Loss 1.7238 (Avg-Loss 1.7212)\tAcc 39.0137 (Avg-Acc 39.2894)\n",
            "Epoch: [21][19/19]\tTime 0.102 (Avg-Time 0.281)\t Loss 1.7713 (Avg-Loss 1.7231)\tAcc 39.0625 (Avg-Acc 39.2875)\n",
            "EPOCH: 21 train Results: Acc 39.288 Loss: 1.7231\n",
            "Epoch: [21][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.5907 (Avg-Loss 1.5907)\tAcc 45.3125 (Avg-Acc 45.3125)\n",
            "Epoch: [21][4/4]\tTime 0.057 (Avg-Time 0.050)\t Loss 1.5860 (Avg-Loss 1.5762)\tAcc 45.0774 (Avg-Acc 45.1900)\n",
            "EPOCH: 21 Validation Results: Acc 45.190 Loss: 1.5762\n",
            "Best Accuracy: 45.1900\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [22][0/19]\tTime 0.187 (Avg-Time 0.187)\t Loss 1.7262 (Avg-Loss 1.7262)\tAcc 38.6230 (Avg-Acc 38.6230)\n",
            "Epoch: [22][4/19]\tTime 0.189 (Avg-Time 0.194)\t Loss 1.7049 (Avg-Loss 1.7185)\tAcc 40.0879 (Avg-Acc 39.5508)\n",
            "Epoch: [22][8/19]\tTime 0.196 (Avg-Time 0.195)\t Loss 1.6708 (Avg-Loss 1.7095)\tAcc 40.7715 (Avg-Acc 39.5454)\n",
            "Epoch: [22][12/19]\tTime 0.188 (Avg-Time 0.195)\t Loss 1.7013 (Avg-Loss 1.7027)\tAcc 39.4531 (Avg-Acc 39.7724)\n",
            "Epoch: [22][16/19]\tTime 0.188 (Avg-Time 0.195)\t Loss 1.6902 (Avg-Loss 1.6991)\tAcc 40.6738 (Avg-Acc 39.9213)\n",
            "Epoch: [22][19/19]\tTime 0.102 (Avg-Time 0.192)\t Loss 1.7174 (Avg-Loss 1.7005)\tAcc 39.0625 (Avg-Acc 39.7650)\n",
            "EPOCH: 22 train Results: Acc 39.765 Loss: 1.7005\n",
            "Epoch: [22][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.5869 (Avg-Loss 1.5869)\tAcc 45.8496 (Avg-Acc 45.8496)\n",
            "Epoch: [22][4/4]\tTime 0.052 (Avg-Time 0.049)\t Loss 1.5805 (Avg-Loss 1.5717)\tAcc 45.6305 (Avg-Acc 45.5500)\n",
            "EPOCH: 22 Validation Results: Acc 45.550 Loss: 1.5717\n",
            "Best Accuracy: 45.5500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [23][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.7180 (Avg-Loss 1.7180)\tAcc 39.9902 (Avg-Acc 39.9902)\n",
            "Epoch: [23][4/19]\tTime 0.186 (Avg-Time 0.196)\t Loss 1.6792 (Avg-Loss 1.6942)\tAcc 41.3574 (Avg-Acc 40.2734)\n",
            "Epoch: [23][8/19]\tTime 0.193 (Avg-Time 0.196)\t Loss 1.6680 (Avg-Loss 1.6945)\tAcc 40.6738 (Avg-Acc 40.2398)\n",
            "Epoch: [23][12/19]\tTime 0.210 (Avg-Time 0.196)\t Loss 1.6689 (Avg-Loss 1.6843)\tAcc 40.6250 (Avg-Acc 40.3320)\n",
            "Epoch: [23][16/19]\tTime 0.185 (Avg-Time 0.195)\t Loss 1.7092 (Avg-Loss 1.6898)\tAcc 38.7695 (Avg-Acc 40.1827)\n",
            "Epoch: [23][19/19]\tTime 0.095 (Avg-Time 0.191)\t Loss 1.6919 (Avg-Loss 1.6895)\tAcc 39.1544 (Avg-Acc 40.1925)\n",
            "EPOCH: 23 train Results: Acc 40.193 Loss: 1.6895\n",
            "Epoch: [23][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.5822 (Avg-Loss 1.5822)\tAcc 46.1914 (Avg-Acc 46.1914)\n",
            "Epoch: [23][4/4]\tTime 0.056 (Avg-Time 0.050)\t Loss 1.5771 (Avg-Loss 1.5676)\tAcc 45.6858 (Avg-Acc 45.7300)\n",
            "EPOCH: 23 Validation Results: Acc 45.730 Loss: 1.5676\n",
            "Best Accuracy: 45.7300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [24][0/19]\tTime 0.183 (Avg-Time 0.183)\t Loss 1.6418 (Avg-Loss 1.6418)\tAcc 42.1387 (Avg-Acc 42.1387)\n",
            "Epoch: [24][4/19]\tTime 0.318 (Avg-Time 0.225)\t Loss 1.7045 (Avg-Loss 1.6821)\tAcc 38.6230 (Avg-Acc 40.4785)\n",
            "Epoch: [24][8/19]\tTime 0.394 (Avg-Time 0.330)\t Loss 1.6881 (Avg-Loss 1.6873)\tAcc 40.5762 (Avg-Acc 40.3971)\n",
            "Epoch: [24][12/19]\tTime 0.194 (Avg-Time 0.327)\t Loss 1.6936 (Avg-Loss 1.6847)\tAcc 41.1621 (Avg-Acc 40.4597)\n",
            "Epoch: [24][16/19]\tTime 0.184 (Avg-Time 0.296)\t Loss 1.7041 (Avg-Loss 1.6809)\tAcc 40.5273 (Avg-Acc 40.4871)\n",
            "Epoch: [24][19/19]\tTime 0.098 (Avg-Time 0.276)\t Loss 1.6422 (Avg-Loss 1.6780)\tAcc 42.4632 (Avg-Acc 40.5450)\n",
            "EPOCH: 24 train Results: Acc 40.545 Loss: 1.6780\n",
            "Epoch: [24][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.5766 (Avg-Loss 1.5766)\tAcc 46.3867 (Avg-Acc 46.3867)\n",
            "Epoch: [24][4/4]\tTime 0.043 (Avg-Time 0.054)\t Loss 1.5713 (Avg-Loss 1.5622)\tAcc 45.6858 (Avg-Acc 45.8000)\n",
            "EPOCH: 24 Validation Results: Acc 45.800 Loss: 1.5622\n",
            "Best Accuracy: 45.8000\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [25][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.7065 (Avg-Loss 1.7065)\tAcc 39.9414 (Avg-Acc 39.9414)\n",
            "Epoch: [25][4/19]\tTime 0.210 (Avg-Time 0.194)\t Loss 1.6345 (Avg-Loss 1.6806)\tAcc 41.1133 (Avg-Acc 40.1367)\n",
            "Epoch: [25][8/19]\tTime 0.185 (Avg-Time 0.191)\t Loss 1.6604 (Avg-Loss 1.6645)\tAcc 41.2109 (Avg-Acc 40.7010)\n",
            "Epoch: [25][12/19]\tTime 0.190 (Avg-Time 0.192)\t Loss 1.6303 (Avg-Loss 1.6628)\tAcc 41.7969 (Avg-Acc 40.7865)\n",
            "Epoch: [25][16/19]\tTime 0.182 (Avg-Time 0.192)\t Loss 1.6516 (Avg-Loss 1.6657)\tAcc 42.5781 (Avg-Acc 40.8174)\n",
            "Epoch: [25][19/19]\tTime 0.097 (Avg-Time 0.187)\t Loss 1.6619 (Avg-Loss 1.6662)\tAcc 39.5221 (Avg-Acc 40.7400)\n",
            "EPOCH: 25 train Results: Acc 40.740 Loss: 1.6662\n",
            "Epoch: [25][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.5740 (Avg-Loss 1.5740)\tAcc 46.1914 (Avg-Acc 46.1914)\n",
            "Epoch: [25][4/4]\tTime 0.052 (Avg-Time 0.049)\t Loss 1.5679 (Avg-Loss 1.5588)\tAcc 45.9624 (Avg-Acc 45.8900)\n",
            "EPOCH: 25 Validation Results: Acc 45.890 Loss: 1.5588\n",
            "Best Accuracy: 45.8900\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [26][0/19]\tTime 0.225 (Avg-Time 0.225)\t Loss 1.6935 (Avg-Loss 1.6935)\tAcc 39.8438 (Avg-Acc 39.8438)\n",
            "Epoch: [26][4/19]\tTime 0.191 (Avg-Time 0.197)\t Loss 1.6382 (Avg-Loss 1.6741)\tAcc 42.1387 (Avg-Acc 40.7031)\n",
            "Epoch: [26][8/19]\tTime 0.190 (Avg-Time 0.198)\t Loss 1.6522 (Avg-Loss 1.6694)\tAcc 41.8457 (Avg-Acc 40.8529)\n",
            "Epoch: [26][12/19]\tTime 0.189 (Avg-Time 0.200)\t Loss 1.6777 (Avg-Loss 1.6631)\tAcc 40.4297 (Avg-Acc 41.0607)\n",
            "Epoch: [26][16/19]\tTime 0.200 (Avg-Time 0.199)\t Loss 1.6217 (Avg-Loss 1.6573)\tAcc 43.6523 (Avg-Acc 41.3115)\n",
            "Epoch: [26][19/19]\tTime 0.104 (Avg-Time 0.194)\t Loss 1.6353 (Avg-Loss 1.6565)\tAcc 42.0037 (Avg-Acc 41.2475)\n",
            "EPOCH: 26 train Results: Acc 41.248 Loss: 1.6565\n",
            "Epoch: [26][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.5690 (Avg-Loss 1.5690)\tAcc 46.0938 (Avg-Acc 46.0938)\n",
            "Epoch: [26][4/4]\tTime 0.053 (Avg-Time 0.049)\t Loss 1.5633 (Avg-Loss 1.5544)\tAcc 46.0177 (Avg-Acc 46.1000)\n",
            "EPOCH: 26 Validation Results: Acc 46.100 Loss: 1.5544\n",
            "Best Accuracy: 46.1000\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [27][0/19]\tTime 0.249 (Avg-Time 0.249)\t Loss 1.6395 (Avg-Loss 1.6395)\tAcc 41.1621 (Avg-Acc 41.1621)\n",
            "Epoch: [27][4/19]\tTime 0.490 (Avg-Time 0.405)\t Loss 1.6427 (Avg-Loss 1.6458)\tAcc 42.6758 (Avg-Acc 41.5234)\n",
            "Epoch: [27][8/19]\tTime 0.305 (Avg-Time 0.390)\t Loss 1.6170 (Avg-Loss 1.6366)\tAcc 41.6016 (Avg-Acc 41.6558)\n",
            "Epoch: [27][12/19]\tTime 0.213 (Avg-Time 0.339)\t Loss 1.6831 (Avg-Loss 1.6416)\tAcc 42.7734 (Avg-Acc 41.5152)\n",
            "Epoch: [27][16/19]\tTime 0.183 (Avg-Time 0.305)\t Loss 1.6305 (Avg-Loss 1.6435)\tAcc 42.4805 (Avg-Acc 41.4580)\n",
            "Epoch: [27][19/19]\tTime 0.097 (Avg-Time 0.284)\t Loss 1.6372 (Avg-Loss 1.6428)\tAcc 41.7279 (Avg-Acc 41.5150)\n",
            "EPOCH: 27 train Results: Acc 41.515 Loss: 1.6428\n",
            "Epoch: [27][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.5632 (Avg-Loss 1.5632)\tAcc 46.2402 (Avg-Acc 46.2402)\n",
            "Epoch: [27][4/4]\tTime 0.061 (Avg-Time 0.052)\t Loss 1.5598 (Avg-Loss 1.5496)\tAcc 46.0730 (Avg-Acc 46.3200)\n",
            "EPOCH: 27 Validation Results: Acc 46.320 Loss: 1.5496\n",
            "Best Accuracy: 46.3200\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [28][0/19]\tTime 0.183 (Avg-Time 0.183)\t Loss 1.6565 (Avg-Loss 1.6565)\tAcc 41.6016 (Avg-Acc 41.6016)\n",
            "Epoch: [28][4/19]\tTime 0.191 (Avg-Time 0.195)\t Loss 1.6319 (Avg-Loss 1.6462)\tAcc 42.0410 (Avg-Acc 41.4648)\n",
            "Epoch: [28][8/19]\tTime 0.193 (Avg-Time 0.195)\t Loss 1.6582 (Avg-Loss 1.6351)\tAcc 40.7715 (Avg-Acc 41.3303)\n",
            "Epoch: [28][12/19]\tTime 0.186 (Avg-Time 0.194)\t Loss 1.6280 (Avg-Loss 1.6360)\tAcc 41.0156 (Avg-Acc 41.3574)\n",
            "Epoch: [28][16/19]\tTime 0.183 (Avg-Time 0.195)\t Loss 1.6188 (Avg-Loss 1.6337)\tAcc 42.7246 (Avg-Acc 41.5728)\n",
            "Epoch: [28][19/19]\tTime 0.102 (Avg-Time 0.191)\t Loss 1.6463 (Avg-Loss 1.6325)\tAcc 41.8199 (Avg-Acc 41.7325)\n",
            "EPOCH: 28 train Results: Acc 41.733 Loss: 1.6325\n",
            "Epoch: [28][0/4]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.5603 (Avg-Loss 1.5603)\tAcc 46.7285 (Avg-Acc 46.7285)\n",
            "Epoch: [28][4/4]\tTime 0.057 (Avg-Time 0.050)\t Loss 1.5553 (Avg-Loss 1.5461)\tAcc 46.2389 (Avg-Acc 46.4200)\n",
            "EPOCH: 28 Validation Results: Acc 46.420 Loss: 1.5461\n",
            "Best Accuracy: 46.4200\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [29][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.6490 (Avg-Loss 1.6490)\tAcc 41.8457 (Avg-Acc 41.8457)\n",
            "Epoch: [29][4/19]\tTime 0.188 (Avg-Time 0.201)\t Loss 1.6276 (Avg-Loss 1.6294)\tAcc 42.0898 (Avg-Acc 42.2168)\n",
            "Epoch: [29][8/19]\tTime 0.210 (Avg-Time 0.199)\t Loss 1.6396 (Avg-Loss 1.6266)\tAcc 40.9180 (Avg-Acc 42.2743)\n",
            "Epoch: [29][12/19]\tTime 0.191 (Avg-Time 0.196)\t Loss 1.6349 (Avg-Loss 1.6306)\tAcc 41.2598 (Avg-Acc 41.8945)\n",
            "Epoch: [29][16/19]\tTime 0.185 (Avg-Time 0.196)\t Loss 1.6392 (Avg-Loss 1.6296)\tAcc 41.7480 (Avg-Acc 41.9434)\n",
            "Epoch: [29][19/19]\tTime 0.180 (Avg-Time 0.199)\t Loss 1.6446 (Avg-Loss 1.6318)\tAcc 41.5441 (Avg-Acc 41.9625)\n",
            "EPOCH: 29 train Results: Acc 41.962 Loss: 1.6318\n",
            "Epoch: [29][0/4]\tTime 0.087 (Avg-Time 0.087)\t Loss 1.5569 (Avg-Loss 1.5569)\tAcc 46.4355 (Avg-Acc 46.4355)\n",
            "Epoch: [29][4/4]\tTime 0.112 (Avg-Time 0.095)\t Loss 1.5500 (Avg-Loss 1.5416)\tAcc 46.5155 (Avg-Acc 46.4900)\n",
            "EPOCH: 29 Validation Results: Acc 46.490 Loss: 1.5416\n",
            "Best Accuracy: 46.4900\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [30][0/19]\tTime 0.520 (Avg-Time 0.520)\t Loss 1.6291 (Avg-Loss 1.6291)\tAcc 42.2363 (Avg-Acc 42.2363)\n",
            "Epoch: [30][4/19]\tTime 0.347 (Avg-Time 0.426)\t Loss 1.6608 (Avg-Loss 1.6235)\tAcc 41.5527 (Avg-Acc 42.2754)\n",
            "Epoch: [30][8/19]\tTime 0.192 (Avg-Time 0.345)\t Loss 1.6617 (Avg-Loss 1.6225)\tAcc 40.0879 (Avg-Acc 42.1332)\n",
            "Epoch: [30][12/19]\tTime 0.192 (Avg-Time 0.299)\t Loss 1.6084 (Avg-Loss 1.6196)\tAcc 43.7012 (Avg-Acc 42.2514)\n",
            "Epoch: [30][16/19]\tTime 0.205 (Avg-Time 0.275)\t Loss 1.6233 (Avg-Loss 1.6188)\tAcc 40.9668 (Avg-Acc 42.1961)\n",
            "Epoch: [30][19/19]\tTime 0.097 (Avg-Time 0.258)\t Loss 1.5947 (Avg-Loss 1.6173)\tAcc 43.5662 (Avg-Acc 42.2600)\n",
            "EPOCH: 30 train Results: Acc 42.260 Loss: 1.6173\n",
            "Epoch: [30][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.5531 (Avg-Loss 1.5531)\tAcc 46.5332 (Avg-Acc 46.5332)\n",
            "Epoch: [30][4/4]\tTime 0.054 (Avg-Time 0.049)\t Loss 1.5478 (Avg-Loss 1.5381)\tAcc 46.4049 (Avg-Acc 46.5500)\n",
            "EPOCH: 30 Validation Results: Acc 46.550 Loss: 1.5381\n",
            "Best Accuracy: 46.5500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [31][0/19]\tTime 0.194 (Avg-Time 0.194)\t Loss 1.6081 (Avg-Loss 1.6081)\tAcc 42.6758 (Avg-Acc 42.6758)\n",
            "Epoch: [31][4/19]\tTime 0.185 (Avg-Time 0.194)\t Loss 1.6200 (Avg-Loss 1.6151)\tAcc 43.6523 (Avg-Acc 42.3730)\n",
            "Epoch: [31][8/19]\tTime 0.187 (Avg-Time 0.194)\t Loss 1.5899 (Avg-Loss 1.6164)\tAcc 45.0195 (Avg-Acc 42.4479)\n",
            "Epoch: [31][12/19]\tTime 0.186 (Avg-Time 0.194)\t Loss 1.6018 (Avg-Loss 1.6089)\tAcc 42.1387 (Avg-Acc 42.5293)\n",
            "Epoch: [31][16/19]\tTime 0.185 (Avg-Time 0.195)\t Loss 1.6147 (Avg-Loss 1.6080)\tAcc 41.4551 (Avg-Acc 42.3857)\n",
            "Epoch: [31][19/19]\tTime 0.097 (Avg-Time 0.191)\t Loss 1.5782 (Avg-Loss 1.6085)\tAcc 43.0147 (Avg-Acc 42.4775)\n",
            "EPOCH: 31 train Results: Acc 42.477 Loss: 1.6085\n",
            "Epoch: [31][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.5483 (Avg-Loss 1.5483)\tAcc 46.8750 (Avg-Acc 46.8750)\n",
            "Epoch: [31][4/4]\tTime 0.060 (Avg-Time 0.051)\t Loss 1.5436 (Avg-Loss 1.5335)\tAcc 46.6261 (Avg-Acc 46.5800)\n",
            "EPOCH: 31 Validation Results: Acc 46.580 Loss: 1.5335\n",
            "Best Accuracy: 46.5800\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [32][0/19]\tTime 0.184 (Avg-Time 0.184)\t Loss 1.6061 (Avg-Loss 1.6061)\tAcc 41.6504 (Avg-Acc 41.6504)\n",
            "Epoch: [32][4/19]\tTime 0.184 (Avg-Time 0.192)\t Loss 1.6005 (Avg-Loss 1.5992)\tAcc 42.9688 (Avg-Acc 42.4707)\n",
            "Epoch: [32][8/19]\tTime 0.187 (Avg-Time 0.193)\t Loss 1.5397 (Avg-Loss 1.5945)\tAcc 45.2637 (Avg-Acc 42.7029)\n",
            "Epoch: [32][12/19]\tTime 0.210 (Avg-Time 0.194)\t Loss 1.6116 (Avg-Loss 1.6026)\tAcc 42.6758 (Avg-Acc 42.8711)\n",
            "Epoch: [32][16/19]\tTime 0.329 (Avg-Time 0.201)\t Loss 1.6204 (Avg-Loss 1.6055)\tAcc 42.6270 (Avg-Acc 42.7849)\n",
            "Epoch: [32][19/19]\tTime 0.391 (Avg-Time 0.244)\t Loss 1.6081 (Avg-Loss 1.6069)\tAcc 43.5662 (Avg-Acc 42.7850)\n",
            "EPOCH: 32 train Results: Acc 42.785 Loss: 1.6069\n",
            "Epoch: [32][0/4]\tTime 0.104 (Avg-Time 0.104)\t Loss 1.5445 (Avg-Loss 1.5445)\tAcc 46.8750 (Avg-Acc 46.8750)\n",
            "Epoch: [32][4/4]\tTime 0.085 (Avg-Time 0.101)\t Loss 1.5395 (Avg-Loss 1.5295)\tAcc 46.4049 (Avg-Acc 46.6500)\n",
            "EPOCH: 32 Validation Results: Acc 46.650 Loss: 1.5295\n",
            "Best Accuracy: 46.6500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [33][0/19]\tTime 0.393 (Avg-Time 0.393)\t Loss 1.5868 (Avg-Loss 1.5868)\tAcc 43.9941 (Avg-Acc 43.9941)\n",
            "Epoch: [33][4/19]\tTime 0.196 (Avg-Time 0.292)\t Loss 1.6166 (Avg-Loss 1.5976)\tAcc 41.6992 (Avg-Acc 43.2422)\n",
            "Epoch: [33][8/19]\tTime 0.188 (Avg-Time 0.245)\t Loss 1.5978 (Avg-Loss 1.5957)\tAcc 43.4082 (Avg-Acc 43.1424)\n",
            "Epoch: [33][12/19]\tTime 0.186 (Avg-Time 0.231)\t Loss 1.5782 (Avg-Loss 1.5937)\tAcc 42.1875 (Avg-Acc 43.1115)\n",
            "Epoch: [33][16/19]\tTime 0.188 (Avg-Time 0.223)\t Loss 1.5712 (Avg-Loss 1.5936)\tAcc 43.3594 (Avg-Acc 43.0808)\n",
            "Epoch: [33][19/19]\tTime 0.097 (Avg-Time 0.214)\t Loss 1.6099 (Avg-Loss 1.5953)\tAcc 41.2684 (Avg-Acc 43.0650)\n",
            "EPOCH: 33 train Results: Acc 43.065 Loss: 1.5953\n",
            "Epoch: [33][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.5411 (Avg-Loss 1.5411)\tAcc 47.1191 (Avg-Acc 47.1191)\n",
            "Epoch: [33][4/4]\tTime 0.041 (Avg-Time 0.059)\t Loss 1.5359 (Avg-Loss 1.5265)\tAcc 47.0686 (Avg-Acc 46.8600)\n",
            "EPOCH: 33 Validation Results: Acc 46.860 Loss: 1.5265\n",
            "Best Accuracy: 46.8600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [34][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.6069 (Avg-Loss 1.6069)\tAcc 42.1875 (Avg-Acc 42.1875)\n",
            "Epoch: [34][4/19]\tTime 0.209 (Avg-Time 0.192)\t Loss 1.6085 (Avg-Loss 1.6042)\tAcc 43.3594 (Avg-Acc 42.8613)\n",
            "Epoch: [34][8/19]\tTime 0.187 (Avg-Time 0.190)\t Loss 1.6167 (Avg-Loss 1.6023)\tAcc 41.9434 (Avg-Acc 42.8657)\n",
            "Epoch: [34][12/19]\tTime 0.182 (Avg-Time 0.190)\t Loss 1.6142 (Avg-Loss 1.6004)\tAcc 42.1387 (Avg-Acc 42.7622)\n",
            "Epoch: [34][16/19]\tTime 0.186 (Avg-Time 0.191)\t Loss 1.6097 (Avg-Loss 1.5958)\tAcc 41.8945 (Avg-Acc 42.9601)\n",
            "Epoch: [34][19/19]\tTime 0.097 (Avg-Time 0.186)\t Loss 1.5636 (Avg-Loss 1.5949)\tAcc 44.7610 (Avg-Acc 43.0275)\n",
            "EPOCH: 34 train Results: Acc 43.028 Loss: 1.5949\n",
            "Epoch: [34][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.5373 (Avg-Loss 1.5373)\tAcc 47.1191 (Avg-Acc 47.1191)\n",
            "Epoch: [34][4/4]\tTime 0.056 (Avg-Time 0.050)\t Loss 1.5325 (Avg-Loss 1.5227)\tAcc 46.7367 (Avg-Acc 46.6900)\n",
            "EPOCH: 34 Validation Results: Acc 46.690 Loss: 1.5227\n",
            "Best Accuracy: 46.8600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [35][0/19]\tTime 0.209 (Avg-Time 0.209)\t Loss 1.5666 (Avg-Loss 1.5666)\tAcc 43.5059 (Avg-Acc 43.5059)\n",
            "Epoch: [35][4/19]\tTime 0.185 (Avg-Time 0.194)\t Loss 1.5766 (Avg-Loss 1.5684)\tAcc 44.1406 (Avg-Acc 43.7988)\n",
            "Epoch: [35][8/19]\tTime 0.192 (Avg-Time 0.196)\t Loss 1.5997 (Avg-Loss 1.5822)\tAcc 43.8965 (Avg-Acc 43.9182)\n",
            "Epoch: [35][12/19]\tTime 0.255 (Avg-Time 0.201)\t Loss 1.5953 (Avg-Loss 1.5839)\tAcc 42.9199 (Avg-Acc 43.8326)\n",
            "Epoch: [35][16/19]\tTime 0.506 (Avg-Time 0.272)\t Loss 1.5978 (Avg-Loss 1.5832)\tAcc 43.3594 (Avg-Acc 43.7672)\n",
            "Epoch: [35][19/19]\tTime 0.198 (Avg-Time 0.277)\t Loss 1.5382 (Avg-Loss 1.5825)\tAcc 46.5074 (Avg-Acc 43.7825)\n",
            "EPOCH: 35 train Results: Acc 43.782 Loss: 1.5825\n",
            "Epoch: [35][0/4]\tTime 0.085 (Avg-Time 0.085)\t Loss 1.5329 (Avg-Loss 1.5329)\tAcc 47.2656 (Avg-Acc 47.2656)\n",
            "Epoch: [35][4/4]\tTime 0.043 (Avg-Time 0.071)\t Loss 1.5295 (Avg-Loss 1.5195)\tAcc 47.1792 (Avg-Acc 46.9200)\n",
            "EPOCH: 35 Validation Results: Acc 46.920 Loss: 1.5195\n",
            "Best Accuracy: 46.9200\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [36][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.6135 (Avg-Loss 1.6135)\tAcc 43.3105 (Avg-Acc 43.3105)\n",
            "Epoch: [36][4/19]\tTime 0.186 (Avg-Time 0.193)\t Loss 1.5585 (Avg-Loss 1.5872)\tAcc 45.4590 (Avg-Acc 43.6133)\n",
            "Epoch: [36][8/19]\tTime 0.182 (Avg-Time 0.193)\t Loss 1.5826 (Avg-Loss 1.5861)\tAcc 44.3848 (Avg-Acc 43.4842)\n",
            "Epoch: [36][12/19]\tTime 0.186 (Avg-Time 0.192)\t Loss 1.5627 (Avg-Loss 1.5845)\tAcc 43.7012 (Avg-Acc 43.5810)\n",
            "Epoch: [36][16/19]\tTime 0.188 (Avg-Time 0.192)\t Loss 1.5408 (Avg-Loss 1.5809)\tAcc 45.1172 (Avg-Acc 43.6724)\n",
            "Epoch: [36][19/19]\tTime 0.095 (Avg-Time 0.188)\t Loss 1.5892 (Avg-Loss 1.5813)\tAcc 43.0147 (Avg-Acc 43.6925)\n",
            "EPOCH: 36 train Results: Acc 43.693 Loss: 1.5813\n",
            "Epoch: [36][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.5289 (Avg-Loss 1.5289)\tAcc 47.9004 (Avg-Acc 47.9004)\n",
            "Epoch: [36][4/4]\tTime 0.057 (Avg-Time 0.050)\t Loss 1.5252 (Avg-Loss 1.5154)\tAcc 47.0133 (Avg-Acc 47.1900)\n",
            "EPOCH: 36 Validation Results: Acc 47.190 Loss: 1.5154\n",
            "Best Accuracy: 47.1900\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [37][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.5896 (Avg-Loss 1.5896)\tAcc 43.4082 (Avg-Acc 43.4082)\n",
            "Epoch: [37][4/19]\tTime 0.195 (Avg-Time 0.192)\t Loss 1.5804 (Avg-Loss 1.5685)\tAcc 44.3359 (Avg-Acc 44.5020)\n",
            "Epoch: [37][8/19]\tTime 0.207 (Avg-Time 0.193)\t Loss 1.5475 (Avg-Loss 1.5721)\tAcc 45.4102 (Avg-Acc 44.4607)\n",
            "Epoch: [37][12/19]\tTime 0.187 (Avg-Time 0.192)\t Loss 1.5547 (Avg-Loss 1.5665)\tAcc 43.0176 (Avg-Acc 44.2758)\n",
            "Epoch: [37][16/19]\tTime 0.190 (Avg-Time 0.193)\t Loss 1.5669 (Avg-Loss 1.5667)\tAcc 44.0430 (Avg-Acc 44.2670)\n",
            "Epoch: [37][19/19]\tTime 0.113 (Avg-Time 0.189)\t Loss 1.5733 (Avg-Loss 1.5671)\tAcc 43.4743 (Avg-Acc 44.1500)\n",
            "EPOCH: 37 train Results: Acc 44.150 Loss: 1.5671\n",
            "Epoch: [37][0/4]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.5242 (Avg-Loss 1.5242)\tAcc 47.7051 (Avg-Acc 47.7051)\n",
            "Epoch: [37][4/4]\tTime 0.054 (Avg-Time 0.052)\t Loss 1.5213 (Avg-Loss 1.5111)\tAcc 46.9580 (Avg-Acc 47.1700)\n",
            "EPOCH: 37 Validation Results: Acc 47.170 Loss: 1.5111\n",
            "Best Accuracy: 47.1900\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [38][0/19]\tTime 0.193 (Avg-Time 0.193)\t Loss 1.5692 (Avg-Loss 1.5692)\tAcc 44.7266 (Avg-Acc 44.7266)\n",
            "Epoch: [38][4/19]\tTime 0.201 (Avg-Time 0.196)\t Loss 1.5526 (Avg-Loss 1.5595)\tAcc 44.2383 (Avg-Acc 44.4531)\n",
            "Epoch: [38][8/19]\tTime 0.185 (Avg-Time 0.193)\t Loss 1.5361 (Avg-Loss 1.5664)\tAcc 44.8242 (Avg-Acc 44.0158)\n",
            "Epoch: [38][12/19]\tTime 0.473 (Avg-Time 0.265)\t Loss 1.5636 (Avg-Loss 1.5634)\tAcc 43.0664 (Avg-Acc 44.1143)\n",
            "Epoch: [38][16/19]\tTime 0.347 (Avg-Time 0.296)\t Loss 1.5646 (Avg-Loss 1.5614)\tAcc 43.8965 (Avg-Acc 44.1090)\n",
            "Epoch: [38][19/19]\tTime 0.100 (Avg-Time 0.280)\t Loss 1.5812 (Avg-Loss 1.5611)\tAcc 43.7500 (Avg-Acc 44.1775)\n",
            "EPOCH: 38 train Results: Acc 44.178 Loss: 1.5611\n",
            "Epoch: [38][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.5198 (Avg-Loss 1.5198)\tAcc 48.3398 (Avg-Acc 48.3398)\n",
            "Epoch: [38][4/4]\tTime 0.055 (Avg-Time 0.052)\t Loss 1.5174 (Avg-Loss 1.5069)\tAcc 46.8473 (Avg-Acc 47.2600)\n",
            "EPOCH: 38 Validation Results: Acc 47.260 Loss: 1.5069\n",
            "Best Accuracy: 47.2600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [39][0/19]\tTime 0.195 (Avg-Time 0.195)\t Loss 1.5786 (Avg-Loss 1.5786)\tAcc 43.7988 (Avg-Acc 43.7988)\n",
            "Epoch: [39][4/19]\tTime 0.188 (Avg-Time 0.195)\t Loss 1.5617 (Avg-Loss 1.5512)\tAcc 43.6523 (Avg-Acc 44.1895)\n",
            "Epoch: [39][8/19]\tTime 0.186 (Avg-Time 0.195)\t Loss 1.5695 (Avg-Loss 1.5507)\tAcc 44.3848 (Avg-Acc 44.5421)\n",
            "Epoch: [39][12/19]\tTime 0.189 (Avg-Time 0.195)\t Loss 1.5339 (Avg-Loss 1.5534)\tAcc 44.6289 (Avg-Acc 44.5538)\n",
            "Epoch: [39][16/19]\tTime 0.187 (Avg-Time 0.194)\t Loss 1.5466 (Avg-Loss 1.5590)\tAcc 44.9219 (Avg-Acc 44.4135)\n",
            "Epoch: [39][19/19]\tTime 0.101 (Avg-Time 0.190)\t Loss 1.5611 (Avg-Loss 1.5590)\tAcc 44.4853 (Avg-Acc 44.3475)\n",
            "EPOCH: 39 train Results: Acc 44.347 Loss: 1.5590\n",
            "Epoch: [39][0/4]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.5163 (Avg-Loss 1.5163)\tAcc 48.6816 (Avg-Acc 48.6816)\n",
            "Epoch: [39][4/4]\tTime 0.051 (Avg-Time 0.049)\t Loss 1.5145 (Avg-Loss 1.5034)\tAcc 46.5708 (Avg-Acc 47.5600)\n",
            "EPOCH: 39 Validation Results: Acc 47.560 Loss: 1.5034\n",
            "Best Accuracy: 47.5600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [40][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.5553 (Avg-Loss 1.5553)\tAcc 44.9219 (Avg-Acc 44.9219)\n",
            "Epoch: [40][4/19]\tTime 0.191 (Avg-Time 0.196)\t Loss 1.5492 (Avg-Loss 1.5622)\tAcc 43.5547 (Avg-Acc 43.9453)\n",
            "Epoch: [40][8/19]\tTime 0.187 (Avg-Time 0.196)\t Loss 1.5264 (Avg-Loss 1.5559)\tAcc 46.6797 (Avg-Acc 44.3359)\n",
            "Epoch: [40][12/19]\tTime 0.200 (Avg-Time 0.198)\t Loss 1.5445 (Avg-Loss 1.5541)\tAcc 45.8008 (Avg-Acc 44.4899)\n",
            "Epoch: [40][16/19]\tTime 0.191 (Avg-Time 0.197)\t Loss 1.5931 (Avg-Loss 1.5547)\tAcc 43.2617 (Avg-Acc 44.3675)\n",
            "Epoch: [40][19/19]\tTime 0.099 (Avg-Time 0.194)\t Loss 1.6239 (Avg-Loss 1.5537)\tAcc 42.7390 (Avg-Acc 44.3925)\n",
            "EPOCH: 40 train Results: Acc 44.392 Loss: 1.5537\n",
            "Epoch: [40][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.5118 (Avg-Loss 1.5118)\tAcc 48.9258 (Avg-Acc 48.9258)\n",
            "Epoch: [40][4/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.5098 (Avg-Loss 1.4993)\tAcc 47.1239 (Avg-Acc 47.6700)\n",
            "EPOCH: 40 Validation Results: Acc 47.670 Loss: 1.4993\n",
            "Best Accuracy: 47.6700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [41][0/19]\tTime 0.192 (Avg-Time 0.192)\t Loss 1.5509 (Avg-Loss 1.5509)\tAcc 44.4824 (Avg-Acc 44.4824)\n",
            "Epoch: [41][4/19]\tTime 0.186 (Avg-Time 0.197)\t Loss 1.5670 (Avg-Loss 1.5594)\tAcc 44.1406 (Avg-Acc 44.3652)\n",
            "Epoch: [41][8/19]\tTime 0.598 (Avg-Time 0.287)\t Loss 1.5547 (Avg-Loss 1.5507)\tAcc 42.2852 (Avg-Acc 44.3793)\n",
            "Epoch: [41][12/19]\tTime 0.360 (Avg-Time 0.322)\t Loss 1.4917 (Avg-Loss 1.5484)\tAcc 46.4355 (Avg-Acc 44.3059)\n",
            "Epoch: [41][16/19]\tTime 0.187 (Avg-Time 0.300)\t Loss 1.5516 (Avg-Loss 1.5463)\tAcc 45.1660 (Avg-Acc 44.5628)\n",
            "Epoch: [41][19/19]\tTime 0.098 (Avg-Time 0.280)\t Loss 1.5723 (Avg-Loss 1.5478)\tAcc 43.6581 (Avg-Acc 44.6700)\n",
            "EPOCH: 41 train Results: Acc 44.670 Loss: 1.5478\n",
            "Epoch: [41][0/4]\tTime 0.052 (Avg-Time 0.052)\t Loss 1.5070 (Avg-Loss 1.5070)\tAcc 48.9258 (Avg-Acc 48.9258)\n",
            "Epoch: [41][4/4]\tTime 0.044 (Avg-Time 0.053)\t Loss 1.5063 (Avg-Loss 1.4955)\tAcc 47.1792 (Avg-Acc 47.7800)\n",
            "EPOCH: 41 Validation Results: Acc 47.780 Loss: 1.4955\n",
            "Best Accuracy: 47.7800\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [42][0/19]\tTime 0.184 (Avg-Time 0.184)\t Loss 1.5034 (Avg-Loss 1.5034)\tAcc 47.0703 (Avg-Acc 47.0703)\n",
            "Epoch: [42][4/19]\tTime 0.211 (Avg-Time 0.195)\t Loss 1.5341 (Avg-Loss 1.5232)\tAcc 44.4824 (Avg-Acc 45.6250)\n",
            "Epoch: [42][8/19]\tTime 0.192 (Avg-Time 0.193)\t Loss 1.5502 (Avg-Loss 1.5303)\tAcc 44.7266 (Avg-Acc 45.1172)\n",
            "Epoch: [42][12/19]\tTime 0.185 (Avg-Time 0.194)\t Loss 1.5709 (Avg-Loss 1.5351)\tAcc 44.3848 (Avg-Acc 45.1247)\n",
            "Epoch: [42][16/19]\tTime 0.189 (Avg-Time 0.195)\t Loss 1.5802 (Avg-Loss 1.5393)\tAcc 43.8965 (Avg-Acc 45.1172)\n",
            "Epoch: [42][19/19]\tTime 0.097 (Avg-Time 0.190)\t Loss 1.6112 (Avg-Loss 1.5451)\tAcc 42.8309 (Avg-Acc 44.9925)\n",
            "EPOCH: 42 train Results: Acc 44.992 Loss: 1.5451\n",
            "Epoch: [42][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.5040 (Avg-Loss 1.5040)\tAcc 48.6328 (Avg-Acc 48.6328)\n",
            "Epoch: [42][4/4]\tTime 0.043 (Avg-Time 0.054)\t Loss 1.5021 (Avg-Loss 1.4923)\tAcc 47.5111 (Avg-Acc 47.8500)\n",
            "EPOCH: 42 Validation Results: Acc 47.850 Loss: 1.4923\n",
            "Best Accuracy: 47.8500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [43][0/19]\tTime 0.187 (Avg-Time 0.187)\t Loss 1.5274 (Avg-Loss 1.5274)\tAcc 45.7031 (Avg-Acc 45.7031)\n",
            "Epoch: [43][4/19]\tTime 0.206 (Avg-Time 0.193)\t Loss 1.5725 (Avg-Loss 1.5500)\tAcc 43.7500 (Avg-Acc 44.4336)\n",
            "Epoch: [43][8/19]\tTime 0.184 (Avg-Time 0.191)\t Loss 1.5553 (Avg-Loss 1.5425)\tAcc 45.0195 (Avg-Acc 44.8296)\n",
            "Epoch: [43][12/19]\tTime 0.185 (Avg-Time 0.191)\t Loss 1.5242 (Avg-Loss 1.5389)\tAcc 45.4590 (Avg-Acc 44.8543)\n",
            "Epoch: [43][16/19]\tTime 0.189 (Avg-Time 0.193)\t Loss 1.5596 (Avg-Loss 1.5413)\tAcc 44.3359 (Avg-Acc 45.0224)\n",
            "Epoch: [43][19/19]\tTime 0.101 (Avg-Time 0.189)\t Loss 1.5377 (Avg-Loss 1.5403)\tAcc 43.2904 (Avg-Acc 44.9775)\n",
            "EPOCH: 43 train Results: Acc 44.977 Loss: 1.5403\n",
            "Epoch: [43][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.5008 (Avg-Loss 1.5008)\tAcc 49.2676 (Avg-Acc 49.2676)\n",
            "Epoch: [43][4/4]\tTime 0.050 (Avg-Time 0.049)\t Loss 1.5005 (Avg-Loss 1.4894)\tAcc 47.5111 (Avg-Acc 48.1100)\n",
            "EPOCH: 43 Validation Results: Acc 48.110 Loss: 1.4894\n",
            "Best Accuracy: 48.1100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [44][0/19]\tTime 0.212 (Avg-Time 0.212)\t Loss 1.5299 (Avg-Loss 1.5299)\tAcc 46.3379 (Avg-Acc 46.3379)\n",
            "Epoch: [44][4/19]\tTime 0.356 (Avg-Time 0.282)\t Loss 1.5410 (Avg-Loss 1.5396)\tAcc 44.6777 (Avg-Acc 45.0488)\n",
            "Epoch: [44][8/19]\tTime 0.373 (Avg-Time 0.348)\t Loss 1.5386 (Avg-Loss 1.5284)\tAcc 44.0430 (Avg-Acc 45.5512)\n",
            "Epoch: [44][12/19]\tTime 0.198 (Avg-Time 0.337)\t Loss 1.5144 (Avg-Loss 1.5277)\tAcc 46.2402 (Avg-Acc 45.5717)\n",
            "Epoch: [44][16/19]\tTime 0.187 (Avg-Time 0.302)\t Loss 1.5162 (Avg-Loss 1.5310)\tAcc 45.0195 (Avg-Acc 45.3527)\n",
            "Epoch: [44][19/19]\tTime 0.103 (Avg-Time 0.282)\t Loss 1.5737 (Avg-Loss 1.5333)\tAcc 43.1066 (Avg-Acc 45.2250)\n",
            "EPOCH: 44 train Results: Acc 45.225 Loss: 1.5333\n",
            "Epoch: [44][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.4965 (Avg-Loss 1.4965)\tAcc 49.3164 (Avg-Acc 49.3164)\n",
            "Epoch: [44][4/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.4964 (Avg-Loss 1.4854)\tAcc 47.4558 (Avg-Acc 48.0800)\n",
            "EPOCH: 44 Validation Results: Acc 48.080 Loss: 1.4854\n",
            "Best Accuracy: 48.1100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [45][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.5680 (Avg-Loss 1.5680)\tAcc 44.0918 (Avg-Acc 44.0918)\n",
            "Epoch: [45][4/19]\tTime 0.186 (Avg-Time 0.194)\t Loss 1.5611 (Avg-Loss 1.5304)\tAcc 43.1641 (Avg-Acc 45.1465)\n",
            "Epoch: [45][8/19]\tTime 0.192 (Avg-Time 0.194)\t Loss 1.5261 (Avg-Loss 1.5273)\tAcc 44.9219 (Avg-Acc 45.2854)\n",
            "Epoch: [45][12/19]\tTime 0.209 (Avg-Time 0.194)\t Loss 1.5126 (Avg-Loss 1.5282)\tAcc 46.6797 (Avg-Acc 45.4252)\n",
            "Epoch: [45][16/19]\tTime 0.189 (Avg-Time 0.194)\t Loss 1.5229 (Avg-Loss 1.5272)\tAcc 45.3613 (Avg-Acc 45.4504)\n",
            "Epoch: [45][19/19]\tTime 0.102 (Avg-Time 0.189)\t Loss 1.5146 (Avg-Loss 1.5266)\tAcc 45.8640 (Avg-Acc 45.5250)\n",
            "EPOCH: 45 train Results: Acc 45.525 Loss: 1.5266\n",
            "Epoch: [45][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.4938 (Avg-Loss 1.4938)\tAcc 49.3652 (Avg-Acc 49.3652)\n",
            "Epoch: [45][4/4]\tTime 0.053 (Avg-Time 0.049)\t Loss 1.4942 (Avg-Loss 1.4833)\tAcc 47.2898 (Avg-Acc 48.1600)\n",
            "EPOCH: 45 Validation Results: Acc 48.160 Loss: 1.4833\n",
            "Best Accuracy: 48.1600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [46][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.4840 (Avg-Loss 1.4840)\tAcc 46.4355 (Avg-Acc 46.4355)\n",
            "Epoch: [46][4/19]\tTime 0.187 (Avg-Time 0.192)\t Loss 1.4961 (Avg-Loss 1.5160)\tAcc 46.6309 (Avg-Acc 46.2695)\n",
            "Epoch: [46][8/19]\tTime 0.199 (Avg-Time 0.192)\t Loss 1.5126 (Avg-Loss 1.5163)\tAcc 46.1426 (Avg-Acc 46.1534)\n",
            "Epoch: [46][12/19]\tTime 0.188 (Avg-Time 0.192)\t Loss 1.5218 (Avg-Loss 1.5180)\tAcc 45.8496 (Avg-Acc 46.1576)\n",
            "Epoch: [46][16/19]\tTime 0.188 (Avg-Time 0.193)\t Loss 1.5134 (Avg-Loss 1.5183)\tAcc 46.6797 (Avg-Acc 46.2632)\n",
            "Epoch: [46][19/19]\tTime 0.103 (Avg-Time 0.189)\t Loss 1.5270 (Avg-Loss 1.5210)\tAcc 43.8419 (Avg-Acc 46.1000)\n",
            "EPOCH: 46 train Results: Acc 46.100 Loss: 1.5210\n",
            "Epoch: [46][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.4896 (Avg-Loss 1.4896)\tAcc 49.6094 (Avg-Acc 49.6094)\n",
            "Epoch: [46][4/4]\tTime 0.058 (Avg-Time 0.050)\t Loss 1.4892 (Avg-Loss 1.4791)\tAcc 48.0088 (Avg-Acc 48.3900)\n",
            "EPOCH: 46 Validation Results: Acc 48.390 Loss: 1.4791\n",
            "Best Accuracy: 48.3900\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [47][0/19]\tTime 0.325 (Avg-Time 0.325)\t Loss 1.5439 (Avg-Loss 1.5439)\tAcc 45.0195 (Avg-Acc 45.0195)\n",
            "Epoch: [47][4/19]\tTime 0.616 (Avg-Time 0.446)\t Loss 1.4945 (Avg-Loss 1.5286)\tAcc 45.8496 (Avg-Acc 45.1855)\n",
            "Epoch: [47][8/19]\tTime 0.186 (Avg-Time 0.394)\t Loss 1.5522 (Avg-Loss 1.5263)\tAcc 44.5801 (Avg-Acc 45.5512)\n",
            "Epoch: [47][12/19]\tTime 0.190 (Avg-Time 0.334)\t Loss 1.4981 (Avg-Loss 1.5187)\tAcc 46.9727 (Avg-Acc 45.9322)\n",
            "Epoch: [47][16/19]\tTime 0.187 (Avg-Time 0.301)\t Loss 1.5084 (Avg-Loss 1.5233)\tAcc 45.2148 (Avg-Acc 45.7117)\n",
            "Epoch: [47][19/19]\tTime 0.102 (Avg-Time 0.281)\t Loss 1.5044 (Avg-Loss 1.5232)\tAcc 47.2426 (Avg-Acc 45.7075)\n",
            "EPOCH: 47 train Results: Acc 45.708 Loss: 1.5232\n",
            "Epoch: [47][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.4855 (Avg-Loss 1.4855)\tAcc 49.4141 (Avg-Acc 49.4141)\n",
            "Epoch: [47][4/4]\tTime 0.057 (Avg-Time 0.050)\t Loss 1.4845 (Avg-Loss 1.4753)\tAcc 48.2301 (Avg-Acc 48.3900)\n",
            "EPOCH: 47 Validation Results: Acc 48.390 Loss: 1.4753\n",
            "Best Accuracy: 48.3900\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [48][0/19]\tTime 0.215 (Avg-Time 0.215)\t Loss 1.5197 (Avg-Loss 1.5197)\tAcc 46.3867 (Avg-Acc 46.3867)\n",
            "Epoch: [48][4/19]\tTime 0.203 (Avg-Time 0.200)\t Loss 1.5000 (Avg-Loss 1.5205)\tAcc 45.9473 (Avg-Acc 45.6836)\n",
            "Epoch: [48][8/19]\tTime 0.190 (Avg-Time 0.200)\t Loss 1.4793 (Avg-Loss 1.5125)\tAcc 47.4609 (Avg-Acc 46.0069)\n",
            "Epoch: [48][12/19]\tTime 0.187 (Avg-Time 0.199)\t Loss 1.5087 (Avg-Loss 1.5088)\tAcc 44.7754 (Avg-Acc 46.1050)\n",
            "Epoch: [48][16/19]\tTime 0.187 (Avg-Time 0.199)\t Loss 1.5072 (Avg-Loss 1.5112)\tAcc 45.8008 (Avg-Acc 46.1139)\n",
            "Epoch: [48][19/19]\tTime 0.101 (Avg-Time 0.193)\t Loss 1.5103 (Avg-Loss 1.5127)\tAcc 43.9338 (Avg-Acc 45.9575)\n",
            "EPOCH: 48 train Results: Acc 45.958 Loss: 1.5127\n",
            "Epoch: [48][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.4821 (Avg-Loss 1.4821)\tAcc 49.6094 (Avg-Acc 49.6094)\n",
            "Epoch: [48][4/4]\tTime 0.051 (Avg-Time 0.049)\t Loss 1.4808 (Avg-Loss 1.4716)\tAcc 48.0642 (Avg-Acc 48.4500)\n",
            "EPOCH: 48 Validation Results: Acc 48.450 Loss: 1.4716\n",
            "Best Accuracy: 48.4500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [49][0/19]\tTime 0.233 (Avg-Time 0.233)\t Loss 1.5039 (Avg-Loss 1.5039)\tAcc 46.6797 (Avg-Acc 46.6797)\n",
            "Epoch: [49][4/19]\tTime 0.187 (Avg-Time 0.200)\t Loss 1.5079 (Avg-Loss 1.5156)\tAcc 46.8750 (Avg-Acc 45.6055)\n",
            "Epoch: [49][8/19]\tTime 0.191 (Avg-Time 0.199)\t Loss 1.5043 (Avg-Loss 1.5180)\tAcc 46.1426 (Avg-Acc 45.4210)\n",
            "Epoch: [49][12/19]\tTime 0.185 (Avg-Time 0.200)\t Loss 1.4759 (Avg-Loss 1.5084)\tAcc 47.1680 (Avg-Acc 45.9097)\n",
            "Epoch: [49][16/19]\tTime 0.231 (Avg-Time 0.200)\t Loss 1.5519 (Avg-Loss 1.5050)\tAcc 44.6289 (Avg-Acc 46.0995)\n",
            "Epoch: [49][19/19]\tTime 0.202 (Avg-Time 0.217)\t Loss 1.5709 (Avg-Loss 1.5079)\tAcc 44.4853 (Avg-Acc 46.0825)\n",
            "EPOCH: 49 train Results: Acc 46.083 Loss: 1.5079\n",
            "Epoch: [49][0/4]\tTime 0.098 (Avg-Time 0.098)\t Loss 1.4786 (Avg-Loss 1.4786)\tAcc 49.6582 (Avg-Acc 49.6582)\n",
            "Epoch: [49][4/4]\tTime 0.109 (Avg-Time 0.112)\t Loss 1.4772 (Avg-Loss 1.4683)\tAcc 48.0642 (Avg-Acc 48.6000)\n",
            "EPOCH: 49 Validation Results: Acc 48.600 Loss: 1.4683\n",
            "Best Accuracy: 48.6000\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [50][0/19]\tTime 0.525 (Avg-Time 0.525)\t Loss 1.4988 (Avg-Loss 1.4988)\tAcc 47.9492 (Avg-Acc 47.9492)\n",
            "Epoch: [50][4/19]\tTime 0.206 (Avg-Time 0.411)\t Loss 1.4867 (Avg-Loss 1.4964)\tAcc 45.8008 (Avg-Acc 46.5137)\n",
            "Epoch: [50][8/19]\tTime 0.188 (Avg-Time 0.315)\t Loss 1.5108 (Avg-Loss 1.4987)\tAcc 46.2891 (Avg-Acc 46.7773)\n",
            "Epoch: [50][12/19]\tTime 0.203 (Avg-Time 0.278)\t Loss 1.5099 (Avg-Loss 1.5034)\tAcc 45.3613 (Avg-Acc 46.2064)\n",
            "Epoch: [50][16/19]\tTime 0.186 (Avg-Time 0.258)\t Loss 1.4935 (Avg-Loss 1.5039)\tAcc 46.7285 (Avg-Acc 46.2086)\n",
            "Epoch: [50][19/19]\tTime 0.098 (Avg-Time 0.244)\t Loss 1.4817 (Avg-Loss 1.5040)\tAcc 47.4265 (Avg-Acc 46.2175)\n",
            "EPOCH: 50 train Results: Acc 46.218 Loss: 1.5040\n",
            "Epoch: [50][0/4]\tTime 0.052 (Avg-Time 0.052)\t Loss 1.4762 (Avg-Loss 1.4762)\tAcc 49.7070 (Avg-Acc 49.7070)\n",
            "Epoch: [50][4/4]\tTime 0.048 (Avg-Time 0.049)\t Loss 1.4750 (Avg-Loss 1.4658)\tAcc 48.2301 (Avg-Acc 48.6100)\n",
            "EPOCH: 50 Validation Results: Acc 48.610 Loss: 1.4658\n",
            "Best Accuracy: 48.6100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [51][0/19]\tTime 0.184 (Avg-Time 0.184)\t Loss 1.5172 (Avg-Loss 1.5172)\tAcc 45.8984 (Avg-Acc 45.8984)\n",
            "Epoch: [51][4/19]\tTime 0.187 (Avg-Time 0.189)\t Loss 1.5014 (Avg-Loss 1.4966)\tAcc 47.1680 (Avg-Acc 46.8652)\n",
            "Epoch: [51][8/19]\tTime 0.210 (Avg-Time 0.191)\t Loss 1.5285 (Avg-Loss 1.4964)\tAcc 46.1426 (Avg-Acc 46.8153)\n",
            "Epoch: [51][12/19]\tTime 0.188 (Avg-Time 0.190)\t Loss 1.5096 (Avg-Loss 1.4997)\tAcc 46.8750 (Avg-Acc 46.6834)\n",
            "Epoch: [51][16/19]\tTime 0.182 (Avg-Time 0.191)\t Loss 1.5210 (Avg-Loss 1.4984)\tAcc 45.5566 (Avg-Acc 46.6567)\n",
            "Epoch: [51][19/19]\tTime 0.119 (Avg-Time 0.187)\t Loss 1.5658 (Avg-Loss 1.4989)\tAcc 43.9338 (Avg-Acc 46.6750)\n",
            "EPOCH: 51 train Results: Acc 46.675 Loss: 1.4989\n",
            "Epoch: [51][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.4727 (Avg-Loss 1.4727)\tAcc 49.6094 (Avg-Acc 49.6094)\n",
            "Epoch: [51][4/4]\tTime 0.058 (Avg-Time 0.051)\t Loss 1.4711 (Avg-Loss 1.4621)\tAcc 48.5066 (Avg-Acc 48.7800)\n",
            "EPOCH: 51 Validation Results: Acc 48.780 Loss: 1.4621\n",
            "Best Accuracy: 48.7800\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [52][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.5045 (Avg-Loss 1.5045)\tAcc 44.5801 (Avg-Acc 44.5801)\n",
            "Epoch: [52][4/19]\tTime 0.198 (Avg-Time 0.197)\t Loss 1.5041 (Avg-Loss 1.4981)\tAcc 46.4844 (Avg-Acc 46.0938)\n",
            "Epoch: [52][8/19]\tTime 0.208 (Avg-Time 0.197)\t Loss 1.4756 (Avg-Loss 1.4933)\tAcc 48.1934 (Avg-Acc 46.5007)\n",
            "Epoch: [52][12/19]\tTime 0.188 (Avg-Time 0.194)\t Loss 1.4774 (Avg-Loss 1.4930)\tAcc 49.2188 (Avg-Acc 46.7210)\n",
            "Epoch: [52][16/19]\tTime 0.615 (Avg-Time 0.235)\t Loss 1.4947 (Avg-Loss 1.4944)\tAcc 45.9473 (Avg-Acc 46.6854)\n",
            "Epoch: [52][19/19]\tTime 0.203 (Avg-Time 0.259)\t Loss 1.4903 (Avg-Loss 1.4941)\tAcc 47.6103 (Avg-Acc 46.6975)\n",
            "EPOCH: 52 train Results: Acc 46.697 Loss: 1.4941\n",
            "Epoch: [52][0/4]\tTime 0.088 (Avg-Time 0.088)\t Loss 1.4705 (Avg-Loss 1.4705)\tAcc 50.0000 (Avg-Acc 50.0000)\n",
            "Epoch: [52][4/4]\tTime 0.089 (Avg-Time 0.107)\t Loss 1.4674 (Avg-Loss 1.4593)\tAcc 48.6173 (Avg-Acc 48.9300)\n",
            "EPOCH: 52 Validation Results: Acc 48.930 Loss: 1.4593\n",
            "Best Accuracy: 48.9300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [53][0/19]\tTime 0.309 (Avg-Time 0.309)\t Loss 1.5302 (Avg-Loss 1.5302)\tAcc 44.3848 (Avg-Acc 44.3848)\n",
            "Epoch: [53][4/19]\tTime 0.192 (Avg-Time 0.219)\t Loss 1.4519 (Avg-Loss 1.4913)\tAcc 47.8027 (Avg-Acc 46.4746)\n",
            "Epoch: [53][8/19]\tTime 0.192 (Avg-Time 0.210)\t Loss 1.4957 (Avg-Loss 1.4901)\tAcc 46.2402 (Avg-Acc 46.8370)\n",
            "Epoch: [53][12/19]\tTime 0.182 (Avg-Time 0.206)\t Loss 1.4723 (Avg-Loss 1.4909)\tAcc 48.4863 (Avg-Acc 46.9163)\n",
            "Epoch: [53][16/19]\tTime 0.213 (Avg-Time 0.204)\t Loss 1.4803 (Avg-Loss 1.4904)\tAcc 46.9238 (Avg-Acc 46.9181)\n",
            "Epoch: [53][19/19]\tTime 0.096 (Avg-Time 0.197)\t Loss 1.5074 (Avg-Loss 1.4914)\tAcc 45.9559 (Avg-Acc 46.8700)\n",
            "EPOCH: 53 train Results: Acc 46.870 Loss: 1.4914\n",
            "Epoch: [53][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.4669 (Avg-Loss 1.4669)\tAcc 49.8047 (Avg-Acc 49.8047)\n",
            "Epoch: [53][4/4]\tTime 0.052 (Avg-Time 0.048)\t Loss 1.4652 (Avg-Loss 1.4557)\tAcc 48.7832 (Avg-Acc 49.1100)\n",
            "EPOCH: 53 Validation Results: Acc 49.110 Loss: 1.4557\n",
            "Best Accuracy: 49.1100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [54][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.4781 (Avg-Loss 1.4781)\tAcc 47.6562 (Avg-Acc 47.6562)\n",
            "Epoch: [54][4/19]\tTime 0.187 (Avg-Time 0.194)\t Loss 1.5094 (Avg-Loss 1.4846)\tAcc 45.6543 (Avg-Acc 46.6895)\n",
            "Epoch: [54][8/19]\tTime 0.182 (Avg-Time 0.193)\t Loss 1.4821 (Avg-Loss 1.4859)\tAcc 48.4375 (Avg-Acc 46.7828)\n",
            "Epoch: [54][12/19]\tTime 0.203 (Avg-Time 0.193)\t Loss 1.5279 (Avg-Loss 1.4884)\tAcc 45.8008 (Avg-Acc 46.8525)\n",
            "Epoch: [54][16/19]\tTime 0.192 (Avg-Time 0.194)\t Loss 1.4779 (Avg-Loss 1.4856)\tAcc 46.7285 (Avg-Acc 47.0100)\n",
            "Epoch: [54][19/19]\tTime 0.099 (Avg-Time 0.190)\t Loss 1.5132 (Avg-Loss 1.4874)\tAcc 45.0368 (Avg-Acc 46.9350)\n",
            "EPOCH: 54 train Results: Acc 46.935 Loss: 1.4874\n",
            "Epoch: [54][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.4632 (Avg-Loss 1.4632)\tAcc 49.8047 (Avg-Acc 49.8047)\n",
            "Epoch: [54][4/4]\tTime 0.054 (Avg-Time 0.050)\t Loss 1.4618 (Avg-Loss 1.4526)\tAcc 48.5619 (Avg-Acc 48.9700)\n",
            "EPOCH: 54 Validation Results: Acc 48.970 Loss: 1.4526\n",
            "Best Accuracy: 49.1100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [55][0/19]\tTime 0.184 (Avg-Time 0.184)\t Loss 1.4746 (Avg-Loss 1.4746)\tAcc 46.0449 (Avg-Acc 46.0449)\n",
            "Epoch: [55][4/19]\tTime 0.190 (Avg-Time 0.193)\t Loss 1.4765 (Avg-Loss 1.4832)\tAcc 47.9004 (Avg-Acc 46.9824)\n",
            "Epoch: [55][8/19]\tTime 0.185 (Avg-Time 0.195)\t Loss 1.4980 (Avg-Loss 1.4808)\tAcc 46.2891 (Avg-Acc 47.0595)\n",
            "Epoch: [55][12/19]\tTime 0.495 (Avg-Time 0.235)\t Loss 1.4508 (Avg-Loss 1.4820)\tAcc 50.0977 (Avg-Acc 47.3107)\n",
            "Epoch: [55][16/19]\tTime 0.321 (Avg-Time 0.285)\t Loss 1.5131 (Avg-Loss 1.4826)\tAcc 45.8008 (Avg-Acc 47.1823)\n",
            "Epoch: [55][19/19]\tTime 0.097 (Avg-Time 0.274)\t Loss 1.4111 (Avg-Loss 1.4801)\tAcc 51.2868 (Avg-Acc 47.3000)\n",
            "EPOCH: 55 train Results: Acc 47.300 Loss: 1.4801\n",
            "Epoch: [55][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.4593 (Avg-Loss 1.4593)\tAcc 50.0488 (Avg-Acc 50.0488)\n",
            "Epoch: [55][4/4]\tTime 0.049 (Avg-Time 0.055)\t Loss 1.4596 (Avg-Loss 1.4496)\tAcc 48.7279 (Avg-Acc 49.1500)\n",
            "EPOCH: 55 Validation Results: Acc 49.150 Loss: 1.4496\n",
            "Best Accuracy: 49.1500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [56][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.4679 (Avg-Loss 1.4679)\tAcc 46.9727 (Avg-Acc 46.9727)\n",
            "Epoch: [56][4/19]\tTime 0.193 (Avg-Time 0.192)\t Loss 1.4580 (Avg-Loss 1.4729)\tAcc 48.8770 (Avg-Acc 47.2070)\n",
            "Epoch: [56][8/19]\tTime 0.186 (Avg-Time 0.194)\t Loss 1.4974 (Avg-Loss 1.4670)\tAcc 46.6797 (Avg-Acc 47.5803)\n",
            "Epoch: [56][12/19]\tTime 0.189 (Avg-Time 0.193)\t Loss 1.5147 (Avg-Loss 1.4710)\tAcc 46.9727 (Avg-Acc 47.5436)\n",
            "Epoch: [56][16/19]\tTime 0.192 (Avg-Time 0.194)\t Loss 1.5065 (Avg-Loss 1.4698)\tAcc 45.4102 (Avg-Acc 47.6304)\n",
            "Epoch: [56][19/19]\tTime 0.097 (Avg-Time 0.189)\t Loss 1.4420 (Avg-Loss 1.4717)\tAcc 48.9890 (Avg-Acc 47.4650)\n",
            "EPOCH: 56 train Results: Acc 47.465 Loss: 1.4717\n",
            "Epoch: [56][0/4]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.4556 (Avg-Loss 1.4556)\tAcc 50.1953 (Avg-Acc 50.1953)\n",
            "Epoch: [56][4/4]\tTime 0.058 (Avg-Time 0.051)\t Loss 1.4557 (Avg-Loss 1.4462)\tAcc 49.0597 (Avg-Acc 49.2800)\n",
            "EPOCH: 56 Validation Results: Acc 49.280 Loss: 1.4462\n",
            "Best Accuracy: 49.2800\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [57][0/19]\tTime 0.219 (Avg-Time 0.219)\t Loss 1.4490 (Avg-Loss 1.4490)\tAcc 48.4863 (Avg-Acc 48.4863)\n",
            "Epoch: [57][4/19]\tTime 0.185 (Avg-Time 0.195)\t Loss 1.5075 (Avg-Loss 1.4803)\tAcc 45.0195 (Avg-Acc 46.6895)\n",
            "Epoch: [57][8/19]\tTime 0.186 (Avg-Time 0.195)\t Loss 1.4784 (Avg-Loss 1.4822)\tAcc 48.2422 (Avg-Acc 47.0269)\n",
            "Epoch: [57][12/19]\tTime 0.189 (Avg-Time 0.196)\t Loss 1.4877 (Avg-Loss 1.4798)\tAcc 47.2168 (Avg-Acc 47.1905)\n",
            "Epoch: [57][16/19]\tTime 0.208 (Avg-Time 0.196)\t Loss 1.4547 (Avg-Loss 1.4773)\tAcc 47.0703 (Avg-Acc 47.2513)\n",
            "Epoch: [57][19/19]\tTime 0.100 (Avg-Time 0.191)\t Loss 1.4296 (Avg-Loss 1.4760)\tAcc 50.6434 (Avg-Acc 47.3475)\n",
            "EPOCH: 57 train Results: Acc 47.347 Loss: 1.4760\n",
            "Epoch: [57][0/4]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.4540 (Avg-Loss 1.4540)\tAcc 49.9512 (Avg-Acc 49.9512)\n",
            "Epoch: [57][4/4]\tTime 0.048 (Avg-Time 0.047)\t Loss 1.4525 (Avg-Loss 1.4434)\tAcc 49.3363 (Avg-Acc 49.3800)\n",
            "EPOCH: 57 Validation Results: Acc 49.380 Loss: 1.4434\n",
            "Best Accuracy: 49.3800\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [58][0/19]\tTime 0.219 (Avg-Time 0.219)\t Loss 1.4460 (Avg-Loss 1.4460)\tAcc 49.4141 (Avg-Acc 49.4141)\n",
            "Epoch: [58][4/19]\tTime 0.189 (Avg-Time 0.197)\t Loss 1.4349 (Avg-Loss 1.4552)\tAcc 48.2422 (Avg-Acc 48.0762)\n",
            "Epoch: [58][8/19]\tTime 0.559 (Avg-Time 0.256)\t Loss 1.4554 (Avg-Loss 1.4627)\tAcc 47.5098 (Avg-Acc 47.7105)\n",
            "Epoch: [58][12/19]\tTime 0.450 (Avg-Time 0.329)\t Loss 1.5199 (Avg-Loss 1.4663)\tAcc 46.3379 (Avg-Acc 47.7126)\n",
            "Epoch: [58][16/19]\tTime 0.190 (Avg-Time 0.303)\t Loss 1.5060 (Avg-Loss 1.4681)\tAcc 45.7031 (Avg-Acc 47.5816)\n",
            "Epoch: [58][19/19]\tTime 0.097 (Avg-Time 0.283)\t Loss 1.4954 (Avg-Loss 1.4680)\tAcc 46.8750 (Avg-Acc 47.5650)\n",
            "EPOCH: 58 train Results: Acc 47.565 Loss: 1.4680\n",
            "Epoch: [58][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.4491 (Avg-Loss 1.4491)\tAcc 50.1465 (Avg-Acc 50.1465)\n",
            "Epoch: [58][4/4]\tTime 0.052 (Avg-Time 0.049)\t Loss 1.4498 (Avg-Loss 1.4398)\tAcc 49.1704 (Avg-Acc 49.3500)\n",
            "EPOCH: 58 Validation Results: Acc 49.350 Loss: 1.4398\n",
            "Best Accuracy: 49.3800\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [59][0/19]\tTime 0.184 (Avg-Time 0.184)\t Loss 1.4664 (Avg-Loss 1.4664)\tAcc 46.2402 (Avg-Acc 46.2402)\n",
            "Epoch: [59][4/19]\tTime 0.187 (Avg-Time 0.193)\t Loss 1.4409 (Avg-Loss 1.4619)\tAcc 48.4375 (Avg-Acc 47.8125)\n",
            "Epoch: [59][8/19]\tTime 0.199 (Avg-Time 0.192)\t Loss 1.4668 (Avg-Loss 1.4622)\tAcc 46.8262 (Avg-Acc 47.6942)\n",
            "Epoch: [59][12/19]\tTime 0.189 (Avg-Time 0.191)\t Loss 1.4573 (Avg-Loss 1.4629)\tAcc 46.7285 (Avg-Acc 47.6750)\n",
            "Epoch: [59][16/19]\tTime 0.186 (Avg-Time 0.195)\t Loss 1.4811 (Avg-Loss 1.4645)\tAcc 45.7520 (Avg-Acc 47.4868)\n",
            "Epoch: [59][19/19]\tTime 0.099 (Avg-Time 0.192)\t Loss 1.4219 (Avg-Loss 1.4622)\tAcc 50.0919 (Avg-Acc 47.6725)\n",
            "EPOCH: 59 train Results: Acc 47.672 Loss: 1.4622\n",
            "Epoch: [59][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.4472 (Avg-Loss 1.4472)\tAcc 50.1465 (Avg-Acc 50.1465)\n",
            "Epoch: [59][4/4]\tTime 0.053 (Avg-Time 0.050)\t Loss 1.4459 (Avg-Loss 1.4375)\tAcc 49.2257 (Avg-Acc 49.5600)\n",
            "EPOCH: 59 Validation Results: Acc 49.560 Loss: 1.4375\n",
            "Best Accuracy: 49.5600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [60][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.4472 (Avg-Loss 1.4472)\tAcc 47.8027 (Avg-Acc 47.8027)\n",
            "Epoch: [60][4/19]\tTime 0.188 (Avg-Time 0.194)\t Loss 1.4529 (Avg-Loss 1.4639)\tAcc 47.4121 (Avg-Acc 47.2168)\n",
            "Epoch: [60][8/19]\tTime 0.200 (Avg-Time 0.193)\t Loss 1.4495 (Avg-Loss 1.4595)\tAcc 48.1445 (Avg-Acc 47.4935)\n",
            "Epoch: [60][12/19]\tTime 0.192 (Avg-Time 0.192)\t Loss 1.4717 (Avg-Loss 1.4617)\tAcc 47.3633 (Avg-Acc 47.7088)\n",
            "Epoch: [60][16/19]\tTime 0.188 (Avg-Time 0.193)\t Loss 1.4653 (Avg-Loss 1.4611)\tAcc 47.1191 (Avg-Acc 47.8401)\n",
            "Epoch: [60][19/19]\tTime 0.099 (Avg-Time 0.188)\t Loss 1.4101 (Avg-Loss 1.4607)\tAcc 50.6434 (Avg-Acc 48.0275)\n",
            "EPOCH: 60 train Results: Acc 48.028 Loss: 1.4607\n",
            "Epoch: [60][0/4]\tTime 0.068 (Avg-Time 0.068)\t Loss 1.4428 (Avg-Loss 1.4428)\tAcc 50.1465 (Avg-Acc 50.1465)\n",
            "Epoch: [60][4/4]\tTime 0.044 (Avg-Time 0.054)\t Loss 1.4413 (Avg-Loss 1.4333)\tAcc 49.6128 (Avg-Acc 49.5500)\n",
            "EPOCH: 60 Validation Results: Acc 49.550 Loss: 1.4333\n",
            "Best Accuracy: 49.5600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [61][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.4352 (Avg-Loss 1.4352)\tAcc 49.1211 (Avg-Acc 49.1211)\n",
            "Epoch: [61][4/19]\tTime 0.476 (Avg-Time 0.294)\t Loss 1.4262 (Avg-Loss 1.4518)\tAcc 48.7305 (Avg-Acc 47.9590)\n",
            "Epoch: [61][8/19]\tTime 0.375 (Avg-Time 0.374)\t Loss 1.4193 (Avg-Loss 1.4513)\tAcc 48.8281 (Avg-Acc 48.1988)\n",
            "Epoch: [61][12/19]\tTime 0.192 (Avg-Time 0.334)\t Loss 1.4512 (Avg-Loss 1.4535)\tAcc 48.3398 (Avg-Acc 48.1558)\n",
            "Epoch: [61][16/19]\tTime 0.211 (Avg-Time 0.302)\t Loss 1.4765 (Avg-Loss 1.4573)\tAcc 48.4375 (Avg-Acc 47.8832)\n",
            "Epoch: [61][19/19]\tTime 0.097 (Avg-Time 0.281)\t Loss 1.4828 (Avg-Loss 1.4589)\tAcc 48.0699 (Avg-Acc 47.7900)\n",
            "EPOCH: 61 train Results: Acc 47.790 Loss: 1.4589\n",
            "Epoch: [61][0/4]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.4403 (Avg-Loss 1.4403)\tAcc 50.2930 (Avg-Acc 50.2930)\n",
            "Epoch: [61][4/4]\tTime 0.058 (Avg-Time 0.051)\t Loss 1.4395 (Avg-Loss 1.4312)\tAcc 49.0044 (Avg-Acc 49.6200)\n",
            "EPOCH: 61 Validation Results: Acc 49.620 Loss: 1.4312\n",
            "Best Accuracy: 49.6200\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [62][0/19]\tTime 0.210 (Avg-Time 0.210)\t Loss 1.4748 (Avg-Loss 1.4748)\tAcc 47.7051 (Avg-Acc 47.7051)\n",
            "Epoch: [62][4/19]\tTime 0.187 (Avg-Time 0.197)\t Loss 1.4254 (Avg-Loss 1.4531)\tAcc 48.5840 (Avg-Acc 47.9102)\n",
            "Epoch: [62][8/19]\tTime 0.184 (Avg-Time 0.197)\t Loss 1.4606 (Avg-Loss 1.4535)\tAcc 47.3145 (Avg-Acc 48.1500)\n",
            "Epoch: [62][12/19]\tTime 0.193 (Avg-Time 0.197)\t Loss 1.4480 (Avg-Loss 1.4518)\tAcc 48.1445 (Avg-Acc 48.2535)\n",
            "Epoch: [62][16/19]\tTime 0.202 (Avg-Time 0.196)\t Loss 1.4532 (Avg-Loss 1.4519)\tAcc 47.9004 (Avg-Acc 48.2250)\n",
            "Epoch: [62][19/19]\tTime 0.098 (Avg-Time 0.192)\t Loss 1.4644 (Avg-Loss 1.4519)\tAcc 48.3456 (Avg-Acc 48.2075)\n",
            "EPOCH: 62 train Results: Acc 48.208 Loss: 1.4519\n",
            "Epoch: [62][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.4372 (Avg-Loss 1.4372)\tAcc 50.5859 (Avg-Acc 50.5859)\n",
            "Epoch: [62][4/4]\tTime 0.055 (Avg-Time 0.050)\t Loss 1.4381 (Avg-Loss 1.4283)\tAcc 48.9491 (Avg-Acc 49.7500)\n",
            "EPOCH: 62 Validation Results: Acc 49.750 Loss: 1.4283\n",
            "Best Accuracy: 49.7500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [63][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.4694 (Avg-Loss 1.4694)\tAcc 46.8750 (Avg-Acc 46.8750)\n",
            "Epoch: [63][4/19]\tTime 0.186 (Avg-Time 0.192)\t Loss 1.4157 (Avg-Loss 1.4413)\tAcc 50.0488 (Avg-Acc 48.7988)\n",
            "Epoch: [63][8/19]\tTime 0.184 (Avg-Time 0.193)\t Loss 1.4500 (Avg-Loss 1.4427)\tAcc 47.6074 (Avg-Acc 48.5786)\n",
            "Epoch: [63][12/19]\tTime 0.200 (Avg-Time 0.194)\t Loss 1.4413 (Avg-Loss 1.4489)\tAcc 47.8516 (Avg-Acc 48.3323)\n",
            "Epoch: [63][16/19]\tTime 0.186 (Avg-Time 0.193)\t Loss 1.4719 (Avg-Loss 1.4484)\tAcc 47.0215 (Avg-Acc 48.2565)\n",
            "Epoch: [63][19/19]\tTime 0.096 (Avg-Time 0.189)\t Loss 1.4611 (Avg-Loss 1.4500)\tAcc 48.3456 (Avg-Acc 48.2325)\n",
            "EPOCH: 63 train Results: Acc 48.233 Loss: 1.4500\n",
            "Epoch: [63][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.4353 (Avg-Loss 1.4353)\tAcc 50.3418 (Avg-Acc 50.3418)\n",
            "Epoch: [63][4/4]\tTime 0.085 (Avg-Time 0.084)\t Loss 1.4366 (Avg-Loss 1.4264)\tAcc 49.3916 (Avg-Acc 49.8700)\n",
            "EPOCH: 63 Validation Results: Acc 49.870 Loss: 1.4264\n",
            "Best Accuracy: 49.8700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [64][0/19]\tTime 0.469 (Avg-Time 0.469)\t Loss 1.4546 (Avg-Loss 1.4546)\tAcc 48.1934 (Avg-Acc 48.1934)\n",
            "Epoch: [64][4/19]\tTime 0.340 (Avg-Time 0.490)\t Loss 1.4703 (Avg-Loss 1.4682)\tAcc 45.4102 (Avg-Acc 46.9922)\n",
            "Epoch: [64][8/19]\tTime 0.193 (Avg-Time 0.372)\t Loss 1.4079 (Avg-Loss 1.4462)\tAcc 49.0234 (Avg-Acc 47.8570)\n",
            "Epoch: [64][12/19]\tTime 0.187 (Avg-Time 0.318)\t Loss 1.4103 (Avg-Loss 1.4458)\tAcc 48.7793 (Avg-Acc 48.0319)\n",
            "Epoch: [64][16/19]\tTime 0.189 (Avg-Time 0.290)\t Loss 1.4470 (Avg-Loss 1.4458)\tAcc 48.7305 (Avg-Acc 48.0325)\n",
            "Epoch: [64][19/19]\tTime 0.099 (Avg-Time 0.271)\t Loss 1.4329 (Avg-Loss 1.4457)\tAcc 49.5404 (Avg-Acc 48.1425)\n",
            "EPOCH: 64 train Results: Acc 48.142 Loss: 1.4457\n",
            "Epoch: [64][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.4314 (Avg-Loss 1.4314)\tAcc 50.7324 (Avg-Acc 50.7324)\n",
            "Epoch: [64][4/4]\tTime 0.046 (Avg-Time 0.055)\t Loss 1.4322 (Avg-Loss 1.4230)\tAcc 49.1150 (Avg-Acc 49.9600)\n",
            "EPOCH: 64 Validation Results: Acc 49.960 Loss: 1.4230\n",
            "Best Accuracy: 49.9600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [65][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.4644 (Avg-Loss 1.4644)\tAcc 47.1680 (Avg-Acc 47.1680)\n",
            "Epoch: [65][4/19]\tTime 0.198 (Avg-Time 0.191)\t Loss 1.4303 (Avg-Loss 1.4547)\tAcc 48.8770 (Avg-Acc 48.0078)\n",
            "Epoch: [65][8/19]\tTime 0.193 (Avg-Time 0.192)\t Loss 1.3904 (Avg-Loss 1.4406)\tAcc 50.9766 (Avg-Acc 48.5514)\n",
            "Epoch: [65][12/19]\tTime 0.186 (Avg-Time 0.192)\t Loss 1.4384 (Avg-Loss 1.4454)\tAcc 48.8770 (Avg-Acc 48.3624)\n",
            "Epoch: [65][16/19]\tTime 0.184 (Avg-Time 0.192)\t Loss 1.4050 (Avg-Loss 1.4404)\tAcc 50.2930 (Avg-Acc 48.4519)\n",
            "Epoch: [65][19/19]\tTime 0.098 (Avg-Time 0.188)\t Loss 1.4286 (Avg-Loss 1.4430)\tAcc 47.8860 (Avg-Acc 48.3125)\n",
            "EPOCH: 65 train Results: Acc 48.312 Loss: 1.4430\n",
            "Epoch: [65][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.4276 (Avg-Loss 1.4276)\tAcc 50.3906 (Avg-Acc 50.3906)\n",
            "Epoch: [65][4/4]\tTime 0.070 (Avg-Time 0.052)\t Loss 1.4287 (Avg-Loss 1.4186)\tAcc 49.5575 (Avg-Acc 50.0700)\n",
            "EPOCH: 65 Validation Results: Acc 50.070 Loss: 1.4186\n",
            "Best Accuracy: 50.0700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [66][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.4255 (Avg-Loss 1.4255)\tAcc 49.4629 (Avg-Acc 49.4629)\n",
            "Epoch: [66][4/19]\tTime 0.216 (Avg-Time 0.194)\t Loss 1.4578 (Avg-Loss 1.4383)\tAcc 48.7793 (Avg-Acc 48.6035)\n",
            "Epoch: [66][8/19]\tTime 0.188 (Avg-Time 0.195)\t Loss 1.4024 (Avg-Loss 1.4357)\tAcc 49.3652 (Avg-Acc 48.3344)\n",
            "Epoch: [66][12/19]\tTime 0.193 (Avg-Time 0.195)\t Loss 1.4336 (Avg-Loss 1.4395)\tAcc 49.3652 (Avg-Acc 48.4450)\n",
            "Epoch: [66][16/19]\tTime 0.324 (Avg-Time 0.204)\t Loss 1.4423 (Avg-Loss 1.4378)\tAcc 49.1211 (Avg-Acc 48.7104)\n",
            "Epoch: [66][19/19]\tTime 0.238 (Avg-Time 0.234)\t Loss 1.3899 (Avg-Loss 1.4377)\tAcc 49.5404 (Avg-Acc 48.6250)\n",
            "EPOCH: 66 train Results: Acc 48.625 Loss: 1.4377\n",
            "Epoch: [66][0/4]\tTime 0.135 (Avg-Time 0.135)\t Loss 1.4255 (Avg-Loss 1.4255)\tAcc 50.3906 (Avg-Acc 50.3906)\n",
            "Epoch: [66][4/4]\tTime 0.123 (Avg-Time 0.121)\t Loss 1.4278 (Avg-Loss 1.4166)\tAcc 49.2810 (Avg-Acc 49.9600)\n",
            "EPOCH: 66 Validation Results: Acc 49.960 Loss: 1.4166\n",
            "Best Accuracy: 50.0700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [67][0/19]\tTime 0.367 (Avg-Time 0.367)\t Loss 1.4399 (Avg-Loss 1.4399)\tAcc 47.6074 (Avg-Acc 47.6074)\n",
            "Epoch: [67][4/19]\tTime 0.185 (Avg-Time 0.308)\t Loss 1.4256 (Avg-Loss 1.4379)\tAcc 48.5840 (Avg-Acc 48.6230)\n",
            "Epoch: [67][8/19]\tTime 0.193 (Avg-Time 0.259)\t Loss 1.4283 (Avg-Loss 1.4354)\tAcc 48.3887 (Avg-Acc 48.3724)\n",
            "Epoch: [67][12/19]\tTime 0.208 (Avg-Time 0.240)\t Loss 1.4272 (Avg-Loss 1.4375)\tAcc 49.9023 (Avg-Acc 48.6028)\n",
            "Epoch: [67][16/19]\tTime 0.189 (Avg-Time 0.228)\t Loss 1.4467 (Avg-Loss 1.4353)\tAcc 48.1934 (Avg-Acc 48.6098)\n",
            "Epoch: [67][19/19]\tTime 0.096 (Avg-Time 0.219)\t Loss 1.4387 (Avg-Loss 1.4372)\tAcc 45.9559 (Avg-Acc 48.5100)\n",
            "EPOCH: 67 train Results: Acc 48.510 Loss: 1.4372\n",
            "Epoch: [67][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.4234 (Avg-Loss 1.4234)\tAcc 50.7324 (Avg-Acc 50.7324)\n",
            "Epoch: [67][4/4]\tTime 0.051 (Avg-Time 0.049)\t Loss 1.4264 (Avg-Loss 1.4157)\tAcc 49.5575 (Avg-Acc 50.1200)\n",
            "EPOCH: 67 Validation Results: Acc 50.120 Loss: 1.4157\n",
            "Best Accuracy: 50.1200\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [68][0/19]\tTime 0.185 (Avg-Time 0.185)\t Loss 1.4165 (Avg-Loss 1.4165)\tAcc 50.1953 (Avg-Acc 50.1953)\n",
            "Epoch: [68][4/19]\tTime 0.186 (Avg-Time 0.194)\t Loss 1.4538 (Avg-Loss 1.4340)\tAcc 48.7305 (Avg-Acc 49.1406)\n",
            "Epoch: [68][8/19]\tTime 0.203 (Avg-Time 0.193)\t Loss 1.4319 (Avg-Loss 1.4325)\tAcc 49.8047 (Avg-Acc 48.9638)\n",
            "Epoch: [68][12/19]\tTime 0.190 (Avg-Time 0.194)\t Loss 1.4367 (Avg-Loss 1.4341)\tAcc 48.3887 (Avg-Acc 48.8469)\n",
            "Epoch: [68][16/19]\tTime 0.191 (Avg-Time 0.194)\t Loss 1.4330 (Avg-Loss 1.4374)\tAcc 46.7773 (Avg-Acc 48.5409)\n",
            "Epoch: [68][19/19]\tTime 0.116 (Avg-Time 0.189)\t Loss 1.4225 (Avg-Loss 1.4345)\tAcc 47.7941 (Avg-Acc 48.6425)\n",
            "EPOCH: 68 train Results: Acc 48.642 Loss: 1.4345\n",
            "Epoch: [68][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.4209 (Avg-Loss 1.4209)\tAcc 50.6836 (Avg-Acc 50.6836)\n",
            "Epoch: [68][4/4]\tTime 0.052 (Avg-Time 0.050)\t Loss 1.4225 (Avg-Loss 1.4116)\tAcc 49.1704 (Avg-Acc 50.2300)\n",
            "EPOCH: 68 Validation Results: Acc 50.230 Loss: 1.4116\n",
            "Best Accuracy: 50.2300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [69][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.4214 (Avg-Loss 1.4214)\tAcc 49.2188 (Avg-Acc 49.2188)\n",
            "Epoch: [69][4/19]\tTime 0.184 (Avg-Time 0.189)\t Loss 1.4354 (Avg-Loss 1.4226)\tAcc 47.4609 (Avg-Acc 49.0234)\n",
            "Epoch: [69][8/19]\tTime 0.248 (Avg-Time 0.196)\t Loss 1.4141 (Avg-Loss 1.4242)\tAcc 49.0234 (Avg-Acc 49.0451)\n",
            "Epoch: [69][12/19]\tTime 0.223 (Avg-Time 0.198)\t Loss 1.4236 (Avg-Loss 1.4266)\tAcc 48.4375 (Avg-Acc 49.0497)\n",
            "Epoch: [69][16/19]\tTime 0.505 (Avg-Time 0.272)\t Loss 1.4470 (Avg-Loss 1.4272)\tAcc 47.8516 (Avg-Acc 49.1211)\n",
            "Epoch: [69][19/19]\tTime 0.254 (Avg-Time 0.281)\t Loss 1.3929 (Avg-Loss 1.4273)\tAcc 51.7463 (Avg-Acc 49.1875)\n",
            "EPOCH: 69 train Results: Acc 49.188 Loss: 1.4273\n",
            "Epoch: [69][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.4155 (Avg-Loss 1.4155)\tAcc 51.1230 (Avg-Acc 51.1230)\n",
            "Epoch: [69][4/4]\tTime 0.055 (Avg-Time 0.049)\t Loss 1.4174 (Avg-Loss 1.4075)\tAcc 49.6681 (Avg-Acc 50.3900)\n",
            "EPOCH: 69 Validation Results: Acc 50.390 Loss: 1.4075\n",
            "Best Accuracy: 50.3900\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [70][0/19]\tTime 0.227 (Avg-Time 0.227)\t Loss 1.4310 (Avg-Loss 1.4310)\tAcc 48.3887 (Avg-Acc 48.3887)\n",
            "Epoch: [70][4/19]\tTime 0.185 (Avg-Time 0.199)\t Loss 1.4053 (Avg-Loss 1.4149)\tAcc 49.4629 (Avg-Acc 49.8340)\n",
            "Epoch: [70][8/19]\tTime 0.189 (Avg-Time 0.199)\t Loss 1.4086 (Avg-Loss 1.4179)\tAcc 48.2422 (Avg-Acc 49.1970)\n",
            "Epoch: [70][12/19]\tTime 0.189 (Avg-Time 0.199)\t Loss 1.4227 (Avg-Loss 1.4201)\tAcc 49.3652 (Avg-Acc 49.0385)\n",
            "Epoch: [70][16/19]\tTime 0.217 (Avg-Time 0.199)\t Loss 1.4576 (Avg-Loss 1.4222)\tAcc 47.1680 (Avg-Acc 48.8770)\n",
            "Epoch: [70][19/19]\tTime 0.099 (Avg-Time 0.194)\t Loss 1.4599 (Avg-Loss 1.4234)\tAcc 48.1618 (Avg-Acc 48.9000)\n",
            "EPOCH: 70 train Results: Acc 48.900 Loss: 1.4234\n",
            "Epoch: [70][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.4122 (Avg-Loss 1.4122)\tAcc 51.2207 (Avg-Acc 51.2207)\n",
            "Epoch: [70][4/4]\tTime 0.055 (Avg-Time 0.051)\t Loss 1.4116 (Avg-Loss 1.4037)\tAcc 49.9447 (Avg-Acc 50.7300)\n",
            "EPOCH: 70 Validation Results: Acc 50.730 Loss: 1.4037\n",
            "Best Accuracy: 50.7300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [71][0/19]\tTime 0.217 (Avg-Time 0.217)\t Loss 1.4200 (Avg-Loss 1.4200)\tAcc 49.2188 (Avg-Acc 49.2188)\n",
            "Epoch: [71][4/19]\tTime 0.190 (Avg-Time 0.199)\t Loss 1.4309 (Avg-Loss 1.4258)\tAcc 48.3887 (Avg-Acc 49.2285)\n",
            "Epoch: [71][8/19]\tTime 0.188 (Avg-Time 0.197)\t Loss 1.4241 (Avg-Loss 1.4259)\tAcc 49.1699 (Avg-Acc 49.2784)\n",
            "Epoch: [71][12/19]\tTime 0.187 (Avg-Time 0.196)\t Loss 1.3908 (Avg-Loss 1.4204)\tAcc 49.9512 (Avg-Acc 49.5080)\n",
            "Epoch: [71][16/19]\tTime 0.194 (Avg-Time 0.194)\t Loss 1.3961 (Avg-Loss 1.4235)\tAcc 50.6836 (Avg-Acc 49.3882)\n",
            "Epoch: [71][19/19]\tTime 0.096 (Avg-Time 0.190)\t Loss 1.4648 (Avg-Loss 1.4228)\tAcc 48.4375 (Avg-Acc 49.3250)\n",
            "EPOCH: 71 train Results: Acc 49.325 Loss: 1.4228\n",
            "Epoch: [71][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.4108 (Avg-Loss 1.4108)\tAcc 51.4648 (Avg-Acc 51.4648)\n",
            "Epoch: [71][4/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.4111 (Avg-Loss 1.4019)\tAcc 50.2212 (Avg-Acc 50.6600)\n",
            "EPOCH: 71 Validation Results: Acc 50.660 Loss: 1.4019\n",
            "Best Accuracy: 50.7300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [72][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.3992 (Avg-Loss 1.3992)\tAcc 51.7578 (Avg-Acc 51.7578)\n",
            "Epoch: [72][4/19]\tTime 0.186 (Avg-Time 0.191)\t Loss 1.4358 (Avg-Loss 1.4173)\tAcc 48.1934 (Avg-Acc 49.5312)\n",
            "Epoch: [72][8/19]\tTime 0.307 (Avg-Time 0.205)\t Loss 1.4784 (Avg-Loss 1.4182)\tAcc 47.6074 (Avg-Acc 49.4954)\n",
            "Epoch: [72][12/19]\tTime 0.328 (Avg-Time 0.296)\t Loss 1.4089 (Avg-Loss 1.4204)\tAcc 48.6328 (Avg-Acc 49.3314)\n",
            "Epoch: [72][16/19]\tTime 0.186 (Avg-Time 0.296)\t Loss 1.3615 (Avg-Loss 1.4166)\tAcc 52.3438 (Avg-Acc 49.5433)\n",
            "Epoch: [72][19/19]\tTime 0.098 (Avg-Time 0.276)\t Loss 1.4089 (Avg-Loss 1.4165)\tAcc 48.3456 (Avg-Acc 49.5475)\n",
            "EPOCH: 72 train Results: Acc 49.547 Loss: 1.4165\n",
            "Epoch: [72][0/4]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.4067 (Avg-Loss 1.4067)\tAcc 50.8789 (Avg-Acc 50.8789)\n",
            "Epoch: [72][4/4]\tTime 0.041 (Avg-Time 0.053)\t Loss 1.4109 (Avg-Loss 1.3997)\tAcc 49.3916 (Avg-Acc 50.5100)\n",
            "EPOCH: 72 Validation Results: Acc 50.510 Loss: 1.3997\n",
            "Best Accuracy: 50.7300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [73][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.4143 (Avg-Loss 1.4143)\tAcc 50.7324 (Avg-Acc 50.7324)\n",
            "Epoch: [73][4/19]\tTime 0.212 (Avg-Time 0.198)\t Loss 1.4157 (Avg-Loss 1.4055)\tAcc 50.1953 (Avg-Acc 50.3125)\n",
            "Epoch: [73][8/19]\tTime 0.186 (Avg-Time 0.196)\t Loss 1.4138 (Avg-Loss 1.4150)\tAcc 50.1953 (Avg-Acc 49.6311)\n",
            "Epoch: [73][12/19]\tTime 0.187 (Avg-Time 0.196)\t Loss 1.4042 (Avg-Loss 1.4115)\tAcc 50.3418 (Avg-Acc 49.7709)\n",
            "Epoch: [73][16/19]\tTime 0.186 (Avg-Time 0.196)\t Loss 1.4262 (Avg-Loss 1.4156)\tAcc 49.1211 (Avg-Acc 49.7099)\n",
            "Epoch: [73][19/19]\tTime 0.114 (Avg-Time 0.192)\t Loss 1.4309 (Avg-Loss 1.4153)\tAcc 50.2757 (Avg-Acc 49.7825)\n",
            "EPOCH: 73 train Results: Acc 49.782 Loss: 1.4153\n",
            "Epoch: [73][0/4]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.4033 (Avg-Loss 1.4033)\tAcc 51.7090 (Avg-Acc 51.7090)\n",
            "Epoch: [73][4/4]\tTime 0.042 (Avg-Time 0.054)\t Loss 1.4072 (Avg-Loss 1.3974)\tAcc 49.9447 (Avg-Acc 50.7200)\n",
            "EPOCH: 73 Validation Results: Acc 50.720 Loss: 1.3974\n",
            "Best Accuracy: 50.7300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [74][0/19]\tTime 0.191 (Avg-Time 0.191)\t Loss 1.4471 (Avg-Loss 1.4471)\tAcc 48.9746 (Avg-Acc 48.9746)\n",
            "Epoch: [74][4/19]\tTime 0.213 (Avg-Time 0.196)\t Loss 1.4261 (Avg-Loss 1.4207)\tAcc 48.7305 (Avg-Acc 49.1602)\n",
            "Epoch: [74][8/19]\tTime 0.186 (Avg-Time 0.193)\t Loss 1.3977 (Avg-Loss 1.4106)\tAcc 50.7324 (Avg-Acc 49.5985)\n",
            "Epoch: [74][12/19]\tTime 0.189 (Avg-Time 0.194)\t Loss 1.4600 (Avg-Loss 1.4181)\tAcc 48.4375 (Avg-Acc 49.3803)\n",
            "Epoch: [74][16/19]\tTime 0.191 (Avg-Time 0.195)\t Loss 1.4194 (Avg-Loss 1.4123)\tAcc 49.7070 (Avg-Acc 49.6639)\n",
            "Epoch: [74][19/19]\tTime 0.097 (Avg-Time 0.190)\t Loss 1.3992 (Avg-Loss 1.4112)\tAcc 49.1728 (Avg-Acc 49.6250)\n",
            "EPOCH: 74 train Results: Acc 49.625 Loss: 1.4112\n",
            "Epoch: [74][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.4030 (Avg-Loss 1.4030)\tAcc 51.6602 (Avg-Acc 51.6602)\n",
            "Epoch: [74][4/4]\tTime 0.070 (Avg-Time 0.052)\t Loss 1.4045 (Avg-Loss 1.3942)\tAcc 50.1106 (Avg-Acc 50.9300)\n",
            "EPOCH: 74 Validation Results: Acc 50.930 Loss: 1.3942\n",
            "Best Accuracy: 50.9300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [75][0/19]\tTime 0.193 (Avg-Time 0.193)\t Loss 1.3480 (Avg-Loss 1.3480)\tAcc 52.3438 (Avg-Acc 52.3438)\n",
            "Epoch: [75][4/19]\tTime 0.362 (Avg-Time 0.226)\t Loss 1.3830 (Avg-Loss 1.3930)\tAcc 51.2207 (Avg-Acc 50.4980)\n",
            "Epoch: [75][8/19]\tTime 0.518 (Avg-Time 0.317)\t Loss 1.4169 (Avg-Loss 1.4010)\tAcc 48.6816 (Avg-Acc 50.1248)\n",
            "Epoch: [75][12/19]\tTime 0.211 (Avg-Time 0.329)\t Loss 1.4025 (Avg-Loss 1.4028)\tAcc 50.1953 (Avg-Acc 49.9962)\n",
            "Epoch: [75][16/19]\tTime 0.187 (Avg-Time 0.296)\t Loss 1.4535 (Avg-Loss 1.4067)\tAcc 47.0215 (Avg-Acc 49.8334)\n",
            "Epoch: [75][19/19]\tTime 0.095 (Avg-Time 0.277)\t Loss 1.4209 (Avg-Loss 1.4076)\tAcc 50.8272 (Avg-Acc 49.8325)\n",
            "EPOCH: 75 train Results: Acc 49.833 Loss: 1.4076\n",
            "Epoch: [75][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.4020 (Avg-Loss 1.4020)\tAcc 51.9043 (Avg-Acc 51.9043)\n",
            "Epoch: [75][4/4]\tTime 0.053 (Avg-Time 0.049)\t Loss 1.4029 (Avg-Loss 1.3927)\tAcc 50.1106 (Avg-Acc 50.9600)\n",
            "EPOCH: 75 Validation Results: Acc 50.960 Loss: 1.3927\n",
            "Best Accuracy: 50.9600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [76][0/19]\tTime 0.187 (Avg-Time 0.187)\t Loss 1.4271 (Avg-Loss 1.4271)\tAcc 48.3398 (Avg-Acc 48.3398)\n",
            "Epoch: [76][4/19]\tTime 0.190 (Avg-Time 0.199)\t Loss 1.4205 (Avg-Loss 1.4191)\tAcc 49.2188 (Avg-Acc 49.3262)\n",
            "Epoch: [76][8/19]\tTime 0.187 (Avg-Time 0.197)\t Loss 1.3775 (Avg-Loss 1.4056)\tAcc 49.9512 (Avg-Acc 49.7613)\n",
            "Epoch: [76][12/19]\tTime 0.194 (Avg-Time 0.195)\t Loss 1.4193 (Avg-Loss 1.4048)\tAcc 48.2910 (Avg-Acc 49.7934)\n",
            "Epoch: [76][16/19]\tTime 0.189 (Avg-Time 0.196)\t Loss 1.3939 (Avg-Loss 1.4045)\tAcc 48.6328 (Avg-Acc 49.7386)\n",
            "Epoch: [76][19/19]\tTime 0.099 (Avg-Time 0.192)\t Loss 1.4403 (Avg-Loss 1.4034)\tAcc 49.9081 (Avg-Acc 49.8025)\n",
            "EPOCH: 76 train Results: Acc 49.803 Loss: 1.4034\n",
            "Epoch: [76][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.3957 (Avg-Loss 1.3957)\tAcc 52.1973 (Avg-Acc 52.1973)\n",
            "Epoch: [76][4/4]\tTime 0.046 (Avg-Time 0.047)\t Loss 1.3985 (Avg-Loss 1.3883)\tAcc 50.2212 (Avg-Acc 51.1600)\n",
            "EPOCH: 76 Validation Results: Acc 51.160 Loss: 1.3883\n",
            "Best Accuracy: 51.1600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [77][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.4102 (Avg-Loss 1.4102)\tAcc 49.5605 (Avg-Acc 49.5605)\n",
            "Epoch: [77][4/19]\tTime 0.183 (Avg-Time 0.192)\t Loss 1.4280 (Avg-Loss 1.4079)\tAcc 48.0957 (Avg-Acc 49.8438)\n",
            "Epoch: [77][8/19]\tTime 0.212 (Avg-Time 0.194)\t Loss 1.3617 (Avg-Loss 1.3976)\tAcc 51.3672 (Avg-Acc 50.0434)\n",
            "Epoch: [77][12/19]\tTime 0.193 (Avg-Time 0.194)\t Loss 1.4127 (Avg-Loss 1.4001)\tAcc 49.8535 (Avg-Acc 49.9662)\n",
            "Epoch: [77][16/19]\tTime 0.198 (Avg-Time 0.195)\t Loss 1.3861 (Avg-Loss 1.4017)\tAcc 52.8320 (Avg-Acc 50.0201)\n",
            "Epoch: [77][19/19]\tTime 0.118 (Avg-Time 0.191)\t Loss 1.3629 (Avg-Loss 1.3985)\tAcc 51.1949 (Avg-Acc 50.0500)\n",
            "EPOCH: 77 train Results: Acc 50.050 Loss: 1.3985\n",
            "Epoch: [77][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.3935 (Avg-Loss 1.3935)\tAcc 51.7090 (Avg-Acc 51.7090)\n",
            "Epoch: [77][4/4]\tTime 0.049 (Avg-Time 0.048)\t Loss 1.3967 (Avg-Loss 1.3858)\tAcc 50.3872 (Avg-Acc 51.1900)\n",
            "EPOCH: 77 Validation Results: Acc 51.190 Loss: 1.3858\n",
            "Best Accuracy: 51.1900\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [78][0/19]\tTime 0.246 (Avg-Time 0.246)\t Loss 1.3961 (Avg-Loss 1.3961)\tAcc 50.0488 (Avg-Acc 50.0488)\n",
            "Epoch: [78][4/19]\tTime 0.633 (Avg-Time 0.442)\t Loss 1.4000 (Avg-Loss 1.3828)\tAcc 50.1465 (Avg-Acc 50.3027)\n",
            "Epoch: [78][8/19]\tTime 0.248 (Avg-Time 0.394)\t Loss 1.3767 (Avg-Loss 1.3896)\tAcc 50.5371 (Avg-Acc 50.3689)\n",
            "Epoch: [78][12/19]\tTime 0.196 (Avg-Time 0.333)\t Loss 1.3927 (Avg-Loss 1.3919)\tAcc 50.0000 (Avg-Acc 50.2855)\n",
            "Epoch: [78][16/19]\tTime 0.245 (Avg-Time 0.304)\t Loss 1.3845 (Avg-Loss 1.3933)\tAcc 51.1719 (Avg-Acc 50.3188)\n",
            "Epoch: [78][19/19]\tTime 0.099 (Avg-Time 0.283)\t Loss 1.3597 (Avg-Loss 1.3940)\tAcc 51.8382 (Avg-Acc 50.2800)\n",
            "EPOCH: 78 train Results: Acc 50.280 Loss: 1.3940\n",
            "Epoch: [78][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.3909 (Avg-Loss 1.3909)\tAcc 52.0996 (Avg-Acc 52.0996)\n",
            "Epoch: [78][4/4]\tTime 0.056 (Avg-Time 0.049)\t Loss 1.3934 (Avg-Loss 1.3834)\tAcc 49.8894 (Avg-Acc 51.2700)\n",
            "EPOCH: 78 Validation Results: Acc 51.270 Loss: 1.3834\n",
            "Best Accuracy: 51.2700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [79][0/19]\tTime 0.214 (Avg-Time 0.214)\t Loss 1.4146 (Avg-Loss 1.4146)\tAcc 49.4141 (Avg-Acc 49.4141)\n",
            "Epoch: [79][4/19]\tTime 0.186 (Avg-Time 0.198)\t Loss 1.3813 (Avg-Loss 1.3708)\tAcc 50.5371 (Avg-Acc 50.4492)\n",
            "Epoch: [79][8/19]\tTime 0.191 (Avg-Time 0.199)\t Loss 1.3761 (Avg-Loss 1.3782)\tAcc 49.8535 (Avg-Acc 50.1519)\n",
            "Epoch: [79][12/19]\tTime 0.189 (Avg-Time 0.197)\t Loss 1.4241 (Avg-Loss 1.3801)\tAcc 48.8281 (Avg-Acc 50.2066)\n",
            "Epoch: [79][16/19]\tTime 0.202 (Avg-Time 0.196)\t Loss 1.4205 (Avg-Loss 1.3855)\tAcc 49.0723 (Avg-Acc 50.2039)\n",
            "Epoch: [79][19/19]\tTime 0.099 (Avg-Time 0.191)\t Loss 1.4214 (Avg-Loss 1.3875)\tAcc 50.6434 (Avg-Acc 50.1675)\n",
            "EPOCH: 79 train Results: Acc 50.167 Loss: 1.3875\n",
            "Epoch: [79][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.3862 (Avg-Loss 1.3862)\tAcc 52.6367 (Avg-Acc 52.6367)\n",
            "Epoch: [79][4/4]\tTime 0.056 (Avg-Time 0.049)\t Loss 1.3874 (Avg-Loss 1.3787)\tAcc 50.6084 (Avg-Acc 51.5700)\n",
            "EPOCH: 79 Validation Results: Acc 51.570 Loss: 1.3787\n",
            "Best Accuracy: 51.5700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [80][0/19]\tTime 0.212 (Avg-Time 0.212)\t Loss 1.3736 (Avg-Loss 1.3736)\tAcc 50.6836 (Avg-Acc 50.6836)\n",
            "Epoch: [80][4/19]\tTime 0.186 (Avg-Time 0.194)\t Loss 1.3836 (Avg-Loss 1.3771)\tAcc 51.4160 (Avg-Acc 51.6211)\n",
            "Epoch: [80][8/19]\tTime 0.190 (Avg-Time 0.197)\t Loss 1.3719 (Avg-Loss 1.3760)\tAcc 50.5859 (Avg-Acc 51.3129)\n",
            "Epoch: [80][12/19]\tTime 0.192 (Avg-Time 0.197)\t Loss 1.3860 (Avg-Loss 1.3843)\tAcc 51.6113 (Avg-Acc 50.9428)\n",
            "Epoch: [80][16/19]\tTime 0.210 (Avg-Time 0.196)\t Loss 1.3803 (Avg-Loss 1.3861)\tAcc 50.6348 (Avg-Acc 50.7899)\n",
            "Epoch: [80][19/19]\tTime 0.330 (Avg-Time 0.213)\t Loss 1.4215 (Avg-Loss 1.3895)\tAcc 48.8971 (Avg-Acc 50.6250)\n",
            "EPOCH: 80 train Results: Acc 50.625 Loss: 1.3895\n",
            "Epoch: [80][0/4]\tTime 0.123 (Avg-Time 0.123)\t Loss 1.3826 (Avg-Loss 1.3826)\tAcc 52.3438 (Avg-Acc 52.3438)\n",
            "Epoch: [80][4/4]\tTime 0.114 (Avg-Time 0.118)\t Loss 1.3861 (Avg-Loss 1.3759)\tAcc 50.4978 (Avg-Acc 51.4600)\n",
            "EPOCH: 80 Validation Results: Acc 51.460 Loss: 1.3759\n",
            "Best Accuracy: 51.5700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [81][0/19]\tTime 0.604 (Avg-Time 0.604)\t Loss 1.3826 (Avg-Loss 1.3826)\tAcc 49.5117 (Avg-Acc 49.5117)\n",
            "Epoch: [81][4/19]\tTime 0.281 (Avg-Time 0.397)\t Loss 1.3925 (Avg-Loss 1.3921)\tAcc 51.8066 (Avg-Acc 50.5273)\n",
            "Epoch: [81][8/19]\tTime 0.222 (Avg-Time 0.309)\t Loss 1.3785 (Avg-Loss 1.3847)\tAcc 50.0977 (Avg-Acc 50.3743)\n",
            "Epoch: [81][12/19]\tTime 0.191 (Avg-Time 0.273)\t Loss 1.4045 (Avg-Loss 1.3875)\tAcc 50.1465 (Avg-Acc 50.2517)\n",
            "Epoch: [81][16/19]\tTime 0.191 (Avg-Time 0.255)\t Loss 1.3842 (Avg-Loss 1.3856)\tAcc 50.3906 (Avg-Acc 50.2298)\n",
            "Epoch: [81][19/19]\tTime 0.126 (Avg-Time 0.243)\t Loss 1.3589 (Avg-Loss 1.3816)\tAcc 52.5735 (Avg-Acc 50.4325)\n",
            "EPOCH: 81 train Results: Acc 50.432 Loss: 1.3816\n",
            "Epoch: [81][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.3809 (Avg-Loss 1.3809)\tAcc 51.9043 (Avg-Acc 51.9043)\n",
            "Epoch: [81][4/4]\tTime 0.051 (Avg-Time 0.048)\t Loss 1.3837 (Avg-Loss 1.3734)\tAcc 50.3319 (Avg-Acc 51.4300)\n",
            "EPOCH: 81 Validation Results: Acc 51.430 Loss: 1.3734\n",
            "Best Accuracy: 51.5700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [82][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.3824 (Avg-Loss 1.3824)\tAcc 49.8047 (Avg-Acc 49.8047)\n",
            "Epoch: [82][4/19]\tTime 0.191 (Avg-Time 0.194)\t Loss 1.3331 (Avg-Loss 1.3741)\tAcc 52.7344 (Avg-Acc 50.8398)\n",
            "Epoch: [82][8/19]\tTime 0.187 (Avg-Time 0.194)\t Loss 1.4087 (Avg-Loss 1.3737)\tAcc 50.2930 (Avg-Acc 50.9440)\n",
            "Epoch: [82][12/19]\tTime 0.194 (Avg-Time 0.194)\t Loss 1.3938 (Avg-Loss 1.3800)\tAcc 51.3184 (Avg-Acc 50.7287)\n",
            "Epoch: [82][16/19]\tTime 0.187 (Avg-Time 0.194)\t Loss 1.3992 (Avg-Loss 1.3812)\tAcc 50.3906 (Avg-Acc 50.6434)\n",
            "Epoch: [82][19/19]\tTime 0.100 (Avg-Time 0.190)\t Loss 1.4041 (Avg-Loss 1.3803)\tAcc 50.1838 (Avg-Acc 50.6850)\n",
            "EPOCH: 82 train Results: Acc 50.685 Loss: 1.3803\n",
            "Epoch: [82][0/4]\tTime 0.069 (Avg-Time 0.069)\t Loss 1.3778 (Avg-Loss 1.3778)\tAcc 52.3438 (Avg-Acc 52.3438)\n",
            "Epoch: [82][4/4]\tTime 0.042 (Avg-Time 0.054)\t Loss 1.3804 (Avg-Loss 1.3702)\tAcc 50.6084 (Avg-Acc 51.7100)\n",
            "EPOCH: 82 Validation Results: Acc 51.710 Loss: 1.3702\n",
            "Best Accuracy: 51.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [83][0/19]\tTime 0.194 (Avg-Time 0.194)\t Loss 1.3718 (Avg-Loss 1.3718)\tAcc 50.4395 (Avg-Acc 50.4395)\n",
            "Epoch: [83][4/19]\tTime 0.201 (Avg-Time 0.195)\t Loss 1.3933 (Avg-Loss 1.3783)\tAcc 49.6094 (Avg-Acc 50.1953)\n",
            "Epoch: [83][8/19]\tTime 0.190 (Avg-Time 0.194)\t Loss 1.3783 (Avg-Loss 1.3758)\tAcc 50.5859 (Avg-Acc 50.5100)\n",
            "Epoch: [83][12/19]\tTime 0.194 (Avg-Time 0.195)\t Loss 1.3980 (Avg-Loss 1.3721)\tAcc 50.1465 (Avg-Acc 50.6949)\n",
            "Epoch: [83][16/19]\tTime 0.610 (Avg-Time 0.241)\t Loss 1.3934 (Avg-Loss 1.3799)\tAcc 51.1719 (Avg-Acc 50.4624)\n",
            "Epoch: [83][19/19]\tTime 0.204 (Avg-Time 0.268)\t Loss 1.3398 (Avg-Loss 1.3793)\tAcc 52.5735 (Avg-Acc 50.4900)\n",
            "EPOCH: 83 train Results: Acc 50.490 Loss: 1.3793\n",
            "Epoch: [83][0/4]\tTime 0.109 (Avg-Time 0.109)\t Loss 1.3751 (Avg-Loss 1.3751)\tAcc 52.1973 (Avg-Acc 52.1973)\n",
            "Epoch: [83][4/4]\tTime 0.054 (Avg-Time 0.100)\t Loss 1.3771 (Avg-Loss 1.3672)\tAcc 50.4425 (Avg-Acc 51.7900)\n",
            "EPOCH: 83 Validation Results: Acc 51.790 Loss: 1.3672\n",
            "Best Accuracy: 51.7900\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [84][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.3933 (Avg-Loss 1.3933)\tAcc 49.4141 (Avg-Acc 49.4141)\n",
            "Epoch: [84][4/19]\tTime 0.185 (Avg-Time 0.194)\t Loss 1.3954 (Avg-Loss 1.3782)\tAcc 50.0977 (Avg-Acc 50.5762)\n",
            "Epoch: [84][8/19]\tTime 0.187 (Avg-Time 0.195)\t Loss 1.3515 (Avg-Loss 1.3770)\tAcc 51.8066 (Avg-Acc 50.6510)\n",
            "Epoch: [84][12/19]\tTime 0.209 (Avg-Time 0.196)\t Loss 1.3626 (Avg-Loss 1.3729)\tAcc 51.5625 (Avg-Acc 50.8714)\n",
            "Epoch: [84][16/19]\tTime 0.185 (Avg-Time 0.194)\t Loss 1.3567 (Avg-Loss 1.3680)\tAcc 51.0742 (Avg-Acc 51.1661)\n",
            "Epoch: [84][19/19]\tTime 0.102 (Avg-Time 0.191)\t Loss 1.3896 (Avg-Loss 1.3686)\tAcc 51.4706 (Avg-Acc 51.2550)\n",
            "EPOCH: 84 train Results: Acc 51.255 Loss: 1.3686\n",
            "Epoch: [84][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.3723 (Avg-Loss 1.3723)\tAcc 52.2461 (Avg-Acc 52.2461)\n",
            "Epoch: [84][4/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.3726 (Avg-Loss 1.3638)\tAcc 51.1062 (Avg-Acc 51.9400)\n",
            "EPOCH: 84 Validation Results: Acc 51.940 Loss: 1.3638\n",
            "Best Accuracy: 51.9400\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [85][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.3951 (Avg-Loss 1.3951)\tAcc 48.8770 (Avg-Acc 48.8770)\n",
            "Epoch: [85][4/19]\tTime 0.184 (Avg-Time 0.195)\t Loss 1.3564 (Avg-Loss 1.3796)\tAcc 52.0508 (Avg-Acc 50.4004)\n",
            "Epoch: [85][8/19]\tTime 0.192 (Avg-Time 0.202)\t Loss 1.3750 (Avg-Loss 1.3677)\tAcc 50.8301 (Avg-Acc 50.8409)\n",
            "Epoch: [85][12/19]\tTime 0.219 (Avg-Time 0.200)\t Loss 1.3313 (Avg-Loss 1.3677)\tAcc 52.5391 (Avg-Acc 50.8827)\n",
            "Epoch: [85][16/19]\tTime 0.186 (Avg-Time 0.197)\t Loss 1.3439 (Avg-Loss 1.3673)\tAcc 52.6367 (Avg-Acc 50.9450)\n",
            "Epoch: [85][19/19]\tTime 0.098 (Avg-Time 0.193)\t Loss 1.3464 (Avg-Loss 1.3686)\tAcc 52.3897 (Avg-Acc 50.7950)\n",
            "EPOCH: 85 train Results: Acc 50.795 Loss: 1.3686\n",
            "Epoch: [85][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.3690 (Avg-Loss 1.3690)\tAcc 52.7832 (Avg-Acc 52.7832)\n",
            "Epoch: [85][4/4]\tTime 0.053 (Avg-Time 0.050)\t Loss 1.3699 (Avg-Loss 1.3622)\tAcc 51.4381 (Avg-Acc 51.9600)\n",
            "EPOCH: 85 Validation Results: Acc 51.960 Loss: 1.3622\n",
            "Best Accuracy: 51.9600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [86][0/19]\tTime 0.192 (Avg-Time 0.192)\t Loss 1.3831 (Avg-Loss 1.3831)\tAcc 50.3906 (Avg-Acc 50.3906)\n",
            "Epoch: [86][4/19]\tTime 0.191 (Avg-Time 0.197)\t Loss 1.3650 (Avg-Loss 1.3693)\tAcc 52.0020 (Avg-Acc 50.8496)\n",
            "Epoch: [86][8/19]\tTime 0.186 (Avg-Time 0.197)\t Loss 1.3030 (Avg-Loss 1.3625)\tAcc 52.7832 (Avg-Acc 51.0634)\n",
            "Epoch: [86][12/19]\tTime 0.606 (Avg-Time 0.281)\t Loss 1.3538 (Avg-Loss 1.3618)\tAcc 50.4395 (Avg-Acc 51.1230)\n",
            "Epoch: [86][16/19]\tTime 0.365 (Avg-Time 0.303)\t Loss 1.3840 (Avg-Loss 1.3664)\tAcc 50.7812 (Avg-Acc 50.9622)\n",
            "Epoch: [86][19/19]\tTime 0.101 (Avg-Time 0.283)\t Loss 1.3740 (Avg-Loss 1.3661)\tAcc 51.9301 (Avg-Acc 50.9350)\n",
            "EPOCH: 86 train Results: Acc 50.935 Loss: 1.3661\n",
            "Epoch: [86][0/4]\tTime 0.069 (Avg-Time 0.069)\t Loss 1.3671 (Avg-Loss 1.3671)\tAcc 52.5879 (Avg-Acc 52.5879)\n",
            "Epoch: [86][4/4]\tTime 0.043 (Avg-Time 0.054)\t Loss 1.3687 (Avg-Loss 1.3594)\tAcc 50.9403 (Avg-Acc 51.9700)\n",
            "EPOCH: 86 Validation Results: Acc 51.970 Loss: 1.3594\n",
            "Best Accuracy: 51.9700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [87][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.3783 (Avg-Loss 1.3783)\tAcc 50.7324 (Avg-Acc 50.7324)\n",
            "Epoch: [87][4/19]\tTime 0.202 (Avg-Time 0.195)\t Loss 1.3405 (Avg-Loss 1.3585)\tAcc 54.5410 (Avg-Acc 52.0020)\n",
            "Epoch: [87][8/19]\tTime 0.188 (Avg-Time 0.193)\t Loss 1.3643 (Avg-Loss 1.3610)\tAcc 49.9023 (Avg-Acc 51.2695)\n",
            "Epoch: [87][12/19]\tTime 0.192 (Avg-Time 0.195)\t Loss 1.3953 (Avg-Loss 1.3661)\tAcc 49.9023 (Avg-Acc 51.0930)\n",
            "Epoch: [87][16/19]\tTime 0.194 (Avg-Time 0.196)\t Loss 1.3893 (Avg-Loss 1.3668)\tAcc 51.3184 (Avg-Acc 51.1001)\n",
            "Epoch: [87][19/19]\tTime 0.098 (Avg-Time 0.191)\t Loss 1.3946 (Avg-Loss 1.3642)\tAcc 50.8272 (Avg-Acc 51.2500)\n",
            "EPOCH: 87 train Results: Acc 51.250 Loss: 1.3642\n",
            "Epoch: [87][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.3649 (Avg-Loss 1.3649)\tAcc 52.3926 (Avg-Acc 52.3926)\n",
            "Epoch: [87][4/4]\tTime 0.042 (Avg-Time 0.054)\t Loss 1.3663 (Avg-Loss 1.3569)\tAcc 51.7146 (Avg-Acc 52.3200)\n",
            "EPOCH: 87 Validation Results: Acc 52.320 Loss: 1.3569\n",
            "Best Accuracy: 52.3200\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [88][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.3443 (Avg-Loss 1.3443)\tAcc 51.2695 (Avg-Acc 51.2695)\n",
            "Epoch: [88][4/19]\tTime 0.211 (Avg-Time 0.194)\t Loss 1.3279 (Avg-Loss 1.3480)\tAcc 51.5625 (Avg-Acc 52.1387)\n",
            "Epoch: [88][8/19]\tTime 0.187 (Avg-Time 0.193)\t Loss 1.3521 (Avg-Loss 1.3556)\tAcc 50.5371 (Avg-Acc 51.5245)\n",
            "Epoch: [88][12/19]\tTime 0.192 (Avg-Time 0.194)\t Loss 1.3862 (Avg-Loss 1.3610)\tAcc 50.3906 (Avg-Acc 51.4761)\n",
            "Epoch: [88][16/19]\tTime 0.186 (Avg-Time 0.194)\t Loss 1.3621 (Avg-Loss 1.3631)\tAcc 50.8301 (Avg-Acc 51.3184)\n",
            "Epoch: [88][19/19]\tTime 0.096 (Avg-Time 0.189)\t Loss 1.3295 (Avg-Loss 1.3631)\tAcc 52.3897 (Avg-Acc 51.2475)\n",
            "EPOCH: 88 train Results: Acc 51.248 Loss: 1.3631\n",
            "Epoch: [88][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.3604 (Avg-Loss 1.3604)\tAcc 52.5391 (Avg-Acc 52.5391)\n",
            "Epoch: [88][4/4]\tTime 0.053 (Avg-Time 0.049)\t Loss 1.3628 (Avg-Loss 1.3543)\tAcc 51.3827 (Avg-Acc 52.1100)\n",
            "EPOCH: 88 Validation Results: Acc 52.110 Loss: 1.3543\n",
            "Best Accuracy: 52.3200\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [89][0/19]\tTime 0.199 (Avg-Time 0.199)\t Loss 1.3258 (Avg-Loss 1.3258)\tAcc 52.0020 (Avg-Acc 52.0020)\n",
            "Epoch: [89][4/19]\tTime 0.183 (Avg-Time 0.191)\t Loss 1.3418 (Avg-Loss 1.3380)\tAcc 52.4414 (Avg-Acc 52.0898)\n",
            "Epoch: [89][8/19]\tTime 0.602 (Avg-Time 0.301)\t Loss 1.3631 (Avg-Loss 1.3461)\tAcc 50.5859 (Avg-Acc 51.5679)\n",
            "Epoch: [89][12/19]\tTime 0.371 (Avg-Time 0.329)\t Loss 1.3805 (Avg-Loss 1.3508)\tAcc 50.2930 (Avg-Acc 51.3747)\n",
            "Epoch: [89][16/19]\tTime 0.182 (Avg-Time 0.296)\t Loss 1.3632 (Avg-Loss 1.3524)\tAcc 50.7812 (Avg-Acc 51.2264)\n",
            "Epoch: [89][19/19]\tTime 0.103 (Avg-Time 0.277)\t Loss 1.3801 (Avg-Loss 1.3568)\tAcc 50.4596 (Avg-Acc 51.1575)\n",
            "EPOCH: 89 train Results: Acc 51.157 Loss: 1.3568\n",
            "Epoch: [89][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.3593 (Avg-Loss 1.3593)\tAcc 52.5391 (Avg-Acc 52.5391)\n",
            "Epoch: [89][4/4]\tTime 0.057 (Avg-Time 0.050)\t Loss 1.3602 (Avg-Loss 1.3509)\tAcc 51.3274 (Avg-Acc 52.2900)\n",
            "EPOCH: 89 Validation Results: Acc 52.290 Loss: 1.3509\n",
            "Best Accuracy: 52.3200\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [90][0/19]\tTime 0.187 (Avg-Time 0.187)\t Loss 1.3724 (Avg-Loss 1.3724)\tAcc 51.3672 (Avg-Acc 51.3672)\n",
            "Epoch: [90][4/19]\tTime 0.190 (Avg-Time 0.195)\t Loss 1.3303 (Avg-Loss 1.3527)\tAcc 51.6602 (Avg-Acc 51.6992)\n",
            "Epoch: [90][8/19]\tTime 0.205 (Avg-Time 0.195)\t Loss 1.3475 (Avg-Loss 1.3537)\tAcc 51.3672 (Avg-Acc 51.5462)\n",
            "Epoch: [90][12/19]\tTime 0.183 (Avg-Time 0.193)\t Loss 1.2944 (Avg-Loss 1.3487)\tAcc 54.7852 (Avg-Acc 51.7240)\n",
            "Epoch: [90][16/19]\tTime 0.180 (Avg-Time 0.193)\t Loss 1.3396 (Avg-Loss 1.3501)\tAcc 50.9277 (Avg-Acc 51.6659)\n",
            "Epoch: [90][19/19]\tTime 0.099 (Avg-Time 0.190)\t Loss 1.3306 (Avg-Loss 1.3505)\tAcc 52.0221 (Avg-Acc 51.6475)\n",
            "EPOCH: 90 train Results: Acc 51.648 Loss: 1.3505\n",
            "Epoch: [90][0/4]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.3553 (Avg-Loss 1.3553)\tAcc 53.0762 (Avg-Acc 53.0762)\n",
            "Epoch: [90][4/4]\tTime 0.052 (Avg-Time 0.048)\t Loss 1.3554 (Avg-Loss 1.3477)\tAcc 51.2168 (Avg-Acc 52.4300)\n",
            "EPOCH: 90 Validation Results: Acc 52.430 Loss: 1.3477\n",
            "Best Accuracy: 52.4300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [91][0/19]\tTime 0.183 (Avg-Time 0.183)\t Loss 1.3138 (Avg-Loss 1.3138)\tAcc 53.7598 (Avg-Acc 53.7598)\n",
            "Epoch: [91][4/19]\tTime 0.183 (Avg-Time 0.193)\t Loss 1.3839 (Avg-Loss 1.3359)\tAcc 50.1465 (Avg-Acc 52.2656)\n",
            "Epoch: [91][8/19]\tTime 0.210 (Avg-Time 0.194)\t Loss 1.3497 (Avg-Loss 1.3385)\tAcc 51.3184 (Avg-Acc 52.4902)\n",
            "Epoch: [91][12/19]\tTime 0.183 (Avg-Time 0.192)\t Loss 1.3585 (Avg-Loss 1.3456)\tAcc 51.4648 (Avg-Acc 51.9681)\n",
            "Epoch: [91][16/19]\tTime 0.185 (Avg-Time 0.193)\t Loss 1.3400 (Avg-Loss 1.3468)\tAcc 52.0508 (Avg-Acc 51.9301)\n",
            "Epoch: [91][19/19]\tTime 0.124 (Avg-Time 0.191)\t Loss 1.3664 (Avg-Loss 1.3456)\tAcc 51.1029 (Avg-Acc 52.0125)\n",
            "EPOCH: 91 train Results: Acc 52.013 Loss: 1.3456\n",
            "Epoch: [91][0/4]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.3512 (Avg-Loss 1.3512)\tAcc 52.7344 (Avg-Acc 52.7344)\n",
            "Epoch: [91][4/4]\tTime 0.054 (Avg-Time 0.048)\t Loss 1.3519 (Avg-Loss 1.3450)\tAcc 52.7655 (Avg-Acc 52.7000)\n",
            "EPOCH: 91 Validation Results: Acc 52.700 Loss: 1.3450\n",
            "Best Accuracy: 52.7000\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [92][0/19]\tTime 0.185 (Avg-Time 0.185)\t Loss 1.3440 (Avg-Loss 1.3440)\tAcc 52.0020 (Avg-Acc 52.0020)\n",
            "Epoch: [92][4/19]\tTime 0.618 (Avg-Time 0.390)\t Loss 1.3314 (Avg-Loss 1.3349)\tAcc 52.4902 (Avg-Acc 52.6074)\n",
            "Epoch: [92][8/19]\tTime 0.306 (Avg-Time 0.393)\t Loss 1.3464 (Avg-Loss 1.3447)\tAcc 51.8066 (Avg-Acc 51.8880)\n",
            "Epoch: [92][12/19]\tTime 0.187 (Avg-Time 0.332)\t Loss 1.3528 (Avg-Loss 1.3462)\tAcc 50.5859 (Avg-Acc 52.0020)\n",
            "Epoch: [92][16/19]\tTime 0.212 (Avg-Time 0.300)\t Loss 1.3503 (Avg-Loss 1.3465)\tAcc 51.9531 (Avg-Acc 51.8871)\n",
            "Epoch: [92][19/19]\tTime 0.103 (Avg-Time 0.282)\t Loss 1.3574 (Avg-Loss 1.3474)\tAcc 51.9301 (Avg-Acc 51.8400)\n",
            "EPOCH: 92 train Results: Acc 51.840 Loss: 1.3474\n",
            "Epoch: [92][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.3498 (Avg-Loss 1.3498)\tAcc 52.6855 (Avg-Acc 52.6855)\n",
            "Epoch: [92][4/4]\tTime 0.057 (Avg-Time 0.050)\t Loss 1.3523 (Avg-Loss 1.3438)\tAcc 52.5996 (Avg-Acc 52.6600)\n",
            "EPOCH: 92 Validation Results: Acc 52.660 Loss: 1.3438\n",
            "Best Accuracy: 52.7000\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [93][0/19]\tTime 0.236 (Avg-Time 0.236)\t Loss 1.3842 (Avg-Loss 1.3842)\tAcc 50.6836 (Avg-Acc 50.6836)\n",
            "Epoch: [93][4/19]\tTime 0.189 (Avg-Time 0.199)\t Loss 1.3331 (Avg-Loss 1.3387)\tAcc 52.1973 (Avg-Acc 52.5000)\n",
            "Epoch: [93][8/19]\tTime 0.191 (Avg-Time 0.198)\t Loss 1.3197 (Avg-Loss 1.3431)\tAcc 52.6367 (Avg-Acc 52.1918)\n",
            "Epoch: [93][12/19]\tTime 0.190 (Avg-Time 0.197)\t Loss 1.3333 (Avg-Loss 1.3403)\tAcc 53.1250 (Avg-Acc 52.3362)\n",
            "Epoch: [93][16/19]\tTime 0.209 (Avg-Time 0.196)\t Loss 1.3761 (Avg-Loss 1.3475)\tAcc 50.5371 (Avg-Acc 51.9445)\n",
            "Epoch: [93][19/19]\tTime 0.097 (Avg-Time 0.191)\t Loss 1.3505 (Avg-Loss 1.3449)\tAcc 52.2059 (Avg-Acc 52.0025)\n",
            "EPOCH: 93 train Results: Acc 52.002 Loss: 1.3449\n",
            "Epoch: [93][0/4]\tTime 0.054 (Avg-Time 0.054)\t Loss 1.3445 (Avg-Loss 1.3445)\tAcc 52.9297 (Avg-Acc 52.9297)\n",
            "Epoch: [93][4/4]\tTime 0.051 (Avg-Time 0.050)\t Loss 1.3446 (Avg-Loss 1.3383)\tAcc 52.2124 (Avg-Acc 52.7000)\n",
            "EPOCH: 93 Validation Results: Acc 52.700 Loss: 1.3383\n",
            "Best Accuracy: 52.7000\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [94][0/19]\tTime 0.192 (Avg-Time 0.192)\t Loss 1.3578 (Avg-Loss 1.3578)\tAcc 51.8066 (Avg-Acc 51.8066)\n",
            "Epoch: [94][4/19]\tTime 0.186 (Avg-Time 0.197)\t Loss 1.3380 (Avg-Loss 1.3404)\tAcc 52.2949 (Avg-Acc 51.7871)\n",
            "Epoch: [94][8/19]\tTime 0.193 (Avg-Time 0.197)\t Loss 1.3680 (Avg-Loss 1.3421)\tAcc 51.4648 (Avg-Acc 52.0020)\n",
            "Epoch: [94][12/19]\tTime 0.185 (Avg-Time 0.195)\t Loss 1.3188 (Avg-Loss 1.3390)\tAcc 52.9785 (Avg-Acc 52.1747)\n",
            "Epoch: [94][16/19]\tTime 0.184 (Avg-Time 0.194)\t Loss 1.3441 (Avg-Loss 1.3392)\tAcc 51.8555 (Avg-Acc 52.0967)\n",
            "Epoch: [94][19/19]\tTime 0.216 (Avg-Time 0.203)\t Loss 1.3900 (Avg-Loss 1.3405)\tAcc 49.8162 (Avg-Acc 51.9400)\n",
            "EPOCH: 94 train Results: Acc 51.940 Loss: 1.3405\n",
            "Epoch: [94][0/4]\tTime 0.116 (Avg-Time 0.116)\t Loss 1.3445 (Avg-Loss 1.3445)\tAcc 53.3203 (Avg-Acc 53.3203)\n",
            "Epoch: [94][4/4]\tTime 0.099 (Avg-Time 0.102)\t Loss 1.3440 (Avg-Loss 1.3390)\tAcc 52.4336 (Avg-Acc 52.4800)\n",
            "EPOCH: 94 Validation Results: Acc 52.480 Loss: 1.3390\n",
            "Best Accuracy: 52.7000\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [95][0/19]\tTime 0.503 (Avg-Time 0.503)\t Loss 1.3428 (Avg-Loss 1.3428)\tAcc 51.3672 (Avg-Acc 51.3672)\n",
            "Epoch: [95][4/19]\tTime 0.350 (Avg-Time 0.409)\t Loss 1.3401 (Avg-Loss 1.3248)\tAcc 51.6113 (Avg-Acc 52.5879)\n",
            "Epoch: [95][8/19]\tTime 0.198 (Avg-Time 0.332)\t Loss 1.3273 (Avg-Loss 1.3334)\tAcc 52.4414 (Avg-Acc 52.0399)\n",
            "Epoch: [95][12/19]\tTime 0.188 (Avg-Time 0.290)\t Loss 1.3027 (Avg-Loss 1.3302)\tAcc 53.0273 (Avg-Acc 52.2912)\n",
            "Epoch: [95][16/19]\tTime 0.182 (Avg-Time 0.269)\t Loss 1.3676 (Avg-Loss 1.3316)\tAcc 50.0488 (Avg-Acc 52.1628)\n",
            "Epoch: [95][19/19]\tTime 0.099 (Avg-Time 0.252)\t Loss 1.3200 (Avg-Loss 1.3312)\tAcc 52.5735 (Avg-Acc 52.2325)\n",
            "EPOCH: 95 train Results: Acc 52.233 Loss: 1.3312\n",
            "Epoch: [95][0/4]\tTime 0.062 (Avg-Time 0.062)\t Loss 1.3383 (Avg-Loss 1.3383)\tAcc 53.3203 (Avg-Acc 53.3203)\n",
            "Epoch: [95][4/4]\tTime 0.046 (Avg-Time 0.056)\t Loss 1.3386 (Avg-Loss 1.3335)\tAcc 52.5442 (Avg-Acc 53.0800)\n",
            "EPOCH: 95 Validation Results: Acc 53.080 Loss: 1.3335\n",
            "Best Accuracy: 53.0800\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [96][0/19]\tTime 0.184 (Avg-Time 0.184)\t Loss 1.2932 (Avg-Loss 1.2932)\tAcc 54.6387 (Avg-Acc 54.6387)\n",
            "Epoch: [96][4/19]\tTime 0.206 (Avg-Time 0.190)\t Loss 1.3422 (Avg-Loss 1.3235)\tAcc 51.0742 (Avg-Acc 52.4805)\n",
            "Epoch: [96][8/19]\tTime 0.186 (Avg-Time 0.190)\t Loss 1.3619 (Avg-Loss 1.3293)\tAcc 51.0254 (Avg-Acc 52.2407)\n",
            "Epoch: [96][12/19]\tTime 0.186 (Avg-Time 0.192)\t Loss 1.3282 (Avg-Loss 1.3300)\tAcc 52.2461 (Avg-Acc 52.0132)\n",
            "Epoch: [96][16/19]\tTime 0.189 (Avg-Time 0.193)\t Loss 1.3510 (Avg-Loss 1.3311)\tAcc 51.5137 (Avg-Acc 52.0393)\n",
            "Epoch: [96][19/19]\tTime 0.097 (Avg-Time 0.188)\t Loss 1.3543 (Avg-Loss 1.3311)\tAcc 53.0331 (Avg-Acc 52.1375)\n",
            "EPOCH: 96 train Results: Acc 52.138 Loss: 1.3311\n",
            "Epoch: [96][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.3397 (Avg-Loss 1.3397)\tAcc 53.3203 (Avg-Acc 53.3203)\n",
            "Epoch: [96][4/4]\tTime 0.061 (Avg-Time 0.054)\t Loss 1.3394 (Avg-Loss 1.3326)\tAcc 52.5996 (Avg-Acc 52.9900)\n",
            "EPOCH: 96 Validation Results: Acc 52.990 Loss: 1.3326\n",
            "Best Accuracy: 53.0800\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [97][0/19]\tTime 0.183 (Avg-Time 0.183)\t Loss 1.3195 (Avg-Loss 1.3195)\tAcc 52.5391 (Avg-Acc 52.5391)\n",
            "Epoch: [97][4/19]\tTime 0.190 (Avg-Time 0.188)\t Loss 1.3362 (Avg-Loss 1.3176)\tAcc 52.1484 (Avg-Acc 52.5195)\n",
            "Epoch: [97][8/19]\tTime 0.187 (Avg-Time 0.191)\t Loss 1.3013 (Avg-Loss 1.3215)\tAcc 53.4668 (Avg-Acc 52.5065)\n",
            "Epoch: [97][12/19]\tTime 0.184 (Avg-Time 0.193)\t Loss 1.3095 (Avg-Loss 1.3257)\tAcc 52.6367 (Avg-Acc 52.3513)\n",
            "Epoch: [97][16/19]\tTime 0.377 (Avg-Time 0.205)\t Loss 1.3392 (Avg-Loss 1.3293)\tAcc 52.5879 (Avg-Acc 52.3868)\n",
            "Epoch: [97][19/19]\tTime 0.426 (Avg-Time 0.246)\t Loss 1.3566 (Avg-Loss 1.3289)\tAcc 50.9191 (Avg-Acc 52.3325)\n",
            "EPOCH: 97 train Results: Acc 52.333 Loss: 1.3289\n",
            "Epoch: [97][0/4]\tTime 0.119 (Avg-Time 0.119)\t Loss 1.3363 (Avg-Loss 1.3363)\tAcc 53.8574 (Avg-Acc 53.8574)\n",
            "Epoch: [97][4/4]\tTime 0.088 (Avg-Time 0.101)\t Loss 1.3349 (Avg-Loss 1.3307)\tAcc 52.5996 (Avg-Acc 53.0100)\n",
            "EPOCH: 97 Validation Results: Acc 53.010 Loss: 1.3307\n",
            "Best Accuracy: 53.0800\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [98][0/19]\tTime 0.403 (Avg-Time 0.403)\t Loss 1.2872 (Avg-Loss 1.2872)\tAcc 54.9805 (Avg-Acc 54.9805)\n",
            "Epoch: [98][4/19]\tTime 0.190 (Avg-Time 0.288)\t Loss 1.2786 (Avg-Loss 1.3177)\tAcc 53.5645 (Avg-Acc 52.8223)\n",
            "Epoch: [98][8/19]\tTime 0.185 (Avg-Time 0.246)\t Loss 1.3208 (Avg-Loss 1.3175)\tAcc 53.0273 (Avg-Acc 52.9839)\n",
            "Epoch: [98][12/19]\tTime 0.207 (Avg-Time 0.231)\t Loss 1.3375 (Avg-Loss 1.3225)\tAcc 52.2461 (Avg-Acc 52.8057)\n",
            "Epoch: [98][16/19]\tTime 0.189 (Avg-Time 0.222)\t Loss 1.3352 (Avg-Loss 1.3221)\tAcc 52.9297 (Avg-Acc 52.6913)\n",
            "Epoch: [98][19/19]\tTime 0.097 (Avg-Time 0.214)\t Loss 1.3868 (Avg-Loss 1.3232)\tAcc 51.8382 (Avg-Acc 52.7500)\n",
            "EPOCH: 98 train Results: Acc 52.750 Loss: 1.3232\n",
            "Epoch: [98][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.3326 (Avg-Loss 1.3326)\tAcc 53.9062 (Avg-Acc 53.9062)\n",
            "Epoch: [98][4/4]\tTime 0.057 (Avg-Time 0.050)\t Loss 1.3319 (Avg-Loss 1.3260)\tAcc 51.8252 (Avg-Acc 53.2200)\n",
            "EPOCH: 98 Validation Results: Acc 53.220 Loss: 1.3260\n",
            "Best Accuracy: 53.2200\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [99][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.3365 (Avg-Loss 1.3365)\tAcc 52.4414 (Avg-Acc 52.4414)\n",
            "Epoch: [99][4/19]\tTime 0.187 (Avg-Time 0.195)\t Loss 1.3062 (Avg-Loss 1.3295)\tAcc 52.8320 (Avg-Acc 52.5977)\n",
            "Epoch: [99][8/19]\tTime 0.191 (Avg-Time 0.196)\t Loss 1.3349 (Avg-Loss 1.3267)\tAcc 51.4648 (Avg-Acc 52.6367)\n",
            "Epoch: [99][12/19]\tTime 0.190 (Avg-Time 0.196)\t Loss 1.3179 (Avg-Loss 1.3232)\tAcc 52.5391 (Avg-Acc 52.6480)\n",
            "Epoch: [99][16/19]\tTime 0.185 (Avg-Time 0.196)\t Loss 1.3086 (Avg-Loss 1.3220)\tAcc 53.6621 (Avg-Acc 52.7028)\n",
            "Epoch: [99][19/19]\tTime 0.097 (Avg-Time 0.192)\t Loss 1.3115 (Avg-Loss 1.3227)\tAcc 54.0441 (Avg-Acc 52.7175)\n",
            "EPOCH: 99 train Results: Acc 52.718 Loss: 1.3227\n",
            "Epoch: [99][0/4]\tTime 0.051 (Avg-Time 0.051)\t Loss 1.3284 (Avg-Loss 1.3284)\tAcc 53.9551 (Avg-Acc 53.9551)\n",
            "Epoch: [99][4/4]\tTime 0.052 (Avg-Time 0.049)\t Loss 1.3280 (Avg-Loss 1.3230)\tAcc 51.9912 (Avg-Acc 53.1300)\n",
            "EPOCH: 99 Validation Results: Acc 53.130 Loss: 1.3230\n",
            "Best Accuracy: 53.2200\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [100][0/19]\tTime 0.191 (Avg-Time 0.191)\t Loss 1.3095 (Avg-Loss 1.3095)\tAcc 52.6367 (Avg-Acc 52.6367)\n",
            "Epoch: [100][4/19]\tTime 0.189 (Avg-Time 0.196)\t Loss 1.3454 (Avg-Loss 1.3310)\tAcc 51.3184 (Avg-Acc 52.3438)\n",
            "Epoch: [100][8/19]\tTime 0.201 (Avg-Time 0.196)\t Loss 1.3133 (Avg-Loss 1.3205)\tAcc 52.5879 (Avg-Acc 52.8158)\n",
            "Epoch: [100][12/19]\tTime 0.322 (Avg-Time 0.208)\t Loss 1.2903 (Avg-Loss 1.3113)\tAcc 52.6855 (Avg-Acc 52.9222)\n",
            "Epoch: [100][16/19]\tTime 0.472 (Avg-Time 0.267)\t Loss 1.3121 (Avg-Loss 1.3122)\tAcc 52.7344 (Avg-Acc 52.8550)\n",
            "Epoch: [100][19/19]\tTime 0.317 (Avg-Time 0.280)\t Loss 1.3668 (Avg-Loss 1.3129)\tAcc 50.6434 (Avg-Acc 52.7725)\n",
            "EPOCH: 100 train Results: Acc 52.773 Loss: 1.3129\n",
            "Epoch: [100][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.3263 (Avg-Loss 1.3263)\tAcc 53.8574 (Avg-Acc 53.8574)\n",
            "Epoch: [100][4/4]\tTime 0.052 (Avg-Time 0.049)\t Loss 1.3269 (Avg-Loss 1.3214)\tAcc 52.4336 (Avg-Acc 53.2800)\n",
            "EPOCH: 100 Validation Results: Acc 53.280 Loss: 1.3214\n",
            "Best Accuracy: 53.2800\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [101][0/19]\tTime 0.197 (Avg-Time 0.197)\t Loss 1.2873 (Avg-Loss 1.2873)\tAcc 53.6133 (Avg-Acc 53.6133)\n",
            "Epoch: [101][4/19]\tTime 0.186 (Avg-Time 0.191)\t Loss 1.3138 (Avg-Loss 1.3001)\tAcc 52.3438 (Avg-Acc 53.3301)\n",
            "Epoch: [101][8/19]\tTime 0.195 (Avg-Time 0.193)\t Loss 1.3320 (Avg-Loss 1.3080)\tAcc 51.2207 (Avg-Acc 53.0979)\n",
            "Epoch: [101][12/19]\tTime 0.191 (Avg-Time 0.194)\t Loss 1.3280 (Avg-Loss 1.3129)\tAcc 52.1484 (Avg-Acc 52.9785)\n",
            "Epoch: [101][16/19]\tTime 0.209 (Avg-Time 0.194)\t Loss 1.3258 (Avg-Loss 1.3141)\tAcc 52.6367 (Avg-Acc 52.9843)\n",
            "Epoch: [101][19/19]\tTime 0.102 (Avg-Time 0.190)\t Loss 1.3334 (Avg-Loss 1.3154)\tAcc 51.7463 (Avg-Acc 52.9050)\n",
            "EPOCH: 101 train Results: Acc 52.905 Loss: 1.3154\n",
            "Epoch: [101][0/4]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.3261 (Avg-Loss 1.3261)\tAcc 53.6621 (Avg-Acc 53.6621)\n",
            "Epoch: [101][4/4]\tTime 0.051 (Avg-Time 0.049)\t Loss 1.3211 (Avg-Loss 1.3182)\tAcc 52.1018 (Avg-Acc 53.3600)\n",
            "EPOCH: 101 Validation Results: Acc 53.360 Loss: 1.3182\n",
            "Best Accuracy: 53.3600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [102][0/19]\tTime 0.226 (Avg-Time 0.226)\t Loss 1.3155 (Avg-Loss 1.3155)\tAcc 51.3184 (Avg-Acc 51.3184)\n",
            "Epoch: [102][4/19]\tTime 0.189 (Avg-Time 0.197)\t Loss 1.3174 (Avg-Loss 1.3091)\tAcc 52.1484 (Avg-Acc 52.6270)\n",
            "Epoch: [102][8/19]\tTime 0.189 (Avg-Time 0.196)\t Loss 1.3071 (Avg-Loss 1.3135)\tAcc 53.3203 (Avg-Acc 52.6313)\n",
            "Epoch: [102][12/19]\tTime 0.189 (Avg-Time 0.196)\t Loss 1.2883 (Avg-Loss 1.3092)\tAcc 54.6387 (Avg-Acc 52.9748)\n",
            "Epoch: [102][16/19]\tTime 0.210 (Avg-Time 0.196)\t Loss 1.3629 (Avg-Loss 1.3121)\tAcc 50.2441 (Avg-Acc 52.8981)\n",
            "Epoch: [102][19/19]\tTime 0.096 (Avg-Time 0.191)\t Loss 1.3128 (Avg-Loss 1.3120)\tAcc 52.3897 (Avg-Acc 52.9750)\n",
            "EPOCH: 102 train Results: Acc 52.975 Loss: 1.3120\n",
            "Epoch: [102][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.3222 (Avg-Loss 1.3222)\tAcc 53.9062 (Avg-Acc 53.9062)\n",
            "Epoch: [102][4/4]\tTime 0.052 (Avg-Time 0.049)\t Loss 1.3236 (Avg-Loss 1.3164)\tAcc 52.3783 (Avg-Acc 53.2900)\n",
            "EPOCH: 102 Validation Results: Acc 53.290 Loss: 1.3164\n",
            "Best Accuracy: 53.3600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [103][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.3265 (Avg-Loss 1.3265)\tAcc 52.6855 (Avg-Acc 52.6855)\n",
            "Epoch: [103][4/19]\tTime 0.191 (Avg-Time 0.194)\t Loss 1.2794 (Avg-Loss 1.3128)\tAcc 52.9785 (Avg-Acc 52.9688)\n",
            "Epoch: [103][8/19]\tTime 0.257 (Avg-Time 0.201)\t Loss 1.2900 (Avg-Loss 1.3090)\tAcc 54.8340 (Avg-Acc 53.0979)\n",
            "Epoch: [103][12/19]\tTime 0.512 (Avg-Time 0.276)\t Loss 1.3463 (Avg-Loss 1.3069)\tAcc 52.5879 (Avg-Acc 53.2565)\n",
            "Epoch: [103][16/19]\tTime 0.288 (Avg-Time 0.298)\t Loss 1.2873 (Avg-Loss 1.3048)\tAcc 55.9570 (Avg-Acc 53.5099)\n",
            "Epoch: [103][19/19]\tTime 0.101 (Avg-Time 0.278)\t Loss 1.3058 (Avg-Loss 1.3084)\tAcc 54.4118 (Avg-Acc 53.3725)\n",
            "EPOCH: 103 train Results: Acc 53.373 Loss: 1.3084\n",
            "Epoch: [103][0/4]\tTime 0.064 (Avg-Time 0.064)\t Loss 1.3222 (Avg-Loss 1.3222)\tAcc 53.9551 (Avg-Acc 53.9551)\n",
            "Epoch: [103][4/4]\tTime 0.041 (Avg-Time 0.051)\t Loss 1.3171 (Avg-Loss 1.3140)\tAcc 52.8761 (Avg-Acc 53.7300)\n",
            "EPOCH: 103 Validation Results: Acc 53.730 Loss: 1.3140\n",
            "Best Accuracy: 53.7300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [104][0/19]\tTime 0.192 (Avg-Time 0.192)\t Loss 1.3184 (Avg-Loss 1.3184)\tAcc 52.0020 (Avg-Acc 52.0020)\n",
            "Epoch: [104][4/19]\tTime 0.209 (Avg-Time 0.196)\t Loss 1.3073 (Avg-Loss 1.3084)\tAcc 52.7832 (Avg-Acc 53.0859)\n",
            "Epoch: [104][8/19]\tTime 0.185 (Avg-Time 0.194)\t Loss 1.3202 (Avg-Loss 1.3053)\tAcc 52.6367 (Avg-Acc 52.9351)\n",
            "Epoch: [104][12/19]\tTime 0.195 (Avg-Time 0.198)\t Loss 1.3157 (Avg-Loss 1.3039)\tAcc 52.7832 (Avg-Acc 52.9372)\n",
            "Epoch: [104][16/19]\tTime 0.186 (Avg-Time 0.197)\t Loss 1.2924 (Avg-Loss 1.3018)\tAcc 53.9062 (Avg-Acc 53.0704)\n",
            "Epoch: [104][19/19]\tTime 0.098 (Avg-Time 0.192)\t Loss 1.3250 (Avg-Loss 1.3042)\tAcc 51.4706 (Avg-Acc 53.1325)\n",
            "EPOCH: 104 train Results: Acc 53.133 Loss: 1.3042\n",
            "Epoch: [104][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.3163 (Avg-Loss 1.3163)\tAcc 54.4434 (Avg-Acc 54.4434)\n",
            "Epoch: [104][4/4]\tTime 0.041 (Avg-Time 0.052)\t Loss 1.3189 (Avg-Loss 1.3106)\tAcc 53.2080 (Avg-Acc 53.7400)\n",
            "EPOCH: 104 Validation Results: Acc 53.740 Loss: 1.3106\n",
            "Best Accuracy: 53.7400\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [105][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.3104 (Avg-Loss 1.3104)\tAcc 52.7832 (Avg-Acc 52.7832)\n",
            "Epoch: [105][4/19]\tTime 0.205 (Avg-Time 0.194)\t Loss 1.2440 (Avg-Loss 1.2876)\tAcc 55.3711 (Avg-Acc 53.5742)\n",
            "Epoch: [105][8/19]\tTime 0.186 (Avg-Time 0.192)\t Loss 1.3268 (Avg-Loss 1.2937)\tAcc 52.0996 (Avg-Acc 53.3095)\n",
            "Epoch: [105][12/19]\tTime 0.186 (Avg-Time 0.191)\t Loss 1.2899 (Avg-Loss 1.2968)\tAcc 52.4902 (Avg-Acc 53.3992)\n",
            "Epoch: [105][16/19]\tTime 0.185 (Avg-Time 0.191)\t Loss 1.3296 (Avg-Loss 1.3042)\tAcc 51.9043 (Avg-Acc 53.2313)\n",
            "Epoch: [105][19/19]\tTime 0.100 (Avg-Time 0.187)\t Loss 1.2966 (Avg-Loss 1.3028)\tAcc 55.4228 (Avg-Acc 53.4325)\n",
            "EPOCH: 105 train Results: Acc 53.432 Loss: 1.3028\n",
            "Epoch: [105][0/4]\tTime 0.053 (Avg-Time 0.053)\t Loss 1.3180 (Avg-Loss 1.3180)\tAcc 54.0527 (Avg-Acc 54.0527)\n",
            "Epoch: [105][4/4]\tTime 0.052 (Avg-Time 0.050)\t Loss 1.3145 (Avg-Loss 1.3102)\tAcc 52.9314 (Avg-Acc 53.5300)\n",
            "EPOCH: 105 Validation Results: Acc 53.530 Loss: 1.3102\n",
            "Best Accuracy: 53.7400\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [106][0/19]\tTime 0.197 (Avg-Time 0.197)\t Loss 1.2825 (Avg-Loss 1.2825)\tAcc 53.1738 (Avg-Acc 53.1738)\n",
            "Epoch: [106][4/19]\tTime 0.188 (Avg-Time 0.191)\t Loss 1.3163 (Avg-Loss 1.2922)\tAcc 51.5625 (Avg-Acc 53.2617)\n",
            "Epoch: [106][8/19]\tTime 0.508 (Avg-Time 0.282)\t Loss 1.3021 (Avg-Loss 1.2906)\tAcc 53.4668 (Avg-Acc 53.5373)\n",
            "Epoch: [106][12/19]\tTime 0.354 (Avg-Time 0.326)\t Loss 1.3004 (Avg-Loss 1.2932)\tAcc 53.4668 (Avg-Acc 53.5569)\n",
            "Epoch: [106][16/19]\tTime 0.188 (Avg-Time 0.298)\t Loss 1.3337 (Avg-Loss 1.2983)\tAcc 51.5625 (Avg-Acc 53.3375)\n",
            "Epoch: [106][19/19]\tTime 0.099 (Avg-Time 0.278)\t Loss 1.3028 (Avg-Loss 1.3000)\tAcc 52.5735 (Avg-Acc 53.2850)\n",
            "EPOCH: 106 train Results: Acc 53.285 Loss: 1.3000\n",
            "Epoch: [106][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.3158 (Avg-Loss 1.3158)\tAcc 54.6387 (Avg-Acc 54.6387)\n",
            "Epoch: [106][4/4]\tTime 0.054 (Avg-Time 0.049)\t Loss 1.3152 (Avg-Loss 1.3090)\tAcc 53.6504 (Avg-Acc 54.0700)\n",
            "EPOCH: 106 Validation Results: Acc 54.070 Loss: 1.3090\n",
            "Best Accuracy: 54.0700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [107][0/19]\tTime 0.193 (Avg-Time 0.193)\t Loss 1.3050 (Avg-Loss 1.3050)\tAcc 53.6621 (Avg-Acc 53.6621)\n",
            "Epoch: [107][4/19]\tTime 0.192 (Avg-Time 0.197)\t Loss 1.3043 (Avg-Loss 1.2977)\tAcc 53.6133 (Avg-Acc 53.6133)\n",
            "Epoch: [107][8/19]\tTime 0.205 (Avg-Time 0.199)\t Loss 1.2727 (Avg-Loss 1.2957)\tAcc 54.3457 (Avg-Acc 53.8032)\n",
            "Epoch: [107][12/19]\tTime 0.184 (Avg-Time 0.195)\t Loss 1.2792 (Avg-Loss 1.2961)\tAcc 53.7598 (Avg-Acc 53.7184)\n",
            "Epoch: [107][16/19]\tTime 0.185 (Avg-Time 0.195)\t Loss 1.3334 (Avg-Loss 1.2983)\tAcc 51.2695 (Avg-Acc 53.7196)\n",
            "Epoch: [107][19/19]\tTime 0.096 (Avg-Time 0.191)\t Loss 1.2825 (Avg-Loss 1.2957)\tAcc 55.3309 (Avg-Acc 53.8425)\n",
            "EPOCH: 107 train Results: Acc 53.843 Loss: 1.2957\n",
            "Epoch: [107][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.3128 (Avg-Loss 1.3128)\tAcc 54.3457 (Avg-Acc 54.3457)\n",
            "Epoch: [107][4/4]\tTime 0.065 (Avg-Time 0.052)\t Loss 1.3089 (Avg-Loss 1.3053)\tAcc 53.7058 (Avg-Acc 53.9500)\n",
            "EPOCH: 107 Validation Results: Acc 53.950 Loss: 1.3053\n",
            "Best Accuracy: 54.0700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [108][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.2859 (Avg-Loss 1.2859)\tAcc 53.5156 (Avg-Acc 53.5156)\n",
            "Epoch: [108][4/19]\tTime 0.185 (Avg-Time 0.193)\t Loss 1.3078 (Avg-Loss 1.2778)\tAcc 53.0762 (Avg-Acc 54.2969)\n",
            "Epoch: [108][8/19]\tTime 0.204 (Avg-Time 0.193)\t Loss 1.3137 (Avg-Loss 1.2881)\tAcc 52.1484 (Avg-Acc 53.8683)\n",
            "Epoch: [108][12/19]\tTime 0.191 (Avg-Time 0.192)\t Loss 1.2977 (Avg-Loss 1.2916)\tAcc 53.4668 (Avg-Acc 53.7147)\n",
            "Epoch: [108][16/19]\tTime 0.186 (Avg-Time 0.193)\t Loss 1.2933 (Avg-Loss 1.2953)\tAcc 52.9785 (Avg-Acc 53.6707)\n",
            "Epoch: [108][19/19]\tTime 0.116 (Avg-Time 0.189)\t Loss 1.2942 (Avg-Loss 1.2971)\tAcc 53.9522 (Avg-Acc 53.5850)\n",
            "EPOCH: 108 train Results: Acc 53.585 Loss: 1.2971\n",
            "Epoch: [108][0/4]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.3066 (Avg-Loss 1.3066)\tAcc 54.1992 (Avg-Acc 54.1992)\n",
            "Epoch: [108][4/4]\tTime 0.045 (Avg-Time 0.051)\t Loss 1.3069 (Avg-Loss 1.3013)\tAcc 53.5951 (Avg-Acc 54.0700)\n",
            "EPOCH: 108 Validation Results: Acc 54.070 Loss: 1.3013\n",
            "Best Accuracy: 54.0700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [109][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.2635 (Avg-Loss 1.2635)\tAcc 55.1270 (Avg-Acc 55.1270)\n",
            "Epoch: [109][4/19]\tTime 0.492 (Avg-Time 0.321)\t Loss 1.2785 (Avg-Loss 1.2817)\tAcc 55.2734 (Avg-Acc 54.3750)\n",
            "Epoch: [109][8/19]\tTime 0.361 (Avg-Time 0.378)\t Loss 1.3138 (Avg-Loss 1.2837)\tAcc 52.4414 (Avg-Acc 54.2806)\n",
            "Epoch: [109][12/19]\tTime 0.189 (Avg-Time 0.333)\t Loss 1.3055 (Avg-Loss 1.2859)\tAcc 52.6855 (Avg-Acc 54.0941)\n",
            "Epoch: [109][16/19]\tTime 0.206 (Avg-Time 0.300)\t Loss 1.3229 (Avg-Loss 1.2879)\tAcc 53.1738 (Avg-Acc 54.1906)\n",
            "Epoch: [109][19/19]\tTime 0.101 (Avg-Time 0.279)\t Loss 1.2863 (Avg-Loss 1.2891)\tAcc 55.1471 (Avg-Acc 54.1175)\n",
            "EPOCH: 109 train Results: Acc 54.117 Loss: 1.2891\n",
            "Epoch: [109][0/4]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.3082 (Avg-Loss 1.3082)\tAcc 54.2480 (Avg-Acc 54.2480)\n",
            "Epoch: [109][4/4]\tTime 0.055 (Avg-Time 0.050)\t Loss 1.3051 (Avg-Loss 1.3000)\tAcc 53.8164 (Avg-Acc 54.0000)\n",
            "EPOCH: 109 Validation Results: Acc 54.000 Loss: 1.3000\n",
            "Best Accuracy: 54.0700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [110][0/19]\tTime 0.199 (Avg-Time 0.199)\t Loss 1.2853 (Avg-Loss 1.2853)\tAcc 54.2969 (Avg-Acc 54.2969)\n",
            "Epoch: [110][4/19]\tTime 0.183 (Avg-Time 0.194)\t Loss 1.2978 (Avg-Loss 1.2799)\tAcc 52.2949 (Avg-Acc 53.8477)\n",
            "Epoch: [110][8/19]\tTime 0.193 (Avg-Time 0.194)\t Loss 1.2911 (Avg-Loss 1.2800)\tAcc 52.9297 (Avg-Acc 53.9768)\n",
            "Epoch: [110][12/19]\tTime 0.186 (Avg-Time 0.193)\t Loss 1.3041 (Avg-Loss 1.2838)\tAcc 53.8086 (Avg-Acc 54.0490)\n",
            "Epoch: [110][16/19]\tTime 0.187 (Avg-Time 0.192)\t Loss 1.2611 (Avg-Loss 1.2832)\tAcc 55.5176 (Avg-Acc 54.1820)\n",
            "Epoch: [110][19/19]\tTime 0.105 (Avg-Time 0.189)\t Loss 1.3037 (Avg-Loss 1.2844)\tAcc 53.8603 (Avg-Acc 54.0900)\n",
            "EPOCH: 110 train Results: Acc 54.090 Loss: 1.2844\n",
            "Epoch: [110][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.3053 (Avg-Loss 1.3053)\tAcc 54.8340 (Avg-Acc 54.8340)\n",
            "Epoch: [110][4/4]\tTime 0.057 (Avg-Time 0.050)\t Loss 1.2990 (Avg-Loss 1.2958)\tAcc 53.1527 (Avg-Acc 54.3400)\n",
            "EPOCH: 110 Validation Results: Acc 54.340 Loss: 1.2958\n",
            "Best Accuracy: 54.3400\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [111][0/19]\tTime 0.229 (Avg-Time 0.229)\t Loss 1.2674 (Avg-Loss 1.2674)\tAcc 54.1504 (Avg-Acc 54.1504)\n",
            "Epoch: [111][4/19]\tTime 0.189 (Avg-Time 0.204)\t Loss 1.2890 (Avg-Loss 1.2902)\tAcc 53.3203 (Avg-Acc 53.8770)\n",
            "Epoch: [111][8/19]\tTime 0.191 (Avg-Time 0.199)\t Loss 1.2995 (Avg-Loss 1.2837)\tAcc 53.2715 (Avg-Acc 53.9062)\n",
            "Epoch: [111][12/19]\tTime 0.198 (Avg-Time 0.197)\t Loss 1.2942 (Avg-Loss 1.2825)\tAcc 52.7344 (Avg-Acc 53.9663)\n",
            "Epoch: [111][16/19]\tTime 0.187 (Avg-Time 0.195)\t Loss 1.2575 (Avg-Loss 1.2784)\tAcc 55.2734 (Avg-Acc 54.2050)\n",
            "Epoch: [111][19/19]\tTime 0.135 (Avg-Time 0.194)\t Loss 1.3033 (Avg-Loss 1.2817)\tAcc 52.2978 (Avg-Acc 54.0100)\n",
            "EPOCH: 111 train Results: Acc 54.010 Loss: 1.2817\n",
            "Epoch: [111][0/4]\tTime 0.107 (Avg-Time 0.107)\t Loss 1.3016 (Avg-Loss 1.3016)\tAcc 54.4922 (Avg-Acc 54.4922)\n",
            "Epoch: [111][4/4]\tTime 0.089 (Avg-Time 0.100)\t Loss 1.3009 (Avg-Loss 1.2975)\tAcc 53.8717 (Avg-Acc 54.0600)\n",
            "EPOCH: 111 Validation Results: Acc 54.060 Loss: 1.2975\n",
            "Best Accuracy: 54.3400\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [112][0/19]\tTime 0.403 (Avg-Time 0.403)\t Loss 1.3297 (Avg-Loss 1.3297)\tAcc 53.8574 (Avg-Acc 53.8574)\n",
            "Epoch: [112][4/19]\tTime 0.385 (Avg-Time 0.452)\t Loss 1.2357 (Avg-Loss 1.2816)\tAcc 54.9805 (Avg-Acc 54.3164)\n",
            "Epoch: [112][8/19]\tTime 0.185 (Avg-Time 0.368)\t Loss 1.2742 (Avg-Loss 1.2806)\tAcc 53.5156 (Avg-Acc 54.5573)\n",
            "Epoch: [112][12/19]\tTime 0.185 (Avg-Time 0.315)\t Loss 1.2550 (Avg-Loss 1.2746)\tAcc 56.4453 (Avg-Acc 54.8265)\n",
            "Epoch: [112][16/19]\tTime 0.188 (Avg-Time 0.286)\t Loss 1.2717 (Avg-Loss 1.2781)\tAcc 53.7109 (Avg-Acc 54.4635)\n",
            "Epoch: [112][19/19]\tTime 0.097 (Avg-Time 0.267)\t Loss 1.2646 (Avg-Loss 1.2784)\tAcc 55.6985 (Avg-Acc 54.5275)\n",
            "EPOCH: 112 train Results: Acc 54.528 Loss: 1.2784\n",
            "Epoch: [112][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.3046 (Avg-Loss 1.3046)\tAcc 54.2969 (Avg-Acc 54.2969)\n",
            "Epoch: [112][4/4]\tTime 0.042 (Avg-Time 0.055)\t Loss 1.2991 (Avg-Loss 1.2958)\tAcc 54.0376 (Avg-Acc 54.1800)\n",
            "EPOCH: 112 Validation Results: Acc 54.180 Loss: 1.2958\n",
            "Best Accuracy: 54.3400\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [113][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.2767 (Avg-Loss 1.2767)\tAcc 54.0039 (Avg-Acc 54.0039)\n",
            "Epoch: [113][4/19]\tTime 0.210 (Avg-Time 0.196)\t Loss 1.2706 (Avg-Loss 1.2936)\tAcc 54.3457 (Avg-Acc 53.6035)\n",
            "Epoch: [113][8/19]\tTime 0.192 (Avg-Time 0.194)\t Loss 1.3096 (Avg-Loss 1.2927)\tAcc 52.9785 (Avg-Acc 53.4722)\n",
            "Epoch: [113][12/19]\tTime 0.184 (Avg-Time 0.193)\t Loss 1.2792 (Avg-Loss 1.2828)\tAcc 53.1738 (Avg-Acc 53.8762)\n",
            "Epoch: [113][16/19]\tTime 0.187 (Avg-Time 0.193)\t Loss 1.2834 (Avg-Loss 1.2797)\tAcc 53.5156 (Avg-Acc 54.0470)\n",
            "Epoch: [113][19/19]\tTime 0.096 (Avg-Time 0.188)\t Loss 1.2826 (Avg-Loss 1.2775)\tAcc 54.5037 (Avg-Acc 54.0950)\n",
            "EPOCH: 113 train Results: Acc 54.095 Loss: 1.2775\n",
            "Epoch: [113][0/4]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.2959 (Avg-Loss 1.2959)\tAcc 54.4434 (Avg-Acc 54.4434)\n",
            "Epoch: [113][4/4]\tTime 0.072 (Avg-Time 0.054)\t Loss 1.2969 (Avg-Loss 1.2905)\tAcc 53.8717 (Avg-Acc 54.4600)\n",
            "EPOCH: 113 Validation Results: Acc 54.460 Loss: 1.2905\n",
            "Best Accuracy: 54.4600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [114][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.2747 (Avg-Loss 1.2747)\tAcc 54.1504 (Avg-Acc 54.1504)\n",
            "Epoch: [114][4/19]\tTime 0.187 (Avg-Time 0.187)\t Loss 1.2538 (Avg-Loss 1.2655)\tAcc 55.4688 (Avg-Acc 54.6484)\n",
            "Epoch: [114][8/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.2902 (Avg-Loss 1.2751)\tAcc 54.1016 (Avg-Acc 54.4542)\n",
            "Epoch: [114][12/19]\tTime 0.183 (Avg-Time 0.190)\t Loss 1.3033 (Avg-Loss 1.2825)\tAcc 52.9297 (Avg-Acc 53.9851)\n",
            "Epoch: [114][16/19]\tTime 0.285 (Avg-Time 0.196)\t Loss 1.2659 (Avg-Loss 1.2779)\tAcc 54.3457 (Avg-Acc 54.1446)\n",
            "Epoch: [114][19/19]\tTime 0.358 (Avg-Time 0.227)\t Loss 1.2310 (Avg-Loss 1.2742)\tAcc 55.2390 (Avg-Acc 54.2075)\n",
            "EPOCH: 114 train Results: Acc 54.208 Loss: 1.2742\n",
            "Epoch: [114][0/4]\tTime 0.106 (Avg-Time 0.106)\t Loss 1.3037 (Avg-Loss 1.3037)\tAcc 54.3457 (Avg-Acc 54.3457)\n",
            "Epoch: [114][4/4]\tTime 0.119 (Avg-Time 0.114)\t Loss 1.2978 (Avg-Loss 1.2927)\tAcc 53.9270 (Avg-Acc 54.3700)\n",
            "EPOCH: 114 Validation Results: Acc 54.370 Loss: 1.2927\n",
            "Best Accuracy: 54.4600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [115][0/19]\tTime 0.350 (Avg-Time 0.350)\t Loss 1.2902 (Avg-Loss 1.2902)\tAcc 54.0039 (Avg-Acc 54.0039)\n",
            "Epoch: [115][4/19]\tTime 0.187 (Avg-Time 0.316)\t Loss 1.2751 (Avg-Loss 1.2737)\tAcc 53.5156 (Avg-Acc 54.5215)\n",
            "Epoch: [115][8/19]\tTime 0.205 (Avg-Time 0.263)\t Loss 1.2710 (Avg-Loss 1.2719)\tAcc 53.0762 (Avg-Acc 54.4162)\n",
            "Epoch: [115][12/19]\tTime 0.187 (Avg-Time 0.241)\t Loss 1.3088 (Avg-Loss 1.2721)\tAcc 53.2227 (Avg-Acc 54.4283)\n",
            "Epoch: [115][16/19]\tTime 0.187 (Avg-Time 0.231)\t Loss 1.2543 (Avg-Loss 1.2751)\tAcc 56.1035 (Avg-Acc 54.4491)\n",
            "Epoch: [115][19/19]\tTime 0.099 (Avg-Time 0.222)\t Loss 1.2756 (Avg-Loss 1.2763)\tAcc 53.4007 (Avg-Acc 54.2900)\n",
            "EPOCH: 115 train Results: Acc 54.290 Loss: 1.2763\n",
            "Epoch: [115][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2936 (Avg-Loss 1.2936)\tAcc 54.2480 (Avg-Acc 54.2480)\n",
            "Epoch: [115][4/4]\tTime 0.050 (Avg-Time 0.049)\t Loss 1.2886 (Avg-Loss 1.2853)\tAcc 53.7611 (Avg-Acc 54.4700)\n",
            "EPOCH: 115 Validation Results: Acc 54.470 Loss: 1.2853\n",
            "Best Accuracy: 54.4700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [116][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.2657 (Avg-Loss 1.2657)\tAcc 54.8828 (Avg-Acc 54.8828)\n",
            "Epoch: [116][4/19]\tTime 0.189 (Avg-Time 0.194)\t Loss 1.2539 (Avg-Loss 1.2648)\tAcc 54.3457 (Avg-Acc 54.1797)\n",
            "Epoch: [116][8/19]\tTime 0.217 (Avg-Time 0.196)\t Loss 1.2820 (Avg-Loss 1.2609)\tAcc 52.5879 (Avg-Acc 54.2480)\n",
            "Epoch: [116][12/19]\tTime 0.183 (Avg-Time 0.193)\t Loss 1.2663 (Avg-Loss 1.2655)\tAcc 55.5176 (Avg-Acc 54.4321)\n",
            "Epoch: [116][16/19]\tTime 0.185 (Avg-Time 0.194)\t Loss 1.2613 (Avg-Loss 1.2675)\tAcc 55.2734 (Avg-Acc 54.5295)\n",
            "Epoch: [116][19/19]\tTime 0.096 (Avg-Time 0.188)\t Loss 1.2646 (Avg-Loss 1.2650)\tAcc 53.6765 (Avg-Acc 54.5550)\n",
            "EPOCH: 116 train Results: Acc 54.555 Loss: 1.2650\n",
            "Epoch: [116][0/4]\tTime 0.070 (Avg-Time 0.070)\t Loss 1.2963 (Avg-Loss 1.2963)\tAcc 54.5898 (Avg-Acc 54.5898)\n",
            "Epoch: [116][4/4]\tTime 0.042 (Avg-Time 0.055)\t Loss 1.2899 (Avg-Loss 1.2866)\tAcc 54.2588 (Avg-Acc 54.5300)\n",
            "EPOCH: 116 Validation Results: Acc 54.530 Loss: 1.2866\n",
            "Best Accuracy: 54.5300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [117][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.2190 (Avg-Loss 1.2190)\tAcc 54.5898 (Avg-Acc 54.5898)\n",
            "Epoch: [117][4/19]\tTime 0.194 (Avg-Time 0.192)\t Loss 1.2306 (Avg-Loss 1.2392)\tAcc 56.0547 (Avg-Acc 55.0977)\n",
            "Epoch: [117][8/19]\tTime 0.183 (Avg-Time 0.191)\t Loss 1.2705 (Avg-Loss 1.2459)\tAcc 54.0527 (Avg-Acc 55.0239)\n",
            "Epoch: [117][12/19]\tTime 0.188 (Avg-Time 0.196)\t Loss 1.3111 (Avg-Loss 1.2556)\tAcc 53.2715 (Avg-Acc 54.7251)\n",
            "Epoch: [117][16/19]\tTime 0.502 (Avg-Time 0.256)\t Loss 1.2847 (Avg-Loss 1.2630)\tAcc 56.0059 (Avg-Acc 54.6415)\n",
            "Epoch: [117][19/19]\tTime 0.229 (Avg-Time 0.267)\t Loss 1.2365 (Avg-Loss 1.2635)\tAcc 54.2279 (Avg-Acc 54.6075)\n",
            "EPOCH: 117 train Results: Acc 54.608 Loss: 1.2635\n",
            "Epoch: [117][0/4]\tTime 0.107 (Avg-Time 0.107)\t Loss 1.2953 (Avg-Loss 1.2953)\tAcc 54.4922 (Avg-Acc 54.4922)\n",
            "Epoch: [117][4/4]\tTime 0.073 (Avg-Time 0.095)\t Loss 1.2928 (Avg-Loss 1.2875)\tAcc 54.0929 (Avg-Acc 54.2300)\n",
            "EPOCH: 117 Validation Results: Acc 54.230 Loss: 1.2875\n",
            "Best Accuracy: 54.5300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [118][0/19]\tTime 0.218 (Avg-Time 0.218)\t Loss 1.2518 (Avg-Loss 1.2518)\tAcc 55.5664 (Avg-Acc 55.5664)\n",
            "Epoch: [118][4/19]\tTime 0.186 (Avg-Time 0.197)\t Loss 1.2832 (Avg-Loss 1.2553)\tAcc 53.4668 (Avg-Acc 55.0586)\n",
            "Epoch: [118][8/19]\tTime 0.187 (Avg-Time 0.196)\t Loss 1.2491 (Avg-Loss 1.2534)\tAcc 55.1758 (Avg-Acc 55.3385)\n",
            "Epoch: [118][12/19]\tTime 0.186 (Avg-Time 0.197)\t Loss 1.2699 (Avg-Loss 1.2575)\tAcc 54.3457 (Avg-Acc 54.8528)\n",
            "Epoch: [118][16/19]\tTime 0.207 (Avg-Time 0.201)\t Loss 1.2824 (Avg-Loss 1.2622)\tAcc 54.1016 (Avg-Acc 54.6904)\n",
            "Epoch: [118][19/19]\tTime 0.096 (Avg-Time 0.195)\t Loss 1.2935 (Avg-Loss 1.2611)\tAcc 52.5735 (Avg-Acc 54.8850)\n",
            "EPOCH: 118 train Results: Acc 54.885 Loss: 1.2611\n",
            "Epoch: [118][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2881 (Avg-Loss 1.2881)\tAcc 55.0293 (Avg-Acc 55.0293)\n",
            "Epoch: [118][4/4]\tTime 0.056 (Avg-Time 0.049)\t Loss 1.2922 (Avg-Loss 1.2853)\tAcc 54.2588 (Avg-Acc 54.3300)\n",
            "EPOCH: 118 Validation Results: Acc 54.330 Loss: 1.2853\n",
            "Best Accuracy: 54.5300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [119][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.2514 (Avg-Loss 1.2514)\tAcc 55.9570 (Avg-Acc 55.9570)\n",
            "Epoch: [119][4/19]\tTime 0.184 (Avg-Time 0.192)\t Loss 1.2418 (Avg-Loss 1.2525)\tAcc 56.0059 (Avg-Acc 55.4980)\n",
            "Epoch: [119][8/19]\tTime 0.186 (Avg-Time 0.192)\t Loss 1.2551 (Avg-Loss 1.2552)\tAcc 55.8594 (Avg-Acc 55.4416)\n",
            "Epoch: [119][12/19]\tTime 0.205 (Avg-Time 0.193)\t Loss 1.2787 (Avg-Loss 1.2516)\tAcc 54.6875 (Avg-Acc 55.3636)\n",
            "Epoch: [119][16/19]\tTime 0.186 (Avg-Time 0.192)\t Loss 1.2051 (Avg-Loss 1.2494)\tAcc 57.0312 (Avg-Acc 55.4314)\n",
            "Epoch: [119][19/19]\tTime 0.097 (Avg-Time 0.188)\t Loss 1.2400 (Avg-Loss 1.2546)\tAcc 55.1471 (Avg-Acc 55.1875)\n",
            "EPOCH: 119 train Results: Acc 55.188 Loss: 1.2546\n",
            "Epoch: [119][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2862 (Avg-Loss 1.2862)\tAcc 55.2246 (Avg-Acc 55.2246)\n",
            "Epoch: [119][4/4]\tTime 0.057 (Avg-Time 0.050)\t Loss 1.2862 (Avg-Loss 1.2805)\tAcc 53.8164 (Avg-Acc 54.7000)\n",
            "EPOCH: 119 Validation Results: Acc 54.700 Loss: 1.2805\n",
            "Best Accuracy: 54.7000\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [120][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.2635 (Avg-Loss 1.2635)\tAcc 53.7109 (Avg-Acc 53.7109)\n",
            "Epoch: [120][4/19]\tTime 0.189 (Avg-Time 0.193)\t Loss 1.2547 (Avg-Loss 1.2541)\tAcc 55.4688 (Avg-Acc 55.3613)\n",
            "Epoch: [120][8/19]\tTime 0.196 (Avg-Time 0.194)\t Loss 1.2744 (Avg-Loss 1.2553)\tAcc 53.3691 (Avg-Acc 55.1487)\n",
            "Epoch: [120][12/19]\tTime 0.557 (Avg-Time 0.256)\t Loss 1.2724 (Avg-Loss 1.2568)\tAcc 55.6152 (Avg-Acc 55.1908)\n",
            "Epoch: [120][16/19]\tTime 0.386 (Avg-Time 0.297)\t Loss 1.2432 (Avg-Loss 1.2570)\tAcc 55.8594 (Avg-Acc 55.2045)\n",
            "Epoch: [120][19/19]\tTime 0.102 (Avg-Time 0.280)\t Loss 1.2117 (Avg-Loss 1.2524)\tAcc 57.3529 (Avg-Acc 55.3075)\n",
            "EPOCH: 120 train Results: Acc 55.307 Loss: 1.2524\n",
            "Epoch: [120][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2811 (Avg-Loss 1.2811)\tAcc 55.3711 (Avg-Acc 55.3711)\n",
            "Epoch: [120][4/4]\tTime 0.042 (Avg-Time 0.053)\t Loss 1.2816 (Avg-Loss 1.2781)\tAcc 55.3650 (Avg-Acc 55.1500)\n",
            "EPOCH: 120 Validation Results: Acc 55.150 Loss: 1.2781\n",
            "Best Accuracy: 55.1500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [121][0/19]\tTime 0.192 (Avg-Time 0.192)\t Loss 1.2601 (Avg-Loss 1.2601)\tAcc 55.5664 (Avg-Acc 55.5664)\n",
            "Epoch: [121][4/19]\tTime 0.215 (Avg-Time 0.196)\t Loss 1.2374 (Avg-Loss 1.2440)\tAcc 55.3223 (Avg-Acc 55.2832)\n",
            "Epoch: [121][8/19]\tTime 0.205 (Avg-Time 0.195)\t Loss 1.2373 (Avg-Loss 1.2420)\tAcc 55.6641 (Avg-Acc 55.1432)\n",
            "Epoch: [121][12/19]\tTime 0.189 (Avg-Time 0.196)\t Loss 1.2633 (Avg-Loss 1.2433)\tAcc 54.1504 (Avg-Acc 55.0856)\n",
            "Epoch: [121][16/19]\tTime 0.189 (Avg-Time 0.196)\t Loss 1.2492 (Avg-Loss 1.2480)\tAcc 54.6387 (Avg-Acc 54.9920)\n",
            "Epoch: [121][19/19]\tTime 0.098 (Avg-Time 0.190)\t Loss 1.2813 (Avg-Loss 1.2486)\tAcc 53.9522 (Avg-Acc 54.9875)\n",
            "EPOCH: 121 train Results: Acc 54.987 Loss: 1.2486\n",
            "Epoch: [121][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2816 (Avg-Loss 1.2816)\tAcc 55.0293 (Avg-Acc 55.0293)\n",
            "Epoch: [121][4/4]\tTime 0.056 (Avg-Time 0.054)\t Loss 1.2838 (Avg-Loss 1.2754)\tAcc 54.4801 (Avg-Acc 54.7900)\n",
            "EPOCH: 121 Validation Results: Acc 54.790 Loss: 1.2754\n",
            "Best Accuracy: 55.1500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [122][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.2226 (Avg-Loss 1.2226)\tAcc 56.7383 (Avg-Acc 56.7383)\n",
            "Epoch: [122][4/19]\tTime 0.219 (Avg-Time 0.197)\t Loss 1.2283 (Avg-Loss 1.2414)\tAcc 55.6152 (Avg-Acc 55.5469)\n",
            "Epoch: [122][8/19]\tTime 0.194 (Avg-Time 0.196)\t Loss 1.2104 (Avg-Loss 1.2470)\tAcc 56.7871 (Avg-Acc 55.3548)\n",
            "Epoch: [122][12/19]\tTime 0.188 (Avg-Time 0.196)\t Loss 1.2554 (Avg-Loss 1.2491)\tAcc 55.7617 (Avg-Acc 55.4913)\n",
            "Epoch: [122][16/19]\tTime 0.190 (Avg-Time 0.196)\t Loss 1.2357 (Avg-Loss 1.2505)\tAcc 56.2012 (Avg-Acc 55.4975)\n",
            "Epoch: [122][19/19]\tTime 0.097 (Avg-Time 0.190)\t Loss 1.2699 (Avg-Loss 1.2498)\tAcc 53.1250 (Avg-Acc 55.3900)\n",
            "EPOCH: 122 train Results: Acc 55.390 Loss: 1.2498\n",
            "Epoch: [122][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.2850 (Avg-Loss 1.2850)\tAcc 55.7617 (Avg-Acc 55.7617)\n",
            "Epoch: [122][4/4]\tTime 0.055 (Avg-Time 0.050)\t Loss 1.2829 (Avg-Loss 1.2777)\tAcc 54.8673 (Avg-Acc 55.0600)\n",
            "EPOCH: 122 Validation Results: Acc 55.060 Loss: 1.2777\n",
            "Best Accuracy: 55.1500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [123][0/19]\tTime 0.196 (Avg-Time 0.196)\t Loss 1.2743 (Avg-Loss 1.2743)\tAcc 54.5410 (Avg-Acc 54.5410)\n",
            "Epoch: [123][4/19]\tTime 0.188 (Avg-Time 0.191)\t Loss 1.2490 (Avg-Loss 1.2487)\tAcc 54.1016 (Avg-Acc 55.2246)\n",
            "Epoch: [123][8/19]\tTime 0.586 (Avg-Time 0.265)\t Loss 1.2354 (Avg-Loss 1.2479)\tAcc 57.2754 (Avg-Acc 55.5935)\n",
            "Epoch: [123][12/19]\tTime 0.460 (Avg-Time 0.328)\t Loss 1.2572 (Avg-Loss 1.2544)\tAcc 55.6152 (Avg-Acc 55.4838)\n",
            "Epoch: [123][16/19]\tTime 0.184 (Avg-Time 0.302)\t Loss 1.2205 (Avg-Loss 1.2515)\tAcc 56.1523 (Avg-Acc 55.3855)\n",
            "Epoch: [123][19/19]\tTime 0.095 (Avg-Time 0.282)\t Loss 1.2400 (Avg-Loss 1.2504)\tAcc 55.5147 (Avg-Acc 55.3700)\n",
            "EPOCH: 123 train Results: Acc 55.370 Loss: 1.2504\n",
            "Epoch: [123][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2790 (Avg-Loss 1.2790)\tAcc 55.6152 (Avg-Acc 55.6152)\n",
            "Epoch: [123][4/4]\tTime 0.051 (Avg-Time 0.049)\t Loss 1.2764 (Avg-Loss 1.2738)\tAcc 55.4757 (Avg-Acc 55.2100)\n",
            "EPOCH: 123 Validation Results: Acc 55.210 Loss: 1.2738\n",
            "Best Accuracy: 55.2100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [124][0/19]\tTime 0.185 (Avg-Time 0.185)\t Loss 1.2252 (Avg-Loss 1.2252)\tAcc 55.2246 (Avg-Acc 55.2246)\n",
            "Epoch: [124][4/19]\tTime 0.185 (Avg-Time 0.192)\t Loss 1.2502 (Avg-Loss 1.2414)\tAcc 54.5410 (Avg-Acc 55.3223)\n",
            "Epoch: [124][8/19]\tTime 0.187 (Avg-Time 0.192)\t Loss 1.2175 (Avg-Loss 1.2428)\tAcc 56.4941 (Avg-Acc 55.3223)\n",
            "Epoch: [124][12/19]\tTime 0.190 (Avg-Time 0.192)\t Loss 1.2543 (Avg-Loss 1.2415)\tAcc 54.9805 (Avg-Acc 55.4387)\n",
            "Epoch: [124][16/19]\tTime 0.188 (Avg-Time 0.192)\t Loss 1.2421 (Avg-Loss 1.2414)\tAcc 55.0293 (Avg-Acc 55.4429)\n",
            "Epoch: [124][19/19]\tTime 0.100 (Avg-Time 0.189)\t Loss 1.2385 (Avg-Loss 1.2405)\tAcc 56.1581 (Avg-Acc 55.4850)\n",
            "EPOCH: 124 train Results: Acc 55.485 Loss: 1.2405\n",
            "Epoch: [124][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2744 (Avg-Loss 1.2744)\tAcc 55.5176 (Avg-Acc 55.5176)\n",
            "Epoch: [124][4/4]\tTime 0.059 (Avg-Time 0.050)\t Loss 1.2790 (Avg-Loss 1.2689)\tAcc 54.9226 (Avg-Acc 55.2600)\n",
            "EPOCH: 124 Validation Results: Acc 55.260 Loss: 1.2689\n",
            "Best Accuracy: 55.2600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [125][0/19]\tTime 0.207 (Avg-Time 0.207)\t Loss 1.2442 (Avg-Loss 1.2442)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [125][4/19]\tTime 0.186 (Avg-Time 0.203)\t Loss 1.2203 (Avg-Loss 1.2347)\tAcc 56.8848 (Avg-Acc 55.8691)\n",
            "Epoch: [125][8/19]\tTime 0.211 (Avg-Time 0.200)\t Loss 1.2378 (Avg-Loss 1.2375)\tAcc 56.1523 (Avg-Acc 55.6152)\n",
            "Epoch: [125][12/19]\tTime 0.189 (Avg-Time 0.197)\t Loss 1.2179 (Avg-Loss 1.2387)\tAcc 56.4453 (Avg-Acc 55.6002)\n",
            "Epoch: [125][16/19]\tTime 0.189 (Avg-Time 0.197)\t Loss 1.2577 (Avg-Loss 1.2394)\tAcc 54.6875 (Avg-Acc 55.5090)\n",
            "Epoch: [125][19/19]\tTime 0.098 (Avg-Time 0.193)\t Loss 1.2680 (Avg-Loss 1.2402)\tAcc 53.1250 (Avg-Acc 55.3850)\n",
            "EPOCH: 125 train Results: Acc 55.385 Loss: 1.2402\n",
            "Epoch: [125][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2806 (Avg-Loss 1.2806)\tAcc 54.8828 (Avg-Acc 54.8828)\n",
            "Epoch: [125][4/4]\tTime 0.052 (Avg-Time 0.050)\t Loss 1.2730 (Avg-Loss 1.2702)\tAcc 55.4757 (Avg-Acc 55.0000)\n",
            "EPOCH: 125 Validation Results: Acc 55.000 Loss: 1.2702\n",
            "Best Accuracy: 55.2600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [126][0/19]\tTime 0.183 (Avg-Time 0.183)\t Loss 1.2498 (Avg-Loss 1.2498)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [126][4/19]\tTime 0.622 (Avg-Time 0.318)\t Loss 1.2187 (Avg-Loss 1.2350)\tAcc 56.6895 (Avg-Acc 55.7324)\n",
            "Epoch: [126][8/19]\tTime 0.367 (Avg-Time 0.362)\t Loss 1.2201 (Avg-Loss 1.2320)\tAcc 56.3965 (Avg-Acc 55.8051)\n",
            "Epoch: [126][12/19]\tTime 0.187 (Avg-Time 0.325)\t Loss 1.2264 (Avg-Loss 1.2288)\tAcc 57.3730 (Avg-Acc 56.0584)\n",
            "Epoch: [126][16/19]\tTime 0.210 (Avg-Time 0.294)\t Loss 1.2577 (Avg-Loss 1.2362)\tAcc 55.1270 (Avg-Acc 55.8910)\n",
            "Epoch: [126][19/19]\tTime 0.096 (Avg-Time 0.274)\t Loss 1.2545 (Avg-Loss 1.2383)\tAcc 54.2279 (Avg-Acc 55.7350)\n",
            "EPOCH: 126 train Results: Acc 55.735 Loss: 1.2383\n",
            "Epoch: [126][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2698 (Avg-Loss 1.2698)\tAcc 55.2734 (Avg-Acc 55.2734)\n",
            "Epoch: [126][4/4]\tTime 0.047 (Avg-Time 0.048)\t Loss 1.2689 (Avg-Loss 1.2643)\tAcc 54.7013 (Avg-Acc 55.1800)\n",
            "EPOCH: 126 Validation Results: Acc 55.180 Loss: 1.2643\n",
            "Best Accuracy: 55.2600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [127][0/19]\tTime 0.203 (Avg-Time 0.203)\t Loss 1.2146 (Avg-Loss 1.2146)\tAcc 56.8848 (Avg-Acc 56.8848)\n",
            "Epoch: [127][4/19]\tTime 0.189 (Avg-Time 0.197)\t Loss 1.2434 (Avg-Loss 1.2285)\tAcc 55.9570 (Avg-Acc 55.8496)\n",
            "Epoch: [127][8/19]\tTime 0.189 (Avg-Time 0.195)\t Loss 1.2498 (Avg-Loss 1.2281)\tAcc 55.4199 (Avg-Acc 55.8322)\n",
            "Epoch: [127][12/19]\tTime 0.186 (Avg-Time 0.194)\t Loss 1.2235 (Avg-Loss 1.2301)\tAcc 56.8848 (Avg-Acc 55.7993)\n",
            "Epoch: [127][16/19]\tTime 0.190 (Avg-Time 0.193)\t Loss 1.2603 (Avg-Loss 1.2352)\tAcc 55.0781 (Avg-Acc 55.6813)\n",
            "Epoch: [127][19/19]\tTime 0.097 (Avg-Time 0.189)\t Loss 1.2618 (Avg-Loss 1.2364)\tAcc 54.7794 (Avg-Acc 55.7025)\n",
            "EPOCH: 127 train Results: Acc 55.703 Loss: 1.2364\n",
            "Epoch: [127][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2774 (Avg-Loss 1.2774)\tAcc 55.5176 (Avg-Acc 55.5176)\n",
            "Epoch: [127][4/4]\tTime 0.051 (Avg-Time 0.049)\t Loss 1.2670 (Avg-Loss 1.2659)\tAcc 55.6969 (Avg-Acc 55.4400)\n",
            "EPOCH: 127 Validation Results: Acc 55.440 Loss: 1.2659\n",
            "Best Accuracy: 55.4400\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [128][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.2267 (Avg-Loss 1.2267)\tAcc 55.7617 (Avg-Acc 55.7617)\n",
            "Epoch: [128][4/19]\tTime 0.188 (Avg-Time 0.192)\t Loss 1.2582 (Avg-Loss 1.2320)\tAcc 55.7617 (Avg-Acc 56.3477)\n",
            "Epoch: [128][8/19]\tTime 0.185 (Avg-Time 0.192)\t Loss 1.2275 (Avg-Loss 1.2289)\tAcc 55.4199 (Avg-Acc 56.0438)\n",
            "Epoch: [128][12/19]\tTime 0.215 (Avg-Time 0.194)\t Loss 1.2233 (Avg-Loss 1.2236)\tAcc 56.3477 (Avg-Acc 56.2200)\n",
            "Epoch: [128][16/19]\tTime 0.189 (Avg-Time 0.194)\t Loss 1.2091 (Avg-Loss 1.2260)\tAcc 56.6406 (Avg-Acc 56.1696)\n",
            "Epoch: [128][19/19]\tTime 0.098 (Avg-Time 0.190)\t Loss 1.2632 (Avg-Loss 1.2269)\tAcc 53.4926 (Avg-Acc 56.0900)\n",
            "EPOCH: 128 train Results: Acc 56.090 Loss: 1.2269\n",
            "Epoch: [128][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2750 (Avg-Loss 1.2750)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [128][4/4]\tTime 0.071 (Avg-Time 0.058)\t Loss 1.2735 (Avg-Loss 1.2651)\tAcc 55.4204 (Avg-Acc 55.3900)\n",
            "EPOCH: 128 Validation Results: Acc 55.390 Loss: 1.2651\n",
            "Best Accuracy: 55.4400\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [129][0/19]\tTime 0.300 (Avg-Time 0.300)\t Loss 1.2313 (Avg-Loss 1.2313)\tAcc 55.4199 (Avg-Acc 55.4199)\n",
            "Epoch: [129][4/19]\tTime 0.454 (Avg-Time 0.435)\t Loss 1.2464 (Avg-Loss 1.2252)\tAcc 55.5176 (Avg-Acc 55.9961)\n",
            "Epoch: [129][8/19]\tTime 0.184 (Avg-Time 0.383)\t Loss 1.2373 (Avg-Loss 1.2249)\tAcc 56.0059 (Avg-Acc 55.9462)\n",
            "Epoch: [129][12/19]\tTime 0.189 (Avg-Time 0.326)\t Loss 1.2627 (Avg-Loss 1.2237)\tAcc 53.8574 (Avg-Acc 56.1035)\n",
            "Epoch: [129][16/19]\tTime 0.189 (Avg-Time 0.295)\t Loss 1.2448 (Avg-Loss 1.2248)\tAcc 56.0059 (Avg-Acc 56.1150)\n",
            "Epoch: [129][19/19]\tTime 0.095 (Avg-Time 0.275)\t Loss 1.2327 (Avg-Loss 1.2252)\tAcc 55.6985 (Avg-Acc 56.0350)\n",
            "EPOCH: 129 train Results: Acc 56.035 Loss: 1.2252\n",
            "Epoch: [129][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2672 (Avg-Loss 1.2672)\tAcc 55.7129 (Avg-Acc 55.7129)\n",
            "Epoch: [129][4/4]\tTime 0.042 (Avg-Time 0.054)\t Loss 1.2705 (Avg-Loss 1.2628)\tAcc 55.3650 (Avg-Acc 55.2900)\n",
            "EPOCH: 129 Validation Results: Acc 55.290 Loss: 1.2628\n",
            "Best Accuracy: 55.4400\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [130][0/19]\tTime 0.192 (Avg-Time 0.192)\t Loss 1.2010 (Avg-Loss 1.2010)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [130][4/19]\tTime 0.250 (Avg-Time 0.201)\t Loss 1.2479 (Avg-Loss 1.2198)\tAcc 55.4199 (Avg-Acc 56.2305)\n",
            "Epoch: [130][8/19]\tTime 0.186 (Avg-Time 0.195)\t Loss 1.2077 (Avg-Loss 1.2213)\tAcc 56.4453 (Avg-Acc 56.1198)\n",
            "Epoch: [130][12/19]\tTime 0.184 (Avg-Time 0.195)\t Loss 1.2316 (Avg-Loss 1.2279)\tAcc 55.7617 (Avg-Acc 55.9458)\n",
            "Epoch: [130][16/19]\tTime 0.190 (Avg-Time 0.195)\t Loss 1.2719 (Avg-Loss 1.2306)\tAcc 55.1758 (Avg-Acc 55.9226)\n",
            "Epoch: [130][19/19]\tTime 0.097 (Avg-Time 0.190)\t Loss 1.2203 (Avg-Loss 1.2312)\tAcc 57.2610 (Avg-Acc 55.9600)\n",
            "EPOCH: 130 train Results: Acc 55.960 Loss: 1.2312\n",
            "Epoch: [130][0/4]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.2734 (Avg-Loss 1.2734)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [130][4/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2671 (Avg-Loss 1.2637)\tAcc 55.3650 (Avg-Acc 55.3000)\n",
            "EPOCH: 130 Validation Results: Acc 55.300 Loss: 1.2637\n",
            "Best Accuracy: 55.4400\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [131][0/19]\tTime 0.214 (Avg-Time 0.214)\t Loss 1.2037 (Avg-Loss 1.2037)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [131][4/19]\tTime 0.187 (Avg-Time 0.194)\t Loss 1.2327 (Avg-Loss 1.2150)\tAcc 56.4941 (Avg-Acc 56.4258)\n",
            "Epoch: [131][8/19]\tTime 0.186 (Avg-Time 0.195)\t Loss 1.2601 (Avg-Loss 1.2207)\tAcc 54.5410 (Avg-Acc 56.2663)\n",
            "Epoch: [131][12/19]\tTime 0.193 (Avg-Time 0.196)\t Loss 1.2420 (Avg-Loss 1.2195)\tAcc 55.4199 (Avg-Acc 56.3552)\n",
            "Epoch: [131][16/19]\tTime 0.206 (Avg-Time 0.196)\t Loss 1.2311 (Avg-Loss 1.2223)\tAcc 55.4688 (Avg-Acc 56.2213)\n",
            "Epoch: [131][19/19]\tTime 0.183 (Avg-Time 0.213)\t Loss 1.2416 (Avg-Loss 1.2229)\tAcc 54.2279 (Avg-Acc 56.1350)\n",
            "EPOCH: 131 train Results: Acc 56.135 Loss: 1.2229\n",
            "Epoch: [131][0/4]\tTime 0.107 (Avg-Time 0.107)\t Loss 1.2754 (Avg-Loss 1.2754)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [131][4/4]\tTime 0.108 (Avg-Time 0.110)\t Loss 1.2696 (Avg-Loss 1.2637)\tAcc 56.0841 (Avg-Acc 55.4700)\n",
            "EPOCH: 131 Validation Results: Acc 55.470 Loss: 1.2637\n",
            "Best Accuracy: 55.4700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [132][0/19]\tTime 0.533 (Avg-Time 0.533)\t Loss 1.2166 (Avg-Loss 1.2166)\tAcc 54.6875 (Avg-Acc 54.6875)\n",
            "Epoch: [132][4/19]\tTime 0.221 (Avg-Time 0.400)\t Loss 1.2198 (Avg-Loss 1.2175)\tAcc 55.7617 (Avg-Acc 55.7715)\n",
            "Epoch: [132][8/19]\tTime 0.195 (Avg-Time 0.310)\t Loss 1.2021 (Avg-Loss 1.2156)\tAcc 57.3242 (Avg-Acc 56.1361)\n",
            "Epoch: [132][12/19]\tTime 0.183 (Avg-Time 0.272)\t Loss 1.2026 (Avg-Loss 1.2182)\tAcc 57.4707 (Avg-Acc 56.1486)\n",
            "Epoch: [132][16/19]\tTime 0.184 (Avg-Time 0.253)\t Loss 1.2265 (Avg-Loss 1.2194)\tAcc 55.5176 (Avg-Acc 56.1724)\n",
            "Epoch: [132][19/19]\tTime 0.099 (Avg-Time 0.240)\t Loss 1.2160 (Avg-Loss 1.2197)\tAcc 55.2390 (Avg-Acc 56.1225)\n",
            "EPOCH: 132 train Results: Acc 56.123 Loss: 1.2197\n",
            "Epoch: [132][0/4]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.2636 (Avg-Loss 1.2636)\tAcc 56.2988 (Avg-Acc 56.2988)\n",
            "Epoch: [132][4/4]\tTime 0.058 (Avg-Time 0.049)\t Loss 1.2675 (Avg-Loss 1.2553)\tAcc 55.2544 (Avg-Acc 55.5300)\n",
            "EPOCH: 132 Validation Results: Acc 55.530 Loss: 1.2553\n",
            "Best Accuracy: 55.5300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [133][0/19]\tTime 0.203 (Avg-Time 0.203)\t Loss 1.2396 (Avg-Loss 1.2396)\tAcc 55.4199 (Avg-Acc 55.4199)\n",
            "Epoch: [133][4/19]\tTime 0.186 (Avg-Time 0.201)\t Loss 1.2273 (Avg-Loss 1.2260)\tAcc 55.7129 (Avg-Acc 56.3086)\n",
            "Epoch: [133][8/19]\tTime 0.213 (Avg-Time 0.198)\t Loss 1.1824 (Avg-Loss 1.2133)\tAcc 59.1309 (Avg-Acc 56.9770)\n",
            "Epoch: [133][12/19]\tTime 0.189 (Avg-Time 0.196)\t Loss 1.2155 (Avg-Loss 1.2131)\tAcc 55.8105 (Avg-Acc 56.7796)\n",
            "Epoch: [133][16/19]\tTime 0.189 (Avg-Time 0.195)\t Loss 1.2223 (Avg-Loss 1.2167)\tAcc 56.2500 (Avg-Acc 56.5918)\n",
            "Epoch: [133][19/19]\tTime 0.120 (Avg-Time 0.191)\t Loss 1.2415 (Avg-Loss 1.2163)\tAcc 55.6985 (Avg-Acc 56.6150)\n",
            "EPOCH: 133 train Results: Acc 56.615 Loss: 1.2163\n",
            "Epoch: [133][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2632 (Avg-Loss 1.2632)\tAcc 55.5664 (Avg-Acc 55.5664)\n",
            "Epoch: [133][4/4]\tTime 0.063 (Avg-Time 0.051)\t Loss 1.2647 (Avg-Loss 1.2573)\tAcc 56.3606 (Avg-Acc 55.3000)\n",
            "EPOCH: 133 Validation Results: Acc 55.300 Loss: 1.2573\n",
            "Best Accuracy: 55.5300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [134][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.2039 (Avg-Loss 1.2039)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [134][4/19]\tTime 0.190 (Avg-Time 0.193)\t Loss 1.1883 (Avg-Loss 1.2168)\tAcc 57.7637 (Avg-Acc 56.5137)\n",
            "Epoch: [134][8/19]\tTime 0.186 (Avg-Time 0.192)\t Loss 1.2035 (Avg-Loss 1.2129)\tAcc 56.6895 (Avg-Acc 56.6135)\n",
            "Epoch: [134][12/19]\tTime 0.191 (Avg-Time 0.194)\t Loss 1.2289 (Avg-Loss 1.2168)\tAcc 55.7617 (Avg-Acc 56.6632)\n",
            "Epoch: [134][16/19]\tTime 0.638 (Avg-Time 0.235)\t Loss 1.1954 (Avg-Loss 1.2164)\tAcc 57.5684 (Avg-Acc 56.6780)\n",
            "Epoch: [134][19/19]\tTime 0.215 (Avg-Time 0.255)\t Loss 1.2364 (Avg-Loss 1.2182)\tAcc 55.6066 (Avg-Acc 56.5975)\n",
            "EPOCH: 134 train Results: Acc 56.597 Loss: 1.2182\n",
            "Epoch: [134][0/4]\tTime 0.113 (Avg-Time 0.113)\t Loss 1.2675 (Avg-Loss 1.2675)\tAcc 55.7129 (Avg-Acc 55.7129)\n",
            "Epoch: [134][4/4]\tTime 0.089 (Avg-Time 0.101)\t Loss 1.2610 (Avg-Loss 1.2594)\tAcc 55.7522 (Avg-Acc 55.4500)\n",
            "EPOCH: 134 Validation Results: Acc 55.450 Loss: 1.2594\n",
            "Best Accuracy: 55.5300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [135][0/19]\tTime 0.401 (Avg-Time 0.401)\t Loss 1.2182 (Avg-Loss 1.2182)\tAcc 56.0059 (Avg-Acc 56.0059)\n",
            "Epoch: [135][4/19]\tTime 0.184 (Avg-Time 0.234)\t Loss 1.1933 (Avg-Loss 1.1987)\tAcc 56.2012 (Avg-Acc 57.0020)\n",
            "Epoch: [135][8/19]\tTime 0.189 (Avg-Time 0.216)\t Loss 1.2149 (Avg-Loss 1.2116)\tAcc 56.6406 (Avg-Acc 56.6786)\n",
            "Epoch: [135][12/19]\tTime 0.200 (Avg-Time 0.208)\t Loss 1.2182 (Avg-Loss 1.2139)\tAcc 57.4219 (Avg-Acc 56.6970)\n",
            "Epoch: [135][16/19]\tTime 0.188 (Avg-Time 0.203)\t Loss 1.2067 (Avg-Loss 1.2100)\tAcc 57.3730 (Avg-Acc 56.7842)\n",
            "Epoch: [135][19/19]\tTime 0.096 (Avg-Time 0.198)\t Loss 1.1936 (Avg-Loss 1.2122)\tAcc 56.4338 (Avg-Acc 56.6875)\n",
            "EPOCH: 135 train Results: Acc 56.688 Loss: 1.2122\n",
            "Epoch: [135][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2621 (Avg-Loss 1.2621)\tAcc 55.9570 (Avg-Acc 55.9570)\n",
            "Epoch: [135][4/4]\tTime 0.051 (Avg-Time 0.048)\t Loss 1.2616 (Avg-Loss 1.2528)\tAcc 55.8075 (Avg-Acc 55.4600)\n",
            "EPOCH: 135 Validation Results: Acc 55.460 Loss: 1.2528\n",
            "Best Accuracy: 55.5300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [136][0/19]\tTime 0.182 (Avg-Time 0.182)\t Loss 1.2423 (Avg-Loss 1.2423)\tAcc 54.7852 (Avg-Acc 54.7852)\n",
            "Epoch: [136][4/19]\tTime 0.185 (Avg-Time 0.192)\t Loss 1.1847 (Avg-Loss 1.2225)\tAcc 59.1309 (Avg-Acc 56.5723)\n",
            "Epoch: [136][8/19]\tTime 0.189 (Avg-Time 0.193)\t Loss 1.2140 (Avg-Loss 1.2155)\tAcc 56.0547 (Avg-Acc 56.6406)\n",
            "Epoch: [136][12/19]\tTime 0.200 (Avg-Time 0.192)\t Loss 1.1803 (Avg-Loss 1.2082)\tAcc 56.8848 (Avg-Acc 56.7345)\n",
            "Epoch: [136][16/19]\tTime 0.207 (Avg-Time 0.194)\t Loss 1.2136 (Avg-Loss 1.2104)\tAcc 57.0801 (Avg-Acc 56.7555)\n",
            "Epoch: [136][19/19]\tTime 0.097 (Avg-Time 0.190)\t Loss 1.2149 (Avg-Loss 1.2117)\tAcc 57.4449 (Avg-Acc 56.6700)\n",
            "EPOCH: 136 train Results: Acc 56.670 Loss: 1.2117\n",
            "Epoch: [136][0/4]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.2668 (Avg-Loss 1.2668)\tAcc 55.1270 (Avg-Acc 55.1270)\n",
            "Epoch: [136][4/4]\tTime 0.053 (Avg-Time 0.049)\t Loss 1.2660 (Avg-Loss 1.2561)\tAcc 55.9735 (Avg-Acc 55.4200)\n",
            "EPOCH: 136 Validation Results: Acc 55.420 Loss: 1.2561\n",
            "Best Accuracy: 55.5300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [137][0/19]\tTime 0.187 (Avg-Time 0.187)\t Loss 1.2239 (Avg-Loss 1.2239)\tAcc 56.0059 (Avg-Acc 56.0059)\n",
            "Epoch: [137][4/19]\tTime 0.185 (Avg-Time 0.197)\t Loss 1.2009 (Avg-Loss 1.2109)\tAcc 56.5430 (Avg-Acc 56.5234)\n",
            "Epoch: [137][8/19]\tTime 0.200 (Avg-Time 0.197)\t Loss 1.1918 (Avg-Loss 1.1999)\tAcc 56.5918 (Avg-Acc 57.1452)\n",
            "Epoch: [137][12/19]\tTime 0.330 (Avg-Time 0.215)\t Loss 1.2292 (Avg-Loss 1.2039)\tAcc 55.9082 (Avg-Acc 56.8960)\n",
            "Epoch: [137][16/19]\tTime 0.380 (Avg-Time 0.279)\t Loss 1.2229 (Avg-Loss 1.2109)\tAcc 56.7383 (Avg-Acc 56.6866)\n",
            "Epoch: [137][19/19]\tTime 0.151 (Avg-Time 0.282)\t Loss 1.2318 (Avg-Loss 1.2097)\tAcc 56.0662 (Avg-Acc 56.7875)\n",
            "EPOCH: 137 train Results: Acc 56.788 Loss: 1.2097\n",
            "Epoch: [137][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2595 (Avg-Loss 1.2595)\tAcc 56.0547 (Avg-Acc 56.0547)\n",
            "Epoch: [137][4/4]\tTime 0.051 (Avg-Time 0.053)\t Loss 1.2563 (Avg-Loss 1.2495)\tAcc 55.6969 (Avg-Acc 55.8300)\n",
            "EPOCH: 137 Validation Results: Acc 55.830 Loss: 1.2495\n",
            "Best Accuracy: 55.8300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [138][0/19]\tTime 0.187 (Avg-Time 0.187)\t Loss 1.1734 (Avg-Loss 1.1734)\tAcc 58.9355 (Avg-Acc 58.9355)\n",
            "Epoch: [138][4/19]\tTime 0.213 (Avg-Time 0.196)\t Loss 1.2232 (Avg-Loss 1.1891)\tAcc 56.5430 (Avg-Acc 57.7246)\n",
            "Epoch: [138][8/19]\tTime 0.186 (Avg-Time 0.193)\t Loss 1.1435 (Avg-Loss 1.1968)\tAcc 57.9590 (Avg-Acc 57.1669)\n",
            "Epoch: [138][12/19]\tTime 0.194 (Avg-Time 0.193)\t Loss 1.2439 (Avg-Loss 1.2027)\tAcc 55.9082 (Avg-Acc 57.0951)\n",
            "Epoch: [138][16/19]\tTime 0.190 (Avg-Time 0.193)\t Loss 1.2220 (Avg-Loss 1.2057)\tAcc 56.0547 (Avg-Acc 56.8589)\n",
            "Epoch: [138][19/19]\tTime 0.097 (Avg-Time 0.188)\t Loss 1.2628 (Avg-Loss 1.2077)\tAcc 55.9743 (Avg-Acc 56.8550)\n",
            "EPOCH: 138 train Results: Acc 56.855 Loss: 1.2077\n",
            "Epoch: [138][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2519 (Avg-Loss 1.2519)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [138][4/4]\tTime 0.053 (Avg-Time 0.049)\t Loss 1.2559 (Avg-Loss 1.2460)\tAcc 55.1438 (Avg-Acc 55.4600)\n",
            "EPOCH: 138 Validation Results: Acc 55.460 Loss: 1.2460\n",
            "Best Accuracy: 55.8300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [139][0/19]\tTime 0.210 (Avg-Time 0.210)\t Loss 1.1772 (Avg-Loss 1.1772)\tAcc 58.6914 (Avg-Acc 58.6914)\n",
            "Epoch: [139][4/19]\tTime 0.189 (Avg-Time 0.196)\t Loss 1.2351 (Avg-Loss 1.2065)\tAcc 54.7852 (Avg-Acc 57.2754)\n",
            "Epoch: [139][8/19]\tTime 0.185 (Avg-Time 0.196)\t Loss 1.1951 (Avg-Loss 1.1949)\tAcc 57.4707 (Avg-Acc 57.4056)\n",
            "Epoch: [139][12/19]\tTime 0.201 (Avg-Time 0.195)\t Loss 1.1893 (Avg-Loss 1.1964)\tAcc 58.7891 (Avg-Acc 57.2416)\n",
            "Epoch: [139][16/19]\tTime 0.206 (Avg-Time 0.195)\t Loss 1.1481 (Avg-Loss 1.1965)\tAcc 59.7656 (Avg-Acc 57.2668)\n",
            "Epoch: [139][19/19]\tTime 0.095 (Avg-Time 0.190)\t Loss 1.2229 (Avg-Loss 1.2015)\tAcc 56.8015 (Avg-Acc 57.1025)\n",
            "EPOCH: 139 train Results: Acc 57.102 Loss: 1.2015\n",
            "Epoch: [139][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2539 (Avg-Loss 1.2539)\tAcc 55.5176 (Avg-Acc 55.5176)\n",
            "Epoch: [139][4/4]\tTime 0.049 (Avg-Time 0.048)\t Loss 1.2560 (Avg-Loss 1.2480)\tAcc 56.3606 (Avg-Acc 55.4700)\n",
            "EPOCH: 139 Validation Results: Acc 55.470 Loss: 1.2480\n",
            "Best Accuracy: 55.8300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [140][0/19]\tTime 0.208 (Avg-Time 0.208)\t Loss 1.1999 (Avg-Loss 1.1999)\tAcc 55.8105 (Avg-Acc 55.8105)\n",
            "Epoch: [140][4/19]\tTime 0.186 (Avg-Time 0.198)\t Loss 1.2073 (Avg-Loss 1.1942)\tAcc 57.3730 (Avg-Acc 57.4414)\n",
            "Epoch: [140][8/19]\tTime 0.308 (Avg-Time 0.210)\t Loss 1.1768 (Avg-Loss 1.1953)\tAcc 58.8867 (Avg-Acc 57.5467)\n",
            "Epoch: [140][12/19]\tTime 0.457 (Avg-Time 0.297)\t Loss 1.2319 (Avg-Loss 1.2006)\tAcc 56.3477 (Avg-Acc 57.2679)\n",
            "Epoch: [140][16/19]\tTime 0.183 (Avg-Time 0.301)\t Loss 1.2054 (Avg-Loss 1.1996)\tAcc 56.7871 (Avg-Acc 57.2898)\n",
            "Epoch: [140][19/19]\tTime 0.095 (Avg-Time 0.281)\t Loss 1.2301 (Avg-Loss 1.2000)\tAcc 57.4449 (Avg-Acc 57.3075)\n",
            "EPOCH: 140 train Results: Acc 57.307 Loss: 1.2000\n",
            "Epoch: [140][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2511 (Avg-Loss 1.2511)\tAcc 56.0547 (Avg-Acc 56.0547)\n",
            "Epoch: [140][4/4]\tTime 0.059 (Avg-Time 0.049)\t Loss 1.2493 (Avg-Loss 1.2449)\tAcc 55.9181 (Avg-Acc 55.9400)\n",
            "EPOCH: 140 Validation Results: Acc 55.940 Loss: 1.2449\n",
            "Best Accuracy: 55.9400\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [141][0/19]\tTime 0.181 (Avg-Time 0.181)\t Loss 1.1754 (Avg-Loss 1.1754)\tAcc 58.1055 (Avg-Acc 58.1055)\n",
            "Epoch: [141][4/19]\tTime 0.181 (Avg-Time 0.190)\t Loss 1.1958 (Avg-Loss 1.1916)\tAcc 56.4453 (Avg-Acc 57.4707)\n",
            "Epoch: [141][8/19]\tTime 0.197 (Avg-Time 0.189)\t Loss 1.1830 (Avg-Loss 1.1990)\tAcc 57.0312 (Avg-Acc 56.9607)\n",
            "Epoch: [141][12/19]\tTime 0.185 (Avg-Time 0.191)\t Loss 1.2307 (Avg-Loss 1.2029)\tAcc 55.2734 (Avg-Acc 56.9073)\n",
            "Epoch: [141][16/19]\tTime 0.185 (Avg-Time 0.192)\t Loss 1.2218 (Avg-Loss 1.2040)\tAcc 55.5176 (Avg-Acc 56.8474)\n",
            "Epoch: [141][19/19]\tTime 0.097 (Avg-Time 0.187)\t Loss 1.2101 (Avg-Loss 1.2042)\tAcc 58.5478 (Avg-Acc 56.9225)\n",
            "EPOCH: 141 train Results: Acc 56.922 Loss: 1.2042\n",
            "Epoch: [141][0/4]\tTime 0.065 (Avg-Time 0.065)\t Loss 1.2431 (Avg-Loss 1.2431)\tAcc 55.7617 (Avg-Acc 55.7617)\n",
            "Epoch: [141][4/4]\tTime 0.041 (Avg-Time 0.052)\t Loss 1.2503 (Avg-Loss 1.2451)\tAcc 57.0796 (Avg-Acc 55.9000)\n",
            "EPOCH: 141 Validation Results: Acc 55.900 Loss: 1.2451\n",
            "Best Accuracy: 55.9400\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [142][0/19]\tTime 0.183 (Avg-Time 0.183)\t Loss 1.2504 (Avg-Loss 1.2504)\tAcc 54.7363 (Avg-Acc 54.7363)\n",
            "Epoch: [142][4/19]\tTime 0.204 (Avg-Time 0.190)\t Loss 1.1510 (Avg-Loss 1.1924)\tAcc 58.5449 (Avg-Acc 57.2949)\n",
            "Epoch: [142][8/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.2046 (Avg-Loss 1.1908)\tAcc 56.0059 (Avg-Acc 57.3296)\n",
            "Epoch: [142][12/19]\tTime 0.186 (Avg-Time 0.190)\t Loss 1.2028 (Avg-Loss 1.1915)\tAcc 56.6406 (Avg-Acc 57.1740)\n",
            "Epoch: [142][16/19]\tTime 0.187 (Avg-Time 0.191)\t Loss 1.2034 (Avg-Loss 1.1918)\tAcc 57.2754 (Avg-Acc 57.3242)\n",
            "Epoch: [142][19/19]\tTime 0.098 (Avg-Time 0.186)\t Loss 1.2830 (Avg-Loss 1.1942)\tAcc 52.8493 (Avg-Acc 57.1750)\n",
            "EPOCH: 142 train Results: Acc 57.175 Loss: 1.1942\n",
            "Epoch: [142][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.2425 (Avg-Loss 1.2425)\tAcc 56.1035 (Avg-Acc 56.1035)\n",
            "Epoch: [142][4/4]\tTime 0.075 (Avg-Time 0.054)\t Loss 1.2466 (Avg-Loss 1.2405)\tAcc 56.4712 (Avg-Acc 56.0900)\n",
            "EPOCH: 142 Validation Results: Acc 56.090 Loss: 1.2405\n",
            "Best Accuracy: 56.0900\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [143][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.1382 (Avg-Loss 1.1382)\tAcc 60.6934 (Avg-Acc 60.6934)\n",
            "Epoch: [143][4/19]\tTime 0.183 (Avg-Time 0.188)\t Loss 1.1770 (Avg-Loss 1.1729)\tAcc 57.3730 (Avg-Acc 58.4570)\n",
            "Epoch: [143][8/19]\tTime 0.571 (Avg-Time 0.285)\t Loss 1.2051 (Avg-Loss 1.1759)\tAcc 57.2266 (Avg-Acc 58.2628)\n",
            "Epoch: [143][12/19]\tTime 0.370 (Avg-Time 0.318)\t Loss 1.1707 (Avg-Loss 1.1837)\tAcc 58.1543 (Avg-Acc 57.7411)\n",
            "Epoch: [143][16/19]\tTime 0.188 (Avg-Time 0.299)\t Loss 1.2707 (Avg-Loss 1.1904)\tAcc 53.7598 (Avg-Acc 57.3673)\n",
            "Epoch: [143][19/19]\tTime 0.104 (Avg-Time 0.280)\t Loss 1.2334 (Avg-Loss 1.1899)\tAcc 55.4228 (Avg-Acc 57.4175)\n",
            "EPOCH: 143 train Results: Acc 57.417 Loss: 1.1899\n",
            "Epoch: [143][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2496 (Avg-Loss 1.2496)\tAcc 56.1035 (Avg-Acc 56.1035)\n",
            "Epoch: [143][4/4]\tTime 0.052 (Avg-Time 0.049)\t Loss 1.2527 (Avg-Loss 1.2449)\tAcc 56.2500 (Avg-Acc 55.8600)\n",
            "EPOCH: 143 Validation Results: Acc 55.860 Loss: 1.2449\n",
            "Best Accuracy: 56.0900\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [144][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.1661 (Avg-Loss 1.1661)\tAcc 57.9590 (Avg-Acc 57.9590)\n",
            "Epoch: [144][4/19]\tTime 0.192 (Avg-Time 0.192)\t Loss 1.1805 (Avg-Loss 1.1718)\tAcc 58.0566 (Avg-Acc 57.9688)\n",
            "Epoch: [144][8/19]\tTime 0.187 (Avg-Time 0.193)\t Loss 1.2059 (Avg-Loss 1.1812)\tAcc 55.4688 (Avg-Acc 57.6931)\n",
            "Epoch: [144][12/19]\tTime 0.213 (Avg-Time 0.195)\t Loss 1.1954 (Avg-Loss 1.1827)\tAcc 56.2012 (Avg-Acc 57.5834)\n",
            "Epoch: [144][16/19]\tTime 0.185 (Avg-Time 0.194)\t Loss 1.1986 (Avg-Loss 1.1862)\tAcc 58.0078 (Avg-Acc 57.5080)\n",
            "Epoch: [144][19/19]\tTime 0.102 (Avg-Time 0.190)\t Loss 1.1945 (Avg-Loss 1.1881)\tAcc 56.7096 (Avg-Acc 57.4450)\n",
            "EPOCH: 144 train Results: Acc 57.445 Loss: 1.1881\n",
            "Epoch: [144][0/4]\tTime 0.052 (Avg-Time 0.052)\t Loss 1.2484 (Avg-Loss 1.2484)\tAcc 56.1523 (Avg-Acc 56.1523)\n",
            "Epoch: [144][4/4]\tTime 0.055 (Avg-Time 0.051)\t Loss 1.2506 (Avg-Loss 1.2423)\tAcc 56.1394 (Avg-Acc 56.0500)\n",
            "EPOCH: 144 Validation Results: Acc 56.050 Loss: 1.2423\n",
            "Best Accuracy: 56.0900\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [145][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.2371 (Avg-Loss 1.2371)\tAcc 56.1523 (Avg-Acc 56.1523)\n",
            "Epoch: [145][4/19]\tTime 0.184 (Avg-Time 0.194)\t Loss 1.1855 (Avg-Loss 1.1961)\tAcc 56.7871 (Avg-Acc 57.0898)\n",
            "Epoch: [145][8/19]\tTime 0.215 (Avg-Time 0.194)\t Loss 1.1779 (Avg-Loss 1.1904)\tAcc 57.1289 (Avg-Acc 57.2537)\n",
            "Epoch: [145][12/19]\tTime 0.183 (Avg-Time 0.191)\t Loss 1.1774 (Avg-Loss 1.1855)\tAcc 58.5938 (Avg-Acc 57.5458)\n",
            "Epoch: [145][16/19]\tTime 0.183 (Avg-Time 0.192)\t Loss 1.1680 (Avg-Loss 1.1872)\tAcc 57.9590 (Avg-Acc 57.4822)\n",
            "Epoch: [145][19/19]\tTime 0.120 (Avg-Time 0.188)\t Loss 1.1913 (Avg-Loss 1.1903)\tAcc 56.5257 (Avg-Acc 57.4450)\n",
            "EPOCH: 145 train Results: Acc 57.445 Loss: 1.1903\n",
            "Epoch: [145][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2548 (Avg-Loss 1.2548)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [145][4/4]\tTime 0.055 (Avg-Time 0.049)\t Loss 1.2554 (Avg-Loss 1.2500)\tAcc 55.9735 (Avg-Acc 55.9600)\n",
            "EPOCH: 145 Validation Results: Acc 55.960 Loss: 1.2500\n",
            "Best Accuracy: 56.0900\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [146][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.1784 (Avg-Loss 1.1784)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [146][4/19]\tTime 0.310 (Avg-Time 0.243)\t Loss 1.1900 (Avg-Loss 1.1770)\tAcc 57.9590 (Avg-Acc 58.0957)\n",
            "Epoch: [146][8/19]\tTime 0.377 (Avg-Time 0.352)\t Loss 1.2242 (Avg-Loss 1.1834)\tAcc 55.5176 (Avg-Acc 57.8071)\n",
            "Epoch: [146][12/19]\tTime 0.183 (Avg-Time 0.329)\t Loss 1.1849 (Avg-Loss 1.1843)\tAcc 57.2754 (Avg-Acc 57.8200)\n",
            "Epoch: [146][16/19]\tTime 0.208 (Avg-Time 0.296)\t Loss 1.2105 (Avg-Loss 1.1881)\tAcc 56.7871 (Avg-Acc 57.5224)\n",
            "Epoch: [146][19/19]\tTime 0.096 (Avg-Time 0.275)\t Loss 1.2432 (Avg-Loss 1.1912)\tAcc 55.1471 (Avg-Acc 57.4650)\n",
            "EPOCH: 146 train Results: Acc 57.465 Loss: 1.1912\n",
            "Epoch: [146][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2486 (Avg-Loss 1.2486)\tAcc 56.6895 (Avg-Acc 56.6895)\n",
            "Epoch: [146][4/4]\tTime 0.053 (Avg-Time 0.049)\t Loss 1.2471 (Avg-Loss 1.2412)\tAcc 56.2500 (Avg-Acc 56.4200)\n",
            "EPOCH: 146 Validation Results: Acc 56.420 Loss: 1.2412\n",
            "Best Accuracy: 56.4200\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [147][0/19]\tTime 0.192 (Avg-Time 0.192)\t Loss 1.1551 (Avg-Loss 1.1551)\tAcc 57.5195 (Avg-Acc 57.5195)\n",
            "Epoch: [147][4/19]\tTime 0.190 (Avg-Time 0.194)\t Loss 1.1528 (Avg-Loss 1.1661)\tAcc 58.5938 (Avg-Acc 58.0762)\n",
            "Epoch: [147][8/19]\tTime 0.189 (Avg-Time 0.197)\t Loss 1.1606 (Avg-Loss 1.1688)\tAcc 57.7148 (Avg-Acc 57.8830)\n",
            "Epoch: [147][12/19]\tTime 0.183 (Avg-Time 0.195)\t Loss 1.1872 (Avg-Loss 1.1759)\tAcc 58.1055 (Avg-Acc 57.6134)\n",
            "Epoch: [147][16/19]\tTime 0.184 (Avg-Time 0.193)\t Loss 1.1865 (Avg-Loss 1.1774)\tAcc 57.5684 (Avg-Acc 57.6718)\n",
            "Epoch: [147][19/19]\tTime 0.097 (Avg-Time 0.190)\t Loss 1.2325 (Avg-Loss 1.1782)\tAcc 55.5147 (Avg-Acc 57.6050)\n",
            "EPOCH: 147 train Results: Acc 57.605 Loss: 1.1782\n",
            "Epoch: [147][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2423 (Avg-Loss 1.2423)\tAcc 55.7617 (Avg-Acc 55.7617)\n",
            "Epoch: [147][4/4]\tTime 0.051 (Avg-Time 0.049)\t Loss 1.2479 (Avg-Loss 1.2382)\tAcc 55.6969 (Avg-Acc 55.9500)\n",
            "EPOCH: 147 Validation Results: Acc 55.950 Loss: 1.2382\n",
            "Best Accuracy: 56.4200\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [148][0/19]\tTime 0.192 (Avg-Time 0.192)\t Loss 1.1608 (Avg-Loss 1.1608)\tAcc 58.3496 (Avg-Acc 58.3496)\n",
            "Epoch: [148][4/19]\tTime 0.186 (Avg-Time 0.196)\t Loss 1.2083 (Avg-Loss 1.1760)\tAcc 57.7637 (Avg-Acc 57.9785)\n",
            "Epoch: [148][8/19]\tTime 0.189 (Avg-Time 0.197)\t Loss 1.1814 (Avg-Loss 1.1752)\tAcc 57.7148 (Avg-Acc 58.1109)\n",
            "Epoch: [148][12/19]\tTime 0.205 (Avg-Time 0.197)\t Loss 1.1658 (Avg-Loss 1.1753)\tAcc 57.7637 (Avg-Acc 57.9026)\n",
            "Epoch: [148][16/19]\tTime 0.189 (Avg-Time 0.195)\t Loss 1.2207 (Avg-Loss 1.1805)\tAcc 56.2012 (Avg-Acc 57.6631)\n",
            "Epoch: [148][19/19]\tTime 0.098 (Avg-Time 0.191)\t Loss 1.1670 (Avg-Loss 1.1810)\tAcc 57.1691 (Avg-Acc 57.5775)\n",
            "EPOCH: 148 train Results: Acc 57.578 Loss: 1.1810\n",
            "Epoch: [148][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2398 (Avg-Loss 1.2398)\tAcc 56.7871 (Avg-Acc 56.7871)\n",
            "Epoch: [148][4/4]\tTime 0.069 (Avg-Time 0.056)\t Loss 1.2370 (Avg-Loss 1.2331)\tAcc 56.1947 (Avg-Acc 56.4300)\n",
            "EPOCH: 148 Validation Results: Acc 56.430 Loss: 1.2331\n",
            "Best Accuracy: 56.4300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [149][0/19]\tTime 0.298 (Avg-Time 0.298)\t Loss 1.1898 (Avg-Loss 1.1898)\tAcc 57.8613 (Avg-Acc 57.8613)\n",
            "Epoch: [149][4/19]\tTime 0.569 (Avg-Time 0.400)\t Loss 1.1974 (Avg-Loss 1.1871)\tAcc 56.5918 (Avg-Acc 57.2266)\n",
            "Epoch: [149][8/19]\tTime 0.278 (Avg-Time 0.385)\t Loss 1.1611 (Avg-Loss 1.1782)\tAcc 59.5703 (Avg-Acc 57.9807)\n",
            "Epoch: [149][12/19]\tTime 0.192 (Avg-Time 0.330)\t Loss 1.1934 (Avg-Loss 1.1766)\tAcc 58.2031 (Avg-Acc 57.9214)\n",
            "Epoch: [149][16/19]\tTime 0.190 (Avg-Time 0.299)\t Loss 1.2022 (Avg-Loss 1.1802)\tAcc 56.7383 (Avg-Acc 57.8355)\n",
            "Epoch: [149][19/19]\tTime 0.097 (Avg-Time 0.278)\t Loss 1.1466 (Avg-Loss 1.1788)\tAcc 60.7537 (Avg-Acc 57.9475)\n",
            "EPOCH: 149 train Results: Acc 57.947 Loss: 1.1788\n",
            "Epoch: [149][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2379 (Avg-Loss 1.2379)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [149][4/4]\tTime 0.042 (Avg-Time 0.055)\t Loss 1.2451 (Avg-Loss 1.2335)\tAcc 56.6925 (Avg-Acc 56.2500)\n",
            "EPOCH: 149 Validation Results: Acc 56.250 Loss: 1.2335\n",
            "Best Accuracy: 56.4300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [150][0/19]\tTime 0.197 (Avg-Time 0.197)\t Loss 1.1518 (Avg-Loss 1.1518)\tAcc 57.9102 (Avg-Acc 57.9102)\n",
            "Epoch: [150][4/19]\tTime 0.216 (Avg-Time 0.199)\t Loss 1.1728 (Avg-Loss 1.1673)\tAcc 57.6660 (Avg-Acc 58.0078)\n",
            "Epoch: [150][8/19]\tTime 0.185 (Avg-Time 0.197)\t Loss 1.1827 (Avg-Loss 1.1689)\tAcc 57.4219 (Avg-Acc 58.1597)\n",
            "Epoch: [150][12/19]\tTime 0.187 (Avg-Time 0.198)\t Loss 1.2099 (Avg-Loss 1.1718)\tAcc 56.4453 (Avg-Acc 58.0341)\n",
            "Epoch: [150][16/19]\tTime 0.192 (Avg-Time 0.198)\t Loss 1.1664 (Avg-Loss 1.1725)\tAcc 58.0566 (Avg-Acc 58.0566)\n",
            "Epoch: [150][19/19]\tTime 0.100 (Avg-Time 0.193)\t Loss 1.1683 (Avg-Loss 1.1748)\tAcc 57.9963 (Avg-Acc 57.9575)\n",
            "EPOCH: 150 train Results: Acc 57.958 Loss: 1.1748\n",
            "Epoch: [150][0/4]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.2501 (Avg-Loss 1.2501)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [150][4/4]\tTime 0.042 (Avg-Time 0.054)\t Loss 1.2420 (Avg-Loss 1.2363)\tAcc 56.5265 (Avg-Acc 55.8900)\n",
            "EPOCH: 150 Validation Results: Acc 55.890 Loss: 1.2363\n",
            "Best Accuracy: 56.4300\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [151][0/19]\tTime 0.199 (Avg-Time 0.199)\t Loss 1.1488 (Avg-Loss 1.1488)\tAcc 60.0586 (Avg-Acc 60.0586)\n",
            "Epoch: [151][4/19]\tTime 0.215 (Avg-Time 0.199)\t Loss 1.1869 (Avg-Loss 1.1682)\tAcc 56.6406 (Avg-Acc 58.1836)\n",
            "Epoch: [151][8/19]\tTime 0.188 (Avg-Time 0.196)\t Loss 1.2160 (Avg-Loss 1.1724)\tAcc 56.2012 (Avg-Acc 58.2031)\n",
            "Epoch: [151][12/19]\tTime 0.191 (Avg-Time 0.198)\t Loss 1.1267 (Avg-Loss 1.1665)\tAcc 59.7656 (Avg-Acc 58.4322)\n",
            "Epoch: [151][16/19]\tTime 0.188 (Avg-Time 0.198)\t Loss 1.1913 (Avg-Loss 1.1715)\tAcc 56.8359 (Avg-Acc 58.2175)\n",
            "Epoch: [151][19/19]\tTime 0.339 (Avg-Time 0.220)\t Loss 1.1734 (Avg-Loss 1.1736)\tAcc 56.0662 (Avg-Acc 58.0250)\n",
            "EPOCH: 151 train Results: Acc 58.025 Loss: 1.1736\n",
            "Epoch: [151][0/4]\tTime 0.110 (Avg-Time 0.110)\t Loss 1.2412 (Avg-Loss 1.2412)\tAcc 56.2988 (Avg-Acc 56.2988)\n",
            "Epoch: [151][4/4]\tTime 0.110 (Avg-Time 0.112)\t Loss 1.2380 (Avg-Loss 1.2339)\tAcc 57.1350 (Avg-Acc 56.5100)\n",
            "EPOCH: 151 Validation Results: Acc 56.510 Loss: 1.2339\n",
            "Best Accuracy: 56.5100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [152][0/19]\tTime 0.336 (Avg-Time 0.336)\t Loss 1.1539 (Avg-Loss 1.1539)\tAcc 58.5449 (Avg-Acc 58.5449)\n",
            "Epoch: [152][4/19]\tTime 0.323 (Avg-Time 0.379)\t Loss 1.1514 (Avg-Loss 1.1613)\tAcc 57.5684 (Avg-Acc 58.1152)\n",
            "Epoch: [152][8/19]\tTime 0.187 (Avg-Time 0.299)\t Loss 1.1744 (Avg-Loss 1.1662)\tAcc 58.2520 (Avg-Acc 58.1868)\n",
            "Epoch: [152][12/19]\tTime 0.208 (Avg-Time 0.268)\t Loss 1.1965 (Avg-Loss 1.1720)\tAcc 57.2754 (Avg-Acc 57.9139)\n",
            "Epoch: [152][16/19]\tTime 0.194 (Avg-Time 0.250)\t Loss 1.1453 (Avg-Loss 1.1718)\tAcc 58.0566 (Avg-Acc 57.9159)\n",
            "Epoch: [152][19/19]\tTime 0.101 (Avg-Time 0.238)\t Loss 1.1999 (Avg-Loss 1.1715)\tAcc 56.8934 (Avg-Acc 57.9325)\n",
            "EPOCH: 152 train Results: Acc 57.932 Loss: 1.1715\n",
            "Epoch: [152][0/4]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.2310 (Avg-Loss 1.2310)\tAcc 56.5918 (Avg-Acc 56.5918)\n",
            "Epoch: [152][4/4]\tTime 0.054 (Avg-Time 0.049)\t Loss 1.2392 (Avg-Loss 1.2292)\tAcc 56.9137 (Avg-Acc 56.3800)\n",
            "EPOCH: 152 Validation Results: Acc 56.380 Loss: 1.2292\n",
            "Best Accuracy: 56.5100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [153][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.1737 (Avg-Loss 1.1737)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [153][4/19]\tTime 0.186 (Avg-Time 0.193)\t Loss 1.1441 (Avg-Loss 1.1680)\tAcc 59.4238 (Avg-Acc 57.8906)\n",
            "Epoch: [153][8/19]\tTime 0.184 (Avg-Time 0.194)\t Loss 1.1768 (Avg-Loss 1.1623)\tAcc 58.1055 (Avg-Acc 58.3767)\n",
            "Epoch: [153][12/19]\tTime 0.186 (Avg-Time 0.193)\t Loss 1.1611 (Avg-Loss 1.1653)\tAcc 59.2285 (Avg-Acc 58.3646)\n",
            "Epoch: [153][16/19]\tTime 0.186 (Avg-Time 0.193)\t Loss 1.1664 (Avg-Loss 1.1675)\tAcc 57.5195 (Avg-Acc 58.3582)\n",
            "Epoch: [153][19/19]\tTime 0.098 (Avg-Time 0.189)\t Loss 1.1916 (Avg-Loss 1.1705)\tAcc 57.7206 (Avg-Acc 58.2650)\n",
            "EPOCH: 153 train Results: Acc 58.265 Loss: 1.1705\n",
            "Epoch: [153][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2299 (Avg-Loss 1.2299)\tAcc 56.4941 (Avg-Acc 56.4941)\n",
            "Epoch: [153][4/4]\tTime 0.052 (Avg-Time 0.048)\t Loss 1.2468 (Avg-Loss 1.2304)\tAcc 56.5265 (Avg-Acc 56.3100)\n",
            "EPOCH: 153 Validation Results: Acc 56.310 Loss: 1.2304\n",
            "Best Accuracy: 56.5100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [154][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.1667 (Avg-Loss 1.1667)\tAcc 58.0566 (Avg-Acc 58.0566)\n",
            "Epoch: [154][4/19]\tTime 0.186 (Avg-Time 0.193)\t Loss 1.1356 (Avg-Loss 1.1444)\tAcc 59.6680 (Avg-Acc 58.7012)\n",
            "Epoch: [154][8/19]\tTime 0.207 (Avg-Time 0.194)\t Loss 1.1682 (Avg-Loss 1.1506)\tAcc 57.5684 (Avg-Acc 58.2899)\n",
            "Epoch: [154][12/19]\tTime 0.187 (Avg-Time 0.194)\t Loss 1.1754 (Avg-Loss 1.1530)\tAcc 57.7148 (Avg-Acc 58.4848)\n",
            "Epoch: [154][16/19]\tTime 0.416 (Avg-Time 0.218)\t Loss 1.1749 (Avg-Loss 1.1572)\tAcc 57.7148 (Avg-Acc 58.4042)\n",
            "Epoch: [154][19/19]\tTime 0.211 (Avg-Time 0.255)\t Loss 1.1031 (Avg-Loss 1.1588)\tAcc 61.0294 (Avg-Acc 58.4200)\n",
            "EPOCH: 154 train Results: Acc 58.420 Loss: 1.1588\n",
            "Epoch: [154][0/4]\tTime 0.071 (Avg-Time 0.071)\t Loss 1.2360 (Avg-Loss 1.2360)\tAcc 56.0059 (Avg-Acc 56.0059)\n",
            "Epoch: [154][4/4]\tTime 0.102 (Avg-Time 0.079)\t Loss 1.2481 (Avg-Loss 1.2371)\tAcc 57.3009 (Avg-Acc 56.1400)\n",
            "EPOCH: 154 Validation Results: Acc 56.140 Loss: 1.2371\n",
            "Best Accuracy: 56.5100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [155][0/19]\tTime 0.360 (Avg-Time 0.360)\t Loss 1.1236 (Avg-Loss 1.1236)\tAcc 59.6680 (Avg-Acc 59.6680)\n",
            "Epoch: [155][4/19]\tTime 0.186 (Avg-Time 0.254)\t Loss 1.1732 (Avg-Loss 1.1507)\tAcc 56.8848 (Avg-Acc 59.1504)\n",
            "Epoch: [155][8/19]\tTime 0.185 (Avg-Time 0.227)\t Loss 1.1899 (Avg-Loss 1.1602)\tAcc 57.8613 (Avg-Acc 58.6426)\n",
            "Epoch: [155][12/19]\tTime 0.188 (Avg-Time 0.216)\t Loss 1.1809 (Avg-Loss 1.1652)\tAcc 58.1055 (Avg-Acc 58.4172)\n",
            "Epoch: [155][16/19]\tTime 0.183 (Avg-Time 0.209)\t Loss 1.1754 (Avg-Loss 1.1641)\tAcc 57.6660 (Avg-Acc 58.4272)\n",
            "Epoch: [155][19/19]\tTime 0.096 (Avg-Time 0.203)\t Loss 1.2128 (Avg-Loss 1.1684)\tAcc 54.5037 (Avg-Acc 58.1975)\n",
            "EPOCH: 155 train Results: Acc 58.197 Loss: 1.1684\n",
            "Epoch: [155][0/4]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.2300 (Avg-Loss 1.2300)\tAcc 56.4941 (Avg-Acc 56.4941)\n",
            "Epoch: [155][4/4]\tTime 0.043 (Avg-Time 0.057)\t Loss 1.2417 (Avg-Loss 1.2294)\tAcc 56.3606 (Avg-Acc 56.3900)\n",
            "EPOCH: 155 Validation Results: Acc 56.390 Loss: 1.2294\n",
            "Best Accuracy: 56.5100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [156][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.1475 (Avg-Loss 1.1475)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [156][4/19]\tTime 0.207 (Avg-Time 0.196)\t Loss 1.1582 (Avg-Loss 1.1466)\tAcc 59.4727 (Avg-Acc 58.8965)\n",
            "Epoch: [156][8/19]\tTime 0.186 (Avg-Time 0.194)\t Loss 1.1727 (Avg-Loss 1.1594)\tAcc 57.8613 (Avg-Acc 58.3984)\n",
            "Epoch: [156][12/19]\tTime 0.214 (Avg-Time 0.194)\t Loss 1.1361 (Avg-Loss 1.1586)\tAcc 60.1562 (Avg-Acc 58.4848)\n",
            "Epoch: [156][16/19]\tTime 0.188 (Avg-Time 0.193)\t Loss 1.2012 (Avg-Loss 1.1619)\tAcc 57.8125 (Avg-Acc 58.5105)\n",
            "Epoch: [156][19/19]\tTime 0.096 (Avg-Time 0.189)\t Loss 1.1793 (Avg-Loss 1.1627)\tAcc 58.5478 (Avg-Acc 58.3975)\n",
            "EPOCH: 156 train Results: Acc 58.398 Loss: 1.1627\n",
            "Epoch: [156][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2364 (Avg-Loss 1.2364)\tAcc 55.7617 (Avg-Acc 55.7617)\n",
            "Epoch: [156][4/4]\tTime 0.054 (Avg-Time 0.049)\t Loss 1.2424 (Avg-Loss 1.2273)\tAcc 56.0841 (Avg-Acc 56.1400)\n",
            "EPOCH: 156 Validation Results: Acc 56.140 Loss: 1.2273\n",
            "Best Accuracy: 56.5100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [157][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.1819 (Avg-Loss 1.1819)\tAcc 57.3242 (Avg-Acc 57.3242)\n",
            "Epoch: [157][4/19]\tTime 0.192 (Avg-Time 0.196)\t Loss 1.1327 (Avg-Loss 1.1555)\tAcc 59.5215 (Avg-Acc 58.9648)\n",
            "Epoch: [157][8/19]\tTime 0.186 (Avg-Time 0.198)\t Loss 1.1511 (Avg-Loss 1.1588)\tAcc 57.6172 (Avg-Acc 58.4256)\n",
            "Epoch: [157][12/19]\tTime 0.276 (Avg-Time 0.204)\t Loss 1.1675 (Avg-Loss 1.1610)\tAcc 56.5918 (Avg-Acc 58.1881)\n",
            "Epoch: [157][16/19]\tTime 0.394 (Avg-Time 0.270)\t Loss 1.1490 (Avg-Loss 1.1612)\tAcc 58.3496 (Avg-Acc 58.3295)\n",
            "Epoch: [157][19/19]\tTime 0.180 (Avg-Time 0.278)\t Loss 1.1652 (Avg-Loss 1.1628)\tAcc 59.6507 (Avg-Acc 58.3025)\n",
            "EPOCH: 157 train Results: Acc 58.303 Loss: 1.1628\n",
            "Epoch: [157][0/4]\tTime 0.056 (Avg-Time 0.056)\t Loss 1.2290 (Avg-Loss 1.2290)\tAcc 56.7871 (Avg-Acc 56.7871)\n",
            "Epoch: [157][4/4]\tTime 0.044 (Avg-Time 0.056)\t Loss 1.2390 (Avg-Loss 1.2246)\tAcc 57.2456 (Avg-Acc 56.4900)\n",
            "EPOCH: 157 Validation Results: Acc 56.490 Loss: 1.2246\n",
            "Best Accuracy: 56.5100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [158][0/19]\tTime 0.196 (Avg-Time 0.196)\t Loss 1.1357 (Avg-Loss 1.1357)\tAcc 60.4980 (Avg-Acc 60.4980)\n",
            "Epoch: [158][4/19]\tTime 0.212 (Avg-Time 0.197)\t Loss 1.1538 (Avg-Loss 1.1622)\tAcc 57.6172 (Avg-Acc 58.4668)\n",
            "Epoch: [158][8/19]\tTime 0.189 (Avg-Time 0.195)\t Loss 1.1490 (Avg-Loss 1.1652)\tAcc 59.1797 (Avg-Acc 58.5341)\n",
            "Epoch: [158][12/19]\tTime 0.190 (Avg-Time 0.196)\t Loss 1.1586 (Avg-Loss 1.1586)\tAcc 57.8613 (Avg-Acc 58.7816)\n",
            "Epoch: [158][16/19]\tTime 0.187 (Avg-Time 0.195)\t Loss 1.1310 (Avg-Loss 1.1613)\tAcc 59.1797 (Avg-Acc 58.5420)\n",
            "Epoch: [158][19/19]\tTime 0.096 (Avg-Time 0.190)\t Loss 1.2161 (Avg-Loss 1.1627)\tAcc 56.7096 (Avg-Acc 58.4575)\n",
            "EPOCH: 158 train Results: Acc 58.458 Loss: 1.1627\n",
            "Epoch: [158][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2361 (Avg-Loss 1.2361)\tAcc 56.3477 (Avg-Acc 56.3477)\n",
            "Epoch: [158][4/4]\tTime 0.053 (Avg-Time 0.052)\t Loss 1.2350 (Avg-Loss 1.2291)\tAcc 56.7478 (Avg-Acc 56.0900)\n",
            "EPOCH: 158 Validation Results: Acc 56.090 Loss: 1.2291\n",
            "Best Accuracy: 56.5100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [159][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.1405 (Avg-Loss 1.1405)\tAcc 59.2285 (Avg-Acc 59.2285)\n",
            "Epoch: [159][4/19]\tTime 0.189 (Avg-Time 0.188)\t Loss 1.1099 (Avg-Loss 1.1348)\tAcc 59.0332 (Avg-Acc 59.1016)\n",
            "Epoch: [159][8/19]\tTime 0.186 (Avg-Time 0.190)\t Loss 1.1779 (Avg-Loss 1.1439)\tAcc 58.5449 (Avg-Acc 58.6480)\n",
            "Epoch: [159][12/19]\tTime 0.191 (Avg-Time 0.192)\t Loss 1.1360 (Avg-Loss 1.1447)\tAcc 58.6914 (Avg-Acc 58.6914)\n",
            "Epoch: [159][16/19]\tTime 0.185 (Avg-Time 0.193)\t Loss 1.1287 (Avg-Loss 1.1475)\tAcc 59.4727 (Avg-Acc 58.8494)\n",
            "Epoch: [159][19/19]\tTime 0.096 (Avg-Time 0.189)\t Loss 1.1619 (Avg-Loss 1.1510)\tAcc 58.8235 (Avg-Acc 58.7925)\n",
            "EPOCH: 159 train Results: Acc 58.792 Loss: 1.1510\n",
            "Epoch: [159][0/4]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.2304 (Avg-Loss 1.2304)\tAcc 56.4941 (Avg-Acc 56.4941)\n",
            "Epoch: [159][4/4]\tTime 0.065 (Avg-Time 0.052)\t Loss 1.2389 (Avg-Loss 1.2229)\tAcc 57.2456 (Avg-Acc 56.5500)\n",
            "EPOCH: 159 Validation Results: Acc 56.550 Loss: 1.2229\n",
            "Best Accuracy: 56.5500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [160][0/19]\tTime 0.210 (Avg-Time 0.210)\t Loss 1.0994 (Avg-Loss 1.0994)\tAcc 60.4004 (Avg-Acc 60.4004)\n",
            "Epoch: [160][4/19]\tTime 0.189 (Avg-Time 0.195)\t Loss 1.1327 (Avg-Loss 1.1312)\tAcc 58.5938 (Avg-Acc 59.2090)\n",
            "Epoch: [160][8/19]\tTime 0.271 (Avg-Time 0.204)\t Loss 1.1467 (Avg-Loss 1.1444)\tAcc 56.7383 (Avg-Acc 58.3984)\n",
            "Epoch: [160][12/19]\tTime 0.578 (Avg-Time 0.286)\t Loss 1.1717 (Avg-Loss 1.1498)\tAcc 58.4473 (Avg-Acc 58.5224)\n",
            "Epoch: [160][16/19]\tTime 0.197 (Avg-Time 0.295)\t Loss 1.1703 (Avg-Loss 1.1501)\tAcc 57.4219 (Avg-Acc 58.4386)\n",
            "Epoch: [160][19/19]\tTime 0.094 (Avg-Time 0.275)\t Loss 1.2343 (Avg-Loss 1.1545)\tAcc 55.0551 (Avg-Acc 58.2950)\n",
            "EPOCH: 160 train Results: Acc 58.295 Loss: 1.1545\n",
            "Epoch: [160][0/4]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.2266 (Avg-Loss 1.2266)\tAcc 55.7129 (Avg-Acc 55.7129)\n",
            "Epoch: [160][4/4]\tTime 0.049 (Avg-Time 0.048)\t Loss 1.2353 (Avg-Loss 1.2226)\tAcc 57.0796 (Avg-Acc 56.3200)\n",
            "EPOCH: 160 Validation Results: Acc 56.320 Loss: 1.2226\n",
            "Best Accuracy: 56.5500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [161][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.1651 (Avg-Loss 1.1651)\tAcc 57.5195 (Avg-Acc 57.5195)\n",
            "Epoch: [161][4/19]\tTime 0.186 (Avg-Time 0.190)\t Loss 1.1513 (Avg-Loss 1.1607)\tAcc 59.1797 (Avg-Acc 58.6816)\n",
            "Epoch: [161][8/19]\tTime 0.205 (Avg-Time 0.191)\t Loss 1.1261 (Avg-Loss 1.1542)\tAcc 59.4238 (Avg-Acc 58.7945)\n",
            "Epoch: [161][12/19]\tTime 0.187 (Avg-Time 0.190)\t Loss 1.1651 (Avg-Loss 1.1562)\tAcc 58.1055 (Avg-Acc 58.7966)\n",
            "Epoch: [161][16/19]\tTime 0.187 (Avg-Time 0.190)\t Loss 1.1328 (Avg-Loss 1.1556)\tAcc 59.5703 (Avg-Acc 58.8264)\n",
            "Epoch: [161][19/19]\tTime 0.098 (Avg-Time 0.186)\t Loss 1.1091 (Avg-Loss 1.1554)\tAcc 61.7647 (Avg-Acc 58.9075)\n",
            "EPOCH: 161 train Results: Acc 58.907 Loss: 1.1554\n",
            "Epoch: [161][0/4]\tTime 0.068 (Avg-Time 0.068)\t Loss 1.2295 (Avg-Loss 1.2295)\tAcc 56.7383 (Avg-Acc 56.7383)\n",
            "Epoch: [161][4/4]\tTime 0.041 (Avg-Time 0.053)\t Loss 1.2373 (Avg-Loss 1.2242)\tAcc 56.5265 (Avg-Acc 56.5600)\n",
            "EPOCH: 161 Validation Results: Acc 56.560 Loss: 1.2242\n",
            "Best Accuracy: 56.5600\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [162][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.1831 (Avg-Loss 1.1831)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [162][4/19]\tTime 0.207 (Avg-Time 0.193)\t Loss 1.1345 (Avg-Loss 1.1387)\tAcc 59.0332 (Avg-Acc 59.0137)\n",
            "Epoch: [162][8/19]\tTime 0.184 (Avg-Time 0.190)\t Loss 1.1335 (Avg-Loss 1.1424)\tAcc 60.2051 (Avg-Acc 59.4076)\n",
            "Epoch: [162][12/19]\tTime 0.185 (Avg-Time 0.193)\t Loss 1.1432 (Avg-Loss 1.1456)\tAcc 59.0332 (Avg-Acc 59.2773)\n",
            "Epoch: [162][16/19]\tTime 0.184 (Avg-Time 0.193)\t Loss 1.1376 (Avg-Loss 1.1463)\tAcc 58.3496 (Avg-Acc 59.1883)\n",
            "Epoch: [162][19/19]\tTime 0.095 (Avg-Time 0.189)\t Loss 1.1385 (Avg-Loss 1.1479)\tAcc 58.8235 (Avg-Acc 59.0875)\n",
            "EPOCH: 162 train Results: Acc 59.087 Loss: 1.1479\n",
            "Epoch: [162][0/4]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.2279 (Avg-Loss 1.2279)\tAcc 56.7383 (Avg-Acc 56.7383)\n",
            "Epoch: [162][4/4]\tTime 0.042 (Avg-Time 0.054)\t Loss 1.2283 (Avg-Loss 1.2184)\tAcc 56.9690 (Avg-Acc 56.6100)\n",
            "EPOCH: 162 Validation Results: Acc 56.610 Loss: 1.2184\n",
            "Best Accuracy: 56.6100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [163][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.1472 (Avg-Loss 1.1472)\tAcc 59.2773 (Avg-Acc 59.2773)\n",
            "Epoch: [163][4/19]\tTime 0.210 (Avg-Time 0.194)\t Loss 1.1034 (Avg-Loss 1.1319)\tAcc 59.3750 (Avg-Acc 59.6875)\n",
            "Epoch: [163][8/19]\tTime 0.618 (Avg-Time 0.319)\t Loss 1.1338 (Avg-Loss 1.1413)\tAcc 59.4238 (Avg-Acc 59.2502)\n",
            "Epoch: [163][12/19]\tTime 0.336 (Avg-Time 0.335)\t Loss 1.1455 (Avg-Loss 1.1419)\tAcc 58.3984 (Avg-Acc 59.2210)\n",
            "Epoch: [163][16/19]\tTime 0.193 (Avg-Time 0.301)\t Loss 1.1345 (Avg-Loss 1.1404)\tAcc 59.0332 (Avg-Acc 59.2055)\n",
            "Epoch: [163][19/19]\tTime 0.099 (Avg-Time 0.281)\t Loss 1.1827 (Avg-Loss 1.1417)\tAcc 58.3640 (Avg-Acc 59.2325)\n",
            "EPOCH: 163 train Results: Acc 59.233 Loss: 1.1417\n",
            "Epoch: [163][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2248 (Avg-Loss 1.2248)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [163][4/4]\tTime 0.054 (Avg-Time 0.049)\t Loss 1.2287 (Avg-Loss 1.2191)\tAcc 57.2456 (Avg-Acc 57.0500)\n",
            "EPOCH: 163 Validation Results: Acc 57.050 Loss: 1.2191\n",
            "Best Accuracy: 57.0500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [164][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.1465 (Avg-Loss 1.1465)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [164][4/19]\tTime 0.187 (Avg-Time 0.191)\t Loss 1.1477 (Avg-Loss 1.1390)\tAcc 58.6426 (Avg-Acc 59.0332)\n",
            "Epoch: [164][8/19]\tTime 0.185 (Avg-Time 0.193)\t Loss 1.1052 (Avg-Loss 1.1458)\tAcc 61.1328 (Avg-Acc 58.7891)\n",
            "Epoch: [164][12/19]\tTime 0.210 (Avg-Time 0.195)\t Loss 1.1265 (Avg-Loss 1.1445)\tAcc 60.0586 (Avg-Acc 58.8116)\n",
            "Epoch: [164][16/19]\tTime 0.188 (Avg-Time 0.193)\t Loss 1.1414 (Avg-Loss 1.1484)\tAcc 59.1797 (Avg-Acc 58.7000)\n",
            "Epoch: [164][19/19]\tTime 0.098 (Avg-Time 0.189)\t Loss 1.1468 (Avg-Loss 1.1463)\tAcc 60.2941 (Avg-Acc 58.8425)\n",
            "EPOCH: 164 train Results: Acc 58.843 Loss: 1.1463\n",
            "Epoch: [164][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.2235 (Avg-Loss 1.2235)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [164][4/4]\tTime 0.051 (Avg-Time 0.048)\t Loss 1.2333 (Avg-Loss 1.2202)\tAcc 57.3009 (Avg-Acc 56.9200)\n",
            "EPOCH: 164 Validation Results: Acc 56.920 Loss: 1.2202\n",
            "Best Accuracy: 57.0500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [165][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.1618 (Avg-Loss 1.1618)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [165][4/19]\tTime 0.185 (Avg-Time 0.191)\t Loss 1.1727 (Avg-Loss 1.1530)\tAcc 58.0566 (Avg-Acc 58.8574)\n",
            "Epoch: [165][8/19]\tTime 0.197 (Avg-Time 0.191)\t Loss 1.1708 (Avg-Loss 1.1449)\tAcc 57.7148 (Avg-Acc 59.0441)\n",
            "Epoch: [165][12/19]\tTime 0.187 (Avg-Time 0.190)\t Loss 1.1527 (Avg-Loss 1.1480)\tAcc 59.2773 (Avg-Acc 58.9543)\n",
            "Epoch: [165][16/19]\tTime 0.185 (Avg-Time 0.191)\t Loss 1.1365 (Avg-Loss 1.1492)\tAcc 59.4238 (Avg-Acc 58.9011)\n",
            "Epoch: [165][19/19]\tTime 0.115 (Avg-Time 0.187)\t Loss 1.1471 (Avg-Loss 1.1474)\tAcc 59.1912 (Avg-Acc 58.9625)\n",
            "EPOCH: 165 train Results: Acc 58.962 Loss: 1.1474\n",
            "Epoch: [165][0/4]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.2225 (Avg-Loss 1.2225)\tAcc 56.6895 (Avg-Acc 56.6895)\n",
            "Epoch: [165][4/4]\tTime 0.053 (Avg-Time 0.050)\t Loss 1.2326 (Avg-Loss 1.2173)\tAcc 57.4115 (Avg-Acc 56.7700)\n",
            "EPOCH: 165 Validation Results: Acc 56.770 Loss: 1.2173\n",
            "Best Accuracy: 57.0500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [166][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.1195 (Avg-Loss 1.1195)\tAcc 60.7910 (Avg-Acc 60.7910)\n",
            "Epoch: [166][4/19]\tTime 0.500 (Avg-Time 0.323)\t Loss 1.1377 (Avg-Loss 1.1281)\tAcc 59.4238 (Avg-Acc 59.5703)\n",
            "Epoch: [166][8/19]\tTime 0.351 (Avg-Time 0.359)\t Loss 1.1328 (Avg-Loss 1.1409)\tAcc 58.9844 (Avg-Acc 59.3316)\n",
            "Epoch: [166][12/19]\tTime 0.187 (Avg-Time 0.324)\t Loss 1.1692 (Avg-Loss 1.1436)\tAcc 57.5195 (Avg-Acc 59.1910)\n",
            "Epoch: [166][16/19]\tTime 0.207 (Avg-Time 0.293)\t Loss 1.1547 (Avg-Loss 1.1457)\tAcc 58.1055 (Avg-Acc 59.1021)\n",
            "Epoch: [166][19/19]\tTime 0.099 (Avg-Time 0.274)\t Loss 1.1509 (Avg-Loss 1.1491)\tAcc 57.4449 (Avg-Acc 58.8325)\n",
            "EPOCH: 166 train Results: Acc 58.833 Loss: 1.1491\n",
            "Epoch: [166][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2284 (Avg-Loss 1.2284)\tAcc 56.9824 (Avg-Acc 56.9824)\n",
            "Epoch: [166][4/4]\tTime 0.056 (Avg-Time 0.049)\t Loss 1.2319 (Avg-Loss 1.2240)\tAcc 57.5221 (Avg-Acc 56.6800)\n",
            "EPOCH: 166 Validation Results: Acc 56.680 Loss: 1.2240\n",
            "Best Accuracy: 57.0500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [167][0/19]\tTime 0.193 (Avg-Time 0.193)\t Loss 1.1267 (Avg-Loss 1.1267)\tAcc 59.9121 (Avg-Acc 59.9121)\n",
            "Epoch: [167][4/19]\tTime 0.183 (Avg-Time 0.193)\t Loss 1.1791 (Avg-Loss 1.1416)\tAcc 57.6660 (Avg-Acc 59.2773)\n",
            "Epoch: [167][8/19]\tTime 0.186 (Avg-Time 0.195)\t Loss 1.1341 (Avg-Loss 1.1481)\tAcc 60.0586 (Avg-Acc 59.2665)\n",
            "Epoch: [167][12/19]\tTime 0.205 (Avg-Time 0.194)\t Loss 1.1654 (Avg-Loss 1.1468)\tAcc 59.1797 (Avg-Acc 59.4501)\n",
            "Epoch: [167][16/19]\tTime 0.185 (Avg-Time 0.192)\t Loss 1.1703 (Avg-Loss 1.1475)\tAcc 56.6406 (Avg-Acc 59.1711)\n",
            "Epoch: [167][19/19]\tTime 0.095 (Avg-Time 0.189)\t Loss 1.1878 (Avg-Loss 1.1493)\tAcc 56.9853 (Avg-Acc 59.0625)\n",
            "EPOCH: 167 train Results: Acc 59.062 Loss: 1.1493\n",
            "Epoch: [167][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2294 (Avg-Loss 1.2294)\tAcc 56.7871 (Avg-Acc 56.7871)\n",
            "Epoch: [167][4/4]\tTime 0.067 (Avg-Time 0.051)\t Loss 1.2328 (Avg-Loss 1.2261)\tAcc 56.9137 (Avg-Acc 56.3700)\n",
            "EPOCH: 167 Validation Results: Acc 56.370 Loss: 1.2261\n",
            "Best Accuracy: 57.0500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [168][0/19]\tTime 0.183 (Avg-Time 0.183)\t Loss 1.1351 (Avg-Loss 1.1351)\tAcc 60.0586 (Avg-Acc 60.0586)\n",
            "Epoch: [168][4/19]\tTime 0.185 (Avg-Time 0.194)\t Loss 1.1592 (Avg-Loss 1.1425)\tAcc 58.7402 (Avg-Acc 59.6973)\n",
            "Epoch: [168][8/19]\tTime 0.189 (Avg-Time 0.194)\t Loss 1.1459 (Avg-Loss 1.1356)\tAcc 56.4941 (Avg-Acc 59.5432)\n",
            "Epoch: [168][12/19]\tTime 0.208 (Avg-Time 0.195)\t Loss 1.1655 (Avg-Loss 1.1401)\tAcc 58.0078 (Avg-Acc 59.2849)\n",
            "Epoch: [168][16/19]\tTime 0.183 (Avg-Time 0.193)\t Loss 1.1464 (Avg-Loss 1.1424)\tAcc 60.1074 (Avg-Acc 59.3118)\n",
            "Epoch: [168][19/19]\tTime 0.101 (Avg-Time 0.190)\t Loss 1.1120 (Avg-Loss 1.1396)\tAcc 58.9154 (Avg-Acc 59.3575)\n",
            "EPOCH: 168 train Results: Acc 59.358 Loss: 1.1396\n",
            "Epoch: [168][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2341 (Avg-Loss 1.2341)\tAcc 56.4941 (Avg-Acc 56.4941)\n",
            "Epoch: [168][4/4]\tTime 0.076 (Avg-Time 0.053)\t Loss 1.2319 (Avg-Loss 1.2231)\tAcc 56.8031 (Avg-Acc 56.7800)\n",
            "EPOCH: 168 Validation Results: Acc 56.780 Loss: 1.2231\n",
            "Best Accuracy: 57.0500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [169][0/19]\tTime 0.382 (Avg-Time 0.382)\t Loss 1.1424 (Avg-Loss 1.1424)\tAcc 58.8867 (Avg-Acc 58.8867)\n",
            "Epoch: [169][4/19]\tTime 0.604 (Avg-Time 0.454)\t Loss 1.1204 (Avg-Loss 1.1245)\tAcc 60.4980 (Avg-Acc 60.1660)\n",
            "Epoch: [169][8/19]\tTime 0.188 (Avg-Time 0.404)\t Loss 1.1297 (Avg-Loss 1.1325)\tAcc 59.4238 (Avg-Acc 59.9230)\n",
            "Epoch: [169][12/19]\tTime 0.185 (Avg-Time 0.339)\t Loss 1.1819 (Avg-Loss 1.1373)\tAcc 57.7148 (Avg-Acc 59.5102)\n",
            "Epoch: [169][16/19]\tTime 0.186 (Avg-Time 0.306)\t Loss 1.1684 (Avg-Loss 1.1389)\tAcc 57.4219 (Avg-Acc 59.4009)\n",
            "Epoch: [169][19/19]\tTime 0.100 (Avg-Time 0.284)\t Loss 1.1761 (Avg-Loss 1.1429)\tAcc 58.2721 (Avg-Acc 59.2750)\n",
            "EPOCH: 169 train Results: Acc 59.275 Loss: 1.1429\n",
            "Epoch: [169][0/4]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.2227 (Avg-Loss 1.2227)\tAcc 57.1777 (Avg-Acc 57.1777)\n",
            "Epoch: [169][4/4]\tTime 0.045 (Avg-Time 0.054)\t Loss 1.2310 (Avg-Loss 1.2185)\tAcc 57.3009 (Avg-Acc 56.9400)\n",
            "EPOCH: 169 Validation Results: Acc 56.940 Loss: 1.2185\n",
            "Best Accuracy: 57.0500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [170][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.1272 (Avg-Loss 1.1272)\tAcc 60.1562 (Avg-Acc 60.1562)\n",
            "Epoch: [170][4/19]\tTime 0.234 (Avg-Time 0.202)\t Loss 1.1226 (Avg-Loss 1.1258)\tAcc 60.6934 (Avg-Acc 60.6445)\n",
            "Epoch: [170][8/19]\tTime 0.191 (Avg-Time 0.199)\t Loss 1.1466 (Avg-Loss 1.1317)\tAcc 59.8145 (Avg-Acc 59.8253)\n",
            "Epoch: [170][12/19]\tTime 0.188 (Avg-Time 0.197)\t Loss 1.1298 (Avg-Loss 1.1359)\tAcc 58.6914 (Avg-Acc 59.4914)\n",
            "Epoch: [170][16/19]\tTime 0.185 (Avg-Time 0.197)\t Loss 1.1308 (Avg-Loss 1.1378)\tAcc 58.8379 (Avg-Acc 59.4324)\n",
            "Epoch: [170][19/19]\tTime 0.097 (Avg-Time 0.191)\t Loss 1.1882 (Avg-Loss 1.1392)\tAcc 57.5368 (Avg-Acc 59.3825)\n",
            "EPOCH: 170 train Results: Acc 59.383 Loss: 1.1392\n",
            "Epoch: [170][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2319 (Avg-Loss 1.2319)\tAcc 56.7871 (Avg-Acc 56.7871)\n",
            "Epoch: [170][4/4]\tTime 0.054 (Avg-Time 0.053)\t Loss 1.2331 (Avg-Loss 1.2188)\tAcc 56.4159 (Avg-Acc 56.6700)\n",
            "EPOCH: 170 Validation Results: Acc 56.670 Loss: 1.2188\n",
            "Best Accuracy: 57.0500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [171][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.0822 (Avg-Loss 1.0822)\tAcc 61.0840 (Avg-Acc 61.0840)\n",
            "Epoch: [171][4/19]\tTime 0.194 (Avg-Time 0.188)\t Loss 1.1109 (Avg-Loss 1.1169)\tAcc 60.8398 (Avg-Acc 60.0391)\n",
            "Epoch: [171][8/19]\tTime 0.186 (Avg-Time 0.187)\t Loss 1.1401 (Avg-Loss 1.1265)\tAcc 59.5215 (Avg-Acc 59.5757)\n",
            "Epoch: [171][12/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.1356 (Avg-Loss 1.1257)\tAcc 59.5703 (Avg-Acc 59.5440)\n",
            "Epoch: [171][16/19]\tTime 0.189 (Avg-Time 0.191)\t Loss 1.1479 (Avg-Loss 1.1337)\tAcc 58.1055 (Avg-Acc 59.2544)\n",
            "Epoch: [171][19/19]\tTime 0.241 (Avg-Time 0.204)\t Loss 1.1413 (Avg-Loss 1.1351)\tAcc 59.0074 (Avg-Acc 59.2200)\n",
            "EPOCH: 171 train Results: Acc 59.220 Loss: 1.1351\n",
            "Epoch: [171][0/4]\tTime 0.096 (Avg-Time 0.096)\t Loss 1.2333 (Avg-Loss 1.2333)\tAcc 56.4941 (Avg-Acc 56.4941)\n",
            "Epoch: [171][4/4]\tTime 0.114 (Avg-Time 0.108)\t Loss 1.2368 (Avg-Loss 1.2287)\tAcc 56.4712 (Avg-Acc 56.2900)\n",
            "EPOCH: 171 Validation Results: Acc 56.290 Loss: 1.2287\n",
            "Best Accuracy: 57.0500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [172][0/19]\tTime 0.499 (Avg-Time 0.499)\t Loss 1.1286 (Avg-Loss 1.1286)\tAcc 59.0820 (Avg-Acc 59.0820)\n",
            "Epoch: [172][4/19]\tTime 0.359 (Avg-Time 0.404)\t Loss 1.1288 (Avg-Loss 1.1272)\tAcc 60.2051 (Avg-Acc 59.4922)\n",
            "Epoch: [172][8/19]\tTime 0.191 (Avg-Time 0.308)\t Loss 1.1543 (Avg-Loss 1.1325)\tAcc 59.6191 (Avg-Acc 59.5323)\n",
            "Epoch: [172][12/19]\tTime 0.184 (Avg-Time 0.271)\t Loss 1.1590 (Avg-Loss 1.1299)\tAcc 59.0820 (Avg-Acc 59.6529)\n",
            "Epoch: [172][16/19]\tTime 0.188 (Avg-Time 0.254)\t Loss 1.1073 (Avg-Loss 1.1317)\tAcc 60.4492 (Avg-Acc 59.5358)\n",
            "Epoch: [172][19/19]\tTime 0.110 (Avg-Time 0.240)\t Loss 1.1152 (Avg-Loss 1.1328)\tAcc 59.6507 (Avg-Acc 59.5900)\n",
            "EPOCH: 172 train Results: Acc 59.590 Loss: 1.1328\n",
            "Epoch: [172][0/4]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.2266 (Avg-Loss 1.2266)\tAcc 56.7383 (Avg-Acc 56.7383)\n",
            "Epoch: [172][4/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.2282 (Avg-Loss 1.2145)\tAcc 56.9137 (Avg-Acc 57.0300)\n",
            "EPOCH: 172 Validation Results: Acc 57.030 Loss: 1.2145\n",
            "Best Accuracy: 57.0500\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [173][0/19]\tTime 0.192 (Avg-Time 0.192)\t Loss 1.1109 (Avg-Loss 1.1109)\tAcc 60.0098 (Avg-Acc 60.0098)\n",
            "Epoch: [173][4/19]\tTime 0.186 (Avg-Time 0.190)\t Loss 1.1173 (Avg-Loss 1.1142)\tAcc 59.6680 (Avg-Acc 60.0586)\n",
            "Epoch: [173][8/19]\tTime 0.191 (Avg-Time 0.189)\t Loss 1.0935 (Avg-Loss 1.1208)\tAcc 60.5957 (Avg-Acc 60.0098)\n",
            "Epoch: [173][12/19]\tTime 0.185 (Avg-Time 0.192)\t Loss 1.1832 (Avg-Loss 1.1280)\tAcc 58.3008 (Avg-Acc 59.6830)\n",
            "Epoch: [173][16/19]\tTime 0.185 (Avg-Time 0.192)\t Loss 1.1623 (Avg-Loss 1.1293)\tAcc 58.0566 (Avg-Acc 59.6306)\n",
            "Epoch: [173][19/19]\tTime 0.100 (Avg-Time 0.188)\t Loss 1.1300 (Avg-Loss 1.1290)\tAcc 59.8346 (Avg-Acc 59.6450)\n",
            "EPOCH: 173 train Results: Acc 59.645 Loss: 1.1290\n",
            "Epoch: [173][0/4]\tTime 0.053 (Avg-Time 0.053)\t Loss 1.2188 (Avg-Loss 1.2188)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [173][4/4]\tTime 0.042 (Avg-Time 0.053)\t Loss 1.2185 (Avg-Loss 1.2084)\tAcc 58.2965 (Avg-Acc 57.7100)\n",
            "EPOCH: 173 Validation Results: Acc 57.710 Loss: 1.2084\n",
            "Best Accuracy: 57.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [174][0/19]\tTime 0.185 (Avg-Time 0.185)\t Loss 1.1438 (Avg-Loss 1.1438)\tAcc 58.6914 (Avg-Acc 58.6914)\n",
            "Epoch: [174][4/19]\tTime 0.206 (Avg-Time 0.190)\t Loss 1.1128 (Avg-Loss 1.1212)\tAcc 59.9121 (Avg-Acc 59.8047)\n",
            "Epoch: [174][8/19]\tTime 0.185 (Avg-Time 0.189)\t Loss 1.1568 (Avg-Loss 1.1216)\tAcc 58.9355 (Avg-Acc 60.1508)\n",
            "Epoch: [174][12/19]\tTime 0.185 (Avg-Time 0.192)\t Loss 1.1083 (Avg-Loss 1.1222)\tAcc 60.3027 (Avg-Acc 59.9910)\n",
            "Epoch: [174][16/19]\tTime 0.318 (Avg-Time 0.206)\t Loss 1.1433 (Avg-Loss 1.1270)\tAcc 59.8633 (Avg-Acc 59.8317)\n",
            "Epoch: [174][19/19]\tTime 0.342 (Avg-Time 0.246)\t Loss 1.0942 (Avg-Loss 1.1295)\tAcc 62.0404 (Avg-Acc 59.7325)\n",
            "EPOCH: 174 train Results: Acc 59.733 Loss: 1.1295\n",
            "Epoch: [174][0/4]\tTime 0.118 (Avg-Time 0.118)\t Loss 1.2159 (Avg-Loss 1.2159)\tAcc 56.7871 (Avg-Acc 56.7871)\n",
            "Epoch: [174][4/4]\tTime 0.071 (Avg-Time 0.094)\t Loss 1.2326 (Avg-Loss 1.2158)\tAcc 57.3009 (Avg-Acc 57.0500)\n",
            "EPOCH: 174 Validation Results: Acc 57.050 Loss: 1.2158\n",
            "Best Accuracy: 57.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [175][0/19]\tTime 0.394 (Avg-Time 0.394)\t Loss 1.1500 (Avg-Loss 1.1500)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [175][4/19]\tTime 0.182 (Avg-Time 0.275)\t Loss 1.1164 (Avg-Loss 1.1266)\tAcc 60.0586 (Avg-Acc 59.5996)\n",
            "Epoch: [175][8/19]\tTime 0.184 (Avg-Time 0.245)\t Loss 1.1564 (Avg-Loss 1.1276)\tAcc 58.4961 (Avg-Acc 59.7168)\n",
            "Epoch: [175][12/19]\tTime 0.203 (Avg-Time 0.230)\t Loss 1.1514 (Avg-Loss 1.1277)\tAcc 60.2539 (Avg-Acc 59.8858)\n",
            "Epoch: [175][16/19]\tTime 0.185 (Avg-Time 0.220)\t Loss 1.1745 (Avg-Loss 1.1288)\tAcc 56.7871 (Avg-Acc 59.7225)\n",
            "Epoch: [175][19/19]\tTime 0.098 (Avg-Time 0.213)\t Loss 1.1336 (Avg-Loss 1.1284)\tAcc 57.8125 (Avg-Acc 59.6875)\n",
            "EPOCH: 175 train Results: Acc 59.688 Loss: 1.1284\n",
            "Epoch: [175][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2238 (Avg-Loss 1.2238)\tAcc 56.9824 (Avg-Acc 56.9824)\n",
            "Epoch: [175][4/4]\tTime 0.060 (Avg-Time 0.050)\t Loss 1.2254 (Avg-Loss 1.2160)\tAcc 57.3562 (Avg-Acc 56.9800)\n",
            "EPOCH: 175 Validation Results: Acc 56.980 Loss: 1.2160\n",
            "Best Accuracy: 57.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [176][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.0942 (Avg-Loss 1.0942)\tAcc 60.9375 (Avg-Acc 60.9375)\n",
            "Epoch: [176][4/19]\tTime 0.185 (Avg-Time 0.193)\t Loss 1.1340 (Avg-Loss 1.1079)\tAcc 59.0820 (Avg-Acc 60.0586)\n",
            "Epoch: [176][8/19]\tTime 0.184 (Avg-Time 0.193)\t Loss 1.1321 (Avg-Loss 1.1150)\tAcc 60.9863 (Avg-Acc 60.1671)\n",
            "Epoch: [176][12/19]\tTime 0.195 (Avg-Time 0.194)\t Loss 1.1508 (Avg-Loss 1.1200)\tAcc 59.6191 (Avg-Acc 60.0436)\n",
            "Epoch: [176][16/19]\tTime 0.185 (Avg-Time 0.194)\t Loss 1.1437 (Avg-Loss 1.1235)\tAcc 59.4238 (Avg-Acc 59.9121)\n",
            "Epoch: [176][19/19]\tTime 0.100 (Avg-Time 0.191)\t Loss 1.1188 (Avg-Loss 1.1223)\tAcc 58.6397 (Avg-Acc 59.8550)\n",
            "EPOCH: 176 train Results: Acc 59.855 Loss: 1.1223\n",
            "Epoch: [176][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2131 (Avg-Loss 1.2131)\tAcc 57.6660 (Avg-Acc 57.6660)\n",
            "Epoch: [176][4/4]\tTime 0.054 (Avg-Time 0.052)\t Loss 1.2250 (Avg-Loss 1.2080)\tAcc 58.0199 (Avg-Acc 57.2500)\n",
            "EPOCH: 176 Validation Results: Acc 57.250 Loss: 1.2080\n",
            "Best Accuracy: 57.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [177][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.0885 (Avg-Loss 1.0885)\tAcc 60.3516 (Avg-Acc 60.3516)\n",
            "Epoch: [177][4/19]\tTime 0.193 (Avg-Time 0.195)\t Loss 1.0875 (Avg-Loss 1.0977)\tAcc 60.8887 (Avg-Acc 60.8594)\n",
            "Epoch: [177][8/19]\tTime 0.188 (Avg-Time 0.196)\t Loss 1.1088 (Avg-Loss 1.1114)\tAcc 59.7168 (Avg-Acc 60.3027)\n",
            "Epoch: [177][12/19]\tTime 0.346 (Avg-Time 0.207)\t Loss 1.0983 (Avg-Loss 1.1168)\tAcc 60.2539 (Avg-Acc 60.0623)\n",
            "Epoch: [177][16/19]\tTime 0.329 (Avg-Time 0.275)\t Loss 1.1438 (Avg-Loss 1.1219)\tAcc 57.7148 (Avg-Acc 59.7742)\n",
            "Epoch: [177][19/19]\tTime 0.174 (Avg-Time 0.276)\t Loss 1.0908 (Avg-Loss 1.1220)\tAcc 59.8346 (Avg-Acc 59.7600)\n",
            "EPOCH: 177 train Results: Acc 59.760 Loss: 1.1220\n",
            "Epoch: [177][0/4]\tTime 0.088 (Avg-Time 0.088)\t Loss 1.2216 (Avg-Loss 1.2216)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [177][4/4]\tTime 0.040 (Avg-Time 0.060)\t Loss 1.2287 (Avg-Loss 1.2131)\tAcc 58.0199 (Avg-Acc 57.2800)\n",
            "EPOCH: 177 Validation Results: Acc 57.280 Loss: 1.2131\n",
            "Best Accuracy: 57.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [178][0/19]\tTime 0.192 (Avg-Time 0.192)\t Loss 1.0628 (Avg-Loss 1.0628)\tAcc 61.4258 (Avg-Acc 61.4258)\n",
            "Epoch: [178][4/19]\tTime 0.185 (Avg-Time 0.187)\t Loss 1.1128 (Avg-Loss 1.0896)\tAcc 60.4980 (Avg-Acc 60.8105)\n",
            "Epoch: [178][8/19]\tTime 0.189 (Avg-Time 0.191)\t Loss 1.1371 (Avg-Loss 1.1064)\tAcc 60.8887 (Avg-Acc 60.5469)\n",
            "Epoch: [178][12/19]\tTime 0.186 (Avg-Time 0.192)\t Loss 1.1449 (Avg-Loss 1.1129)\tAcc 60.3027 (Avg-Acc 60.4004)\n",
            "Epoch: [178][16/19]\tTime 0.194 (Avg-Time 0.192)\t Loss 1.1095 (Avg-Loss 1.1138)\tAcc 60.3516 (Avg-Acc 60.3487)\n",
            "Epoch: [178][19/19]\tTime 0.098 (Avg-Time 0.187)\t Loss 1.1651 (Avg-Loss 1.1171)\tAcc 58.0882 (Avg-Acc 60.1850)\n",
            "EPOCH: 178 train Results: Acc 60.185 Loss: 1.1171\n",
            "Epoch: [178][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2153 (Avg-Loss 1.2153)\tAcc 57.3730 (Avg-Acc 57.3730)\n",
            "Epoch: [178][4/4]\tTime 0.044 (Avg-Time 0.047)\t Loss 1.2262 (Avg-Loss 1.2065)\tAcc 56.4159 (Avg-Acc 56.9400)\n",
            "EPOCH: 178 Validation Results: Acc 56.940 Loss: 1.2065\n",
            "Best Accuracy: 57.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [179][0/19]\tTime 0.221 (Avg-Time 0.221)\t Loss 1.1001 (Avg-Loss 1.1001)\tAcc 61.6211 (Avg-Acc 61.6211)\n",
            "Epoch: [179][4/19]\tTime 0.186 (Avg-Time 0.197)\t Loss 1.1012 (Avg-Loss 1.0957)\tAcc 62.1094 (Avg-Acc 61.5430)\n",
            "Epoch: [179][8/19]\tTime 0.187 (Avg-Time 0.196)\t Loss 1.0989 (Avg-Loss 1.1014)\tAcc 61.5234 (Avg-Acc 61.0135)\n",
            "Epoch: [179][12/19]\tTime 0.190 (Avg-Time 0.197)\t Loss 1.1344 (Avg-Loss 1.1102)\tAcc 59.2285 (Avg-Acc 60.5431)\n",
            "Epoch: [179][16/19]\tTime 0.212 (Avg-Time 0.197)\t Loss 1.1326 (Avg-Loss 1.1165)\tAcc 59.3750 (Avg-Acc 60.2625)\n",
            "Epoch: [179][19/19]\tTime 0.100 (Avg-Time 0.191)\t Loss 1.0943 (Avg-Loss 1.1159)\tAcc 60.9375 (Avg-Acc 60.2800)\n",
            "EPOCH: 179 train Results: Acc 60.280 Loss: 1.1159\n",
            "Epoch: [179][0/4]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.2261 (Avg-Loss 1.2261)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [179][4/4]\tTime 0.051 (Avg-Time 0.048)\t Loss 1.2287 (Avg-Loss 1.2083)\tAcc 57.3009 (Avg-Acc 57.2000)\n",
            "EPOCH: 179 Validation Results: Acc 57.200 Loss: 1.2083\n",
            "Best Accuracy: 57.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [180][0/19]\tTime 0.187 (Avg-Time 0.187)\t Loss 1.1011 (Avg-Loss 1.1011)\tAcc 61.6211 (Avg-Acc 61.6211)\n",
            "Epoch: [180][4/19]\tTime 0.186 (Avg-Time 0.193)\t Loss 1.1312 (Avg-Loss 1.1083)\tAcc 58.8379 (Avg-Acc 60.9082)\n",
            "Epoch: [180][8/19]\tTime 0.203 (Avg-Time 0.198)\t Loss 1.1235 (Avg-Loss 1.1093)\tAcc 60.3027 (Avg-Acc 60.7151)\n",
            "Epoch: [180][12/19]\tTime 0.599 (Avg-Time 0.284)\t Loss 1.1487 (Avg-Loss 1.1152)\tAcc 58.4961 (Avg-Acc 60.2764)\n",
            "Epoch: [180][16/19]\tTime 0.266 (Avg-Time 0.301)\t Loss 1.1434 (Avg-Loss 1.1151)\tAcc 58.2520 (Avg-Acc 60.1074)\n",
            "Epoch: [180][19/19]\tTime 0.121 (Avg-Time 0.281)\t Loss 1.1434 (Avg-Loss 1.1203)\tAcc 59.6507 (Avg-Acc 59.8975)\n",
            "EPOCH: 180 train Results: Acc 59.898 Loss: 1.1203\n",
            "Epoch: [180][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2101 (Avg-Loss 1.2101)\tAcc 57.9102 (Avg-Acc 57.9102)\n",
            "Epoch: [180][4/4]\tTime 0.056 (Avg-Time 0.049)\t Loss 1.2265 (Avg-Loss 1.2075)\tAcc 57.0243 (Avg-Acc 57.3500)\n",
            "EPOCH: 180 Validation Results: Acc 57.350 Loss: 1.2075\n",
            "Best Accuracy: 57.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [181][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.0526 (Avg-Loss 1.0526)\tAcc 62.1094 (Avg-Acc 62.1094)\n",
            "Epoch: [181][4/19]\tTime 0.186 (Avg-Time 0.191)\t Loss 1.1174 (Avg-Loss 1.1009)\tAcc 59.8633 (Avg-Acc 60.6445)\n",
            "Epoch: [181][8/19]\tTime 0.186 (Avg-Time 0.192)\t Loss 1.1420 (Avg-Loss 1.1070)\tAcc 58.8867 (Avg-Acc 60.3895)\n",
            "Epoch: [181][12/19]\tTime 0.189 (Avg-Time 0.193)\t Loss 1.1420 (Avg-Loss 1.1175)\tAcc 59.9121 (Avg-Acc 59.9572)\n",
            "Epoch: [181][16/19]\tTime 0.184 (Avg-Time 0.193)\t Loss 1.0733 (Avg-Loss 1.1129)\tAcc 61.6211 (Avg-Acc 60.2137)\n",
            "Epoch: [181][19/19]\tTime 0.097 (Avg-Time 0.190)\t Loss 1.1352 (Avg-Loss 1.1106)\tAcc 59.7426 (Avg-Acc 60.3500)\n",
            "EPOCH: 181 train Results: Acc 60.350 Loss: 1.1106\n",
            "Epoch: [181][0/4]\tTime 0.071 (Avg-Time 0.071)\t Loss 1.2121 (Avg-Loss 1.2121)\tAcc 57.0801 (Avg-Acc 57.0801)\n",
            "Epoch: [181][4/4]\tTime 0.041 (Avg-Time 0.054)\t Loss 1.2275 (Avg-Loss 1.2091)\tAcc 57.1903 (Avg-Acc 57.4400)\n",
            "EPOCH: 181 Validation Results: Acc 57.440 Loss: 1.2091\n",
            "Best Accuracy: 57.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [182][0/19]\tTime 0.188 (Avg-Time 0.188)\t Loss 1.1225 (Avg-Loss 1.1225)\tAcc 59.3262 (Avg-Acc 59.3262)\n",
            "Epoch: [182][4/19]\tTime 0.199 (Avg-Time 0.193)\t Loss 1.1184 (Avg-Loss 1.1200)\tAcc 59.2285 (Avg-Acc 59.6289)\n",
            "Epoch: [182][8/19]\tTime 0.183 (Avg-Time 0.191)\t Loss 1.0911 (Avg-Loss 1.1147)\tAcc 61.7188 (Avg-Acc 60.1128)\n",
            "Epoch: [182][12/19]\tTime 0.191 (Avg-Time 0.192)\t Loss 1.1223 (Avg-Loss 1.1172)\tAcc 59.8633 (Avg-Acc 60.0210)\n",
            "Epoch: [182][16/19]\tTime 0.188 (Avg-Time 0.191)\t Loss 1.1899 (Avg-Loss 1.1172)\tAcc 56.6406 (Avg-Acc 60.0758)\n",
            "Epoch: [182][19/19]\tTime 0.096 (Avg-Time 0.186)\t Loss 1.1165 (Avg-Loss 1.1181)\tAcc 60.9375 (Avg-Acc 60.0125)\n",
            "EPOCH: 182 train Results: Acc 60.013 Loss: 1.1181\n",
            "Epoch: [182][0/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.2105 (Avg-Loss 1.2105)\tAcc 57.5684 (Avg-Acc 57.5684)\n",
            "Epoch: [182][4/4]\tTime 0.069 (Avg-Time 0.053)\t Loss 1.2251 (Avg-Loss 1.2096)\tAcc 56.9690 (Avg-Acc 57.1100)\n",
            "EPOCH: 182 Validation Results: Acc 57.110 Loss: 1.2096\n",
            "Best Accuracy: 57.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [183][0/19]\tTime 0.191 (Avg-Time 0.191)\t Loss 1.1131 (Avg-Loss 1.1131)\tAcc 60.9375 (Avg-Acc 60.9375)\n",
            "Epoch: [183][4/19]\tTime 0.191 (Avg-Time 0.191)\t Loss 1.1577 (Avg-Loss 1.1288)\tAcc 57.4707 (Avg-Acc 59.8828)\n",
            "Epoch: [183][8/19]\tTime 0.591 (Avg-Time 0.282)\t Loss 1.0906 (Avg-Loss 1.1156)\tAcc 61.1816 (Avg-Acc 60.2431)\n",
            "Epoch: [183][12/19]\tTime 0.320 (Avg-Time 0.318)\t Loss 1.1363 (Avg-Loss 1.1189)\tAcc 58.5449 (Avg-Acc 60.1262)\n",
            "Epoch: [183][16/19]\tTime 0.186 (Avg-Time 0.297)\t Loss 1.1009 (Avg-Loss 1.1148)\tAcc 60.5957 (Avg-Acc 60.1591)\n",
            "Epoch: [183][19/19]\tTime 0.097 (Avg-Time 0.278)\t Loss 1.1573 (Avg-Loss 1.1144)\tAcc 58.4559 (Avg-Acc 60.1450)\n",
            "EPOCH: 183 train Results: Acc 60.145 Loss: 1.1144\n",
            "Epoch: [183][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.1999 (Avg-Loss 1.1999)\tAcc 57.5195 (Avg-Acc 57.5195)\n",
            "Epoch: [183][4/4]\tTime 0.054 (Avg-Time 0.049)\t Loss 1.2200 (Avg-Loss 1.2022)\tAcc 56.4159 (Avg-Acc 57.0900)\n",
            "EPOCH: 183 Validation Results: Acc 57.090 Loss: 1.2022\n",
            "Best Accuracy: 57.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [184][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.1048 (Avg-Loss 1.1048)\tAcc 60.4980 (Avg-Acc 60.4980)\n",
            "Epoch: [184][4/19]\tTime 0.186 (Avg-Time 0.195)\t Loss 1.1189 (Avg-Loss 1.1036)\tAcc 59.6680 (Avg-Acc 60.6934)\n",
            "Epoch: [184][8/19]\tTime 0.191 (Avg-Time 0.197)\t Loss 1.1200 (Avg-Loss 1.1068)\tAcc 59.0332 (Avg-Acc 60.3516)\n",
            "Epoch: [184][12/19]\tTime 0.205 (Avg-Time 0.196)\t Loss 1.0865 (Avg-Loss 1.0992)\tAcc 62.1094 (Avg-Acc 60.7009)\n",
            "Epoch: [184][16/19]\tTime 0.188 (Avg-Time 0.196)\t Loss 1.1167 (Avg-Loss 1.1063)\tAcc 60.4004 (Avg-Acc 60.5354)\n",
            "Epoch: [184][19/19]\tTime 0.098 (Avg-Time 0.192)\t Loss 1.1107 (Avg-Loss 1.1072)\tAcc 60.3860 (Avg-Acc 60.5650)\n",
            "EPOCH: 184 train Results: Acc 60.565 Loss: 1.1072\n",
            "Epoch: [184][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.2117 (Avg-Loss 1.2117)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [184][4/4]\tTime 0.052 (Avg-Time 0.049)\t Loss 1.2225 (Avg-Loss 1.2084)\tAcc 58.0199 (Avg-Acc 57.3300)\n",
            "EPOCH: 184 Validation Results: Acc 57.330 Loss: 1.2084\n",
            "Best Accuracy: 57.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [185][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.1347 (Avg-Loss 1.1347)\tAcc 59.6680 (Avg-Acc 59.6680)\n",
            "Epoch: [185][4/19]\tTime 0.187 (Avg-Time 0.193)\t Loss 1.1149 (Avg-Loss 1.1033)\tAcc 59.6191 (Avg-Acc 60.4395)\n",
            "Epoch: [185][8/19]\tTime 0.207 (Avg-Time 0.194)\t Loss 1.0471 (Avg-Loss 1.1023)\tAcc 63.0371 (Avg-Acc 60.6174)\n",
            "Epoch: [185][12/19]\tTime 0.185 (Avg-Time 0.193)\t Loss 1.1006 (Avg-Loss 1.1065)\tAcc 61.5234 (Avg-Acc 60.5056)\n",
            "Epoch: [185][16/19]\tTime 0.191 (Avg-Time 0.194)\t Loss 1.1200 (Avg-Loss 1.1106)\tAcc 59.4727 (Avg-Acc 60.2769)\n",
            "Epoch: [185][19/19]\tTime 0.096 (Avg-Time 0.190)\t Loss 1.1202 (Avg-Loss 1.1107)\tAcc 59.7426 (Avg-Acc 60.2325)\n",
            "EPOCH: 185 train Results: Acc 60.233 Loss: 1.1107\n",
            "Epoch: [185][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2080 (Avg-Loss 1.2080)\tAcc 57.3242 (Avg-Acc 57.3242)\n",
            "Epoch: [185][4/4]\tTime 0.056 (Avg-Time 0.050)\t Loss 1.2154 (Avg-Loss 1.2031)\tAcc 57.3009 (Avg-Acc 57.5000)\n",
            "EPOCH: 185 Validation Results: Acc 57.500 Loss: 1.2031\n",
            "Best Accuracy: 57.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [186][0/19]\tTime 0.185 (Avg-Time 0.185)\t Loss 1.0697 (Avg-Loss 1.0697)\tAcc 60.7422 (Avg-Acc 60.7422)\n",
            "Epoch: [186][4/19]\tTime 0.409 (Avg-Time 0.266)\t Loss 1.1214 (Avg-Loss 1.1022)\tAcc 61.1816 (Avg-Acc 60.4688)\n",
            "Epoch: [186][8/19]\tTime 0.340 (Avg-Time 0.337)\t Loss 1.1079 (Avg-Loss 1.1041)\tAcc 60.5469 (Avg-Acc 60.3190)\n",
            "Epoch: [186][12/19]\tTime 0.188 (Avg-Time 0.329)\t Loss 1.1134 (Avg-Loss 1.1061)\tAcc 59.2285 (Avg-Acc 60.0098)\n",
            "Epoch: [186][16/19]\tTime 0.204 (Avg-Time 0.297)\t Loss 1.1163 (Avg-Loss 1.1100)\tAcc 60.9375 (Avg-Acc 60.0299)\n",
            "Epoch: [186][19/19]\tTime 0.098 (Avg-Time 0.277)\t Loss 1.1191 (Avg-Loss 1.1109)\tAcc 61.3971 (Avg-Acc 60.1000)\n",
            "EPOCH: 186 train Results: Acc 60.100 Loss: 1.1109\n",
            "Epoch: [186][0/4]\tTime 0.051 (Avg-Time 0.051)\t Loss 1.2125 (Avg-Loss 1.2125)\tAcc 57.3242 (Avg-Acc 57.3242)\n",
            "Epoch: [186][4/4]\tTime 0.053 (Avg-Time 0.050)\t Loss 1.2195 (Avg-Loss 1.2080)\tAcc 57.3009 (Avg-Acc 57.3700)\n",
            "EPOCH: 186 Validation Results: Acc 57.370 Loss: 1.2080\n",
            "Best Accuracy: 57.7100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [187][0/19]\tTime 0.209 (Avg-Time 0.209)\t Loss 1.0950 (Avg-Loss 1.0950)\tAcc 61.3770 (Avg-Acc 61.3770)\n",
            "Epoch: [187][4/19]\tTime 0.184 (Avg-Time 0.197)\t Loss 1.1465 (Avg-Loss 1.1066)\tAcc 58.5449 (Avg-Acc 60.3613)\n",
            "Epoch: [187][8/19]\tTime 0.189 (Avg-Time 0.195)\t Loss 1.0722 (Avg-Loss 1.0994)\tAcc 61.6211 (Avg-Acc 60.5740)\n",
            "Epoch: [187][12/19]\tTime 0.183 (Avg-Time 0.196)\t Loss 1.1481 (Avg-Loss 1.1027)\tAcc 58.3496 (Avg-Acc 60.2952)\n",
            "Epoch: [187][16/19]\tTime 0.201 (Avg-Time 0.195)\t Loss 1.1088 (Avg-Loss 1.1042)\tAcc 60.1562 (Avg-Acc 60.2597)\n",
            "Epoch: [187][19/19]\tTime 0.095 (Avg-Time 0.190)\t Loss 1.1152 (Avg-Loss 1.1042)\tAcc 59.9265 (Avg-Acc 60.2300)\n",
            "EPOCH: 187 train Results: Acc 60.230 Loss: 1.1042\n",
            "Epoch: [187][0/4]\tTime 0.051 (Avg-Time 0.051)\t Loss 1.2022 (Avg-Loss 1.2022)\tAcc 58.0566 (Avg-Acc 58.0566)\n",
            "Epoch: [187][4/4]\tTime 0.050 (Avg-Time 0.048)\t Loss 1.2127 (Avg-Loss 1.1974)\tAcc 57.9646 (Avg-Acc 57.8100)\n",
            "EPOCH: 187 Validation Results: Acc 57.810 Loss: 1.1974\n",
            "Best Accuracy: 57.8100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [188][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.0898 (Avg-Loss 1.0898)\tAcc 60.9863 (Avg-Acc 60.9863)\n",
            "Epoch: [188][4/19]\tTime 0.185 (Avg-Time 0.189)\t Loss 1.0705 (Avg-Loss 1.0863)\tAcc 61.1816 (Avg-Acc 60.9961)\n",
            "Epoch: [188][8/19]\tTime 0.222 (Avg-Time 0.194)\t Loss 1.1076 (Avg-Loss 1.0992)\tAcc 60.2051 (Avg-Acc 60.5957)\n",
            "Epoch: [188][12/19]\tTime 0.202 (Avg-Time 0.195)\t Loss 1.1039 (Avg-Loss 1.1016)\tAcc 60.6445 (Avg-Acc 60.5694)\n",
            "Epoch: [188][16/19]\tTime 0.186 (Avg-Time 0.194)\t Loss 1.1012 (Avg-Loss 1.1017)\tAcc 60.5469 (Avg-Acc 60.4377)\n",
            "Epoch: [188][19/19]\tTime 0.101 (Avg-Time 0.190)\t Loss 1.1241 (Avg-Loss 1.1013)\tAcc 60.7537 (Avg-Acc 60.4175)\n",
            "EPOCH: 188 train Results: Acc 60.417 Loss: 1.1013\n",
            "Epoch: [188][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2140 (Avg-Loss 1.2140)\tAcc 57.1289 (Avg-Acc 57.1289)\n",
            "Epoch: [188][4/4]\tTime 0.049 (Avg-Time 0.049)\t Loss 1.2156 (Avg-Loss 1.2035)\tAcc 57.5221 (Avg-Acc 57.5400)\n",
            "EPOCH: 188 Validation Results: Acc 57.540 Loss: 1.2035\n",
            "Best Accuracy: 57.8100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [189][0/19]\tTime 0.233 (Avg-Time 0.233)\t Loss 1.0871 (Avg-Loss 1.0871)\tAcc 62.7441 (Avg-Acc 62.7441)\n",
            "Epoch: [189][4/19]\tTime 0.630 (Avg-Time 0.446)\t Loss 1.1036 (Avg-Loss 1.0852)\tAcc 61.6211 (Avg-Acc 61.7578)\n",
            "Epoch: [189][8/19]\tTime 0.223 (Avg-Time 0.400)\t Loss 1.0876 (Avg-Loss 1.0909)\tAcc 60.9375 (Avg-Acc 61.1057)\n",
            "Epoch: [189][12/19]\tTime 0.184 (Avg-Time 0.338)\t Loss 1.0964 (Avg-Loss 1.0938)\tAcc 59.4238 (Avg-Acc 60.8812)\n",
            "Epoch: [189][16/19]\tTime 0.187 (Avg-Time 0.304)\t Loss 1.1034 (Avg-Loss 1.0990)\tAcc 61.3770 (Avg-Acc 60.6445)\n",
            "Epoch: [189][19/19]\tTime 0.097 (Avg-Time 0.283)\t Loss 1.1200 (Avg-Loss 1.1008)\tAcc 61.3971 (Avg-Acc 60.6575)\n",
            "EPOCH: 189 train Results: Acc 60.657 Loss: 1.1008\n",
            "Epoch: [189][0/4]\tTime 0.070 (Avg-Time 0.070)\t Loss 1.2062 (Avg-Loss 1.2062)\tAcc 57.9102 (Avg-Acc 57.9102)\n",
            "Epoch: [189][4/4]\tTime 0.041 (Avg-Time 0.054)\t Loss 1.2218 (Avg-Loss 1.1981)\tAcc 56.4712 (Avg-Acc 57.4800)\n",
            "EPOCH: 189 Validation Results: Acc 57.480 Loss: 1.1981\n",
            "Best Accuracy: 57.8100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [190][0/19]\tTime 0.189 (Avg-Time 0.189)\t Loss 1.1101 (Avg-Loss 1.1101)\tAcc 60.0586 (Avg-Acc 60.0586)\n",
            "Epoch: [190][4/19]\tTime 0.198 (Avg-Time 0.194)\t Loss 1.0820 (Avg-Loss 1.1059)\tAcc 61.3281 (Avg-Acc 60.5566)\n",
            "Epoch: [190][8/19]\tTime 0.184 (Avg-Time 0.192)\t Loss 1.1091 (Avg-Loss 1.1029)\tAcc 59.9121 (Avg-Acc 60.4655)\n",
            "Epoch: [190][12/19]\tTime 0.187 (Avg-Time 0.193)\t Loss 1.0986 (Avg-Loss 1.1040)\tAcc 60.2051 (Avg-Acc 60.3253)\n",
            "Epoch: [190][16/19]\tTime 0.191 (Avg-Time 0.194)\t Loss 1.1061 (Avg-Loss 1.1035)\tAcc 61.1328 (Avg-Acc 60.3975)\n",
            "Epoch: [190][19/19]\tTime 0.097 (Avg-Time 0.189)\t Loss 1.0745 (Avg-Loss 1.1028)\tAcc 61.3051 (Avg-Acc 60.4450)\n",
            "EPOCH: 190 train Results: Acc 60.445 Loss: 1.1028\n",
            "Epoch: [190][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2032 (Avg-Loss 1.2032)\tAcc 58.5449 (Avg-Acc 58.5449)\n",
            "Epoch: [190][4/4]\tTime 0.044 (Avg-Time 0.054)\t Loss 1.2172 (Avg-Loss 1.2035)\tAcc 57.0243 (Avg-Acc 57.3300)\n",
            "EPOCH: 190 Validation Results: Acc 57.330 Loss: 1.2035\n",
            "Best Accuracy: 57.8100\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [191][0/19]\tTime 0.194 (Avg-Time 0.194)\t Loss 1.0883 (Avg-Loss 1.0883)\tAcc 62.2070 (Avg-Acc 62.2070)\n",
            "Epoch: [191][4/19]\tTime 0.204 (Avg-Time 0.194)\t Loss 1.1529 (Avg-Loss 1.0999)\tAcc 57.8613 (Avg-Acc 60.1270)\n",
            "Epoch: [191][8/19]\tTime 0.189 (Avg-Time 0.195)\t Loss 1.0929 (Avg-Loss 1.0944)\tAcc 60.7422 (Avg-Acc 60.5143)\n",
            "Epoch: [191][12/19]\tTime 0.183 (Avg-Time 0.195)\t Loss 1.1128 (Avg-Loss 1.1053)\tAcc 59.9121 (Avg-Acc 60.1825)\n",
            "Epoch: [191][16/19]\tTime 0.191 (Avg-Time 0.195)\t Loss 1.0799 (Avg-Loss 1.1059)\tAcc 62.3535 (Avg-Acc 60.3315)\n",
            "Epoch: [191][19/19]\tTime 0.195 (Avg-Time 0.201)\t Loss 1.0669 (Avg-Loss 1.1030)\tAcc 61.0294 (Avg-Acc 60.4975)\n",
            "EPOCH: 191 train Results: Acc 60.498 Loss: 1.1030\n",
            "Epoch: [191][0/4]\tTime 0.082 (Avg-Time 0.082)\t Loss 1.2037 (Avg-Loss 1.2037)\tAcc 57.8613 (Avg-Acc 57.8613)\n",
            "Epoch: [191][4/4]\tTime 0.072 (Avg-Time 0.084)\t Loss 1.2187 (Avg-Loss 1.1974)\tAcc 57.6881 (Avg-Acc 57.8700)\n",
            "EPOCH: 191 Validation Results: Acc 57.870 Loss: 1.1974\n",
            "Best Accuracy: 57.8700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [192][0/19]\tTime 0.562 (Avg-Time 0.562)\t Loss 1.1104 (Avg-Loss 1.1104)\tAcc 61.5723 (Avg-Acc 61.5723)\n",
            "Epoch: [192][4/19]\tTime 0.407 (Avg-Time 0.461)\t Loss 1.1283 (Avg-Loss 1.1059)\tAcc 60.8887 (Avg-Acc 60.9766)\n",
            "Epoch: [192][8/19]\tTime 0.187 (Avg-Time 0.344)\t Loss 1.1020 (Avg-Loss 1.1029)\tAcc 61.0840 (Avg-Acc 61.0297)\n",
            "Epoch: [192][12/19]\tTime 0.185 (Avg-Time 0.296)\t Loss 1.0704 (Avg-Loss 1.1044)\tAcc 62.2070 (Avg-Acc 60.8361)\n",
            "Epoch: [192][16/19]\tTime 0.188 (Avg-Time 0.271)\t Loss 1.0724 (Avg-Loss 1.1042)\tAcc 61.8652 (Avg-Acc 60.8284)\n",
            "Epoch: [192][19/19]\tTime 0.097 (Avg-Time 0.255)\t Loss 1.0940 (Avg-Loss 1.1020)\tAcc 61.3971 (Avg-Acc 60.9400)\n",
            "EPOCH: 192 train Results: Acc 60.940 Loss: 1.1020\n",
            "Epoch: [192][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2019 (Avg-Loss 1.2019)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [192][4/4]\tTime 0.052 (Avg-Time 0.049)\t Loss 1.2112 (Avg-Loss 1.1973)\tAcc 57.5221 (Avg-Acc 57.4100)\n",
            "EPOCH: 192 Validation Results: Acc 57.410 Loss: 1.1973\n",
            "Best Accuracy: 57.8700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [193][0/19]\tTime 0.191 (Avg-Time 0.191)\t Loss 1.0899 (Avg-Loss 1.0899)\tAcc 61.4258 (Avg-Acc 61.4258)\n",
            "Epoch: [193][4/19]\tTime 0.185 (Avg-Time 0.194)\t Loss 1.0649 (Avg-Loss 1.0860)\tAcc 63.2324 (Avg-Acc 61.7578)\n",
            "Epoch: [193][8/19]\tTime 0.211 (Avg-Time 0.193)\t Loss 1.1103 (Avg-Loss 1.0919)\tAcc 60.7422 (Avg-Acc 61.2196)\n",
            "Epoch: [193][12/19]\tTime 0.190 (Avg-Time 0.192)\t Loss 1.0787 (Avg-Loss 1.0945)\tAcc 62.1582 (Avg-Acc 61.1629)\n",
            "Epoch: [193][16/19]\tTime 0.186 (Avg-Time 0.193)\t Loss 1.0984 (Avg-Loss 1.0971)\tAcc 61.6699 (Avg-Acc 61.0869)\n",
            "Epoch: [193][19/19]\tTime 0.118 (Avg-Time 0.190)\t Loss 1.0976 (Avg-Loss 1.0970)\tAcc 60.1103 (Avg-Acc 60.9750)\n",
            "EPOCH: 193 train Results: Acc 60.975 Loss: 1.0970\n",
            "Epoch: [193][0/4]\tTime 0.050 (Avg-Time 0.050)\t Loss 1.2056 (Avg-Loss 1.2056)\tAcc 56.9824 (Avg-Acc 56.9824)\n",
            "Epoch: [193][4/4]\tTime 0.053 (Avg-Time 0.048)\t Loss 1.2173 (Avg-Loss 1.2023)\tAcc 57.4115 (Avg-Acc 57.7000)\n",
            "EPOCH: 193 Validation Results: Acc 57.700 Loss: 1.2023\n",
            "Best Accuracy: 57.8700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [194][0/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.1082 (Avg-Loss 1.1082)\tAcc 60.1562 (Avg-Acc 60.1562)\n",
            "Epoch: [194][4/19]\tTime 0.186 (Avg-Time 0.192)\t Loss 1.0952 (Avg-Loss 1.0959)\tAcc 60.5957 (Avg-Acc 60.2930)\n",
            "Epoch: [194][8/19]\tTime 0.184 (Avg-Time 0.192)\t Loss 1.0879 (Avg-Loss 1.0854)\tAcc 60.3027 (Avg-Acc 60.8290)\n",
            "Epoch: [194][12/19]\tTime 0.187 (Avg-Time 0.194)\t Loss 1.1135 (Avg-Loss 1.0917)\tAcc 59.6191 (Avg-Acc 60.5882)\n",
            "Epoch: [194][16/19]\tTime 0.414 (Avg-Time 0.215)\t Loss 1.1208 (Avg-Loss 1.0947)\tAcc 60.4492 (Avg-Acc 60.5584)\n",
            "Epoch: [194][19/19]\tTime 0.339 (Avg-Time 0.254)\t Loss 1.0941 (Avg-Loss 1.0937)\tAcc 62.1324 (Avg-Acc 60.7275)\n",
            "EPOCH: 194 train Results: Acc 60.727 Loss: 1.0937\n",
            "Epoch: [194][0/4]\tTime 0.084 (Avg-Time 0.084)\t Loss 1.1966 (Avg-Loss 1.1966)\tAcc 57.8613 (Avg-Acc 57.8613)\n",
            "Epoch: [194][4/4]\tTime 0.098 (Avg-Time 0.093)\t Loss 1.2123 (Avg-Loss 1.1987)\tAcc 57.6327 (Avg-Acc 57.5700)\n",
            "EPOCH: 194 Validation Results: Acc 57.570 Loss: 1.1987\n",
            "Best Accuracy: 57.8700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [195][0/19]\tTime 0.344 (Avg-Time 0.344)\t Loss 1.0401 (Avg-Loss 1.0401)\tAcc 62.9395 (Avg-Acc 62.9395)\n",
            "Epoch: [195][4/19]\tTime 0.188 (Avg-Time 0.258)\t Loss 1.0979 (Avg-Loss 1.0732)\tAcc 59.6191 (Avg-Acc 61.4062)\n",
            "Epoch: [195][8/19]\tTime 0.185 (Avg-Time 0.229)\t Loss 1.0699 (Avg-Loss 1.0827)\tAcc 60.3516 (Avg-Acc 61.1654)\n",
            "Epoch: [195][12/19]\tTime 0.193 (Avg-Time 0.219)\t Loss 1.0791 (Avg-Loss 1.0849)\tAcc 60.5469 (Avg-Acc 61.0051)\n",
            "Epoch: [195][16/19]\tTime 0.207 (Avg-Time 0.214)\t Loss 1.0714 (Avg-Loss 1.0860)\tAcc 61.3281 (Avg-Acc 61.0323)\n",
            "Epoch: [195][19/19]\tTime 0.096 (Avg-Time 0.206)\t Loss 1.0797 (Avg-Loss 1.0875)\tAcc 60.5699 (Avg-Acc 61.0375)\n",
            "EPOCH: 195 train Results: Acc 61.038 Loss: 1.0875\n",
            "Epoch: [195][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.1932 (Avg-Loss 1.1932)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [195][4/4]\tTime 0.052 (Avg-Time 0.049)\t Loss 1.2073 (Avg-Loss 1.1918)\tAcc 57.1903 (Avg-Acc 57.8100)\n",
            "EPOCH: 195 Validation Results: Acc 57.810 Loss: 1.1918\n",
            "Best Accuracy: 57.8700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [196][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.0812 (Avg-Loss 1.0812)\tAcc 60.6445 (Avg-Acc 60.6445)\n",
            "Epoch: [196][4/19]\tTime 0.186 (Avg-Time 0.197)\t Loss 1.0672 (Avg-Loss 1.0895)\tAcc 61.9141 (Avg-Acc 61.0840)\n",
            "Epoch: [196][8/19]\tTime 0.184 (Avg-Time 0.197)\t Loss 1.0980 (Avg-Loss 1.0829)\tAcc 60.6934 (Avg-Acc 61.4800)\n",
            "Epoch: [196][12/19]\tTime 0.203 (Avg-Time 0.196)\t Loss 1.1271 (Avg-Loss 1.0898)\tAcc 60.1074 (Avg-Acc 61.0877)\n",
            "Epoch: [196][16/19]\tTime 0.199 (Avg-Time 0.195)\t Loss 1.0723 (Avg-Loss 1.0859)\tAcc 62.0605 (Avg-Acc 61.2965)\n",
            "Epoch: [196][19/19]\tTime 0.095 (Avg-Time 0.191)\t Loss 1.1073 (Avg-Loss 1.0883)\tAcc 60.2022 (Avg-Acc 61.2700)\n",
            "EPOCH: 196 train Results: Acc 61.270 Loss: 1.0883\n",
            "Epoch: [196][0/4]\tTime 0.046 (Avg-Time 0.046)\t Loss 1.2063 (Avg-Loss 1.2063)\tAcc 58.1543 (Avg-Acc 58.1543)\n",
            "Epoch: [196][4/4]\tTime 0.053 (Avg-Time 0.048)\t Loss 1.2252 (Avg-Loss 1.2063)\tAcc 56.9690 (Avg-Acc 57.2900)\n",
            "EPOCH: 196 Validation Results: Acc 57.290 Loss: 1.2063\n",
            "Best Accuracy: 57.8700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [197][0/19]\tTime 0.186 (Avg-Time 0.186)\t Loss 1.0970 (Avg-Loss 1.0970)\tAcc 60.6445 (Avg-Acc 60.6445)\n",
            "Epoch: [197][4/19]\tTime 0.186 (Avg-Time 0.191)\t Loss 1.0929 (Avg-Loss 1.0860)\tAcc 59.8633 (Avg-Acc 61.3086)\n",
            "Epoch: [197][8/19]\tTime 0.188 (Avg-Time 0.192)\t Loss 1.0743 (Avg-Loss 1.0868)\tAcc 62.5000 (Avg-Acc 61.3227)\n",
            "Epoch: [197][12/19]\tTime 0.329 (Avg-Time 0.202)\t Loss 1.1091 (Avg-Loss 1.0926)\tAcc 60.5469 (Avg-Acc 60.9788)\n",
            "Epoch: [197][16/19]\tTime 0.579 (Avg-Time 0.264)\t Loss 1.1096 (Avg-Loss 1.0948)\tAcc 60.2051 (Avg-Acc 60.9088)\n",
            "Epoch: [197][19/19]\tTime 0.200 (Avg-Time 0.268)\t Loss 1.1044 (Avg-Loss 1.0962)\tAcc 60.7537 (Avg-Acc 60.9050)\n",
            "EPOCH: 197 train Results: Acc 60.905 Loss: 1.0962\n",
            "Epoch: [197][0/4]\tTime 0.085 (Avg-Time 0.085)\t Loss 1.1989 (Avg-Loss 1.1989)\tAcc 57.7637 (Avg-Acc 57.7637)\n",
            "Epoch: [197][4/4]\tTime 0.073 (Avg-Time 0.090)\t Loss 1.2070 (Avg-Loss 1.1966)\tAcc 57.1903 (Avg-Acc 57.7100)\n",
            "EPOCH: 197 Validation Results: Acc 57.710 Loss: 1.1966\n",
            "Best Accuracy: 57.8700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [198][0/19]\tTime 0.181 (Avg-Time 0.181)\t Loss 1.1008 (Avg-Loss 1.1008)\tAcc 61.0352 (Avg-Acc 61.0352)\n",
            "Epoch: [198][4/19]\tTime 0.204 (Avg-Time 0.191)\t Loss 1.0526 (Avg-Loss 1.0710)\tAcc 62.5000 (Avg-Acc 61.9141)\n",
            "Epoch: [198][8/19]\tTime 0.188 (Avg-Time 0.190)\t Loss 1.1401 (Avg-Loss 1.0810)\tAcc 58.9844 (Avg-Acc 61.1925)\n",
            "Epoch: [198][12/19]\tTime 0.190 (Avg-Time 0.190)\t Loss 1.1317 (Avg-Loss 1.0899)\tAcc 59.1797 (Avg-Acc 61.0765)\n",
            "Epoch: [198][16/19]\tTime 0.185 (Avg-Time 0.193)\t Loss 1.0969 (Avg-Loss 1.0888)\tAcc 62.1094 (Avg-Acc 61.1443)\n",
            "Epoch: [198][19/19]\tTime 0.097 (Avg-Time 0.188)\t Loss 1.1073 (Avg-Loss 1.0896)\tAcc 60.8456 (Avg-Acc 61.1325)\n",
            "EPOCH: 198 train Results: Acc 61.133 Loss: 1.0896\n",
            "Epoch: [198][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.1954 (Avg-Loss 1.1954)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [198][4/4]\tTime 0.056 (Avg-Time 0.050)\t Loss 1.1998 (Avg-Loss 1.1944)\tAcc 58.0199 (Avg-Acc 57.8400)\n",
            "EPOCH: 198 Validation Results: Acc 57.840 Loss: 1.1944\n",
            "Best Accuracy: 57.8700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [199][0/19]\tTime 0.225 (Avg-Time 0.225)\t Loss 1.0923 (Avg-Loss 1.0923)\tAcc 61.4746 (Avg-Acc 61.4746)\n",
            "Epoch: [199][4/19]\tTime 0.186 (Avg-Time 0.199)\t Loss 1.0564 (Avg-Loss 1.0851)\tAcc 61.9629 (Avg-Acc 61.2109)\n",
            "Epoch: [199][8/19]\tTime 0.190 (Avg-Time 0.199)\t Loss 1.0918 (Avg-Loss 1.0832)\tAcc 60.0098 (Avg-Acc 61.1437)\n",
            "Epoch: [199][12/19]\tTime 0.192 (Avg-Time 0.197)\t Loss 1.0956 (Avg-Loss 1.0810)\tAcc 61.9141 (Avg-Acc 61.2718)\n",
            "Epoch: [199][16/19]\tTime 0.203 (Avg-Time 0.195)\t Loss 1.0870 (Avg-Loss 1.0808)\tAcc 62.6465 (Avg-Acc 61.4660)\n",
            "Epoch: [199][19/19]\tTime 0.101 (Avg-Time 0.190)\t Loss 1.1253 (Avg-Loss 1.0854)\tAcc 59.0993 (Avg-Acc 61.3500)\n",
            "EPOCH: 199 train Results: Acc 61.350 Loss: 1.0854\n",
            "Epoch: [199][0/4]\tTime 0.048 (Avg-Time 0.048)\t Loss 1.2018 (Avg-Loss 1.2018)\tAcc 57.7637 (Avg-Acc 57.7637)\n",
            "Epoch: [199][4/4]\tTime 0.052 (Avg-Time 0.049)\t Loss 1.2041 (Avg-Loss 1.1969)\tAcc 57.4668 (Avg-Acc 57.6500)\n",
            "EPOCH: 199 Validation Results: Acc 57.650 Loss: 1.1969\n",
            "Best Accuracy: 57.8700\n",
            "\n",
            "current lr 2.00000e-02\n",
            "Epoch: [200][0/19]\tTime 0.217 (Avg-Time 0.217)\t Loss 1.0890 (Avg-Loss 1.0890)\tAcc 62.3535 (Avg-Acc 62.3535)\n",
            "Epoch: [200][4/19]\tTime 0.185 (Avg-Time 0.198)\t Loss 1.1015 (Avg-Loss 1.0862)\tAcc 60.4980 (Avg-Acc 61.3379)\n",
            "Epoch: [200][8/19]\tTime 0.186 (Avg-Time 0.197)\t Loss 1.0608 (Avg-Loss 1.0762)\tAcc 63.3789 (Avg-Acc 61.5723)\n",
            "Epoch: [200][12/19]\tTime 0.484 (Avg-Time 0.241)\t Loss 1.0954 (Avg-Loss 1.0787)\tAcc 61.5723 (Avg-Acc 61.5610)\n",
            "Epoch: [200][16/19]\tTime 0.365 (Avg-Time 0.279)\t Loss 1.0662 (Avg-Loss 1.0826)\tAcc 61.3281 (Avg-Acc 61.3712)\n",
            "Epoch: [200][19/19]\tTime 0.123 (Avg-Time 0.275)\t Loss 1.0578 (Avg-Loss 1.0817)\tAcc 61.4890 (Avg-Acc 61.3325)\n",
            "EPOCH: 200 train Results: Acc 61.333 Loss: 1.0817\n",
            "Epoch: [200][0/4]\tTime 0.047 (Avg-Time 0.047)\t Loss 1.1987 (Avg-Loss 1.1987)\tAcc 57.9102 (Avg-Acc 57.9102)\n",
            "Epoch: [200][4/4]\tTime 0.057 (Avg-Time 0.050)\t Loss 1.2052 (Avg-Loss 1.1928)\tAcc 57.9093 (Avg-Acc 57.6400)\n",
            "EPOCH: 200 Validation Results: Acc 57.640 Loss: 1.1928\n",
            "Best Accuracy: 57.8700\n",
            "\n",
            "End time:  Sat Mar 29 19:01:20 2025\n",
            "train executed in 937.7419 seconds\n",
            "\n",
            "Test Accuracy for Config: 58.60%\n",
            "\n",
            "\n",
            "=== Testing New Configuration ===\n",
            "Config: {'lr': 0.01, 'batch_size': 1024, 'hidden_units': [256, 256], 'dropout_rates': [0.3, 0.3], 'pre-process': None, 'epoch': 200, 'weight_decay': 0.0005, 'momentum': 0.9, 'optimizer': 'sgd'}\n",
            "No pre-process\n",
            "No pre-process\n",
            "No pre-process\n",
            "Start time:  Sat Mar 29 19:01:20 2025\n",
            "current lr 1.00000e-02\n",
            "Epoch: [1][0/39]\tTime 0.113 (Avg-Time 0.113)\t Loss 8.8571 (Avg-Loss 8.8571)\tAcc 9.4727 (Avg-Acc 9.4727)\n",
            "Epoch: [1][9/39]\tTime 0.057 (Avg-Time 0.067)\t Loss 6.4971 (Avg-Loss 7.7391)\tAcc 17.4805 (Avg-Acc 12.5098)\n",
            "Epoch: [1][18/39]\tTime 0.058 (Avg-Time 0.063)\t Loss 5.3307 (Avg-Loss 6.8281)\tAcc 22.6562 (Avg-Acc 16.1338)\n",
            "Epoch: [1][27/39]\tTime 0.057 (Avg-Time 0.063)\t Loss 4.9509 (Avg-Loss 6.2970)\tAcc 23.4375 (Avg-Acc 18.0664)\n",
            "Epoch: [1][36/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 4.7296 (Avg-Loss 5.8864)\tAcc 24.9023 (Avg-Acc 19.6527)\n",
            "Epoch: [1][39/39]\tTime 0.006 (Avg-Time 0.061)\t Loss 4.0538 (Avg-Loss 5.8061)\tAcc 25.0000 (Avg-Acc 19.9500)\n",
            "EPOCH: 1 train Results: Acc 19.950 Loss: 5.8061\n",
            "Epoch: [1][0/9]\tTime 0.017 (Avg-Time 0.017)\t Loss 2.6079 (Avg-Loss 2.6079)\tAcc 36.0352 (Avg-Acc 36.0352)\n",
            "Epoch: [1][9/9]\tTime 0.012 (Avg-Time 0.019)\t Loss 2.6331 (Avg-Loss 2.6356)\tAcc 36.7347 (Avg-Acc 34.1000)\n",
            "EPOCH: 1 Validation Results: Acc 34.100 Loss: 2.6356\n",
            "Best Accuracy: 34.1000\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [2][0/39]\tTime 0.063 (Avg-Time 0.063)\t Loss 4.1907 (Avg-Loss 4.1907)\tAcc 24.9023 (Avg-Acc 24.9023)\n",
            "Epoch: [2][9/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 3.9708 (Avg-Loss 4.1546)\tAcc 27.2461 (Avg-Acc 25.9180)\n",
            "Epoch: [2][18/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 3.9285 (Avg-Loss 4.0465)\tAcc 25.3906 (Avg-Acc 26.0742)\n",
            "Epoch: [2][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 3.5371 (Avg-Loss 3.9571)\tAcc 28.3203 (Avg-Acc 26.2277)\n",
            "Epoch: [2][36/39]\tTime 0.064 (Avg-Time 0.062)\t Loss 3.4939 (Avg-Loss 3.8689)\tAcc 28.6133 (Avg-Acc 26.4385)\n",
            "Epoch: [2][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 3.8480 (Avg-Loss 3.8526)\tAcc 29.6875 (Avg-Acc 26.4375)\n",
            "EPOCH: 2 train Results: Acc 26.438 Loss: 3.8526\n",
            "Epoch: [2][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 2.1275 (Avg-Loss 2.1275)\tAcc 38.8672 (Avg-Acc 38.8672)\n",
            "Epoch: [2][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 2.1210 (Avg-Loss 2.1289)\tAcc 38.5204 (Avg-Acc 37.2500)\n",
            "EPOCH: 2 Validation Results: Acc 37.250 Loss: 2.1289\n",
            "Best Accuracy: 37.2500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [3][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 3.4095 (Avg-Loss 3.4095)\tAcc 26.0742 (Avg-Acc 26.0742)\n",
            "Epoch: [3][9/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 3.2458 (Avg-Loss 3.3725)\tAcc 29.3945 (Avg-Acc 27.1387)\n",
            "Epoch: [3][18/39]\tTime 0.059 (Avg-Time 0.061)\t Loss 3.2556 (Avg-Loss 3.3450)\tAcc 29.3945 (Avg-Acc 27.2307)\n",
            "Epoch: [3][27/39]\tTime 0.059 (Avg-Time 0.061)\t Loss 3.2118 (Avg-Loss 3.2936)\tAcc 27.6367 (Avg-Acc 27.3717)\n",
            "Epoch: [3][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 3.0596 (Avg-Loss 3.2508)\tAcc 28.0273 (Avg-Acc 27.4045)\n",
            "Epoch: [3][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 2.8006 (Avg-Loss 3.2375)\tAcc 34.3750 (Avg-Acc 27.4575)\n",
            "EPOCH: 3 train Results: Acc 27.457 Loss: 3.2375\n",
            "Epoch: [3][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 2.0064 (Avg-Loss 2.0064)\tAcc 38.6719 (Avg-Acc 38.6719)\n",
            "Epoch: [3][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.9943 (Avg-Loss 1.9791)\tAcc 39.1582 (Avg-Acc 38.6700)\n",
            "EPOCH: 3 Validation Results: Acc 38.670 Loss: 1.9791\n",
            "Best Accuracy: 38.6700\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [4][0/39]\tTime 0.072 (Avg-Time 0.072)\t Loss 3.0148 (Avg-Loss 3.0148)\tAcc 29.7852 (Avg-Acc 29.7852)\n",
            "Epoch: [4][9/39]\tTime 0.057 (Avg-Time 0.062)\t Loss 3.0816 (Avg-Loss 2.9789)\tAcc 27.3438 (Avg-Acc 29.0527)\n",
            "Epoch: [4][18/39]\tTime 0.059 (Avg-Time 0.062)\t Loss 2.8134 (Avg-Loss 2.9573)\tAcc 29.7852 (Avg-Acc 29.0296)\n",
            "Epoch: [4][27/39]\tTime 0.131 (Avg-Time 0.070)\t Loss 2.8082 (Avg-Loss 2.9146)\tAcc 28.6133 (Avg-Acc 29.0109)\n",
            "Epoch: [4][36/39]\tTime 0.123 (Avg-Time 0.101)\t Loss 2.7944 (Avg-Loss 2.8808)\tAcc 30.0781 (Avg-Acc 29.0963)\n",
            "Epoch: [4][39/39]\tTime 0.015 (Avg-Time 0.102)\t Loss 3.1795 (Avg-Loss 2.8778)\tAcc 26.5625 (Avg-Acc 29.1175)\n",
            "EPOCH: 4 train Results: Acc 29.117 Loss: 2.8778\n",
            "Epoch: [4][0/9]\tTime 0.028 (Avg-Time 0.028)\t Loss 1.8917 (Avg-Loss 1.8917)\tAcc 39.9414 (Avg-Acc 39.9414)\n",
            "Epoch: [4][9/9]\tTime 0.019 (Avg-Time 0.029)\t Loss 1.8807 (Avg-Loss 1.8730)\tAcc 40.6888 (Avg-Acc 39.5400)\n",
            "EPOCH: 4 Validation Results: Acc 39.540 Loss: 1.8730\n",
            "Best Accuracy: 39.5400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [5][0/39]\tTime 0.114 (Avg-Time 0.114)\t Loss 2.7655 (Avg-Loss 2.7655)\tAcc 29.0039 (Avg-Acc 29.0039)\n",
            "Epoch: [5][9/39]\tTime 0.060 (Avg-Time 0.080)\t Loss 2.6396 (Avg-Loss 2.6973)\tAcc 29.7852 (Avg-Acc 29.8730)\n",
            "Epoch: [5][18/39]\tTime 0.056 (Avg-Time 0.071)\t Loss 2.6022 (Avg-Loss 2.6828)\tAcc 31.9336 (Avg-Acc 29.9394)\n",
            "Epoch: [5][27/39]\tTime 0.057 (Avg-Time 0.067)\t Loss 2.6297 (Avg-Loss 2.6490)\tAcc 27.5391 (Avg-Acc 29.9526)\n",
            "Epoch: [5][36/39]\tTime 0.061 (Avg-Time 0.066)\t Loss 2.5262 (Avg-Loss 2.6212)\tAcc 29.8828 (Avg-Acc 30.0069)\n",
            "Epoch: [5][39/39]\tTime 0.006 (Avg-Time 0.064)\t Loss 2.7594 (Avg-Loss 2.6178)\tAcc 25.0000 (Avg-Acc 30.0250)\n",
            "EPOCH: 5 train Results: Acc 30.025 Loss: 2.6178\n",
            "Epoch: [5][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.8079 (Avg-Loss 1.8079)\tAcc 41.7969 (Avg-Acc 41.7969)\n",
            "Epoch: [5][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.8095 (Avg-Loss 1.7925)\tAcc 41.3265 (Avg-Acc 40.5900)\n",
            "EPOCH: 5 Validation Results: Acc 40.590 Loss: 1.7925\n",
            "Best Accuracy: 40.5900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [6][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 2.4788 (Avg-Loss 2.4788)\tAcc 30.6641 (Avg-Acc 30.6641)\n",
            "Epoch: [6][9/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 2.4430 (Avg-Loss 2.4546)\tAcc 30.1758 (Avg-Acc 30.8496)\n",
            "Epoch: [6][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 2.3681 (Avg-Loss 2.4474)\tAcc 31.4453 (Avg-Acc 31.0290)\n",
            "Epoch: [6][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 2.4120 (Avg-Loss 2.4371)\tAcc 32.2266 (Avg-Acc 30.9222)\n",
            "Epoch: [6][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 2.4050 (Avg-Loss 2.4189)\tAcc 31.7383 (Avg-Acc 31.0389)\n",
            "Epoch: [6][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 2.1316 (Avg-Loss 2.4157)\tAcc 37.5000 (Avg-Acc 31.1125)\n",
            "EPOCH: 6 train Results: Acc 31.113 Loss: 2.4157\n",
            "Epoch: [6][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.7542 (Avg-Loss 1.7542)\tAcc 41.6016 (Avg-Acc 41.6016)\n",
            "Epoch: [6][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.7540 (Avg-Loss 1.7383)\tAcc 41.7092 (Avg-Acc 40.8900)\n",
            "EPOCH: 6 Validation Results: Acc 40.890 Loss: 1.7383\n",
            "Best Accuracy: 40.8900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [7][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 2.4273 (Avg-Loss 2.4273)\tAcc 30.7617 (Avg-Acc 30.7617)\n",
            "Epoch: [7][9/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 2.3531 (Avg-Loss 2.2926)\tAcc 29.1992 (Avg-Acc 32.1582)\n",
            "Epoch: [7][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 2.2547 (Avg-Loss 2.2799)\tAcc 31.2500 (Avg-Acc 32.3808)\n",
            "Epoch: [7][27/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 2.1591 (Avg-Loss 2.2727)\tAcc 35.2539 (Avg-Acc 32.7672)\n",
            "Epoch: [7][36/39]\tTime 0.064 (Avg-Time 0.061)\t Loss 2.2209 (Avg-Loss 2.2619)\tAcc 32.5195 (Avg-Acc 32.6541)\n",
            "Epoch: [7][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.8584 (Avg-Loss 2.2564)\tAcc 43.7500 (Avg-Acc 32.7025)\n",
            "EPOCH: 7 train Results: Acc 32.703 Loss: 2.2564\n",
            "Epoch: [7][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.7120 (Avg-Loss 1.7120)\tAcc 43.2617 (Avg-Acc 43.2617)\n",
            "Epoch: [7][9/9]\tTime 0.014 (Avg-Time 0.016)\t Loss 1.7110 (Avg-Loss 1.6962)\tAcc 42.4745 (Avg-Acc 41.8100)\n",
            "EPOCH: 7 Validation Results: Acc 41.810 Loss: 1.6962\n",
            "Best Accuracy: 41.8100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [8][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 2.1960 (Avg-Loss 2.1960)\tAcc 31.9336 (Avg-Acc 31.9336)\n",
            "Epoch: [8][9/39]\tTime 0.060 (Avg-Time 0.062)\t Loss 2.2040 (Avg-Loss 2.1607)\tAcc 33.3984 (Avg-Acc 33.4180)\n",
            "Epoch: [8][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 2.1207 (Avg-Loss 2.1569)\tAcc 34.9609 (Avg-Acc 33.6657)\n",
            "Epoch: [8][27/39]\tTime 0.064 (Avg-Time 0.062)\t Loss 2.1918 (Avg-Loss 2.1547)\tAcc 32.7148 (Avg-Acc 33.4089)\n",
            "Epoch: [8][36/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 2.0846 (Avg-Loss 2.1480)\tAcc 33.6914 (Avg-Acc 33.5251)\n",
            "Epoch: [8][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 2.4192 (Avg-Loss 2.1502)\tAcc 29.6875 (Avg-Acc 33.4825)\n",
            "EPOCH: 8 train Results: Acc 33.483 Loss: 2.1502\n",
            "Epoch: [8][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.6878 (Avg-Loss 1.6878)\tAcc 44.1406 (Avg-Acc 44.1406)\n",
            "Epoch: [8][9/9]\tTime 0.011 (Avg-Time 0.018)\t Loss 1.6882 (Avg-Loss 1.6720)\tAcc 42.3469 (Avg-Acc 42.2400)\n",
            "EPOCH: 8 Validation Results: Acc 42.240 Loss: 1.6720\n",
            "Best Accuracy: 42.2400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [9][0/39]\tTime 0.083 (Avg-Time 0.083)\t Loss 2.1802 (Avg-Loss 2.1802)\tAcc 32.2266 (Avg-Acc 32.2266)\n",
            "Epoch: [9][9/39]\tTime 0.337 (Avg-Time 0.179)\t Loss 2.1170 (Avg-Loss 2.1059)\tAcc 32.6172 (Avg-Acc 33.2031)\n",
            "Epoch: [9][18/39]\tTime 0.112 (Avg-Time 0.157)\t Loss 2.1771 (Avg-Loss 2.1068)\tAcc 33.3008 (Avg-Acc 33.4344)\n",
            "Epoch: [9][27/39]\tTime 0.057 (Avg-Time 0.132)\t Loss 2.0409 (Avg-Loss 2.0916)\tAcc 32.9102 (Avg-Acc 33.5519)\n",
            "Epoch: [9][36/39]\tTime 0.057 (Avg-Time 0.115)\t Loss 2.0390 (Avg-Loss 2.0829)\tAcc 31.5430 (Avg-Acc 33.4776)\n",
            "Epoch: [9][39/39]\tTime 0.006 (Avg-Time 0.110)\t Loss 2.2610 (Avg-Loss 2.0793)\tAcc 28.1250 (Avg-Acc 33.4900)\n",
            "EPOCH: 9 train Results: Acc 33.490 Loss: 2.0793\n",
            "Epoch: [9][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.6718 (Avg-Loss 1.6718)\tAcc 44.4336 (Avg-Acc 44.4336)\n",
            "Epoch: [9][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.6757 (Avg-Loss 1.6577)\tAcc 42.8571 (Avg-Acc 42.3400)\n",
            "EPOCH: 9 Validation Results: Acc 42.340 Loss: 1.6577\n",
            "Best Accuracy: 42.3400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [10][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 2.1143 (Avg-Loss 2.1143)\tAcc 30.9570 (Avg-Acc 30.9570)\n",
            "Epoch: [10][9/39]\tTime 0.058 (Avg-Time 0.059)\t Loss 2.0208 (Avg-Loss 2.0178)\tAcc 33.9844 (Avg-Acc 34.3848)\n",
            "Epoch: [10][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 2.0263 (Avg-Loss 2.0107)\tAcc 35.7422 (Avg-Acc 34.8324)\n",
            "Epoch: [10][27/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.9766 (Avg-Loss 2.0110)\tAcc 33.9844 (Avg-Acc 34.7273)\n",
            "Epoch: [10][36/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.9160 (Avg-Loss 1.9949)\tAcc 36.8164 (Avg-Acc 34.8211)\n",
            "Epoch: [10][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 2.2957 (Avg-Loss 1.9921)\tAcc 28.1250 (Avg-Acc 34.9000)\n",
            "EPOCH: 10 train Results: Acc 34.900 Loss: 1.9921\n",
            "Epoch: [10][0/9]\tTime 0.016 (Avg-Time 0.016)\t Loss 1.6462 (Avg-Loss 1.6462)\tAcc 45.1172 (Avg-Acc 45.1172)\n",
            "Epoch: [10][9/9]\tTime 0.011 (Avg-Time 0.017)\t Loss 1.6524 (Avg-Loss 1.6365)\tAcc 42.2194 (Avg-Acc 42.7500)\n",
            "EPOCH: 10 Validation Results: Acc 42.750 Loss: 1.6365\n",
            "Best Accuracy: 42.7500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [11][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.8515 (Avg-Loss 1.8515)\tAcc 36.2305 (Avg-Acc 36.2305)\n",
            "Epoch: [11][9/39]\tTime 0.073 (Avg-Time 0.066)\t Loss 1.9430 (Avg-Loss 1.9358)\tAcc 34.9609 (Avg-Acc 35.4004)\n",
            "Epoch: [11][18/39]\tTime 0.060 (Avg-Time 0.063)\t Loss 1.9672 (Avg-Loss 1.9382)\tAcc 34.4727 (Avg-Acc 35.2488)\n",
            "Epoch: [11][27/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 2.0222 (Avg-Loss 1.9310)\tAcc 33.5938 (Avg-Acc 35.5957)\n",
            "Epoch: [11][36/39]\tTime 0.056 (Avg-Time 0.062)\t Loss 1.8467 (Avg-Loss 1.9268)\tAcc 36.9141 (Avg-Acc 35.5574)\n",
            "Epoch: [11][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.7454 (Avg-Loss 1.9244)\tAcc 42.1875 (Avg-Acc 35.5850)\n",
            "EPOCH: 11 train Results: Acc 35.585 Loss: 1.9244\n",
            "Epoch: [11][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.6294 (Avg-Loss 1.6294)\tAcc 45.8984 (Avg-Acc 45.8984)\n",
            "Epoch: [11][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.6334 (Avg-Loss 1.6189)\tAcc 43.1122 (Avg-Acc 43.3000)\n",
            "EPOCH: 11 Validation Results: Acc 43.300 Loss: 1.6189\n",
            "Best Accuracy: 43.3000\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [12][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.9560 (Avg-Loss 1.9560)\tAcc 34.4727 (Avg-Acc 34.4727)\n",
            "Epoch: [12][9/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.8915 (Avg-Loss 1.8740)\tAcc 35.0586 (Avg-Acc 36.1914)\n",
            "Epoch: [12][18/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.8721 (Avg-Loss 1.8690)\tAcc 37.0117 (Avg-Acc 36.6725)\n",
            "Epoch: [12][27/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.8726 (Avg-Loss 1.8704)\tAcc 39.2578 (Avg-Acc 36.6839)\n",
            "Epoch: [12][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.8405 (Avg-Loss 1.8676)\tAcc 37.3047 (Avg-Acc 36.6237)\n",
            "Epoch: [12][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.8747 (Avg-Loss 1.8645)\tAcc 37.5000 (Avg-Acc 36.7550)\n",
            "EPOCH: 12 train Results: Acc 36.755 Loss: 1.8645\n",
            "Epoch: [12][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.6211 (Avg-Loss 1.6211)\tAcc 45.9961 (Avg-Acc 45.9961)\n",
            "Epoch: [12][9/9]\tTime 0.011 (Avg-Time 0.017)\t Loss 1.6207 (Avg-Loss 1.6076)\tAcc 42.7296 (Avg-Acc 43.6000)\n",
            "EPOCH: 12 Validation Results: Acc 43.600 Loss: 1.6076\n",
            "Best Accuracy: 43.6000\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [13][0/39]\tTime 0.069 (Avg-Time 0.069)\t Loss 1.8559 (Avg-Loss 1.8559)\tAcc 37.0117 (Avg-Acc 37.0117)\n",
            "Epoch: [13][9/39]\tTime 0.056 (Avg-Time 0.062)\t Loss 1.9109 (Avg-Loss 1.8537)\tAcc 37.0117 (Avg-Acc 37.1289)\n",
            "Epoch: [13][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.8175 (Avg-Loss 1.8487)\tAcc 36.7188 (Avg-Acc 37.0477)\n",
            "Epoch: [13][27/39]\tTime 0.303 (Avg-Time 0.093)\t Loss 1.7982 (Avg-Loss 1.8409)\tAcc 38.8672 (Avg-Acc 37.3744)\n",
            "Epoch: [13][36/39]\tTime 0.186 (Avg-Time 0.110)\t Loss 1.8188 (Avg-Loss 1.8352)\tAcc 35.6445 (Avg-Acc 37.4419)\n",
            "Epoch: [13][39/39]\tTime 0.018 (Avg-Time 0.107)\t Loss 1.7079 (Avg-Loss 1.8321)\tAcc 35.9375 (Avg-Acc 37.5325)\n",
            "EPOCH: 13 train Results: Acc 37.532 Loss: 1.8321\n",
            "Epoch: [13][0/9]\tTime 0.030 (Avg-Time 0.030)\t Loss 1.6152 (Avg-Loss 1.6152)\tAcc 46.0938 (Avg-Acc 46.0938)\n",
            "Epoch: [13][9/9]\tTime 0.012 (Avg-Time 0.023)\t Loss 1.6079 (Avg-Loss 1.5992)\tAcc 42.8571 (Avg-Acc 43.9500)\n",
            "EPOCH: 13 Validation Results: Acc 43.950 Loss: 1.5992\n",
            "Best Accuracy: 43.9500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [14][0/39]\tTime 0.075 (Avg-Time 0.075)\t Loss 1.8370 (Avg-Loss 1.8370)\tAcc 35.8398 (Avg-Acc 35.8398)\n",
            "Epoch: [14][9/39]\tTime 0.067 (Avg-Time 0.063)\t Loss 1.8282 (Avg-Loss 1.8167)\tAcc 36.2305 (Avg-Acc 37.6953)\n",
            "Epoch: [14][18/39]\tTime 0.074 (Avg-Time 0.062)\t Loss 1.8346 (Avg-Loss 1.8034)\tAcc 36.7188 (Avg-Acc 37.9523)\n",
            "Epoch: [14][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.7387 (Avg-Loss 1.8004)\tAcc 37.6953 (Avg-Acc 37.9639)\n",
            "Epoch: [14][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.8125 (Avg-Loss 1.7950)\tAcc 37.3047 (Avg-Acc 38.0411)\n",
            "Epoch: [14][39/39]\tTime 0.008 (Avg-Time 0.059)\t Loss 1.6820 (Avg-Loss 1.7965)\tAcc 35.9375 (Avg-Acc 38.0025)\n",
            "EPOCH: 14 train Results: Acc 38.002 Loss: 1.7965\n",
            "Epoch: [14][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.5998 (Avg-Loss 1.5998)\tAcc 46.3867 (Avg-Acc 46.3867)\n",
            "Epoch: [14][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.5941 (Avg-Loss 1.5887)\tAcc 43.4949 (Avg-Acc 44.2300)\n",
            "EPOCH: 14 Validation Results: Acc 44.230 Loss: 1.5887\n",
            "Best Accuracy: 44.2300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [15][0/39]\tTime 0.065 (Avg-Time 0.065)\t Loss 1.7512 (Avg-Loss 1.7512)\tAcc 38.5742 (Avg-Acc 38.5742)\n",
            "Epoch: [15][9/39]\tTime 0.063 (Avg-Time 0.060)\t Loss 1.9149 (Avg-Loss 1.7792)\tAcc 35.7422 (Avg-Acc 38.1445)\n",
            "Epoch: [15][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.7519 (Avg-Loss 1.7739)\tAcc 38.0859 (Avg-Acc 38.4200)\n",
            "Epoch: [15][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.7570 (Avg-Loss 1.7669)\tAcc 40.8203 (Avg-Acc 38.6196)\n",
            "Epoch: [15][36/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.7550 (Avg-Loss 1.7649)\tAcc 40.0391 (Avg-Acc 38.6824)\n",
            "Epoch: [15][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.9227 (Avg-Loss 1.7628)\tAcc 23.4375 (Avg-Acc 38.6900)\n",
            "EPOCH: 15 train Results: Acc 38.690 Loss: 1.7628\n",
            "Epoch: [15][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.5873 (Avg-Loss 1.5873)\tAcc 46.0938 (Avg-Acc 46.0938)\n",
            "Epoch: [15][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.5860 (Avg-Loss 1.5789)\tAcc 42.8571 (Avg-Acc 44.2400)\n",
            "EPOCH: 15 Validation Results: Acc 44.240 Loss: 1.5789\n",
            "Best Accuracy: 44.2400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [16][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.7671 (Avg-Loss 1.7671)\tAcc 39.4531 (Avg-Acc 39.4531)\n",
            "Epoch: [16][9/39]\tTime 0.056 (Avg-Time 0.062)\t Loss 1.7563 (Avg-Loss 1.7617)\tAcc 38.1836 (Avg-Acc 38.6230)\n",
            "Epoch: [16][18/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.7280 (Avg-Loss 1.7506)\tAcc 41.0156 (Avg-Acc 39.0162)\n",
            "Epoch: [16][27/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.7290 (Avg-Loss 1.7384)\tAcc 40.6250 (Avg-Acc 39.2473)\n",
            "Epoch: [16][36/39]\tTime 0.060 (Avg-Time 0.061)\t Loss 1.7368 (Avg-Loss 1.7346)\tAcc 39.2578 (Avg-Acc 39.4109)\n",
            "Epoch: [16][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.7025 (Avg-Loss 1.7368)\tAcc 40.6250 (Avg-Acc 39.3775)\n",
            "EPOCH: 16 train Results: Acc 39.377 Loss: 1.7368\n",
            "Epoch: [16][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.5746 (Avg-Loss 1.5746)\tAcc 46.8750 (Avg-Acc 46.8750)\n",
            "Epoch: [16][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.5726 (Avg-Loss 1.5663)\tAcc 43.6224 (Avg-Acc 44.8100)\n",
            "EPOCH: 16 Validation Results: Acc 44.810 Loss: 1.5663\n",
            "Best Accuracy: 44.8100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [17][0/39]\tTime 0.063 (Avg-Time 0.063)\t Loss 1.7043 (Avg-Loss 1.7043)\tAcc 39.1602 (Avg-Acc 39.1602)\n",
            "Epoch: [17][9/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.6972 (Avg-Loss 1.7184)\tAcc 40.3320 (Avg-Acc 39.8926)\n",
            "Epoch: [17][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.8103 (Avg-Loss 1.7165)\tAcc 37.2070 (Avg-Acc 39.8335)\n",
            "Epoch: [17][27/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.7067 (Avg-Loss 1.7136)\tAcc 37.6953 (Avg-Acc 39.8158)\n",
            "Epoch: [17][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.7019 (Avg-Loss 1.7131)\tAcc 42.2852 (Avg-Acc 39.9572)\n",
            "Epoch: [17][39/39]\tTime 0.014 (Avg-Time 0.061)\t Loss 1.8248 (Avg-Loss 1.7115)\tAcc 40.6250 (Avg-Acc 40.0675)\n",
            "EPOCH: 17 train Results: Acc 40.068 Loss: 1.7115\n",
            "Epoch: [17][0/9]\tTime 0.036 (Avg-Time 0.036)\t Loss 1.5645 (Avg-Loss 1.5645)\tAcc 46.6797 (Avg-Acc 46.6797)\n",
            "Epoch: [17][9/9]\tTime 0.022 (Avg-Time 0.031)\t Loss 1.5665 (Avg-Loss 1.5587)\tAcc 43.2398 (Avg-Acc 44.8500)\n",
            "EPOCH: 17 Validation Results: Acc 44.850 Loss: 1.5587\n",
            "Best Accuracy: 44.8500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [18][0/39]\tTime 0.152 (Avg-Time 0.152)\t Loss 1.6750 (Avg-Loss 1.6750)\tAcc 41.3086 (Avg-Acc 41.3086)\n",
            "Epoch: [18][9/39]\tTime 0.102 (Avg-Time 0.192)\t Loss 1.6128 (Avg-Loss 1.6764)\tAcc 43.2617 (Avg-Acc 41.3770)\n",
            "Epoch: [18][18/39]\tTime 0.067 (Avg-Time 0.158)\t Loss 1.7122 (Avg-Loss 1.6862)\tAcc 40.9180 (Avg-Acc 40.5839)\n",
            "Epoch: [18][27/39]\tTime 0.057 (Avg-Time 0.127)\t Loss 1.7005 (Avg-Loss 1.6956)\tAcc 40.0391 (Avg-Acc 40.3948)\n",
            "Epoch: [18][36/39]\tTime 0.061 (Avg-Time 0.111)\t Loss 1.7126 (Avg-Loss 1.6930)\tAcc 40.8203 (Avg-Acc 40.4244)\n",
            "Epoch: [18][39/39]\tTime 0.006 (Avg-Time 0.105)\t Loss 2.0177 (Avg-Loss 1.6932)\tAcc 34.3750 (Avg-Acc 40.4350)\n",
            "EPOCH: 18 train Results: Acc 40.435 Loss: 1.6932\n",
            "Epoch: [18][0/9]\tTime 0.017 (Avg-Time 0.017)\t Loss 1.5546 (Avg-Loss 1.5546)\tAcc 46.9727 (Avg-Acc 46.9727)\n",
            "Epoch: [18][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.5582 (Avg-Loss 1.5503)\tAcc 43.7500 (Avg-Acc 45.0700)\n",
            "EPOCH: 18 Validation Results: Acc 45.070 Loss: 1.5503\n",
            "Best Accuracy: 45.0700\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [19][0/39]\tTime 0.069 (Avg-Time 0.069)\t Loss 1.7754 (Avg-Loss 1.7754)\tAcc 38.4766 (Avg-Acc 38.4766)\n",
            "Epoch: [19][9/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.6714 (Avg-Loss 1.6818)\tAcc 40.6250 (Avg-Acc 40.5664)\n",
            "Epoch: [19][18/39]\tTime 0.072 (Avg-Time 0.061)\t Loss 1.6756 (Avg-Loss 1.6837)\tAcc 41.8945 (Avg-Acc 40.4811)\n",
            "Epoch: [19][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.6726 (Avg-Loss 1.6813)\tAcc 41.1133 (Avg-Acc 40.4122)\n",
            "Epoch: [19][36/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.6335 (Avg-Loss 1.6762)\tAcc 43.0664 (Avg-Acc 40.5960)\n",
            "Epoch: [19][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.7043 (Avg-Loss 1.6771)\tAcc 48.4375 (Avg-Acc 40.6475)\n",
            "EPOCH: 19 train Results: Acc 40.648 Loss: 1.6771\n",
            "Epoch: [19][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.5477 (Avg-Loss 1.5477)\tAcc 47.3633 (Avg-Acc 47.3633)\n",
            "Epoch: [19][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.5513 (Avg-Loss 1.5459)\tAcc 44.8980 (Avg-Acc 45.3600)\n",
            "EPOCH: 19 Validation Results: Acc 45.360 Loss: 1.5459\n",
            "Best Accuracy: 45.3600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [20][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.6607 (Avg-Loss 1.6607)\tAcc 43.5547 (Avg-Acc 43.5547)\n",
            "Epoch: [20][9/39]\tTime 0.058 (Avg-Time 0.059)\t Loss 1.6282 (Avg-Loss 1.6504)\tAcc 40.9180 (Avg-Acc 41.4551)\n",
            "Epoch: [20][18/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.6647 (Avg-Loss 1.6630)\tAcc 41.5039 (Avg-Acc 41.3497)\n",
            "Epoch: [20][27/39]\tTime 0.055 (Avg-Time 0.059)\t Loss 1.5786 (Avg-Loss 1.6553)\tAcc 42.4805 (Avg-Acc 41.4934)\n",
            "Epoch: [20][36/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.6894 (Avg-Loss 1.6524)\tAcc 39.8438 (Avg-Acc 41.4590)\n",
            "Epoch: [20][39/39]\tTime 0.010 (Avg-Time 0.058)\t Loss 1.8710 (Avg-Loss 1.6534)\tAcc 42.1875 (Avg-Acc 41.5175)\n",
            "EPOCH: 20 train Results: Acc 41.517 Loss: 1.6534\n",
            "Epoch: [20][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.5384 (Avg-Loss 1.5384)\tAcc 48.1445 (Avg-Acc 48.1445)\n",
            "Epoch: [20][9/9]\tTime 0.015 (Avg-Time 0.016)\t Loss 1.5470 (Avg-Loss 1.5389)\tAcc 44.6429 (Avg-Acc 45.5700)\n",
            "EPOCH: 20 Validation Results: Acc 45.570 Loss: 1.5389\n",
            "Best Accuracy: 45.5700\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [21][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.5945 (Avg-Loss 1.5945)\tAcc 43.4570 (Avg-Acc 43.4570)\n",
            "Epoch: [21][9/39]\tTime 0.055 (Avg-Time 0.060)\t Loss 1.6648 (Avg-Loss 1.6235)\tAcc 40.7227 (Avg-Acc 42.4609)\n",
            "Epoch: [21][18/39]\tTime 0.055 (Avg-Time 0.059)\t Loss 1.6638 (Avg-Loss 1.6406)\tAcc 39.8438 (Avg-Acc 41.9100)\n",
            "Epoch: [21][27/39]\tTime 0.056 (Avg-Time 0.062)\t Loss 1.6562 (Avg-Loss 1.6416)\tAcc 40.8203 (Avg-Acc 42.0619)\n",
            "Epoch: [21][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.6510 (Avg-Loss 1.6435)\tAcc 42.6758 (Avg-Acc 42.0819)\n",
            "Epoch: [21][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.7086 (Avg-Loss 1.6408)\tAcc 42.1875 (Avg-Acc 42.2100)\n",
            "EPOCH: 21 train Results: Acc 42.210 Loss: 1.6408\n",
            "Epoch: [21][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.5316 (Avg-Loss 1.5316)\tAcc 48.3398 (Avg-Acc 48.3398)\n",
            "Epoch: [21][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.5417 (Avg-Loss 1.5315)\tAcc 45.2806 (Avg-Acc 45.9400)\n",
            "EPOCH: 21 Validation Results: Acc 45.940 Loss: 1.5315\n",
            "Best Accuracy: 45.9400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [22][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.6110 (Avg-Loss 1.6110)\tAcc 42.6758 (Avg-Acc 42.6758)\n",
            "Epoch: [22][9/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.6057 (Avg-Loss 1.6242)\tAcc 43.3594 (Avg-Acc 42.6758)\n",
            "Epoch: [22][18/39]\tTime 0.121 (Avg-Time 0.066)\t Loss 1.7067 (Avg-Loss 1.6306)\tAcc 37.5977 (Avg-Acc 42.0384)\n",
            "Epoch: [22][27/39]\tTime 0.171 (Avg-Time 0.090)\t Loss 1.5458 (Avg-Loss 1.6246)\tAcc 45.1172 (Avg-Acc 42.2852)\n",
            "Epoch: [22][36/39]\tTime 0.130 (Avg-Time 0.107)\t Loss 1.6408 (Avg-Loss 1.6235)\tAcc 42.3828 (Avg-Acc 42.3247)\n",
            "Epoch: [22][39/39]\tTime 0.098 (Avg-Time 0.108)\t Loss 1.8398 (Avg-Loss 1.6251)\tAcc 37.5000 (Avg-Acc 42.2775)\n",
            "EPOCH: 22 train Results: Acc 42.278 Loss: 1.6251\n",
            "Epoch: [22][0/9]\tTime 0.023 (Avg-Time 0.023)\t Loss 1.5211 (Avg-Loss 1.5211)\tAcc 48.1445 (Avg-Acc 48.1445)\n",
            "Epoch: [22][9/9]\tTime 0.014 (Avg-Time 0.016)\t Loss 1.5314 (Avg-Loss 1.5232)\tAcc 45.5357 (Avg-Acc 46.1800)\n",
            "EPOCH: 22 Validation Results: Acc 46.180 Loss: 1.5232\n",
            "Best Accuracy: 46.1800\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [23][0/39]\tTime 0.076 (Avg-Time 0.076)\t Loss 1.6058 (Avg-Loss 1.6058)\tAcc 42.1875 (Avg-Acc 42.1875)\n",
            "Epoch: [23][9/39]\tTime 0.061 (Avg-Time 0.064)\t Loss 1.5564 (Avg-Loss 1.6024)\tAcc 44.3359 (Avg-Acc 42.7441)\n",
            "Epoch: [23][18/39]\tTime 0.057 (Avg-Time 0.062)\t Loss 1.5853 (Avg-Loss 1.6066)\tAcc 42.9688 (Avg-Acc 42.7375)\n",
            "Epoch: [23][27/39]\tTime 0.057 (Avg-Time 0.062)\t Loss 1.6429 (Avg-Loss 1.6059)\tAcc 40.2344 (Avg-Acc 42.5886)\n",
            "Epoch: [23][36/39]\tTime 0.057 (Avg-Time 0.062)\t Loss 1.6229 (Avg-Loss 1.6039)\tAcc 43.2617 (Avg-Acc 42.6837)\n",
            "Epoch: [23][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.5726 (Avg-Loss 1.6057)\tAcc 42.1875 (Avg-Acc 42.7000)\n",
            "EPOCH: 23 train Results: Acc 42.700 Loss: 1.6057\n",
            "Epoch: [23][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.5188 (Avg-Loss 1.5188)\tAcc 48.2422 (Avg-Acc 48.2422)\n",
            "Epoch: [23][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.5255 (Avg-Loss 1.5196)\tAcc 46.3010 (Avg-Acc 46.4600)\n",
            "EPOCH: 23 Validation Results: Acc 46.460 Loss: 1.5196\n",
            "Best Accuracy: 46.4600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [24][0/39]\tTime 0.067 (Avg-Time 0.067)\t Loss 1.6027 (Avg-Loss 1.6027)\tAcc 44.2383 (Avg-Acc 44.2383)\n",
            "Epoch: [24][9/39]\tTime 0.058 (Avg-Time 0.063)\t Loss 1.5499 (Avg-Loss 1.5990)\tAcc 45.7031 (Avg-Acc 43.0664)\n",
            "Epoch: [24][18/39]\tTime 0.056 (Avg-Time 0.063)\t Loss 1.6772 (Avg-Loss 1.5975)\tAcc 40.9180 (Avg-Acc 43.2000)\n",
            "Epoch: [24][27/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 1.5338 (Avg-Loss 1.6007)\tAcc 46.1914 (Avg-Acc 42.8990)\n",
            "Epoch: [24][36/39]\tTime 0.060 (Avg-Time 0.061)\t Loss 1.5663 (Avg-Loss 1.5943)\tAcc 44.9219 (Avg-Acc 43.2274)\n",
            "Epoch: [24][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.7154 (Avg-Loss 1.5966)\tAcc 40.6250 (Avg-Acc 43.1300)\n",
            "EPOCH: 24 train Results: Acc 43.130 Loss: 1.5966\n",
            "Epoch: [24][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.5116 (Avg-Loss 1.5116)\tAcc 48.3398 (Avg-Acc 48.3398)\n",
            "Epoch: [24][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.5190 (Avg-Loss 1.5128)\tAcc 46.5561 (Avg-Acc 46.6200)\n",
            "EPOCH: 24 Validation Results: Acc 46.620 Loss: 1.5128\n",
            "Best Accuracy: 46.6200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [25][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.5802 (Avg-Loss 1.5802)\tAcc 41.9922 (Avg-Acc 41.9922)\n",
            "Epoch: [25][9/39]\tTime 0.058 (Avg-Time 0.059)\t Loss 1.6005 (Avg-Loss 1.5633)\tAcc 43.9453 (Avg-Acc 44.7949)\n",
            "Epoch: [25][18/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.5673 (Avg-Loss 1.5809)\tAcc 43.8477 (Avg-Acc 43.9042)\n",
            "Epoch: [25][27/39]\tTime 0.072 (Avg-Time 0.061)\t Loss 1.5319 (Avg-Loss 1.5855)\tAcc 45.3125 (Avg-Acc 43.8232)\n",
            "Epoch: [25][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.5742 (Avg-Loss 1.5843)\tAcc 42.5781 (Avg-Acc 43.9559)\n",
            "Epoch: [25][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.4820 (Avg-Loss 1.5842)\tAcc 42.1875 (Avg-Acc 44.0350)\n",
            "EPOCH: 25 train Results: Acc 44.035 Loss: 1.5842\n",
            "Epoch: [25][0/9]\tTime 0.016 (Avg-Time 0.016)\t Loss 1.5036 (Avg-Loss 1.5036)\tAcc 48.7305 (Avg-Acc 48.7305)\n",
            "Epoch: [25][9/9]\tTime 0.012 (Avg-Time 0.016)\t Loss 1.5136 (Avg-Loss 1.5064)\tAcc 46.5561 (Avg-Acc 46.9200)\n",
            "EPOCH: 25 Validation Results: Acc 46.920 Loss: 1.5064\n",
            "Best Accuracy: 46.9200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [26][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.6166 (Avg-Loss 1.6166)\tAcc 41.2109 (Avg-Acc 41.2109)\n",
            "Epoch: [26][9/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.5934 (Avg-Loss 1.5881)\tAcc 44.0430 (Avg-Acc 43.0664)\n",
            "Epoch: [26][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.5576 (Avg-Loss 1.5776)\tAcc 44.4336 (Avg-Acc 43.7346)\n",
            "Epoch: [26][27/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.5411 (Avg-Loss 1.5751)\tAcc 45.0195 (Avg-Acc 43.9628)\n",
            "Epoch: [26][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.5783 (Avg-Loss 1.5758)\tAcc 46.2891 (Avg-Acc 44.1723)\n",
            "Epoch: [26][39/39]\tTime 0.024 (Avg-Time 0.062)\t Loss 1.5339 (Avg-Loss 1.5745)\tAcc 48.4375 (Avg-Acc 44.2125)\n",
            "EPOCH: 26 train Results: Acc 44.212 Loss: 1.5745\n",
            "Epoch: [26][0/9]\tTime 0.022 (Avg-Time 0.022)\t Loss 1.4997 (Avg-Loss 1.4997)\tAcc 49.2188 (Avg-Acc 49.2188)\n",
            "Epoch: [26][9/9]\tTime 0.028 (Avg-Time 0.029)\t Loss 1.5104 (Avg-Loss 1.5019)\tAcc 46.6837 (Avg-Acc 47.2800)\n",
            "EPOCH: 26 Validation Results: Acc 47.280 Loss: 1.5019\n",
            "Best Accuracy: 47.2800\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [27][0/39]\tTime 0.160 (Avg-Time 0.160)\t Loss 1.6426 (Avg-Loss 1.6426)\tAcc 41.6016 (Avg-Acc 41.6016)\n",
            "Epoch: [27][9/39]\tTime 0.121 (Avg-Time 0.179)\t Loss 1.5961 (Avg-Loss 1.5728)\tAcc 43.4570 (Avg-Acc 44.2188)\n",
            "Epoch: [27][18/39]\tTime 0.110 (Avg-Time 0.150)\t Loss 1.5792 (Avg-Loss 1.5663)\tAcc 45.3125 (Avg-Acc 44.4593)\n",
            "Epoch: [27][27/39]\tTime 0.056 (Avg-Time 0.121)\t Loss 1.6018 (Avg-Loss 1.5681)\tAcc 41.8945 (Avg-Acc 44.0744)\n",
            "Epoch: [27][36/39]\tTime 0.056 (Avg-Time 0.106)\t Loss 1.5787 (Avg-Loss 1.5597)\tAcc 45.0195 (Avg-Acc 44.4811)\n",
            "Epoch: [27][39/39]\tTime 0.006 (Avg-Time 0.101)\t Loss 1.4342 (Avg-Loss 1.5589)\tAcc 48.4375 (Avg-Acc 44.5675)\n",
            "EPOCH: 27 train Results: Acc 44.568 Loss: 1.5589\n",
            "Epoch: [27][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.4941 (Avg-Loss 1.4941)\tAcc 49.3164 (Avg-Acc 49.3164)\n",
            "Epoch: [27][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.5003 (Avg-Loss 1.4966)\tAcc 47.4490 (Avg-Acc 47.4900)\n",
            "EPOCH: 27 Validation Results: Acc 47.490 Loss: 1.4966\n",
            "Best Accuracy: 47.4900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [28][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.5721 (Avg-Loss 1.5721)\tAcc 43.3594 (Avg-Acc 43.3594)\n",
            "Epoch: [28][9/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.5522 (Avg-Loss 1.5584)\tAcc 45.6055 (Avg-Acc 44.1602)\n",
            "Epoch: [28][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.5369 (Avg-Loss 1.5554)\tAcc 46.4844 (Avg-Acc 44.5467)\n",
            "Epoch: [28][27/39]\tTime 0.083 (Avg-Time 0.061)\t Loss 1.6216 (Avg-Loss 1.5552)\tAcc 44.0430 (Avg-Acc 44.6010)\n",
            "Epoch: [28][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.5630 (Avg-Loss 1.5518)\tAcc 47.1680 (Avg-Acc 44.6711)\n",
            "Epoch: [28][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.6403 (Avg-Loss 1.5516)\tAcc 35.9375 (Avg-Acc 44.6475)\n",
            "EPOCH: 28 train Results: Acc 44.648 Loss: 1.5516\n",
            "Epoch: [28][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4902 (Avg-Loss 1.4902)\tAcc 49.3164 (Avg-Acc 49.3164)\n",
            "Epoch: [28][9/9]\tTime 0.013 (Avg-Time 0.016)\t Loss 1.4933 (Avg-Loss 1.4919)\tAcc 47.5765 (Avg-Acc 47.6200)\n",
            "EPOCH: 28 Validation Results: Acc 47.620 Loss: 1.4919\n",
            "Best Accuracy: 47.6200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [29][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.5565 (Avg-Loss 1.5565)\tAcc 45.8984 (Avg-Acc 45.8984)\n",
            "Epoch: [29][9/39]\tTime 0.060 (Avg-Time 0.064)\t Loss 1.5347 (Avg-Loss 1.5525)\tAcc 45.6055 (Avg-Acc 44.2969)\n",
            "Epoch: [29][18/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.5053 (Avg-Loss 1.5541)\tAcc 46.6797 (Avg-Acc 44.6752)\n",
            "Epoch: [29][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.4936 (Avg-Loss 1.5486)\tAcc 46.1914 (Avg-Acc 44.9916)\n",
            "Epoch: [29][36/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.5200 (Avg-Loss 1.5451)\tAcc 46.8750 (Avg-Acc 45.1489)\n",
            "Epoch: [29][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.6108 (Avg-Loss 1.5443)\tAcc 39.0625 (Avg-Acc 45.1200)\n",
            "EPOCH: 29 train Results: Acc 45.120 Loss: 1.5443\n",
            "Epoch: [29][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4840 (Avg-Loss 1.4840)\tAcc 48.7305 (Avg-Acc 48.7305)\n",
            "Epoch: [29][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.4866 (Avg-Loss 1.4886)\tAcc 47.5765 (Avg-Acc 47.6400)\n",
            "EPOCH: 29 Validation Results: Acc 47.640 Loss: 1.4886\n",
            "Best Accuracy: 47.6400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [30][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.5954 (Avg-Loss 1.5954)\tAcc 44.2383 (Avg-Acc 44.2383)\n",
            "Epoch: [30][9/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.5295 (Avg-Loss 1.5483)\tAcc 45.2148 (Avg-Acc 44.7266)\n",
            "Epoch: [30][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.5213 (Avg-Loss 1.5403)\tAcc 45.3125 (Avg-Acc 45.1223)\n",
            "Epoch: [30][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.5138 (Avg-Loss 1.5393)\tAcc 45.8984 (Avg-Acc 44.9777)\n",
            "Epoch: [30][36/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.5392 (Avg-Loss 1.5360)\tAcc 46.4844 (Avg-Acc 45.0406)\n",
            "Epoch: [30][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.7633 (Avg-Loss 1.5366)\tAcc 39.0625 (Avg-Acc 45.0650)\n",
            "EPOCH: 30 train Results: Acc 45.065 Loss: 1.5366\n",
            "Epoch: [30][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.4763 (Avg-Loss 1.4763)\tAcc 49.6094 (Avg-Acc 49.6094)\n",
            "Epoch: [30][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.4842 (Avg-Loss 1.4817)\tAcc 48.4694 (Avg-Acc 48.0600)\n",
            "EPOCH: 30 Validation Results: Acc 48.060 Loss: 1.4817\n",
            "Best Accuracy: 48.0600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [31][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.5089 (Avg-Loss 1.5089)\tAcc 48.5352 (Avg-Acc 48.5352)\n",
            "Epoch: [31][9/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.4685 (Avg-Loss 1.5137)\tAcc 46.4844 (Avg-Acc 46.5332)\n",
            "Epoch: [31][18/39]\tTime 0.103 (Avg-Time 0.064)\t Loss 1.5152 (Avg-Loss 1.5255)\tAcc 48.0469 (Avg-Acc 46.1863)\n",
            "Epoch: [31][27/39]\tTime 0.285 (Avg-Time 0.091)\t Loss 1.5164 (Avg-Loss 1.5299)\tAcc 46.8750 (Avg-Acc 45.8740)\n",
            "Epoch: [31][36/39]\tTime 0.167 (Avg-Time 0.109)\t Loss 1.5238 (Avg-Loss 1.5285)\tAcc 46.2891 (Avg-Acc 45.7823)\n",
            "Epoch: [31][39/39]\tTime 0.013 (Avg-Time 0.107)\t Loss 1.5100 (Avg-Loss 1.5280)\tAcc 43.7500 (Avg-Acc 45.8050)\n",
            "EPOCH: 31 train Results: Acc 45.805 Loss: 1.5280\n",
            "Epoch: [31][0/9]\tTime 0.017 (Avg-Time 0.017)\t Loss 1.4687 (Avg-Loss 1.4687)\tAcc 50.0977 (Avg-Acc 50.0977)\n",
            "Epoch: [31][9/9]\tTime 0.011 (Avg-Time 0.023)\t Loss 1.4833 (Avg-Loss 1.4763)\tAcc 47.7041 (Avg-Acc 48.2100)\n",
            "EPOCH: 31 Validation Results: Acc 48.210 Loss: 1.4763\n",
            "Best Accuracy: 48.2100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [32][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.5151 (Avg-Loss 1.5151)\tAcc 47.5586 (Avg-Acc 47.5586)\n",
            "Epoch: [32][9/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.5264 (Avg-Loss 1.5090)\tAcc 44.7266 (Avg-Acc 46.1133)\n",
            "Epoch: [32][18/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.5844 (Avg-Loss 1.5149)\tAcc 45.0195 (Avg-Acc 45.6877)\n",
            "Epoch: [32][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.5553 (Avg-Loss 1.5156)\tAcc 46.6797 (Avg-Acc 45.9821)\n",
            "Epoch: [32][36/39]\tTime 0.084 (Avg-Time 0.061)\t Loss 1.4974 (Avg-Loss 1.5180)\tAcc 47.0703 (Avg-Acc 45.9961)\n",
            "Epoch: [32][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.4641 (Avg-Loss 1.5173)\tAcc 57.8125 (Avg-Acc 46.0450)\n",
            "EPOCH: 32 train Results: Acc 46.045 Loss: 1.5173\n",
            "Epoch: [32][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4662 (Avg-Loss 1.4662)\tAcc 49.4141 (Avg-Acc 49.4141)\n",
            "Epoch: [32][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.4777 (Avg-Loss 1.4727)\tAcc 48.7245 (Avg-Acc 48.3000)\n",
            "EPOCH: 32 Validation Results: Acc 48.300 Loss: 1.4727\n",
            "Best Accuracy: 48.3000\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [33][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.4870 (Avg-Loss 1.4870)\tAcc 46.0938 (Avg-Acc 46.0938)\n",
            "Epoch: [33][9/39]\tTime 0.060 (Avg-Time 0.059)\t Loss 1.4903 (Avg-Loss 1.5111)\tAcc 45.2148 (Avg-Acc 45.4004)\n",
            "Epoch: [33][18/39]\tTime 0.067 (Avg-Time 0.062)\t Loss 1.5062 (Avg-Loss 1.5080)\tAcc 47.0703 (Avg-Acc 45.8882)\n",
            "Epoch: [33][27/39]\tTime 0.069 (Avg-Time 0.064)\t Loss 1.4959 (Avg-Loss 1.5059)\tAcc 47.1680 (Avg-Acc 46.1217)\n",
            "Epoch: [33][36/39]\tTime 0.057 (Avg-Time 0.062)\t Loss 1.5137 (Avg-Loss 1.5072)\tAcc 48.2422 (Avg-Acc 46.1201)\n",
            "Epoch: [33][39/39]\tTime 0.006 (Avg-Time 0.061)\t Loss 1.5204 (Avg-Loss 1.5068)\tAcc 40.6250 (Avg-Acc 46.1550)\n",
            "EPOCH: 33 train Results: Acc 46.155 Loss: 1.5068\n",
            "Epoch: [33][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4616 (Avg-Loss 1.4616)\tAcc 49.7070 (Avg-Acc 49.7070)\n",
            "Epoch: [33][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.4710 (Avg-Loss 1.4690)\tAcc 49.2347 (Avg-Acc 48.3900)\n",
            "EPOCH: 33 Validation Results: Acc 48.390 Loss: 1.4690\n",
            "Best Accuracy: 48.3900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [34][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.4430 (Avg-Loss 1.4430)\tAcc 50.8789 (Avg-Acc 50.8789)\n",
            "Epoch: [34][9/39]\tTime 0.057 (Avg-Time 0.062)\t Loss 1.4779 (Avg-Loss 1.5090)\tAcc 46.7773 (Avg-Acc 46.9922)\n",
            "Epoch: [34][18/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.4708 (Avg-Loss 1.5025)\tAcc 48.4375 (Avg-Acc 47.2605)\n",
            "Epoch: [34][27/39]\tTime 0.059 (Avg-Time 0.061)\t Loss 1.5119 (Avg-Loss 1.5012)\tAcc 46.8750 (Avg-Acc 47.1157)\n",
            "Epoch: [34][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.4956 (Avg-Loss 1.4973)\tAcc 46.7773 (Avg-Acc 47.1152)\n",
            "Epoch: [34][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.4995 (Avg-Loss 1.4996)\tAcc 43.7500 (Avg-Acc 47.1100)\n",
            "EPOCH: 34 train Results: Acc 47.110 Loss: 1.4996\n",
            "Epoch: [34][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4529 (Avg-Loss 1.4529)\tAcc 50.0977 (Avg-Acc 50.0977)\n",
            "Epoch: [34][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.4641 (Avg-Loss 1.4621)\tAcc 48.7245 (Avg-Acc 48.5200)\n",
            "EPOCH: 34 Validation Results: Acc 48.520 Loss: 1.4621\n",
            "Best Accuracy: 48.5200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [35][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.5072 (Avg-Loss 1.5072)\tAcc 46.8750 (Avg-Acc 46.8750)\n",
            "Epoch: [35][9/39]\tTime 0.057 (Avg-Time 0.058)\t Loss 1.4892 (Avg-Loss 1.4949)\tAcc 46.2891 (Avg-Acc 46.7188)\n",
            "Epoch: [35][18/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.4553 (Avg-Loss 1.4967)\tAcc 49.1211 (Avg-Acc 46.6591)\n",
            "Epoch: [35][27/39]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.5316 (Avg-Loss 1.4917)\tAcc 46.1914 (Avg-Acc 47.0180)\n",
            "Epoch: [35][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.5285 (Avg-Loss 1.4930)\tAcc 46.1914 (Avg-Acc 46.9753)\n",
            "Epoch: [35][39/39]\tTime 0.013 (Avg-Time 0.062)\t Loss 1.8889 (Avg-Loss 1.4958)\tAcc 32.8125 (Avg-Acc 46.8975)\n",
            "EPOCH: 35 train Results: Acc 46.898 Loss: 1.4958\n",
            "Epoch: [35][0/9]\tTime 0.032 (Avg-Time 0.032)\t Loss 1.4456 (Avg-Loss 1.4456)\tAcc 49.6094 (Avg-Acc 49.6094)\n",
            "Epoch: [35][9/9]\tTime 0.055 (Avg-Time 0.033)\t Loss 1.4589 (Avg-Loss 1.4569)\tAcc 48.0867 (Avg-Acc 48.4800)\n",
            "EPOCH: 35 Validation Results: Acc 48.480 Loss: 1.4569\n",
            "Best Accuracy: 48.5200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [36][0/39]\tTime 0.301 (Avg-Time 0.301)\t Loss 1.5505 (Avg-Loss 1.5505)\tAcc 45.1172 (Avg-Acc 45.1172)\n",
            "Epoch: [36][9/39]\tTime 0.124 (Avg-Time 0.200)\t Loss 1.4944 (Avg-Loss 1.4978)\tAcc 46.6797 (Avg-Acc 46.9629)\n",
            "Epoch: [36][18/39]\tTime 0.058 (Avg-Time 0.152)\t Loss 1.3958 (Avg-Loss 1.4903)\tAcc 50.6836 (Avg-Acc 46.8390)\n",
            "Epoch: [36][27/39]\tTime 0.060 (Avg-Time 0.123)\t Loss 1.5153 (Avg-Loss 1.4879)\tAcc 45.5078 (Avg-Acc 47.0878)\n",
            "Epoch: [36][36/39]\tTime 0.058 (Avg-Time 0.108)\t Loss 1.4888 (Avg-Loss 1.4898)\tAcc 47.1680 (Avg-Acc 47.1152)\n",
            "Epoch: [36][39/39]\tTime 0.006 (Avg-Time 0.103)\t Loss 1.3721 (Avg-Loss 1.4900)\tAcc 50.0000 (Avg-Acc 47.0950)\n",
            "EPOCH: 36 train Results: Acc 47.095 Loss: 1.4900\n",
            "Epoch: [36][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4428 (Avg-Loss 1.4428)\tAcc 50.1953 (Avg-Acc 50.1953)\n",
            "Epoch: [36][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.4527 (Avg-Loss 1.4531)\tAcc 48.0867 (Avg-Acc 48.7300)\n",
            "EPOCH: 36 Validation Results: Acc 48.730 Loss: 1.4531\n",
            "Best Accuracy: 48.7300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [37][0/39]\tTime 0.084 (Avg-Time 0.084)\t Loss 1.4717 (Avg-Loss 1.4717)\tAcc 47.5586 (Avg-Acc 47.5586)\n",
            "Epoch: [37][9/39]\tTime 0.057 (Avg-Time 0.063)\t Loss 1.4917 (Avg-Loss 1.4724)\tAcc 48.4375 (Avg-Acc 47.3340)\n",
            "Epoch: [37][18/39]\tTime 0.059 (Avg-Time 0.062)\t Loss 1.5557 (Avg-Loss 1.4832)\tAcc 44.8242 (Avg-Acc 47.0395)\n",
            "Epoch: [37][27/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 1.5310 (Avg-Loss 1.4873)\tAcc 45.8008 (Avg-Acc 47.0110)\n",
            "Epoch: [37][36/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 1.4723 (Avg-Loss 1.4878)\tAcc 47.6562 (Avg-Acc 47.0439)\n",
            "Epoch: [37][39/39]\tTime 0.006 (Avg-Time 0.061)\t Loss 1.4822 (Avg-Loss 1.4869)\tAcc 46.8750 (Avg-Acc 47.0950)\n",
            "EPOCH: 37 train Results: Acc 47.095 Loss: 1.4869\n",
            "Epoch: [37][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.4416 (Avg-Loss 1.4416)\tAcc 50.1953 (Avg-Acc 50.1953)\n",
            "Epoch: [37][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.4464 (Avg-Loss 1.4491)\tAcc 48.8520 (Avg-Acc 48.9600)\n",
            "EPOCH: 37 Validation Results: Acc 48.960 Loss: 1.4491\n",
            "Best Accuracy: 48.9600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [38][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.5031 (Avg-Loss 1.5031)\tAcc 47.7539 (Avg-Acc 47.7539)\n",
            "Epoch: [38][9/39]\tTime 0.072 (Avg-Time 0.062)\t Loss 1.4814 (Avg-Loss 1.4819)\tAcc 45.6055 (Avg-Acc 47.4121)\n",
            "Epoch: [38][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.4314 (Avg-Loss 1.4714)\tAcc 47.2656 (Avg-Acc 47.8361)\n",
            "Epoch: [38][27/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.5667 (Avg-Loss 1.4726)\tAcc 43.7500 (Avg-Acc 47.7225)\n",
            "Epoch: [38][36/39]\tTime 0.064 (Avg-Time 0.061)\t Loss 1.4576 (Avg-Loss 1.4686)\tAcc 48.2422 (Avg-Acc 47.8410)\n",
            "Epoch: [38][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.3981 (Avg-Loss 1.4686)\tAcc 50.0000 (Avg-Acc 47.8550)\n",
            "EPOCH: 38 train Results: Acc 47.855 Loss: 1.4686\n",
            "Epoch: [38][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.4274 (Avg-Loss 1.4274)\tAcc 51.2695 (Avg-Acc 51.2695)\n",
            "Epoch: [38][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.4423 (Avg-Loss 1.4409)\tAcc 49.3622 (Avg-Acc 49.3100)\n",
            "EPOCH: 38 Validation Results: Acc 49.310 Loss: 1.4409\n",
            "Best Accuracy: 49.3100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [39][0/39]\tTime 0.063 (Avg-Time 0.063)\t Loss 1.4653 (Avg-Loss 1.4653)\tAcc 47.8516 (Avg-Acc 47.8516)\n",
            "Epoch: [39][9/39]\tTime 0.056 (Avg-Time 0.062)\t Loss 1.4596 (Avg-Loss 1.4697)\tAcc 49.9023 (Avg-Acc 47.9199)\n",
            "Epoch: [39][18/39]\tTime 0.060 (Avg-Time 0.061)\t Loss 1.4854 (Avg-Loss 1.4759)\tAcc 46.8750 (Avg-Acc 47.8978)\n",
            "Epoch: [39][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.4863 (Avg-Loss 1.4708)\tAcc 47.1680 (Avg-Acc 47.9667)\n",
            "Epoch: [39][36/39]\tTime 0.076 (Avg-Time 0.061)\t Loss 1.4063 (Avg-Loss 1.4684)\tAcc 48.9258 (Avg-Acc 48.0310)\n",
            "Epoch: [39][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.5500 (Avg-Loss 1.4685)\tAcc 48.4375 (Avg-Acc 47.9525)\n",
            "EPOCH: 39 train Results: Acc 47.953 Loss: 1.4685\n",
            "Epoch: [39][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4224 (Avg-Loss 1.4224)\tAcc 51.3672 (Avg-Acc 51.3672)\n",
            "Epoch: [39][9/9]\tTime 0.011 (Avg-Time 0.017)\t Loss 1.4428 (Avg-Loss 1.4380)\tAcc 48.3418 (Avg-Acc 49.0600)\n",
            "EPOCH: 39 Validation Results: Acc 49.060 Loss: 1.4380\n",
            "Best Accuracy: 49.3100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [40][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.4915 (Avg-Loss 1.4915)\tAcc 45.3125 (Avg-Acc 45.3125)\n",
            "Epoch: [40][9/39]\tTime 0.055 (Avg-Time 0.059)\t Loss 1.4126 (Avg-Loss 1.4640)\tAcc 50.2930 (Avg-Acc 48.0469)\n",
            "Epoch: [40][18/39]\tTime 0.310 (Avg-Time 0.096)\t Loss 1.4083 (Avg-Loss 1.4605)\tAcc 50.4883 (Avg-Acc 48.0315)\n",
            "Epoch: [40][27/39]\tTime 0.143 (Avg-Time 0.121)\t Loss 1.4115 (Avg-Loss 1.4596)\tAcc 50.0977 (Avg-Acc 48.0120)\n",
            "Epoch: [40][36/39]\tTime 0.061 (Avg-Time 0.115)\t Loss 1.4731 (Avg-Loss 1.4572)\tAcc 47.5586 (Avg-Acc 48.0020)\n",
            "Epoch: [40][39/39]\tTime 0.006 (Avg-Time 0.109)\t Loss 1.7722 (Avg-Loss 1.4576)\tAcc 31.2500 (Avg-Acc 47.9575)\n",
            "EPOCH: 40 train Results: Acc 47.958 Loss: 1.4576\n",
            "Epoch: [40][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4190 (Avg-Loss 1.4190)\tAcc 51.3672 (Avg-Acc 51.3672)\n",
            "Epoch: [40][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.4369 (Avg-Loss 1.4332)\tAcc 48.9796 (Avg-Acc 49.1700)\n",
            "EPOCH: 40 Validation Results: Acc 49.170 Loss: 1.4332\n",
            "Best Accuracy: 49.3100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [41][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.5323 (Avg-Loss 1.5323)\tAcc 45.7031 (Avg-Acc 45.7031)\n",
            "Epoch: [41][9/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.5079 (Avg-Loss 1.4557)\tAcc 45.9961 (Avg-Acc 48.4375)\n",
            "Epoch: [41][18/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 1.4295 (Avg-Loss 1.4577)\tAcc 49.6094 (Avg-Acc 48.3039)\n",
            "Epoch: [41][27/39]\tTime 0.061 (Avg-Time 0.062)\t Loss 1.4261 (Avg-Loss 1.4508)\tAcc 47.9492 (Avg-Acc 48.3050)\n",
            "Epoch: [41][36/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.4733 (Avg-Loss 1.4509)\tAcc 48.8281 (Avg-Acc 48.4613)\n",
            "Epoch: [41][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.6279 (Avg-Loss 1.4503)\tAcc 39.0625 (Avg-Acc 48.4475)\n",
            "EPOCH: 41 train Results: Acc 48.447 Loss: 1.4503\n",
            "Epoch: [41][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4152 (Avg-Loss 1.4152)\tAcc 51.8555 (Avg-Acc 51.8555)\n",
            "Epoch: [41][9/9]\tTime 0.011 (Avg-Time 0.017)\t Loss 1.4334 (Avg-Loss 1.4297)\tAcc 49.6173 (Avg-Acc 49.4900)\n",
            "EPOCH: 41 Validation Results: Acc 49.490 Loss: 1.4297\n",
            "Best Accuracy: 49.4900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [42][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.4621 (Avg-Loss 1.4621)\tAcc 47.5586 (Avg-Acc 47.5586)\n",
            "Epoch: [42][9/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.4340 (Avg-Loss 1.4488)\tAcc 52.0508 (Avg-Acc 48.8379)\n",
            "Epoch: [42][18/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.4246 (Avg-Loss 1.4513)\tAcc 51.0742 (Avg-Acc 48.6688)\n",
            "Epoch: [42][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.4890 (Avg-Loss 1.4488)\tAcc 47.4609 (Avg-Acc 48.6154)\n",
            "Epoch: [42][36/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.3982 (Avg-Loss 1.4462)\tAcc 52.7344 (Avg-Acc 48.6592)\n",
            "Epoch: [42][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.3710 (Avg-Loss 1.4454)\tAcc 53.1250 (Avg-Acc 48.6725)\n",
            "EPOCH: 42 train Results: Acc 48.672 Loss: 1.4454\n",
            "Epoch: [42][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4091 (Avg-Loss 1.4091)\tAcc 51.2695 (Avg-Acc 51.2695)\n",
            "Epoch: [42][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.4251 (Avg-Loss 1.4250)\tAcc 49.7449 (Avg-Acc 49.3300)\n",
            "EPOCH: 42 Validation Results: Acc 49.330 Loss: 1.4250\n",
            "Best Accuracy: 49.4900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [43][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.4711 (Avg-Loss 1.4711)\tAcc 47.8516 (Avg-Acc 47.8516)\n",
            "Epoch: [43][9/39]\tTime 0.062 (Avg-Time 0.060)\t Loss 1.4009 (Avg-Loss 1.4497)\tAcc 50.1953 (Avg-Acc 48.5547)\n",
            "Epoch: [43][18/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.4240 (Avg-Loss 1.4429)\tAcc 49.8047 (Avg-Acc 48.7253)\n",
            "Epoch: [43][27/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.4421 (Avg-Loss 1.4427)\tAcc 49.8047 (Avg-Acc 48.7409)\n",
            "Epoch: [43][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.4557 (Avg-Loss 1.4419)\tAcc 50.0000 (Avg-Acc 48.8149)\n",
            "Epoch: [43][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.6958 (Avg-Loss 1.4419)\tAcc 32.8125 (Avg-Acc 48.7800)\n",
            "EPOCH: 43 train Results: Acc 48.780 Loss: 1.4419\n",
            "Epoch: [43][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.4017 (Avg-Loss 1.4017)\tAcc 51.8555 (Avg-Acc 51.8555)\n",
            "Epoch: [43][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.4181 (Avg-Loss 1.4180)\tAcc 49.3622 (Avg-Acc 49.6400)\n",
            "EPOCH: 43 Validation Results: Acc 49.640 Loss: 1.4180\n",
            "Best Accuracy: 49.6400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [44][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.4383 (Avg-Loss 1.4383)\tAcc 48.0469 (Avg-Acc 48.0469)\n",
            "Epoch: [44][9/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.4415 (Avg-Loss 1.4390)\tAcc 48.6328 (Avg-Acc 48.7988)\n",
            "Epoch: [44][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.4982 (Avg-Loss 1.4378)\tAcc 48.2422 (Avg-Acc 48.9463)\n",
            "Epoch: [44][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.4766 (Avg-Loss 1.4392)\tAcc 46.7773 (Avg-Acc 49.1281)\n",
            "Epoch: [44][36/39]\tTime 0.185 (Avg-Time 0.070)\t Loss 1.4137 (Avg-Loss 1.4373)\tAcc 48.1445 (Avg-Acc 49.0314)\n",
            "Epoch: [44][39/39]\tTime 0.127 (Avg-Time 0.082)\t Loss 1.3559 (Avg-Loss 1.4374)\tAcc 56.2500 (Avg-Acc 49.0200)\n",
            "EPOCH: 44 train Results: Acc 49.020 Loss: 1.4374\n",
            "Epoch: [44][0/9]\tTime 0.027 (Avg-Time 0.027)\t Loss 1.4007 (Avg-Loss 1.4007)\tAcc 52.0508 (Avg-Acc 52.0508)\n",
            "Epoch: [44][9/9]\tTime 0.070 (Avg-Time 0.043)\t Loss 1.4150 (Avg-Loss 1.4155)\tAcc 50.2551 (Avg-Acc 50.1500)\n",
            "EPOCH: 44 Validation Results: Acc 50.150 Loss: 1.4155\n",
            "Best Accuracy: 50.1500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [45][0/39]\tTime 0.264 (Avg-Time 0.264)\t Loss 1.4187 (Avg-Loss 1.4187)\tAcc 49.2188 (Avg-Acc 49.2188)\n",
            "Epoch: [45][9/39]\tTime 0.104 (Avg-Time 0.147)\t Loss 1.4891 (Avg-Loss 1.4317)\tAcc 47.1680 (Avg-Acc 49.6777)\n",
            "Epoch: [45][18/39]\tTime 0.057 (Avg-Time 0.107)\t Loss 1.4231 (Avg-Loss 1.4322)\tAcc 48.6328 (Avg-Acc 49.4655)\n",
            "Epoch: [45][27/39]\tTime 0.063 (Avg-Time 0.092)\t Loss 1.4206 (Avg-Loss 1.4334)\tAcc 49.3164 (Avg-Acc 49.2222)\n",
            "Epoch: [45][36/39]\tTime 0.060 (Avg-Time 0.084)\t Loss 1.4057 (Avg-Loss 1.4285)\tAcc 48.4375 (Avg-Acc 49.3164)\n",
            "Epoch: [45][39/39]\tTime 0.006 (Avg-Time 0.081)\t Loss 1.4014 (Avg-Loss 1.4268)\tAcc 53.1250 (Avg-Acc 49.3650)\n",
            "EPOCH: 45 train Results: Acc 49.365 Loss: 1.4268\n",
            "Epoch: [45][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.3977 (Avg-Loss 1.3977)\tAcc 52.1484 (Avg-Acc 52.1484)\n",
            "Epoch: [45][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.4116 (Avg-Loss 1.4129)\tAcc 49.4898 (Avg-Acc 49.9100)\n",
            "EPOCH: 45 Validation Results: Acc 49.910 Loss: 1.4129\n",
            "Best Accuracy: 50.1500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [46][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.4156 (Avg-Loss 1.4156)\tAcc 51.4648 (Avg-Acc 51.4648)\n",
            "Epoch: [46][9/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.4252 (Avg-Loss 1.4219)\tAcc 50.6836 (Avg-Acc 49.5801)\n",
            "Epoch: [46][18/39]\tTime 0.063 (Avg-Time 0.061)\t Loss 1.3844 (Avg-Loss 1.4242)\tAcc 50.8789 (Avg-Acc 49.3986)\n",
            "Epoch: [46][27/39]\tTime 0.059 (Avg-Time 0.061)\t Loss 1.4390 (Avg-Loss 1.4204)\tAcc 48.1445 (Avg-Acc 49.5710)\n",
            "Epoch: [46][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.4449 (Avg-Loss 1.4213)\tAcc 47.7539 (Avg-Acc 49.3798)\n",
            "Epoch: [46][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.5280 (Avg-Loss 1.4217)\tAcc 42.1875 (Avg-Acc 49.3300)\n",
            "EPOCH: 46 train Results: Acc 49.330 Loss: 1.4217\n",
            "Epoch: [46][0/9]\tTime 0.016 (Avg-Time 0.016)\t Loss 1.3858 (Avg-Loss 1.3858)\tAcc 52.2461 (Avg-Acc 52.2461)\n",
            "Epoch: [46][9/9]\tTime 0.011 (Avg-Time 0.018)\t Loss 1.4048 (Avg-Loss 1.4054)\tAcc 49.8724 (Avg-Acc 50.1200)\n",
            "EPOCH: 46 Validation Results: Acc 50.120 Loss: 1.4054\n",
            "Best Accuracy: 50.1500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [47][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.3760 (Avg-Loss 1.3760)\tAcc 51.4648 (Avg-Acc 51.4648)\n",
            "Epoch: [47][9/39]\tTime 0.059 (Avg-Time 0.058)\t Loss 1.4175 (Avg-Loss 1.4170)\tAcc 48.7305 (Avg-Acc 49.3164)\n",
            "Epoch: [47][18/39]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.4206 (Avg-Loss 1.4149)\tAcc 49.8047 (Avg-Acc 49.6299)\n",
            "Epoch: [47][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.3229 (Avg-Loss 1.4151)\tAcc 52.3438 (Avg-Acc 49.6896)\n",
            "Epoch: [47][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.4265 (Avg-Loss 1.4158)\tAcc 47.8516 (Avg-Acc 49.6754)\n",
            "Epoch: [47][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.6392 (Avg-Loss 1.4157)\tAcc 46.8750 (Avg-Acc 49.7200)\n",
            "EPOCH: 47 train Results: Acc 49.720 Loss: 1.4157\n",
            "Epoch: [47][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.3824 (Avg-Loss 1.3824)\tAcc 52.2461 (Avg-Acc 52.2461)\n",
            "Epoch: [47][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.3987 (Avg-Loss 1.4005)\tAcc 50.1276 (Avg-Acc 50.2800)\n",
            "EPOCH: 47 Validation Results: Acc 50.280 Loss: 1.4005\n",
            "Best Accuracy: 50.2800\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [48][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.4351 (Avg-Loss 1.4351)\tAcc 48.8281 (Avg-Acc 48.8281)\n",
            "Epoch: [48][9/39]\tTime 0.060 (Avg-Time 0.063)\t Loss 1.4097 (Avg-Loss 1.4104)\tAcc 49.5117 (Avg-Acc 49.6191)\n",
            "Epoch: [48][18/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.4351 (Avg-Loss 1.4133)\tAcc 50.2930 (Avg-Acc 49.9949)\n",
            "Epoch: [48][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.3999 (Avg-Loss 1.4115)\tAcc 51.7578 (Avg-Acc 49.9930)\n",
            "Epoch: [48][36/39]\tTime 0.059 (Avg-Time 0.061)\t Loss 1.3760 (Avg-Loss 1.4134)\tAcc 51.6602 (Avg-Acc 49.7545)\n",
            "Epoch: [48][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.4137 (Avg-Loss 1.4121)\tAcc 45.3125 (Avg-Acc 49.8700)\n",
            "EPOCH: 48 train Results: Acc 49.870 Loss: 1.4121\n",
            "Epoch: [48][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.3795 (Avg-Loss 1.3795)\tAcc 51.2695 (Avg-Acc 51.2695)\n",
            "Epoch: [48][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.3982 (Avg-Loss 1.3969)\tAcc 49.8724 (Avg-Acc 50.4800)\n",
            "EPOCH: 48 Validation Results: Acc 50.480 Loss: 1.3969\n",
            "Best Accuracy: 50.4800\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [49][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.4174 (Avg-Loss 1.4174)\tAcc 50.0977 (Avg-Acc 50.0977)\n",
            "Epoch: [49][9/39]\tTime 0.081 (Avg-Time 0.069)\t Loss 1.3593 (Avg-Loss 1.4110)\tAcc 50.4883 (Avg-Acc 50.2734)\n",
            "Epoch: [49][18/39]\tTime 0.113 (Avg-Time 0.139)\t Loss 1.4015 (Avg-Loss 1.3984)\tAcc 50.6836 (Avg-Acc 50.4420)\n",
            "Epoch: [49][27/39]\tTime 0.115 (Avg-Time 0.129)\t Loss 1.4092 (Avg-Loss 1.4011)\tAcc 49.5117 (Avg-Acc 50.2197)\n",
            "Epoch: [49][36/39]\tTime 0.057 (Avg-Time 0.112)\t Loss 1.4205 (Avg-Loss 1.4012)\tAcc 52.0508 (Avg-Acc 50.0607)\n",
            "Epoch: [49][39/39]\tTime 0.006 (Avg-Time 0.107)\t Loss 1.3704 (Avg-Loss 1.4032)\tAcc 54.6875 (Avg-Acc 49.9425)\n",
            "EPOCH: 49 train Results: Acc 49.943 Loss: 1.4032\n",
            "Epoch: [49][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.3731 (Avg-Loss 1.3731)\tAcc 52.0508 (Avg-Acc 52.0508)\n",
            "Epoch: [49][9/9]\tTime 0.011 (Avg-Time 0.017)\t Loss 1.3916 (Avg-Loss 1.3923)\tAcc 49.8724 (Avg-Acc 50.4900)\n",
            "EPOCH: 49 Validation Results: Acc 50.490 Loss: 1.3923\n",
            "Best Accuracy: 50.4900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [50][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.4023 (Avg-Loss 1.4023)\tAcc 49.2188 (Avg-Acc 49.2188)\n",
            "Epoch: [50][9/39]\tTime 0.056 (Avg-Time 0.058)\t Loss 1.3837 (Avg-Loss 1.3855)\tAcc 51.2695 (Avg-Acc 50.9180)\n",
            "Epoch: [50][18/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.4415 (Avg-Loss 1.3975)\tAcc 48.9258 (Avg-Acc 50.3187)\n",
            "Epoch: [50][27/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.3914 (Avg-Loss 1.3917)\tAcc 51.5625 (Avg-Acc 50.5301)\n",
            "Epoch: [50][36/39]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.3903 (Avg-Loss 1.3948)\tAcc 51.9531 (Avg-Acc 50.4091)\n",
            "Epoch: [50][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.5079 (Avg-Loss 1.3965)\tAcc 42.1875 (Avg-Acc 50.3650)\n",
            "EPOCH: 50 train Results: Acc 50.365 Loss: 1.3965\n",
            "Epoch: [50][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.3709 (Avg-Loss 1.3709)\tAcc 51.5625 (Avg-Acc 51.5625)\n",
            "Epoch: [50][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.3919 (Avg-Loss 1.3870)\tAcc 50.2551 (Avg-Acc 50.6100)\n",
            "EPOCH: 50 Validation Results: Acc 50.610 Loss: 1.3870\n",
            "Best Accuracy: 50.6100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [51][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.3910 (Avg-Loss 1.3910)\tAcc 50.2930 (Avg-Acc 50.2930)\n",
            "Epoch: [51][9/39]\tTime 0.069 (Avg-Time 0.059)\t Loss 1.3748 (Avg-Loss 1.3845)\tAcc 50.1953 (Avg-Acc 50.7324)\n",
            "Epoch: [51][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.3876 (Avg-Loss 1.3890)\tAcc 51.3672 (Avg-Acc 50.5191)\n",
            "Epoch: [51][27/39]\tTime 0.059 (Avg-Time 0.060)\t Loss 1.3849 (Avg-Loss 1.3863)\tAcc 52.2461 (Avg-Acc 50.7324)\n",
            "Epoch: [51][36/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.3955 (Avg-Loss 1.3890)\tAcc 51.5625 (Avg-Acc 50.7258)\n",
            "Epoch: [51][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.4714 (Avg-Loss 1.3910)\tAcc 53.1250 (Avg-Acc 50.6925)\n",
            "EPOCH: 51 train Results: Acc 50.693 Loss: 1.3910\n",
            "Epoch: [51][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.3667 (Avg-Loss 1.3667)\tAcc 52.2461 (Avg-Acc 52.2461)\n",
            "Epoch: [51][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.3828 (Avg-Loss 1.3835)\tAcc 51.1480 (Avg-Acc 50.5400)\n",
            "EPOCH: 51 Validation Results: Acc 50.540 Loss: 1.3835\n",
            "Best Accuracy: 50.6100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [52][0/39]\tTime 0.071 (Avg-Time 0.071)\t Loss 1.3672 (Avg-Loss 1.3672)\tAcc 50.0000 (Avg-Acc 50.0000)\n",
            "Epoch: [52][9/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.3530 (Avg-Loss 1.3839)\tAcc 51.3672 (Avg-Acc 50.2441)\n",
            "Epoch: [52][18/39]\tTime 0.078 (Avg-Time 0.061)\t Loss 1.4266 (Avg-Loss 1.3898)\tAcc 49.8047 (Avg-Acc 50.4215)\n",
            "Epoch: [52][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.3498 (Avg-Loss 1.3897)\tAcc 50.9766 (Avg-Acc 50.3836)\n",
            "Epoch: [52][36/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.3884 (Avg-Loss 1.3861)\tAcc 50.0977 (Avg-Acc 50.6519)\n",
            "Epoch: [52][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.3822 (Avg-Loss 1.3863)\tAcc 53.1250 (Avg-Acc 50.6300)\n",
            "EPOCH: 52 train Results: Acc 50.630 Loss: 1.3863\n",
            "Epoch: [52][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.3627 (Avg-Loss 1.3627)\tAcc 52.6367 (Avg-Acc 52.6367)\n",
            "Epoch: [52][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.3798 (Avg-Loss 1.3790)\tAcc 51.1480 (Avg-Acc 50.5500)\n",
            "EPOCH: 52 Validation Results: Acc 50.550 Loss: 1.3790\n",
            "Best Accuracy: 50.6100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [53][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.3475 (Avg-Loss 1.3475)\tAcc 54.8828 (Avg-Acc 54.8828)\n",
            "Epoch: [53][9/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.3700 (Avg-Loss 1.3726)\tAcc 51.7578 (Avg-Acc 51.7090)\n",
            "Epoch: [53][18/39]\tTime 0.058 (Avg-Time 0.059)\t Loss 1.3363 (Avg-Loss 1.3742)\tAcc 53.6133 (Avg-Acc 51.6036)\n",
            "Epoch: [53][27/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.4192 (Avg-Loss 1.3772)\tAcc 50.2930 (Avg-Acc 51.3463)\n",
            "Epoch: [53][36/39]\tTime 0.183 (Avg-Time 0.084)\t Loss 1.4281 (Avg-Loss 1.3811)\tAcc 48.4375 (Avg-Acc 51.1270)\n",
            "Epoch: [53][39/39]\tTime 0.015 (Avg-Time 0.093)\t Loss 1.5043 (Avg-Loss 1.3813)\tAcc 46.8750 (Avg-Acc 51.0150)\n",
            "EPOCH: 53 train Results: Acc 51.015 Loss: 1.3813\n",
            "Epoch: [53][0/9]\tTime 0.036 (Avg-Time 0.036)\t Loss 1.3550 (Avg-Loss 1.3550)\tAcc 52.3438 (Avg-Acc 52.3438)\n",
            "Epoch: [53][9/9]\tTime 0.027 (Avg-Time 0.032)\t Loss 1.3741 (Avg-Loss 1.3736)\tAcc 51.4031 (Avg-Acc 51.0200)\n",
            "EPOCH: 53 Validation Results: Acc 51.020 Loss: 1.3736\n",
            "Best Accuracy: 51.0200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [54][0/39]\tTime 0.141 (Avg-Time 0.141)\t Loss 1.3342 (Avg-Loss 1.3342)\tAcc 52.3438 (Avg-Acc 52.3438)\n",
            "Epoch: [54][9/39]\tTime 0.057 (Avg-Time 0.109)\t Loss 1.3454 (Avg-Loss 1.3809)\tAcc 53.1250 (Avg-Acc 51.5039)\n",
            "Epoch: [54][18/39]\tTime 0.058 (Avg-Time 0.086)\t Loss 1.3877 (Avg-Loss 1.3785)\tAcc 52.6367 (Avg-Acc 51.2901)\n",
            "Epoch: [54][27/39]\tTime 0.057 (Avg-Time 0.078)\t Loss 1.3699 (Avg-Loss 1.3780)\tAcc 50.3906 (Avg-Acc 50.9591)\n",
            "Epoch: [54][36/39]\tTime 0.057 (Avg-Time 0.073)\t Loss 1.3914 (Avg-Loss 1.3756)\tAcc 51.1719 (Avg-Acc 51.1244)\n",
            "Epoch: [54][39/39]\tTime 0.006 (Avg-Time 0.071)\t Loss 1.2105 (Avg-Loss 1.3765)\tAcc 67.1875 (Avg-Acc 51.0725)\n",
            "EPOCH: 54 train Results: Acc 51.072 Loss: 1.3765\n",
            "Epoch: [54][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.3562 (Avg-Loss 1.3562)\tAcc 52.3438 (Avg-Acc 52.3438)\n",
            "Epoch: [54][9/9]\tTime 0.012 (Avg-Time 0.018)\t Loss 1.3711 (Avg-Loss 1.3724)\tAcc 51.9133 (Avg-Acc 50.9200)\n",
            "EPOCH: 54 Validation Results: Acc 50.920 Loss: 1.3724\n",
            "Best Accuracy: 51.0200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [55][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.3470 (Avg-Loss 1.3470)\tAcc 53.7109 (Avg-Acc 53.7109)\n",
            "Epoch: [55][9/39]\tTime 0.058 (Avg-Time 0.059)\t Loss 1.4069 (Avg-Loss 1.3726)\tAcc 49.9023 (Avg-Acc 51.3574)\n",
            "Epoch: [55][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.4056 (Avg-Loss 1.3692)\tAcc 48.7305 (Avg-Acc 50.8326)\n",
            "Epoch: [55][27/39]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.3861 (Avg-Loss 1.3698)\tAcc 50.3906 (Avg-Acc 50.9835)\n",
            "Epoch: [55][36/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.4359 (Avg-Loss 1.3711)\tAcc 48.3398 (Avg-Acc 51.1059)\n",
            "Epoch: [55][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.4110 (Avg-Loss 1.3710)\tAcc 51.5625 (Avg-Acc 51.1250)\n",
            "EPOCH: 55 train Results: Acc 51.125 Loss: 1.3710\n",
            "Epoch: [55][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.3487 (Avg-Loss 1.3487)\tAcc 52.2461 (Avg-Acc 52.2461)\n",
            "Epoch: [55][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.3715 (Avg-Loss 1.3657)\tAcc 51.9133 (Avg-Acc 51.2400)\n",
            "EPOCH: 55 Validation Results: Acc 51.240 Loss: 1.3657\n",
            "Best Accuracy: 51.2400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [56][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.3133 (Avg-Loss 1.3133)\tAcc 52.1484 (Avg-Acc 52.1484)\n",
            "Epoch: [56][9/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 1.3019 (Avg-Loss 1.3651)\tAcc 52.1484 (Avg-Acc 51.7285)\n",
            "Epoch: [56][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.3676 (Avg-Loss 1.3735)\tAcc 52.2461 (Avg-Acc 51.4443)\n",
            "Epoch: [56][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.3527 (Avg-Loss 1.3693)\tAcc 50.9766 (Avg-Acc 51.5834)\n",
            "Epoch: [56][36/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.3823 (Avg-Loss 1.3686)\tAcc 49.8047 (Avg-Acc 51.4121)\n",
            "Epoch: [56][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.3629 (Avg-Loss 1.3659)\tAcc 51.5625 (Avg-Acc 51.5550)\n",
            "EPOCH: 56 train Results: Acc 51.555 Loss: 1.3659\n",
            "Epoch: [56][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.3437 (Avg-Loss 1.3437)\tAcc 53.3203 (Avg-Acc 53.3203)\n",
            "Epoch: [56][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.3643 (Avg-Loss 1.3620)\tAcc 52.1684 (Avg-Acc 51.6000)\n",
            "EPOCH: 56 Validation Results: Acc 51.600 Loss: 1.3620\n",
            "Best Accuracy: 51.6000\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [57][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.2947 (Avg-Loss 1.2947)\tAcc 54.3945 (Avg-Acc 54.3945)\n",
            "Epoch: [57][9/39]\tTime 0.057 (Avg-Time 0.062)\t Loss 1.3592 (Avg-Loss 1.3475)\tAcc 51.0742 (Avg-Acc 51.9824)\n",
            "Epoch: [57][18/39]\tTime 0.072 (Avg-Time 0.062)\t Loss 1.4166 (Avg-Loss 1.3573)\tAcc 49.7070 (Avg-Acc 51.7887)\n",
            "Epoch: [57][27/39]\tTime 0.062 (Avg-Time 0.061)\t Loss 1.3688 (Avg-Loss 1.3584)\tAcc 52.2461 (Avg-Acc 51.7683)\n",
            "Epoch: [57][36/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.3722 (Avg-Loss 1.3615)\tAcc 51.6602 (Avg-Acc 51.7024)\n",
            "Epoch: [57][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.2674 (Avg-Loss 1.3620)\tAcc 57.8125 (Avg-Acc 51.6575)\n",
            "EPOCH: 57 train Results: Acc 51.657 Loss: 1.3620\n",
            "Epoch: [57][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.3417 (Avg-Loss 1.3417)\tAcc 53.0273 (Avg-Acc 53.0273)\n",
            "Epoch: [57][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.3607 (Avg-Loss 1.3607)\tAcc 52.0408 (Avg-Acc 51.3900)\n",
            "EPOCH: 57 Validation Results: Acc 51.390 Loss: 1.3607\n",
            "Best Accuracy: 51.6000\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [58][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.4013 (Avg-Loss 1.4013)\tAcc 50.0000 (Avg-Acc 50.0000)\n",
            "Epoch: [58][9/39]\tTime 0.111 (Avg-Time 0.106)\t Loss 1.3681 (Avg-Loss 1.3522)\tAcc 50.0977 (Avg-Acc 51.9434)\n",
            "Epoch: [58][18/39]\tTime 0.132 (Avg-Time 0.135)\t Loss 1.3463 (Avg-Loss 1.3565)\tAcc 54.1992 (Avg-Acc 52.1330)\n",
            "Epoch: [58][27/39]\tTime 0.057 (Avg-Time 0.131)\t Loss 1.3773 (Avg-Loss 1.3550)\tAcc 49.0234 (Avg-Acc 52.0368)\n",
            "Epoch: [58][36/39]\tTime 0.060 (Avg-Time 0.114)\t Loss 1.3554 (Avg-Loss 1.3539)\tAcc 50.2930 (Avg-Acc 52.0165)\n",
            "Epoch: [58][39/39]\tTime 0.006 (Avg-Time 0.109)\t Loss 1.4570 (Avg-Loss 1.3569)\tAcc 39.0625 (Avg-Acc 51.8475)\n",
            "EPOCH: 58 train Results: Acc 51.847 Loss: 1.3569\n",
            "Epoch: [58][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.3388 (Avg-Loss 1.3388)\tAcc 53.3203 (Avg-Acc 53.3203)\n",
            "Epoch: [58][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.3513 (Avg-Loss 1.3563)\tAcc 52.9337 (Avg-Acc 51.8600)\n",
            "EPOCH: 58 Validation Results: Acc 51.860 Loss: 1.3563\n",
            "Best Accuracy: 51.8600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [59][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.3321 (Avg-Loss 1.3321)\tAcc 52.9297 (Avg-Acc 52.9297)\n",
            "Epoch: [59][9/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.3378 (Avg-Loss 1.3426)\tAcc 53.7109 (Avg-Acc 52.7051)\n",
            "Epoch: [59][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2966 (Avg-Loss 1.3495)\tAcc 54.1016 (Avg-Acc 52.2358)\n",
            "Epoch: [59][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.3590 (Avg-Loss 1.3503)\tAcc 50.7812 (Avg-Acc 52.0961)\n",
            "Epoch: [59][36/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.3915 (Avg-Loss 1.3482)\tAcc 50.8789 (Avg-Acc 52.2435)\n",
            "Epoch: [59][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.2960 (Avg-Loss 1.3479)\tAcc 54.6875 (Avg-Acc 52.2625)\n",
            "EPOCH: 59 train Results: Acc 52.263 Loss: 1.3479\n",
            "Epoch: [59][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.3342 (Avg-Loss 1.3342)\tAcc 52.7344 (Avg-Acc 52.7344)\n",
            "Epoch: [59][9/9]\tTime 0.018 (Avg-Time 0.016)\t Loss 1.3472 (Avg-Loss 1.3502)\tAcc 52.9337 (Avg-Acc 51.8800)\n",
            "EPOCH: 59 Validation Results: Acc 51.880 Loss: 1.3502\n",
            "Best Accuracy: 51.8800\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [60][0/39]\tTime 0.083 (Avg-Time 0.083)\t Loss 1.4030 (Avg-Loss 1.4030)\tAcc 49.7070 (Avg-Acc 49.7070)\n",
            "Epoch: [60][9/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 1.3192 (Avg-Loss 1.3485)\tAcc 52.8320 (Avg-Acc 51.9141)\n",
            "Epoch: [60][18/39]\tTime 0.056 (Avg-Time 0.062)\t Loss 1.3573 (Avg-Loss 1.3465)\tAcc 50.6836 (Avg-Acc 51.9377)\n",
            "Epoch: [60][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.3033 (Avg-Loss 1.3406)\tAcc 54.5898 (Avg-Acc 52.2356)\n",
            "Epoch: [60][36/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.3578 (Avg-Loss 1.3420)\tAcc 49.6094 (Avg-Acc 52.1326)\n",
            "Epoch: [60][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.3199 (Avg-Loss 1.3417)\tAcc 56.2500 (Avg-Acc 52.1775)\n",
            "EPOCH: 60 train Results: Acc 52.178 Loss: 1.3417\n",
            "Epoch: [60][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.3294 (Avg-Loss 1.3294)\tAcc 53.6133 (Avg-Acc 53.6133)\n",
            "Epoch: [60][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.3471 (Avg-Loss 1.3495)\tAcc 53.1888 (Avg-Acc 51.7200)\n",
            "EPOCH: 60 Validation Results: Acc 51.720 Loss: 1.3495\n",
            "Best Accuracy: 51.8800\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [61][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.3214 (Avg-Loss 1.3214)\tAcc 52.8320 (Avg-Acc 52.8320)\n",
            "Epoch: [61][9/39]\tTime 0.055 (Avg-Time 0.058)\t Loss 1.3326 (Avg-Loss 1.3352)\tAcc 52.5391 (Avg-Acc 51.9727)\n",
            "Epoch: [61][18/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.3229 (Avg-Loss 1.3370)\tAcc 52.7344 (Avg-Acc 51.9531)\n",
            "Epoch: [61][27/39]\tTime 0.079 (Avg-Time 0.059)\t Loss 1.3245 (Avg-Loss 1.3377)\tAcc 54.0039 (Avg-Acc 52.2600)\n",
            "Epoch: [61][36/39]\tTime 0.055 (Avg-Time 0.059)\t Loss 1.3169 (Avg-Loss 1.3381)\tAcc 53.4180 (Avg-Acc 52.3543)\n",
            "Epoch: [61][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.2930 (Avg-Loss 1.3377)\tAcc 51.5625 (Avg-Acc 52.3275)\n",
            "EPOCH: 61 train Results: Acc 52.328 Loss: 1.3377\n",
            "Epoch: [61][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.3275 (Avg-Loss 1.3275)\tAcc 53.2227 (Avg-Acc 53.2227)\n",
            "Epoch: [61][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.3366 (Avg-Loss 1.3440)\tAcc 52.2959 (Avg-Acc 51.6600)\n",
            "EPOCH: 61 Validation Results: Acc 51.660 Loss: 1.3440\n",
            "Best Accuracy: 51.8800\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [62][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.3709 (Avg-Loss 1.3709)\tAcc 50.0977 (Avg-Acc 50.0977)\n",
            "Epoch: [62][9/39]\tTime 0.056 (Avg-Time 0.065)\t Loss 1.3195 (Avg-Loss 1.3362)\tAcc 53.9062 (Avg-Acc 52.1289)\n",
            "Epoch: [62][18/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 1.3120 (Avg-Loss 1.3351)\tAcc 53.1250 (Avg-Acc 52.4208)\n",
            "Epoch: [62][27/39]\tTime 0.086 (Avg-Time 0.066)\t Loss 1.3455 (Avg-Loss 1.3353)\tAcc 52.8320 (Avg-Acc 52.6297)\n",
            "Epoch: [62][36/39]\tTime 0.096 (Avg-Time 0.086)\t Loss 1.3582 (Avg-Loss 1.3339)\tAcc 50.0977 (Avg-Acc 52.4995)\n",
            "Epoch: [62][39/39]\tTime 0.016 (Avg-Time 0.093)\t Loss 1.1338 (Avg-Loss 1.3345)\tAcc 59.3750 (Avg-Acc 52.4725)\n",
            "EPOCH: 62 train Results: Acc 52.472 Loss: 1.3345\n",
            "Epoch: [62][0/9]\tTime 0.031 (Avg-Time 0.031)\t Loss 1.3273 (Avg-Loss 1.3273)\tAcc 54.2969 (Avg-Acc 54.2969)\n",
            "Epoch: [62][9/9]\tTime 0.023 (Avg-Time 0.026)\t Loss 1.3409 (Avg-Loss 1.3429)\tAcc 52.6786 (Avg-Acc 52.3000)\n",
            "EPOCH: 62 Validation Results: Acc 52.300 Loss: 1.3429\n",
            "Best Accuracy: 52.3000\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [63][0/39]\tTime 0.116 (Avg-Time 0.116)\t Loss 1.3790 (Avg-Loss 1.3790)\tAcc 51.2695 (Avg-Acc 51.2695)\n",
            "Epoch: [63][9/39]\tTime 0.056 (Avg-Time 0.110)\t Loss 1.3535 (Avg-Loss 1.3484)\tAcc 52.2461 (Avg-Acc 51.9727)\n",
            "Epoch: [63][18/39]\tTime 0.057 (Avg-Time 0.087)\t Loss 1.3339 (Avg-Loss 1.3351)\tAcc 50.7812 (Avg-Acc 52.3386)\n",
            "Epoch: [63][27/39]\tTime 0.059 (Avg-Time 0.078)\t Loss 1.3381 (Avg-Loss 1.3347)\tAcc 51.0742 (Avg-Acc 52.3193)\n",
            "Epoch: [63][36/39]\tTime 0.058 (Avg-Time 0.074)\t Loss 1.3398 (Avg-Loss 1.3309)\tAcc 53.9062 (Avg-Acc 52.5048)\n",
            "Epoch: [63][39/39]\tTime 0.006 (Avg-Time 0.072)\t Loss 1.5775 (Avg-Loss 1.3305)\tAcc 45.3125 (Avg-Acc 52.5875)\n",
            "EPOCH: 63 train Results: Acc 52.587 Loss: 1.3305\n",
            "Epoch: [63][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.3179 (Avg-Loss 1.3179)\tAcc 54.0039 (Avg-Acc 54.0039)\n",
            "Epoch: [63][9/9]\tTime 0.012 (Avg-Time 0.016)\t Loss 1.3309 (Avg-Loss 1.3352)\tAcc 53.4439 (Avg-Acc 52.4400)\n",
            "EPOCH: 63 Validation Results: Acc 52.440 Loss: 1.3352\n",
            "Best Accuracy: 52.4400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [64][0/39]\tTime 0.063 (Avg-Time 0.063)\t Loss 1.3314 (Avg-Loss 1.3314)\tAcc 51.1719 (Avg-Acc 51.1719)\n",
            "Epoch: [64][9/39]\tTime 0.079 (Avg-Time 0.061)\t Loss 1.3211 (Avg-Loss 1.3191)\tAcc 52.0508 (Avg-Acc 52.8125)\n",
            "Epoch: [64][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.3416 (Avg-Loss 1.3237)\tAcc 52.4414 (Avg-Acc 52.7087)\n",
            "Epoch: [64][27/39]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.3050 (Avg-Loss 1.3218)\tAcc 52.4414 (Avg-Acc 52.8495)\n",
            "Epoch: [64][36/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.2954 (Avg-Loss 1.3232)\tAcc 53.3203 (Avg-Acc 52.8109)\n",
            "Epoch: [64][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.3959 (Avg-Loss 1.3230)\tAcc 43.7500 (Avg-Acc 52.8625)\n",
            "EPOCH: 64 train Results: Acc 52.862 Loss: 1.3230\n",
            "Epoch: [64][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.3171 (Avg-Loss 1.3171)\tAcc 53.2227 (Avg-Acc 53.2227)\n",
            "Epoch: [64][9/9]\tTime 0.014 (Avg-Time 0.016)\t Loss 1.3208 (Avg-Loss 1.3338)\tAcc 53.3163 (Avg-Acc 52.2500)\n",
            "EPOCH: 64 Validation Results: Acc 52.250 Loss: 1.3338\n",
            "Best Accuracy: 52.4400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [65][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.3604 (Avg-Loss 1.3604)\tAcc 52.6367 (Avg-Acc 52.6367)\n",
            "Epoch: [65][9/39]\tTime 0.060 (Avg-Time 0.063)\t Loss 1.3624 (Avg-Loss 1.3475)\tAcc 52.9297 (Avg-Acc 51.9043)\n",
            "Epoch: [65][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2842 (Avg-Loss 1.3232)\tAcc 54.8828 (Avg-Acc 52.6881)\n",
            "Epoch: [65][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.3041 (Avg-Loss 1.3214)\tAcc 53.7109 (Avg-Acc 52.8809)\n",
            "Epoch: [65][36/39]\tTime 0.076 (Avg-Time 0.061)\t Loss 1.3217 (Avg-Loss 1.3209)\tAcc 52.3438 (Avg-Acc 52.8743)\n",
            "Epoch: [65][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.3074 (Avg-Loss 1.3207)\tAcc 57.8125 (Avg-Acc 52.8650)\n",
            "EPOCH: 65 train Results: Acc 52.865 Loss: 1.3207\n",
            "Epoch: [65][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.3097 (Avg-Loss 1.3097)\tAcc 53.4180 (Avg-Acc 53.4180)\n",
            "Epoch: [65][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.3231 (Avg-Loss 1.3285)\tAcc 53.3163 (Avg-Acc 52.2200)\n",
            "EPOCH: 65 Validation Results: Acc 52.220 Loss: 1.3285\n",
            "Best Accuracy: 52.4400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [66][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.3095 (Avg-Loss 1.3095)\tAcc 53.1250 (Avg-Acc 53.1250)\n",
            "Epoch: [66][9/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.2782 (Avg-Loss 1.3185)\tAcc 53.7109 (Avg-Acc 52.8516)\n",
            "Epoch: [66][18/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.3189 (Avg-Loss 1.3141)\tAcc 51.8555 (Avg-Acc 52.9297)\n",
            "Epoch: [66][27/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.3539 (Avg-Loss 1.3181)\tAcc 51.7578 (Avg-Acc 53.1599)\n",
            "Epoch: [66][36/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.2905 (Avg-Loss 1.3160)\tAcc 54.3945 (Avg-Acc 53.1382)\n",
            "Epoch: [66][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.1272 (Avg-Loss 1.3152)\tAcc 59.3750 (Avg-Acc 53.1300)\n",
            "EPOCH: 66 train Results: Acc 53.130 Loss: 1.3152\n",
            "Epoch: [66][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.3030 (Avg-Loss 1.3030)\tAcc 54.7852 (Avg-Acc 54.7852)\n",
            "Epoch: [66][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.3136 (Avg-Loss 1.3244)\tAcc 54.5918 (Avg-Acc 52.8600)\n",
            "EPOCH: 66 Validation Results: Acc 52.860 Loss: 1.3244\n",
            "Best Accuracy: 52.8600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [67][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.2478 (Avg-Loss 1.2478)\tAcc 55.3711 (Avg-Acc 55.3711)\n",
            "Epoch: [67][9/39]\tTime 0.118 (Avg-Time 0.080)\t Loss 1.3147 (Avg-Loss 1.3221)\tAcc 50.0977 (Avg-Acc 52.4805)\n",
            "Epoch: [67][18/39]\tTime 0.303 (Avg-Time 0.124)\t Loss 1.2884 (Avg-Loss 1.3136)\tAcc 52.5391 (Avg-Acc 52.9040)\n",
            "Epoch: [67][27/39]\tTime 0.128 (Avg-Time 0.127)\t Loss 1.3092 (Avg-Loss 1.3158)\tAcc 55.0781 (Avg-Acc 53.0169)\n",
            "Epoch: [67][36/39]\tTime 0.056 (Avg-Time 0.112)\t Loss 1.2660 (Avg-Loss 1.3135)\tAcc 53.7109 (Avg-Acc 53.0432)\n",
            "Epoch: [67][39/39]\tTime 0.006 (Avg-Time 0.107)\t Loss 1.4081 (Avg-Loss 1.3120)\tAcc 53.1250 (Avg-Acc 53.0725)\n",
            "EPOCH: 67 train Results: Acc 53.072 Loss: 1.3120\n",
            "Epoch: [67][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.3089 (Avg-Loss 1.3089)\tAcc 53.1250 (Avg-Acc 53.1250)\n",
            "Epoch: [67][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.3109 (Avg-Loss 1.3222)\tAcc 53.9541 (Avg-Acc 52.7200)\n",
            "EPOCH: 67 Validation Results: Acc 52.720 Loss: 1.3222\n",
            "Best Accuracy: 52.8600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [68][0/39]\tTime 0.062 (Avg-Time 0.062)\t Loss 1.3223 (Avg-Loss 1.3223)\tAcc 53.2227 (Avg-Acc 53.2227)\n",
            "Epoch: [68][9/39]\tTime 0.056 (Avg-Time 0.062)\t Loss 1.3001 (Avg-Loss 1.3109)\tAcc 53.4180 (Avg-Acc 53.2129)\n",
            "Epoch: [68][18/39]\tTime 0.078 (Avg-Time 0.061)\t Loss 1.3170 (Avg-Loss 1.3081)\tAcc 54.2969 (Avg-Acc 53.4334)\n",
            "Epoch: [68][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.2906 (Avg-Loss 1.3069)\tAcc 53.4180 (Avg-Acc 53.4947)\n",
            "Epoch: [68][36/39]\tTime 0.064 (Avg-Time 0.061)\t Loss 1.2883 (Avg-Loss 1.3046)\tAcc 53.1250 (Avg-Acc 53.5895)\n",
            "Epoch: [68][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.3305 (Avg-Loss 1.3052)\tAcc 43.7500 (Avg-Acc 53.5100)\n",
            "EPOCH: 68 train Results: Acc 53.510 Loss: 1.3052\n",
            "Epoch: [68][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2949 (Avg-Loss 1.2949)\tAcc 55.3711 (Avg-Acc 55.3711)\n",
            "Epoch: [68][9/9]\tTime 0.012 (Avg-Time 0.016)\t Loss 1.3078 (Avg-Loss 1.3198)\tAcc 53.3163 (Avg-Acc 52.7600)\n",
            "EPOCH: 68 Validation Results: Acc 52.760 Loss: 1.3198\n",
            "Best Accuracy: 52.8600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [69][0/39]\tTime 0.062 (Avg-Time 0.062)\t Loss 1.3162 (Avg-Loss 1.3162)\tAcc 54.1016 (Avg-Acc 54.1016)\n",
            "Epoch: [69][9/39]\tTime 0.058 (Avg-Time 0.059)\t Loss 1.3183 (Avg-Loss 1.3052)\tAcc 52.6367 (Avg-Acc 53.6914)\n",
            "Epoch: [69][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2947 (Avg-Loss 1.3025)\tAcc 53.3203 (Avg-Acc 53.5979)\n",
            "Epoch: [69][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2943 (Avg-Loss 1.3014)\tAcc 52.8320 (Avg-Acc 53.6586)\n",
            "Epoch: [69][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.3296 (Avg-Loss 1.3017)\tAcc 52.6367 (Avg-Acc 53.6555)\n",
            "Epoch: [69][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.4394 (Avg-Loss 1.3026)\tAcc 43.7500 (Avg-Acc 53.6250)\n",
            "EPOCH: 69 train Results: Acc 53.625 Loss: 1.3026\n",
            "Epoch: [69][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2982 (Avg-Loss 1.2982)\tAcc 53.5156 (Avg-Acc 53.5156)\n",
            "Epoch: [69][9/9]\tTime 0.012 (Avg-Time 0.015)\t Loss 1.3107 (Avg-Loss 1.3148)\tAcc 54.3367 (Avg-Acc 52.9000)\n",
            "EPOCH: 69 Validation Results: Acc 52.900 Loss: 1.3148\n",
            "Best Accuracy: 52.9000\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [70][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.2917 (Avg-Loss 1.2917)\tAcc 53.8086 (Avg-Acc 53.8086)\n",
            "Epoch: [70][9/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.3312 (Avg-Loss 1.2958)\tAcc 52.3438 (Avg-Acc 53.9355)\n",
            "Epoch: [70][18/39]\tTime 0.066 (Avg-Time 0.060)\t Loss 1.2738 (Avg-Loss 1.3000)\tAcc 52.6367 (Avg-Acc 53.7109)\n",
            "Epoch: [70][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.3215 (Avg-Loss 1.3034)\tAcc 51.4648 (Avg-Acc 53.5610)\n",
            "Epoch: [70][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2429 (Avg-Loss 1.2990)\tAcc 56.3477 (Avg-Acc 53.7426)\n",
            "Epoch: [70][39/39]\tTime 0.015 (Avg-Time 0.059)\t Loss 1.2896 (Avg-Loss 1.2979)\tAcc 62.5000 (Avg-Acc 53.7550)\n",
            "EPOCH: 70 train Results: Acc 53.755 Loss: 1.2979\n",
            "Epoch: [70][0/9]\tTime 0.024 (Avg-Time 0.024)\t Loss 1.3001 (Avg-Loss 1.3001)\tAcc 54.5898 (Avg-Acc 54.5898)\n",
            "Epoch: [70][9/9]\tTime 0.014 (Avg-Time 0.017)\t Loss 1.3076 (Avg-Loss 1.3156)\tAcc 53.4439 (Avg-Acc 52.8900)\n",
            "EPOCH: 70 Validation Results: Acc 52.890 Loss: 1.3156\n",
            "Best Accuracy: 52.9000\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [71][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.2784 (Avg-Loss 1.2784)\tAcc 54.4922 (Avg-Acc 54.4922)\n",
            "Epoch: [71][9/39]\tTime 0.056 (Avg-Time 0.058)\t Loss 1.3289 (Avg-Loss 1.2934)\tAcc 51.5625 (Avg-Acc 53.9453)\n",
            "Epoch: [71][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.3012 (Avg-Loss 1.2920)\tAcc 54.0039 (Avg-Acc 53.8806)\n",
            "Epoch: [71][27/39]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.2441 (Avg-Loss 1.2931)\tAcc 54.4922 (Avg-Acc 53.9097)\n",
            "Epoch: [71][36/39]\tTime 0.185 (Avg-Time 0.077)\t Loss 1.3284 (Avg-Loss 1.2929)\tAcc 51.9531 (Avg-Acc 54.0356)\n",
            "Epoch: [71][39/39]\tTime 0.137 (Avg-Time 0.088)\t Loss 1.0852 (Avg-Loss 1.2924)\tAcc 65.6250 (Avg-Acc 54.0100)\n",
            "EPOCH: 71 train Results: Acc 54.010 Loss: 1.2924\n",
            "Epoch: [71][0/9]\tTime 0.069 (Avg-Time 0.069)\t Loss 1.2912 (Avg-Loss 1.2912)\tAcc 55.0781 (Avg-Acc 55.0781)\n",
            "Epoch: [71][9/9]\tTime 0.027 (Avg-Time 0.051)\t Loss 1.3020 (Avg-Loss 1.3094)\tAcc 54.3367 (Avg-Acc 53.3700)\n",
            "EPOCH: 71 Validation Results: Acc 53.370 Loss: 1.3094\n",
            "Best Accuracy: 53.3700\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [72][0/39]\tTime 0.112 (Avg-Time 0.112)\t Loss 1.2940 (Avg-Loss 1.2940)\tAcc 54.3945 (Avg-Acc 54.3945)\n",
            "Epoch: [72][9/39]\tTime 0.103 (Avg-Time 0.108)\t Loss 1.3043 (Avg-Loss 1.2995)\tAcc 52.0508 (Avg-Acc 53.4668)\n",
            "Epoch: [72][18/39]\tTime 0.056 (Avg-Time 0.086)\t Loss 1.2743 (Avg-Loss 1.2962)\tAcc 55.6641 (Avg-Acc 53.7315)\n",
            "Epoch: [72][27/39]\tTime 0.074 (Avg-Time 0.078)\t Loss 1.2370 (Avg-Loss 1.2904)\tAcc 55.6641 (Avg-Acc 54.0946)\n",
            "Epoch: [72][36/39]\tTime 0.057 (Avg-Time 0.073)\t Loss 1.3190 (Avg-Loss 1.2898)\tAcc 54.5898 (Avg-Acc 54.1649)\n",
            "Epoch: [72][39/39]\tTime 0.006 (Avg-Time 0.070)\t Loss 1.3928 (Avg-Loss 1.2892)\tAcc 57.8125 (Avg-Acc 54.2775)\n",
            "EPOCH: 72 train Results: Acc 54.278 Loss: 1.2892\n",
            "Epoch: [72][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2885 (Avg-Loss 1.2885)\tAcc 54.8828 (Avg-Acc 54.8828)\n",
            "Epoch: [72][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.3077 (Avg-Loss 1.3068)\tAcc 54.3367 (Avg-Acc 53.2400)\n",
            "EPOCH: 72 Validation Results: Acc 53.240 Loss: 1.3068\n",
            "Best Accuracy: 53.3700\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [73][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.2237 (Avg-Loss 1.2237)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [73][9/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.3283 (Avg-Loss 1.2985)\tAcc 51.0742 (Avg-Acc 53.3203)\n",
            "Epoch: [73][18/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.2457 (Avg-Loss 1.2908)\tAcc 56.0547 (Avg-Acc 53.8651)\n",
            "Epoch: [73][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.3274 (Avg-Loss 1.2924)\tAcc 52.2461 (Avg-Acc 53.7667)\n",
            "Epoch: [73][36/39]\tTime 0.055 (Avg-Time 0.060)\t Loss 1.2631 (Avg-Loss 1.2924)\tAcc 54.1016 (Avg-Acc 53.9326)\n",
            "Epoch: [73][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.3263 (Avg-Loss 1.2922)\tAcc 56.2500 (Avg-Acc 53.9825)\n",
            "EPOCH: 73 train Results: Acc 53.983 Loss: 1.2922\n",
            "Epoch: [73][0/9]\tTime 0.017 (Avg-Time 0.017)\t Loss 1.2902 (Avg-Loss 1.2902)\tAcc 53.5156 (Avg-Acc 53.5156)\n",
            "Epoch: [73][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.3000 (Avg-Loss 1.3060)\tAcc 53.6990 (Avg-Acc 53.2600)\n",
            "EPOCH: 73 Validation Results: Acc 53.260 Loss: 1.3060\n",
            "Best Accuracy: 53.3700\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [74][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.2338 (Avg-Loss 1.2338)\tAcc 56.5430 (Avg-Acc 56.5430)\n",
            "Epoch: [74][9/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2582 (Avg-Loss 1.2712)\tAcc 54.9805 (Avg-Acc 53.9844)\n",
            "Epoch: [74][18/39]\tTime 0.062 (Avg-Time 0.060)\t Loss 1.2404 (Avg-Loss 1.2739)\tAcc 55.8594 (Avg-Acc 54.5333)\n",
            "Epoch: [74][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.2366 (Avg-Loss 1.2775)\tAcc 56.5430 (Avg-Acc 54.4538)\n",
            "Epoch: [74][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.2374 (Avg-Loss 1.2775)\tAcc 57.0312 (Avg-Acc 54.5793)\n",
            "Epoch: [74][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1998 (Avg-Loss 1.2780)\tAcc 53.1250 (Avg-Acc 54.5175)\n",
            "EPOCH: 74 train Results: Acc 54.517 Loss: 1.2780\n",
            "Epoch: [74][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2897 (Avg-Loss 1.2897)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [74][9/9]\tTime 0.016 (Avg-Time 0.016)\t Loss 1.2809 (Avg-Loss 1.2974)\tAcc 55.3571 (Avg-Acc 53.8100)\n",
            "EPOCH: 74 Validation Results: Acc 53.810 Loss: 1.2974\n",
            "Best Accuracy: 53.8100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [75][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.2726 (Avg-Loss 1.2726)\tAcc 53.7109 (Avg-Acc 53.7109)\n",
            "Epoch: [75][9/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2708 (Avg-Loss 1.2830)\tAcc 56.0547 (Avg-Acc 53.7793)\n",
            "Epoch: [75][18/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.2989 (Avg-Loss 1.2796)\tAcc 52.3438 (Avg-Acc 54.0913)\n",
            "Epoch: [75][27/39]\tTime 0.060 (Avg-Time 0.061)\t Loss 1.2655 (Avg-Loss 1.2810)\tAcc 54.0039 (Avg-Acc 54.1434)\n",
            "Epoch: [75][36/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.2778 (Avg-Loss 1.2816)\tAcc 53.1250 (Avg-Acc 54.1755)\n",
            "Epoch: [75][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.2619 (Avg-Loss 1.2819)\tAcc 56.2500 (Avg-Acc 54.2675)\n",
            "EPOCH: 75 train Results: Acc 54.267 Loss: 1.2819\n",
            "Epoch: [75][0/9]\tTime 0.018 (Avg-Time 0.018)\t Loss 1.2914 (Avg-Loss 1.2914)\tAcc 54.3945 (Avg-Acc 54.3945)\n",
            "Epoch: [75][9/9]\tTime 0.018 (Avg-Time 0.017)\t Loss 1.2979 (Avg-Loss 1.3030)\tAcc 54.3367 (Avg-Acc 53.4900)\n",
            "EPOCH: 75 Validation Results: Acc 53.490 Loss: 1.3030\n",
            "Best Accuracy: 53.8100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [76][0/39]\tTime 0.070 (Avg-Time 0.070)\t Loss 1.3011 (Avg-Loss 1.3011)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [76][9/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.2449 (Avg-Loss 1.2717)\tAcc 55.0781 (Avg-Acc 54.5898)\n",
            "Epoch: [76][18/39]\tTime 0.269 (Avg-Time 0.107)\t Loss 1.3064 (Avg-Loss 1.2723)\tAcc 54.5898 (Avg-Acc 54.6772)\n",
            "Epoch: [76][27/39]\tTime 0.146 (Avg-Time 0.120)\t Loss 1.2502 (Avg-Loss 1.2809)\tAcc 56.3477 (Avg-Acc 54.6073)\n",
            "Epoch: [76][36/39]\tTime 0.081 (Avg-Time 0.114)\t Loss 1.2802 (Avg-Loss 1.2772)\tAcc 53.4180 (Avg-Acc 54.7931)\n",
            "Epoch: [76][39/39]\tTime 0.006 (Avg-Time 0.109)\t Loss 1.3128 (Avg-Loss 1.2767)\tAcc 50.0000 (Avg-Acc 54.8125)\n",
            "EPOCH: 76 train Results: Acc 54.812 Loss: 1.2767\n",
            "Epoch: [76][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2855 (Avg-Loss 1.2855)\tAcc 56.1523 (Avg-Acc 56.1523)\n",
            "Epoch: [76][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2903 (Avg-Loss 1.2995)\tAcc 55.3571 (Avg-Acc 53.7200)\n",
            "EPOCH: 76 Validation Results: Acc 53.720 Loss: 1.2995\n",
            "Best Accuracy: 53.8100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [77][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.2904 (Avg-Loss 1.2904)\tAcc 54.0039 (Avg-Acc 54.0039)\n",
            "Epoch: [77][9/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.2873 (Avg-Loss 1.2666)\tAcc 56.0547 (Avg-Acc 55.4688)\n",
            "Epoch: [77][18/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.2826 (Avg-Loss 1.2740)\tAcc 57.2266 (Avg-Acc 55.0730)\n",
            "Epoch: [77][27/39]\tTime 0.059 (Avg-Time 0.060)\t Loss 1.3331 (Avg-Loss 1.2739)\tAcc 52.1484 (Avg-Acc 54.9386)\n",
            "Epoch: [77][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.2402 (Avg-Loss 1.2738)\tAcc 55.2734 (Avg-Acc 54.7772)\n",
            "Epoch: [77][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.0823 (Avg-Loss 1.2742)\tAcc 62.5000 (Avg-Acc 54.7350)\n",
            "EPOCH: 77 train Results: Acc 54.735 Loss: 1.2742\n",
            "Epoch: [77][0/9]\tTime 0.016 (Avg-Time 0.016)\t Loss 1.2898 (Avg-Loss 1.2898)\tAcc 54.2969 (Avg-Acc 54.2969)\n",
            "Epoch: [77][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2866 (Avg-Loss 1.2984)\tAcc 54.2092 (Avg-Acc 53.4600)\n",
            "EPOCH: 77 Validation Results: Acc 53.460 Loss: 1.2984\n",
            "Best Accuracy: 53.8100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [78][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.2786 (Avg-Loss 1.2786)\tAcc 53.0273 (Avg-Acc 53.0273)\n",
            "Epoch: [78][9/39]\tTime 0.056 (Avg-Time 0.063)\t Loss 1.2435 (Avg-Loss 1.2695)\tAcc 56.0547 (Avg-Acc 54.6582)\n",
            "Epoch: [78][18/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.2921 (Avg-Loss 1.2647)\tAcc 54.3945 (Avg-Acc 55.0267)\n",
            "Epoch: [78][27/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.2592 (Avg-Loss 1.2638)\tAcc 54.5898 (Avg-Acc 55.1828)\n",
            "Epoch: [78][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2217 (Avg-Loss 1.2648)\tAcc 56.0547 (Avg-Acc 55.0095)\n",
            "Epoch: [78][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.3273 (Avg-Loss 1.2664)\tAcc 59.3750 (Avg-Acc 55.0275)\n",
            "EPOCH: 78 train Results: Acc 55.028 Loss: 1.2664\n",
            "Epoch: [78][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2737 (Avg-Loss 1.2737)\tAcc 55.5664 (Avg-Acc 55.5664)\n",
            "Epoch: [78][9/9]\tTime 0.011 (Avg-Time 0.017)\t Loss 1.2845 (Avg-Loss 1.2947)\tAcc 55.6122 (Avg-Acc 53.7200)\n",
            "EPOCH: 78 Validation Results: Acc 53.720 Loss: 1.2947\n",
            "Best Accuracy: 53.8100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [79][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.2716 (Avg-Loss 1.2716)\tAcc 54.2969 (Avg-Acc 54.2969)\n",
            "Epoch: [79][9/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.2241 (Avg-Loss 1.2527)\tAcc 54.7852 (Avg-Acc 54.9414)\n",
            "Epoch: [79][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2467 (Avg-Loss 1.2633)\tAcc 56.2500 (Avg-Acc 54.6155)\n",
            "Epoch: [79][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2646 (Avg-Loss 1.2685)\tAcc 55.7617 (Avg-Acc 54.5306)\n",
            "Epoch: [79][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2739 (Avg-Loss 1.2681)\tAcc 54.3945 (Avg-Acc 54.4895)\n",
            "Epoch: [79][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.4610 (Avg-Loss 1.2676)\tAcc 51.5625 (Avg-Acc 54.4850)\n",
            "EPOCH: 79 train Results: Acc 54.485 Loss: 1.2676\n",
            "Epoch: [79][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2734 (Avg-Loss 1.2734)\tAcc 55.2734 (Avg-Acc 55.2734)\n",
            "Epoch: [79][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2802 (Avg-Loss 1.2909)\tAcc 55.6122 (Avg-Acc 53.9400)\n",
            "EPOCH: 79 Validation Results: Acc 53.940 Loss: 1.2909\n",
            "Best Accuracy: 53.9400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [80][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.2551 (Avg-Loss 1.2551)\tAcc 56.1523 (Avg-Acc 56.1523)\n",
            "Epoch: [80][9/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.3125 (Avg-Loss 1.2602)\tAcc 52.6367 (Avg-Acc 54.8242)\n",
            "Epoch: [80][18/39]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.3118 (Avg-Loss 1.2668)\tAcc 51.5625 (Avg-Acc 54.5847)\n",
            "Epoch: [80][27/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.2220 (Avg-Loss 1.2588)\tAcc 56.8359 (Avg-Acc 55.0398)\n",
            "Epoch: [80][36/39]\tTime 0.123 (Avg-Time 0.074)\t Loss 1.2755 (Avg-Loss 1.2618)\tAcc 55.8594 (Avg-Acc 54.9409)\n",
            "Epoch: [80][39/39]\tTime 0.172 (Avg-Time 0.088)\t Loss 1.2591 (Avg-Loss 1.2630)\tAcc 60.9375 (Avg-Acc 54.8850)\n",
            "EPOCH: 80 train Results: Acc 54.885 Loss: 1.2630\n",
            "Epoch: [80][0/9]\tTime 0.087 (Avg-Time 0.087)\t Loss 1.2645 (Avg-Loss 1.2645)\tAcc 56.5430 (Avg-Acc 56.5430)\n",
            "Epoch: [80][9/9]\tTime 0.024 (Avg-Time 0.050)\t Loss 1.2791 (Avg-Loss 1.2878)\tAcc 55.2296 (Avg-Acc 54.1700)\n",
            "EPOCH: 80 Validation Results: Acc 54.170 Loss: 1.2878\n",
            "Best Accuracy: 54.1700\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [81][0/39]\tTime 0.122 (Avg-Time 0.122)\t Loss 1.2269 (Avg-Loss 1.2269)\tAcc 54.9805 (Avg-Acc 54.9805)\n",
            "Epoch: [81][9/39]\tTime 0.057 (Avg-Time 0.112)\t Loss 1.3403 (Avg-Loss 1.2773)\tAcc 49.8047 (Avg-Acc 54.2090)\n",
            "Epoch: [81][18/39]\tTime 0.058 (Avg-Time 0.087)\t Loss 1.3028 (Avg-Loss 1.2752)\tAcc 54.1016 (Avg-Acc 54.6464)\n",
            "Epoch: [81][27/39]\tTime 0.057 (Avg-Time 0.079)\t Loss 1.3207 (Avg-Loss 1.2709)\tAcc 52.4414 (Avg-Acc 54.7956)\n",
            "Epoch: [81][36/39]\tTime 0.078 (Avg-Time 0.074)\t Loss 1.2200 (Avg-Loss 1.2633)\tAcc 58.4961 (Avg-Acc 55.0676)\n",
            "Epoch: [81][39/39]\tTime 0.006 (Avg-Time 0.072)\t Loss 1.2021 (Avg-Loss 1.2631)\tAcc 59.3750 (Avg-Acc 55.1050)\n",
            "EPOCH: 81 train Results: Acc 55.105 Loss: 1.2631\n",
            "Epoch: [81][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2688 (Avg-Loss 1.2688)\tAcc 55.5664 (Avg-Acc 55.5664)\n",
            "Epoch: [81][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2802 (Avg-Loss 1.2891)\tAcc 54.7194 (Avg-Acc 53.8200)\n",
            "EPOCH: 81 Validation Results: Acc 53.820 Loss: 1.2891\n",
            "Best Accuracy: 54.1700\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [82][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.2084 (Avg-Loss 1.2084)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [82][9/39]\tTime 0.057 (Avg-Time 0.064)\t Loss 1.2582 (Avg-Loss 1.2601)\tAcc 55.2734 (Avg-Acc 55.3906)\n",
            "Epoch: [82][18/39]\tTime 0.057 (Avg-Time 0.064)\t Loss 1.2566 (Avg-Loss 1.2510)\tAcc 55.5664 (Avg-Acc 55.7463)\n",
            "Epoch: [82][27/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 1.2798 (Avg-Loss 1.2467)\tAcc 53.5156 (Avg-Acc 55.8524)\n",
            "Epoch: [82][36/39]\tTime 0.060 (Avg-Time 0.062)\t Loss 1.1926 (Avg-Loss 1.2517)\tAcc 58.2031 (Avg-Acc 55.6509)\n",
            "Epoch: [82][39/39]\tTime 0.006 (Avg-Time 0.061)\t Loss 1.1824 (Avg-Loss 1.2516)\tAcc 57.8125 (Avg-Acc 55.6250)\n",
            "EPOCH: 82 train Results: Acc 55.625 Loss: 1.2516\n",
            "Epoch: [82][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2866 (Avg-Loss 1.2866)\tAcc 54.4922 (Avg-Acc 54.4922)\n",
            "Epoch: [82][9/9]\tTime 0.012 (Avg-Time 0.016)\t Loss 1.2842 (Avg-Loss 1.2892)\tAcc 56.1224 (Avg-Acc 53.8100)\n",
            "EPOCH: 82 Validation Results: Acc 53.810 Loss: 1.2892\n",
            "Best Accuracy: 54.1700\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [83][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.2744 (Avg-Loss 1.2744)\tAcc 55.5664 (Avg-Acc 55.5664)\n",
            "Epoch: [83][9/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2735 (Avg-Loss 1.2645)\tAcc 54.1992 (Avg-Acc 54.6680)\n",
            "Epoch: [83][18/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.3036 (Avg-Loss 1.2558)\tAcc 54.6875 (Avg-Acc 55.2477)\n",
            "Epoch: [83][27/39]\tTime 0.063 (Avg-Time 0.062)\t Loss 1.2289 (Avg-Loss 1.2537)\tAcc 56.2500 (Avg-Acc 55.1444)\n",
            "Epoch: [83][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2548 (Avg-Loss 1.2503)\tAcc 57.5195 (Avg-Acc 55.4160)\n",
            "Epoch: [83][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.3935 (Avg-Loss 1.2526)\tAcc 48.4375 (Avg-Acc 55.4050)\n",
            "EPOCH: 83 train Results: Acc 55.405 Loss: 1.2526\n",
            "Epoch: [83][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2704 (Avg-Loss 1.2704)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [83][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2795 (Avg-Loss 1.2831)\tAcc 55.8673 (Avg-Acc 54.1700)\n",
            "EPOCH: 83 Validation Results: Acc 54.170 Loss: 1.2831\n",
            "Best Accuracy: 54.1700\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [84][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.2654 (Avg-Loss 1.2654)\tAcc 54.2969 (Avg-Acc 54.2969)\n",
            "Epoch: [84][9/39]\tTime 0.058 (Avg-Time 0.059)\t Loss 1.2411 (Avg-Loss 1.2589)\tAcc 57.3242 (Avg-Acc 55.4785)\n",
            "Epoch: [84][18/39]\tTime 0.060 (Avg-Time 0.061)\t Loss 1.3245 (Avg-Loss 1.2557)\tAcc 50.4883 (Avg-Acc 55.0730)\n",
            "Epoch: [84][27/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.2159 (Avg-Loss 1.2490)\tAcc 56.7383 (Avg-Acc 55.3327)\n",
            "Epoch: [84][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1986 (Avg-Loss 1.2428)\tAcc 56.0547 (Avg-Acc 55.5770)\n",
            "Epoch: [84][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.2328 (Avg-Loss 1.2431)\tAcc 60.9375 (Avg-Acc 55.6325)\n",
            "EPOCH: 84 train Results: Acc 55.633 Loss: 1.2431\n",
            "Epoch: [84][0/9]\tTime 0.017 (Avg-Time 0.017)\t Loss 1.2567 (Avg-Loss 1.2567)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [84][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2739 (Avg-Loss 1.2761)\tAcc 54.4643 (Avg-Acc 54.3400)\n",
            "EPOCH: 84 Validation Results: Acc 54.340 Loss: 1.2761\n",
            "Best Accuracy: 54.3400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [85][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1994 (Avg-Loss 1.1994)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [85][9/39]\tTime 0.202 (Avg-Time 0.106)\t Loss 1.2851 (Avg-Loss 1.2412)\tAcc 55.8594 (Avg-Acc 56.1816)\n",
            "Epoch: [85][18/39]\tTime 0.087 (Avg-Time 0.147)\t Loss 1.2602 (Avg-Loss 1.2421)\tAcc 56.8359 (Avg-Acc 55.8799)\n",
            "Epoch: [85][27/39]\tTime 0.057 (Avg-Time 0.133)\t Loss 1.2608 (Avg-Loss 1.2477)\tAcc 55.8594 (Avg-Acc 55.6466)\n",
            "Epoch: [85][36/39]\tTime 0.056 (Avg-Time 0.115)\t Loss 1.1994 (Avg-Loss 1.2436)\tAcc 56.9336 (Avg-Acc 55.6509)\n",
            "Epoch: [85][39/39]\tTime 0.006 (Avg-Time 0.109)\t Loss 1.4733 (Avg-Loss 1.2450)\tAcc 48.4375 (Avg-Acc 55.6275)\n",
            "EPOCH: 85 train Results: Acc 55.627 Loss: 1.2450\n",
            "Epoch: [85][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2553 (Avg-Loss 1.2553)\tAcc 56.5430 (Avg-Acc 56.5430)\n",
            "Epoch: [85][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2719 (Avg-Loss 1.2761)\tAcc 55.3571 (Avg-Acc 54.6900)\n",
            "EPOCH: 85 Validation Results: Acc 54.690 Loss: 1.2761\n",
            "Best Accuracy: 54.6900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [86][0/39]\tTime 0.072 (Avg-Time 0.072)\t Loss 1.1839 (Avg-Loss 1.1839)\tAcc 58.6914 (Avg-Acc 58.6914)\n",
            "Epoch: [86][9/39]\tTime 0.062 (Avg-Time 0.061)\t Loss 1.1911 (Avg-Loss 1.2431)\tAcc 57.9102 (Avg-Acc 56.1230)\n",
            "Epoch: [86][18/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.2570 (Avg-Loss 1.2434)\tAcc 55.3711 (Avg-Acc 55.8851)\n",
            "Epoch: [86][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.2177 (Avg-Loss 1.2406)\tAcc 57.2266 (Avg-Acc 55.8524)\n",
            "Epoch: [86][36/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.2673 (Avg-Loss 1.2433)\tAcc 53.8086 (Avg-Acc 55.7168)\n",
            "Epoch: [86][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.5416 (Avg-Loss 1.2451)\tAcc 43.7500 (Avg-Acc 55.6375)\n",
            "EPOCH: 86 train Results: Acc 55.638 Loss: 1.2451\n",
            "Epoch: [86][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2742 (Avg-Loss 1.2742)\tAcc 55.6641 (Avg-Acc 55.6641)\n",
            "Epoch: [86][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2825 (Avg-Loss 1.2813)\tAcc 54.9745 (Avg-Acc 54.2200)\n",
            "EPOCH: 86 Validation Results: Acc 54.220 Loss: 1.2813\n",
            "Best Accuracy: 54.6900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [87][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.2776 (Avg-Loss 1.2776)\tAcc 55.3711 (Avg-Acc 55.3711)\n",
            "Epoch: [87][9/39]\tTime 0.060 (Avg-Time 0.059)\t Loss 1.2529 (Avg-Loss 1.2595)\tAcc 56.7383 (Avg-Acc 55.7324)\n",
            "Epoch: [87][18/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.2054 (Avg-Loss 1.2597)\tAcc 57.5195 (Avg-Acc 55.4071)\n",
            "Epoch: [87][27/39]\tTime 0.063 (Avg-Time 0.059)\t Loss 1.2483 (Avg-Loss 1.2506)\tAcc 57.9102 (Avg-Acc 55.5490)\n",
            "Epoch: [87][36/39]\tTime 0.058 (Avg-Time 0.059)\t Loss 1.2341 (Avg-Loss 1.2453)\tAcc 56.2500 (Avg-Acc 55.6641)\n",
            "Epoch: [87][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.3838 (Avg-Loss 1.2445)\tAcc 46.8750 (Avg-Acc 55.7750)\n",
            "EPOCH: 87 train Results: Acc 55.775 Loss: 1.2445\n",
            "Epoch: [87][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2539 (Avg-Loss 1.2539)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [87][9/9]\tTime 0.012 (Avg-Time 0.016)\t Loss 1.2779 (Avg-Loss 1.2726)\tAcc 55.8673 (Avg-Acc 54.6900)\n",
            "EPOCH: 87 Validation Results: Acc 54.690 Loss: 1.2726\n",
            "Best Accuracy: 54.6900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [88][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.2090 (Avg-Loss 1.2090)\tAcc 56.7383 (Avg-Acc 56.7383)\n",
            "Epoch: [88][9/39]\tTime 0.060 (Avg-Time 0.061)\t Loss 1.2832 (Avg-Loss 1.2416)\tAcc 53.4180 (Avg-Acc 55.9570)\n",
            "Epoch: [88][18/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.2093 (Avg-Loss 1.2419)\tAcc 57.4219 (Avg-Acc 55.8080)\n",
            "Epoch: [88][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1958 (Avg-Loss 1.2350)\tAcc 56.8359 (Avg-Acc 56.0198)\n",
            "Epoch: [88][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.2295 (Avg-Loss 1.2383)\tAcc 57.3242 (Avg-Acc 55.9359)\n",
            "Epoch: [88][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.5126 (Avg-Loss 1.2383)\tAcc 50.0000 (Avg-Acc 55.9200)\n",
            "EPOCH: 88 train Results: Acc 55.920 Loss: 1.2383\n",
            "Epoch: [88][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2575 (Avg-Loss 1.2575)\tAcc 55.4688 (Avg-Acc 55.4688)\n",
            "Epoch: [88][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2773 (Avg-Loss 1.2726)\tAcc 55.3571 (Avg-Acc 54.6700)\n",
            "EPOCH: 88 Validation Results: Acc 54.670 Loss: 1.2726\n",
            "Best Accuracy: 54.6900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [89][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1771 (Avg-Loss 1.1771)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [89][9/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.2259 (Avg-Loss 1.2286)\tAcc 55.4688 (Avg-Acc 56.1328)\n",
            "Epoch: [89][18/39]\tTime 0.066 (Avg-Time 0.061)\t Loss 1.2235 (Avg-Loss 1.2289)\tAcc 56.4453 (Avg-Acc 56.2449)\n",
            "Epoch: [89][27/39]\tTime 0.116 (Avg-Time 0.071)\t Loss 1.2511 (Avg-Loss 1.2317)\tAcc 55.4688 (Avg-Acc 56.1558)\n",
            "Epoch: [89][36/39]\tTime 0.099 (Avg-Time 0.103)\t Loss 1.2357 (Avg-Loss 1.2352)\tAcc 54.9805 (Avg-Acc 56.0494)\n",
            "Epoch: [89][39/39]\tTime 0.016 (Avg-Time 0.102)\t Loss 1.4859 (Avg-Loss 1.2345)\tAcc 37.5000 (Avg-Acc 56.0600)\n",
            "EPOCH: 89 train Results: Acc 56.060 Loss: 1.2345\n",
            "Epoch: [89][0/9]\tTime 0.024 (Avg-Time 0.024)\t Loss 1.2522 (Avg-Loss 1.2522)\tAcc 56.5430 (Avg-Acc 56.5430)\n",
            "Epoch: [89][9/9]\tTime 0.024 (Avg-Time 0.028)\t Loss 1.2722 (Avg-Loss 1.2738)\tAcc 56.7602 (Avg-Acc 54.8800)\n",
            "EPOCH: 89 Validation Results: Acc 54.880 Loss: 1.2738\n",
            "Best Accuracy: 54.8800\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [90][0/39]\tTime 0.155 (Avg-Time 0.155)\t Loss 1.2191 (Avg-Loss 1.2191)\tAcc 57.1289 (Avg-Acc 57.1289)\n",
            "Epoch: [90][9/39]\tTime 0.075 (Avg-Time 0.078)\t Loss 1.2665 (Avg-Loss 1.2484)\tAcc 55.4688 (Avg-Acc 56.1230)\n",
            "Epoch: [90][18/39]\tTime 0.056 (Avg-Time 0.069)\t Loss 1.2641 (Avg-Loss 1.2457)\tAcc 54.3945 (Avg-Acc 55.8337)\n",
            "Epoch: [90][27/39]\tTime 0.059 (Avg-Time 0.066)\t Loss 1.1907 (Avg-Loss 1.2418)\tAcc 56.2500 (Avg-Acc 55.9849)\n",
            "Epoch: [90][36/39]\tTime 0.057 (Avg-Time 0.064)\t Loss 1.2096 (Avg-Loss 1.2375)\tAcc 57.6172 (Avg-Acc 56.1629)\n",
            "Epoch: [90][39/39]\tTime 0.006 (Avg-Time 0.063)\t Loss 1.4965 (Avg-Loss 1.2375)\tAcc 48.4375 (Avg-Acc 56.2425)\n",
            "EPOCH: 90 train Results: Acc 56.242 Loss: 1.2375\n",
            "Epoch: [90][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2515 (Avg-Loss 1.2515)\tAcc 56.2500 (Avg-Acc 56.2500)\n",
            "Epoch: [90][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2725 (Avg-Loss 1.2666)\tAcc 55.3571 (Avg-Acc 54.9500)\n",
            "EPOCH: 90 Validation Results: Acc 54.950 Loss: 1.2666\n",
            "Best Accuracy: 54.9500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [91][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1925 (Avg-Loss 1.1925)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [91][9/39]\tTime 0.057 (Avg-Time 0.062)\t Loss 1.2838 (Avg-Loss 1.2367)\tAcc 54.1992 (Avg-Acc 56.1328)\n",
            "Epoch: [91][18/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.1937 (Avg-Loss 1.2380)\tAcc 58.5938 (Avg-Acc 56.2808)\n",
            "Epoch: [91][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2382 (Avg-Loss 1.2426)\tAcc 55.8594 (Avg-Acc 56.0931)\n",
            "Epoch: [91][36/39]\tTime 0.082 (Avg-Time 0.061)\t Loss 1.2364 (Avg-Loss 1.2399)\tAcc 55.1758 (Avg-Acc 56.1312)\n",
            "Epoch: [91][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.3156 (Avg-Loss 1.2390)\tAcc 60.9375 (Avg-Acc 56.1925)\n",
            "EPOCH: 91 train Results: Acc 56.193 Loss: 1.2390\n",
            "Epoch: [91][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2682 (Avg-Loss 1.2682)\tAcc 55.3711 (Avg-Acc 55.3711)\n",
            "Epoch: [91][9/9]\tTime 0.014 (Avg-Time 0.017)\t Loss 1.2698 (Avg-Loss 1.2774)\tAcc 55.6122 (Avg-Acc 54.2300)\n",
            "EPOCH: 91 Validation Results: Acc 54.230 Loss: 1.2774\n",
            "Best Accuracy: 54.9500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [92][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.2474 (Avg-Loss 1.2474)\tAcc 53.8086 (Avg-Acc 53.8086)\n",
            "Epoch: [92][9/39]\tTime 0.064 (Avg-Time 0.059)\t Loss 1.1989 (Avg-Loss 1.2372)\tAcc 55.8594 (Avg-Acc 55.6543)\n",
            "Epoch: [92][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2457 (Avg-Loss 1.2440)\tAcc 54.1992 (Avg-Acc 55.2118)\n",
            "Epoch: [92][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1947 (Avg-Loss 1.2390)\tAcc 58.3984 (Avg-Acc 55.7582)\n",
            "Epoch: [92][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1963 (Avg-Loss 1.2349)\tAcc 56.0547 (Avg-Acc 55.7142)\n",
            "Epoch: [92][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1774 (Avg-Loss 1.2369)\tAcc 56.2500 (Avg-Acc 55.7350)\n",
            "EPOCH: 92 train Results: Acc 55.735 Loss: 1.2369\n",
            "Epoch: [92][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2546 (Avg-Loss 1.2546)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [92][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2640 (Avg-Loss 1.2704)\tAcc 55.1020 (Avg-Acc 54.8400)\n",
            "EPOCH: 92 Validation Results: Acc 54.840 Loss: 1.2704\n",
            "Best Accuracy: 54.9500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [93][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.1842 (Avg-Loss 1.1842)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [93][9/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.2986 (Avg-Loss 1.2317)\tAcc 53.0273 (Avg-Acc 55.5957)\n",
            "Epoch: [93][18/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.2615 (Avg-Loss 1.2349)\tAcc 54.4922 (Avg-Acc 55.8285)\n",
            "Epoch: [93][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.2069 (Avg-Loss 1.2262)\tAcc 57.2266 (Avg-Acc 56.1035)\n",
            "Epoch: [93][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2817 (Avg-Loss 1.2249)\tAcc 54.6875 (Avg-Acc 56.2606)\n",
            "Epoch: [93][39/39]\tTime 0.014 (Avg-Time 0.059)\t Loss 1.3287 (Avg-Loss 1.2253)\tAcc 59.3750 (Avg-Acc 56.2625)\n",
            "EPOCH: 93 train Results: Acc 56.263 Loss: 1.2253\n",
            "Epoch: [93][0/9]\tTime 0.018 (Avg-Time 0.018)\t Loss 1.2436 (Avg-Loss 1.2436)\tAcc 56.3477 (Avg-Acc 56.3477)\n",
            "Epoch: [93][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2697 (Avg-Loss 1.2694)\tAcc 56.7602 (Avg-Acc 54.6400)\n",
            "EPOCH: 93 Validation Results: Acc 54.640 Loss: 1.2694\n",
            "Best Accuracy: 54.9500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [94][0/39]\tTime 0.105 (Avg-Time 0.105)\t Loss 1.1889 (Avg-Loss 1.1889)\tAcc 58.1055 (Avg-Acc 58.1055)\n",
            "Epoch: [94][9/39]\tTime 0.164 (Avg-Time 0.148)\t Loss 1.2071 (Avg-Loss 1.2391)\tAcc 56.7383 (Avg-Acc 55.8398)\n",
            "Epoch: [94][18/39]\tTime 0.142 (Avg-Time 0.147)\t Loss 1.2310 (Avg-Loss 1.2286)\tAcc 57.7148 (Avg-Acc 56.5584)\n",
            "Epoch: [94][27/39]\tTime 0.059 (Avg-Time 0.127)\t Loss 1.2183 (Avg-Loss 1.2267)\tAcc 55.3711 (Avg-Acc 56.5081)\n",
            "Epoch: [94][36/39]\tTime 0.082 (Avg-Time 0.111)\t Loss 1.1977 (Avg-Loss 1.2229)\tAcc 57.6172 (Avg-Acc 56.5641)\n",
            "Epoch: [94][39/39]\tTime 0.006 (Avg-Time 0.106)\t Loss 1.3347 (Avg-Loss 1.2237)\tAcc 50.0000 (Avg-Acc 56.6000)\n",
            "EPOCH: 94 train Results: Acc 56.600 Loss: 1.2237\n",
            "Epoch: [94][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2497 (Avg-Loss 1.2497)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [94][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2600 (Avg-Loss 1.2648)\tAcc 56.8878 (Avg-Acc 54.7000)\n",
            "EPOCH: 94 Validation Results: Acc 54.700 Loss: 1.2648\n",
            "Best Accuracy: 54.9500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [95][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.1978 (Avg-Loss 1.1978)\tAcc 57.5195 (Avg-Acc 57.5195)\n",
            "Epoch: [95][9/39]\tTime 0.060 (Avg-Time 0.058)\t Loss 1.1745 (Avg-Loss 1.2155)\tAcc 58.3984 (Avg-Acc 57.1680)\n",
            "Epoch: [95][18/39]\tTime 0.058 (Avg-Time 0.059)\t Loss 1.2361 (Avg-Loss 1.2215)\tAcc 57.1289 (Avg-Acc 56.9439)\n",
            "Epoch: [95][27/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.2717 (Avg-Loss 1.2224)\tAcc 53.2227 (Avg-Acc 56.7801)\n",
            "Epoch: [95][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2305 (Avg-Loss 1.2258)\tAcc 58.0078 (Avg-Acc 56.5746)\n",
            "Epoch: [95][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.3493 (Avg-Loss 1.2241)\tAcc 42.1875 (Avg-Acc 56.6125)\n",
            "EPOCH: 95 train Results: Acc 56.612 Loss: 1.2241\n",
            "Epoch: [95][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2649 (Avg-Loss 1.2649)\tAcc 55.3711 (Avg-Acc 55.3711)\n",
            "Epoch: [95][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2597 (Avg-Loss 1.2726)\tAcc 55.6122 (Avg-Acc 54.3300)\n",
            "EPOCH: 95 Validation Results: Acc 54.330 Loss: 1.2726\n",
            "Best Accuracy: 54.9500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [96][0/39]\tTime 0.062 (Avg-Time 0.062)\t Loss 1.2073 (Avg-Loss 1.2073)\tAcc 56.8359 (Avg-Acc 56.8359)\n",
            "Epoch: [96][9/39]\tTime 0.057 (Avg-Time 0.062)\t Loss 1.2623 (Avg-Loss 1.2372)\tAcc 55.1758 (Avg-Acc 56.2109)\n",
            "Epoch: [96][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2219 (Avg-Loss 1.2325)\tAcc 56.6406 (Avg-Acc 56.3579)\n",
            "Epoch: [96][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1894 (Avg-Loss 1.2254)\tAcc 56.1523 (Avg-Acc 56.4802)\n",
            "Epoch: [96][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2133 (Avg-Loss 1.2232)\tAcc 58.9844 (Avg-Acc 56.6010)\n",
            "Epoch: [96][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.2168 (Avg-Loss 1.2227)\tAcc 57.8125 (Avg-Acc 56.6075)\n",
            "EPOCH: 96 train Results: Acc 56.608 Loss: 1.2227\n",
            "Epoch: [96][0/9]\tTime 0.027 (Avg-Time 0.027)\t Loss 1.2398 (Avg-Loss 1.2398)\tAcc 57.5195 (Avg-Acc 57.5195)\n",
            "Epoch: [96][9/9]\tTime 0.011 (Avg-Time 0.019)\t Loss 1.2526 (Avg-Loss 1.2619)\tAcc 55.7398 (Avg-Acc 54.5200)\n",
            "EPOCH: 96 Validation Results: Acc 54.520 Loss: 1.2619\n",
            "Best Accuracy: 54.9500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [97][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1833 (Avg-Loss 1.1833)\tAcc 56.9336 (Avg-Acc 56.9336)\n",
            "Epoch: [97][9/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.2240 (Avg-Loss 1.2088)\tAcc 58.0078 (Avg-Acc 57.1875)\n",
            "Epoch: [97][18/39]\tTime 0.059 (Avg-Time 0.061)\t Loss 1.1823 (Avg-Loss 1.2140)\tAcc 58.7891 (Avg-Acc 56.7280)\n",
            "Epoch: [97][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2063 (Avg-Loss 1.2171)\tAcc 56.2500 (Avg-Acc 56.5116)\n",
            "Epoch: [97][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2404 (Avg-Loss 1.2159)\tAcc 53.7109 (Avg-Acc 56.4691)\n",
            "Epoch: [97][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.4071 (Avg-Loss 1.2155)\tAcc 43.7500 (Avg-Acc 56.4575)\n",
            "EPOCH: 97 train Results: Acc 56.458 Loss: 1.2155\n",
            "Epoch: [97][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2540 (Avg-Loss 1.2540)\tAcc 55.9570 (Avg-Acc 55.9570)\n",
            "Epoch: [97][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2528 (Avg-Loss 1.2694)\tAcc 55.7398 (Avg-Acc 54.7400)\n",
            "EPOCH: 97 Validation Results: Acc 54.740 Loss: 1.2694\n",
            "Best Accuracy: 54.9500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [98][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1966 (Avg-Loss 1.1966)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [98][9/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 1.2243 (Avg-Loss 1.2270)\tAcc 57.7148 (Avg-Acc 57.0898)\n",
            "Epoch: [98][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2226 (Avg-Loss 1.2357)\tAcc 57.0312 (Avg-Acc 56.4402)\n",
            "Epoch: [98][27/39]\tTime 0.104 (Avg-Time 0.077)\t Loss 1.1223 (Avg-Loss 1.2343)\tAcc 60.3516 (Avg-Acc 56.3511)\n",
            "Epoch: [98][36/39]\tTime 0.140 (Avg-Time 0.100)\t Loss 1.2688 (Avg-Loss 1.2290)\tAcc 53.7109 (Avg-Acc 56.5166)\n",
            "Epoch: [98][39/39]\tTime 0.013 (Avg-Time 0.100)\t Loss 1.2119 (Avg-Loss 1.2282)\tAcc 57.8125 (Avg-Acc 56.6125)\n",
            "EPOCH: 98 train Results: Acc 56.612 Loss: 1.2282\n",
            "Epoch: [98][0/9]\tTime 0.037 (Avg-Time 0.037)\t Loss 1.2492 (Avg-Loss 1.2492)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [98][9/9]\tTime 0.029 (Avg-Time 0.031)\t Loss 1.2547 (Avg-Loss 1.2639)\tAcc 56.3776 (Avg-Acc 54.5900)\n",
            "EPOCH: 98 Validation Results: Acc 54.590 Loss: 1.2639\n",
            "Best Accuracy: 54.9500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [99][0/39]\tTime 0.130 (Avg-Time 0.130)\t Loss 1.2086 (Avg-Loss 1.2086)\tAcc 55.9570 (Avg-Acc 55.9570)\n",
            "Epoch: [99][9/39]\tTime 0.058 (Avg-Time 0.089)\t Loss 1.2294 (Avg-Loss 1.2000)\tAcc 56.1523 (Avg-Acc 57.0410)\n",
            "Epoch: [99][18/39]\tTime 0.079 (Avg-Time 0.076)\t Loss 1.2339 (Avg-Loss 1.2076)\tAcc 55.5664 (Avg-Acc 57.0107)\n",
            "Epoch: [99][27/39]\tTime 0.057 (Avg-Time 0.071)\t Loss 1.1732 (Avg-Loss 1.2094)\tAcc 58.5938 (Avg-Acc 57.1045)\n",
            "Epoch: [99][36/39]\tTime 0.056 (Avg-Time 0.069)\t Loss 1.1886 (Avg-Loss 1.2102)\tAcc 56.5430 (Avg-Acc 57.0946)\n",
            "Epoch: [99][39/39]\tTime 0.006 (Avg-Time 0.066)\t Loss 1.0654 (Avg-Loss 1.2115)\tAcc 68.7500 (Avg-Acc 57.0850)\n",
            "EPOCH: 99 train Results: Acc 57.085 Loss: 1.2115\n",
            "Epoch: [99][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2479 (Avg-Loss 1.2479)\tAcc 56.5430 (Avg-Acc 56.5430)\n",
            "Epoch: [99][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2473 (Avg-Loss 1.2608)\tAcc 54.5918 (Avg-Acc 54.8700)\n",
            "EPOCH: 99 Validation Results: Acc 54.870 Loss: 1.2608\n",
            "Best Accuracy: 54.9500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [100][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.1578 (Avg-Loss 1.1578)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [100][9/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.2413 (Avg-Loss 1.2234)\tAcc 57.3242 (Avg-Acc 56.5234)\n",
            "Epoch: [100][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2213 (Avg-Loss 1.2165)\tAcc 55.5664 (Avg-Acc 56.5532)\n",
            "Epoch: [100][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2335 (Avg-Loss 1.2096)\tAcc 57.3242 (Avg-Acc 56.8220)\n",
            "Epoch: [100][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2268 (Avg-Loss 1.2117)\tAcc 55.2734 (Avg-Acc 56.8518)\n",
            "Epoch: [100][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1204 (Avg-Loss 1.2112)\tAcc 68.7500 (Avg-Acc 56.9150)\n",
            "EPOCH: 100 train Results: Acc 56.915 Loss: 1.2112\n",
            "Epoch: [100][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2476 (Avg-Loss 1.2476)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [100][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2450 (Avg-Loss 1.2523)\tAcc 56.1224 (Avg-Acc 55.0300)\n",
            "EPOCH: 100 Validation Results: Acc 55.030 Loss: 1.2523\n",
            "Best Accuracy: 55.0300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [101][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1764 (Avg-Loss 1.1764)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [101][9/39]\tTime 0.058 (Avg-Time 0.063)\t Loss 1.2763 (Avg-Loss 1.2072)\tAcc 54.0039 (Avg-Acc 57.0996)\n",
            "Epoch: [101][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2280 (Avg-Loss 1.2111)\tAcc 56.1523 (Avg-Acc 56.7640)\n",
            "Epoch: [101][27/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.1774 (Avg-Loss 1.2112)\tAcc 57.6172 (Avg-Acc 56.5604)\n",
            "Epoch: [101][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2394 (Avg-Loss 1.2092)\tAcc 53.9062 (Avg-Acc 56.7462)\n",
            "Epoch: [101][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1782 (Avg-Loss 1.2075)\tAcc 59.3750 (Avg-Acc 56.8200)\n",
            "EPOCH: 101 train Results: Acc 56.820 Loss: 1.2075\n",
            "Epoch: [101][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2323 (Avg-Loss 1.2323)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [101][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2492 (Avg-Loss 1.2468)\tAcc 55.8673 (Avg-Acc 55.7400)\n",
            "EPOCH: 101 Validation Results: Acc 55.740 Loss: 1.2468\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [102][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.2063 (Avg-Loss 1.2063)\tAcc 58.3008 (Avg-Acc 58.3008)\n",
            "Epoch: [102][9/39]\tTime 0.060 (Avg-Time 0.062)\t Loss 1.1591 (Avg-Loss 1.2104)\tAcc 57.8125 (Avg-Acc 57.0117)\n",
            "Epoch: [102][18/39]\tTime 0.057 (Avg-Time 0.062)\t Loss 1.1632 (Avg-Loss 1.1993)\tAcc 57.8125 (Avg-Acc 57.3294)\n",
            "Epoch: [102][27/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.2291 (Avg-Loss 1.1994)\tAcc 57.7148 (Avg-Acc 57.4428)\n",
            "Epoch: [102][36/39]\tTime 0.065 (Avg-Time 0.063)\t Loss 1.2117 (Avg-Loss 1.1993)\tAcc 55.9570 (Avg-Acc 57.3717)\n",
            "Epoch: [102][39/39]\tTime 0.006 (Avg-Time 0.061)\t Loss 1.2879 (Avg-Loss 1.1988)\tAcc 54.6875 (Avg-Acc 57.3775)\n",
            "EPOCH: 102 train Results: Acc 57.377 Loss: 1.1988\n",
            "Epoch: [102][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2352 (Avg-Loss 1.2352)\tAcc 56.1523 (Avg-Acc 56.1523)\n",
            "Epoch: [102][9/9]\tTime 0.012 (Avg-Time 0.016)\t Loss 1.2483 (Avg-Loss 1.2482)\tAcc 55.8673 (Avg-Acc 55.2200)\n",
            "EPOCH: 102 Validation Results: Acc 55.220 Loss: 1.2482\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [103][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1473 (Avg-Loss 1.1473)\tAcc 58.4961 (Avg-Acc 58.4961)\n",
            "Epoch: [103][9/39]\tTime 0.118 (Avg-Time 0.101)\t Loss 1.2053 (Avg-Loss 1.1923)\tAcc 56.0547 (Avg-Acc 57.5684)\n",
            "Epoch: [103][18/39]\tTime 0.119 (Avg-Time 0.144)\t Loss 1.2548 (Avg-Loss 1.1951)\tAcc 56.5430 (Avg-Acc 57.6326)\n",
            "Epoch: [103][27/39]\tTime 0.058 (Avg-Time 0.130)\t Loss 1.2179 (Avg-Loss 1.1996)\tAcc 54.8828 (Avg-Acc 57.4184)\n",
            "Epoch: [103][36/39]\tTime 0.059 (Avg-Time 0.113)\t Loss 1.2245 (Avg-Loss 1.2011)\tAcc 55.6641 (Avg-Acc 57.3480)\n",
            "Epoch: [103][39/39]\tTime 0.006 (Avg-Time 0.108)\t Loss 1.0817 (Avg-Loss 1.2033)\tAcc 64.0625 (Avg-Acc 57.2350)\n",
            "EPOCH: 103 train Results: Acc 57.235 Loss: 1.2033\n",
            "Epoch: [103][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2385 (Avg-Loss 1.2385)\tAcc 55.7617 (Avg-Acc 55.7617)\n",
            "Epoch: [103][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2504 (Avg-Loss 1.2498)\tAcc 54.9745 (Avg-Acc 55.0400)\n",
            "EPOCH: 103 Validation Results: Acc 55.040 Loss: 1.2498\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [104][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.2073 (Avg-Loss 1.2073)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [104][9/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.1897 (Avg-Loss 1.1912)\tAcc 58.3984 (Avg-Acc 57.2363)\n",
            "Epoch: [104][18/39]\tTime 0.080 (Avg-Time 0.061)\t Loss 1.1834 (Avg-Loss 1.1938)\tAcc 58.9844 (Avg-Acc 57.3808)\n",
            "Epoch: [104][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1878 (Avg-Loss 1.1963)\tAcc 57.6172 (Avg-Acc 57.3870)\n",
            "Epoch: [104][36/39]\tTime 0.070 (Avg-Time 0.060)\t Loss 1.1849 (Avg-Loss 1.1925)\tAcc 58.4961 (Avg-Acc 57.5882)\n",
            "Epoch: [104][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.0870 (Avg-Loss 1.1917)\tAcc 64.0625 (Avg-Acc 57.6050)\n",
            "EPOCH: 104 train Results: Acc 57.605 Loss: 1.1917\n",
            "Epoch: [104][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2505 (Avg-Loss 1.2505)\tAcc 54.5898 (Avg-Acc 54.5898)\n",
            "Epoch: [104][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2649 (Avg-Loss 1.2586)\tAcc 55.2296 (Avg-Acc 54.5600)\n",
            "EPOCH: 104 Validation Results: Acc 54.560 Loss: 1.2586\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [105][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.1765 (Avg-Loss 1.1765)\tAcc 59.4727 (Avg-Acc 59.4727)\n",
            "Epoch: [105][9/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1854 (Avg-Loss 1.1962)\tAcc 58.9844 (Avg-Acc 57.3828)\n",
            "Epoch: [105][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2132 (Avg-Loss 1.2030)\tAcc 56.9336 (Avg-Acc 57.2009)\n",
            "Epoch: [105][27/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1984 (Avg-Loss 1.2011)\tAcc 57.1289 (Avg-Acc 57.2824)\n",
            "Epoch: [105][36/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1927 (Avg-Loss 1.1987)\tAcc 58.4961 (Avg-Acc 57.3005)\n",
            "Epoch: [105][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.0833 (Avg-Loss 1.2013)\tAcc 54.6875 (Avg-Acc 57.2325)\n",
            "EPOCH: 105 train Results: Acc 57.233 Loss: 1.2013\n",
            "Epoch: [105][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2312 (Avg-Loss 1.2312)\tAcc 56.7383 (Avg-Acc 56.7383)\n",
            "Epoch: [105][9/9]\tTime 0.012 (Avg-Time 0.015)\t Loss 1.2513 (Avg-Loss 1.2480)\tAcc 55.7398 (Avg-Acc 55.6600)\n",
            "EPOCH: 105 Validation Results: Acc 55.660 Loss: 1.2480\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [106][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1005 (Avg-Loss 1.1005)\tAcc 60.5469 (Avg-Acc 60.5469)\n",
            "Epoch: [106][9/39]\tTime 0.056 (Avg-Time 0.062)\t Loss 1.2535 (Avg-Loss 1.1831)\tAcc 55.0781 (Avg-Acc 57.9199)\n",
            "Epoch: [106][18/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.1904 (Avg-Loss 1.1875)\tAcc 56.5430 (Avg-Acc 57.6789)\n",
            "Epoch: [106][27/39]\tTime 0.060 (Avg-Time 0.061)\t Loss 1.2376 (Avg-Loss 1.1897)\tAcc 57.2266 (Avg-Acc 57.6137)\n",
            "Epoch: [106][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2347 (Avg-Loss 1.1908)\tAcc 55.7617 (Avg-Acc 57.6251)\n",
            "Epoch: [106][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.2962 (Avg-Loss 1.1905)\tAcc 53.1250 (Avg-Acc 57.6650)\n",
            "EPOCH: 106 train Results: Acc 57.665 Loss: 1.1905\n",
            "Epoch: [106][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2249 (Avg-Loss 1.2249)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [106][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2454 (Avg-Loss 1.2435)\tAcc 55.6122 (Avg-Acc 55.4800)\n",
            "EPOCH: 106 Validation Results: Acc 55.480 Loss: 1.2435\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [107][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1662 (Avg-Loss 1.1662)\tAcc 60.3516 (Avg-Acc 60.3516)\n",
            "Epoch: [107][9/39]\tTime 0.057 (Avg-Time 0.058)\t Loss 1.1963 (Avg-Loss 1.2005)\tAcc 57.2266 (Avg-Acc 57.7539)\n",
            "Epoch: [107][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2076 (Avg-Loss 1.2013)\tAcc 58.1055 (Avg-Acc 57.6120)\n",
            "Epoch: [107][27/39]\tTime 0.111 (Avg-Time 0.070)\t Loss 1.1945 (Avg-Loss 1.2007)\tAcc 58.2031 (Avg-Acc 57.4219)\n",
            "Epoch: [107][36/39]\tTime 0.124 (Avg-Time 0.099)\t Loss 1.1673 (Avg-Loss 1.1993)\tAcc 59.7656 (Avg-Acc 57.4852)\n",
            "Epoch: [107][39/39]\tTime 0.014 (Avg-Time 0.098)\t Loss 1.2672 (Avg-Loss 1.1979)\tAcc 50.0000 (Avg-Acc 57.5125)\n",
            "EPOCH: 107 train Results: Acc 57.513 Loss: 1.1979\n",
            "Epoch: [107][0/9]\tTime 0.035 (Avg-Time 0.035)\t Loss 1.2391 (Avg-Loss 1.2391)\tAcc 55.8594 (Avg-Acc 55.8594)\n",
            "Epoch: [107][9/9]\tTime 0.023 (Avg-Time 0.031)\t Loss 1.2499 (Avg-Loss 1.2495)\tAcc 55.6122 (Avg-Acc 55.2100)\n",
            "EPOCH: 107 Validation Results: Acc 55.210 Loss: 1.2495\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [108][0/39]\tTime 0.111 (Avg-Time 0.111)\t Loss 1.1322 (Avg-Loss 1.1322)\tAcc 60.4492 (Avg-Acc 60.4492)\n",
            "Epoch: [108][9/39]\tTime 0.077 (Avg-Time 0.086)\t Loss 1.1531 (Avg-Loss 1.1816)\tAcc 58.7891 (Avg-Acc 57.9395)\n",
            "Epoch: [108][18/39]\tTime 0.057 (Avg-Time 0.073)\t Loss 1.2188 (Avg-Loss 1.2010)\tAcc 57.1289 (Avg-Acc 57.2934)\n",
            "Epoch: [108][27/39]\tTime 0.060 (Avg-Time 0.070)\t Loss 1.0983 (Avg-Loss 1.1932)\tAcc 60.5469 (Avg-Acc 57.4114)\n",
            "Epoch: [108][36/39]\tTime 0.058 (Avg-Time 0.067)\t Loss 1.1774 (Avg-Loss 1.1914)\tAcc 57.4219 (Avg-Acc 57.5011)\n",
            "Epoch: [108][39/39]\tTime 0.006 (Avg-Time 0.065)\t Loss 1.3435 (Avg-Loss 1.1939)\tAcc 48.4375 (Avg-Acc 57.4300)\n",
            "EPOCH: 108 train Results: Acc 57.430 Loss: 1.1939\n",
            "Epoch: [108][0/9]\tTime 0.018 (Avg-Time 0.018)\t Loss 1.2526 (Avg-Loss 1.2526)\tAcc 55.9570 (Avg-Acc 55.9570)\n",
            "Epoch: [108][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2640 (Avg-Loss 1.2554)\tAcc 55.1020 (Avg-Acc 55.5900)\n",
            "EPOCH: 108 Validation Results: Acc 55.590 Loss: 1.2554\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [109][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.2410 (Avg-Loss 1.2410)\tAcc 55.7617 (Avg-Acc 55.7617)\n",
            "Epoch: [109][9/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1753 (Avg-Loss 1.1954)\tAcc 58.9844 (Avg-Acc 57.3633)\n",
            "Epoch: [109][18/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.1966 (Avg-Loss 1.1942)\tAcc 58.2031 (Avg-Acc 57.5144)\n",
            "Epoch: [109][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1640 (Avg-Loss 1.1881)\tAcc 57.6172 (Avg-Acc 57.6869)\n",
            "Epoch: [109][36/39]\tTime 0.080 (Avg-Time 0.061)\t Loss 1.2109 (Avg-Loss 1.1867)\tAcc 56.1523 (Avg-Acc 57.5618)\n",
            "Epoch: [109][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.0689 (Avg-Loss 1.1863)\tAcc 57.8125 (Avg-Acc 57.5650)\n",
            "EPOCH: 109 train Results: Acc 57.565 Loss: 1.1863\n",
            "Epoch: [109][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2411 (Avg-Loss 1.2411)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [109][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2462 (Avg-Loss 1.2517)\tAcc 56.6327 (Avg-Acc 55.5500)\n",
            "EPOCH: 109 Validation Results: Acc 55.550 Loss: 1.2517\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [110][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1533 (Avg-Loss 1.1533)\tAcc 59.1797 (Avg-Acc 59.1797)\n",
            "Epoch: [110][9/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1800 (Avg-Loss 1.1834)\tAcc 58.5938 (Avg-Acc 57.9980)\n",
            "Epoch: [110][18/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1841 (Avg-Loss 1.1881)\tAcc 57.0312 (Avg-Acc 58.0644)\n",
            "Epoch: [110][27/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.2812 (Avg-Loss 1.1874)\tAcc 51.6602 (Avg-Acc 57.9973)\n",
            "Epoch: [110][36/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1679 (Avg-Loss 1.1883)\tAcc 59.5703 (Avg-Acc 58.0448)\n",
            "Epoch: [110][39/39]\tTime 0.010 (Avg-Time 0.059)\t Loss 1.3660 (Avg-Loss 1.1869)\tAcc 48.4375 (Avg-Acc 58.1350)\n",
            "EPOCH: 110 train Results: Acc 58.135 Loss: 1.1869\n",
            "Epoch: [110][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2364 (Avg-Loss 1.2364)\tAcc 55.0781 (Avg-Acc 55.0781)\n",
            "Epoch: [110][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2505 (Avg-Loss 1.2472)\tAcc 54.7194 (Avg-Acc 55.3300)\n",
            "EPOCH: 110 Validation Results: Acc 55.330 Loss: 1.2472\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [111][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1906 (Avg-Loss 1.1906)\tAcc 56.5430 (Avg-Acc 56.5430)\n",
            "Epoch: [111][9/39]\tTime 0.059 (Avg-Time 0.062)\t Loss 1.1744 (Avg-Loss 1.1871)\tAcc 57.4219 (Avg-Acc 57.3828)\n",
            "Epoch: [111][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1828 (Avg-Loss 1.1958)\tAcc 60.1562 (Avg-Acc 57.4990)\n",
            "Epoch: [111][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1871 (Avg-Loss 1.1924)\tAcc 58.3008 (Avg-Acc 57.6695)\n",
            "Epoch: [111][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1683 (Avg-Loss 1.1893)\tAcc 58.3984 (Avg-Acc 57.8151)\n",
            "Epoch: [111][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.0706 (Avg-Loss 1.1888)\tAcc 60.9375 (Avg-Acc 57.8150)\n",
            "EPOCH: 111 train Results: Acc 57.815 Loss: 1.1888\n",
            "Epoch: [111][0/9]\tTime 0.018 (Avg-Time 0.018)\t Loss 1.2273 (Avg-Loss 1.2273)\tAcc 56.5430 (Avg-Acc 56.5430)\n",
            "Epoch: [111][9/9]\tTime 0.011 (Avg-Time 0.017)\t Loss 1.2462 (Avg-Loss 1.2457)\tAcc 55.9949 (Avg-Acc 55.6500)\n",
            "EPOCH: 111 Validation Results: Acc 55.650 Loss: 1.2457\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [112][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1958 (Avg-Loss 1.1958)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [112][9/39]\tTime 0.125 (Avg-Time 0.152)\t Loss 1.2058 (Avg-Loss 1.1695)\tAcc 57.0312 (Avg-Acc 58.5449)\n",
            "Epoch: [112][18/39]\tTime 0.128 (Avg-Time 0.152)\t Loss 1.1755 (Avg-Loss 1.1733)\tAcc 56.6406 (Avg-Acc 58.4447)\n",
            "Epoch: [112][27/39]\tTime 0.057 (Avg-Time 0.132)\t Loss 1.2007 (Avg-Loss 1.1750)\tAcc 55.1758 (Avg-Acc 58.3426)\n",
            "Epoch: [112][36/39]\tTime 0.057 (Avg-Time 0.115)\t Loss 1.2061 (Avg-Loss 1.1806)\tAcc 58.0078 (Avg-Acc 58.1767)\n",
            "Epoch: [112][39/39]\tTime 0.009 (Avg-Time 0.109)\t Loss 1.1957 (Avg-Loss 1.1805)\tAcc 50.0000 (Avg-Acc 58.1325)\n",
            "EPOCH: 112 train Results: Acc 58.133 Loss: 1.1805\n",
            "Epoch: [112][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2348 (Avg-Loss 1.2348)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [112][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2558 (Avg-Loss 1.2523)\tAcc 54.8469 (Avg-Acc 55.1900)\n",
            "EPOCH: 112 Validation Results: Acc 55.190 Loss: 1.2523\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [113][0/39]\tTime 0.062 (Avg-Time 0.062)\t Loss 1.1140 (Avg-Loss 1.1140)\tAcc 61.5234 (Avg-Acc 61.5234)\n",
            "Epoch: [113][9/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.2051 (Avg-Loss 1.1913)\tAcc 56.7383 (Avg-Acc 57.1191)\n",
            "Epoch: [113][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1548 (Avg-Loss 1.1880)\tAcc 60.5469 (Avg-Acc 57.5966)\n",
            "Epoch: [113][27/39]\tTime 0.077 (Avg-Time 0.061)\t Loss 1.2342 (Avg-Loss 1.1845)\tAcc 55.2734 (Avg-Acc 57.6556)\n",
            "Epoch: [113][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1344 (Avg-Loss 1.1830)\tAcc 60.7422 (Avg-Acc 57.8099)\n",
            "Epoch: [113][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1302 (Avg-Loss 1.1835)\tAcc 53.1250 (Avg-Acc 57.7875)\n",
            "EPOCH: 113 train Results: Acc 57.788 Loss: 1.1835\n",
            "Epoch: [113][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2301 (Avg-Loss 1.2301)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [113][9/9]\tTime 0.013 (Avg-Time 0.016)\t Loss 1.2337 (Avg-Loss 1.2381)\tAcc 55.6122 (Avg-Acc 55.6500)\n",
            "EPOCH: 113 Validation Results: Acc 55.650 Loss: 1.2381\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [114][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1469 (Avg-Loss 1.1469)\tAcc 57.9102 (Avg-Acc 57.9102)\n",
            "Epoch: [114][9/39]\tTime 0.057 (Avg-Time 0.062)\t Loss 1.1941 (Avg-Loss 1.1885)\tAcc 56.1523 (Avg-Acc 58.0176)\n",
            "Epoch: [114][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1776 (Avg-Loss 1.1773)\tAcc 58.5938 (Avg-Acc 58.2597)\n",
            "Epoch: [114][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1677 (Avg-Loss 1.1757)\tAcc 57.3242 (Avg-Acc 58.3670)\n",
            "Epoch: [114][36/39]\tTime 0.070 (Avg-Time 0.061)\t Loss 1.2004 (Avg-Loss 1.1793)\tAcc 57.0312 (Avg-Acc 58.2031)\n",
            "Epoch: [114][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.1086 (Avg-Loss 1.1808)\tAcc 56.2500 (Avg-Acc 58.0900)\n",
            "EPOCH: 114 train Results: Acc 58.090 Loss: 1.1808\n",
            "Epoch: [114][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2253 (Avg-Loss 1.2253)\tAcc 56.9336 (Avg-Acc 56.9336)\n",
            "Epoch: [114][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2398 (Avg-Loss 1.2423)\tAcc 56.7602 (Avg-Acc 55.4500)\n",
            "EPOCH: 114 Validation Results: Acc 55.450 Loss: 1.2423\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [115][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1485 (Avg-Loss 1.1485)\tAcc 58.8867 (Avg-Acc 58.8867)\n",
            "Epoch: [115][9/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1938 (Avg-Loss 1.1984)\tAcc 55.6641 (Avg-Acc 57.2363)\n",
            "Epoch: [115][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1745 (Avg-Loss 1.1886)\tAcc 58.2031 (Avg-Acc 57.3859)\n",
            "Epoch: [115][27/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.2084 (Avg-Loss 1.1863)\tAcc 56.8359 (Avg-Acc 57.4847)\n",
            "Epoch: [115][36/39]\tTime 0.055 (Avg-Time 0.060)\t Loss 1.1920 (Avg-Loss 1.1805)\tAcc 57.2266 (Avg-Acc 57.8495)\n",
            "Epoch: [115][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 0.9285 (Avg-Loss 1.1818)\tAcc 70.3125 (Avg-Acc 57.7500)\n",
            "EPOCH: 115 train Results: Acc 57.750 Loss: 1.1818\n",
            "Epoch: [115][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2331 (Avg-Loss 1.2331)\tAcc 56.6406 (Avg-Acc 56.6406)\n",
            "Epoch: [115][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2416 (Avg-Loss 1.2412)\tAcc 56.8878 (Avg-Acc 55.4000)\n",
            "EPOCH: 115 Validation Results: Acc 55.400 Loss: 1.2412\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [116][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1117 (Avg-Loss 1.1117)\tAcc 62.5977 (Avg-Acc 62.5977)\n",
            "Epoch: [116][9/39]\tTime 0.057 (Avg-Time 0.062)\t Loss 1.1759 (Avg-Loss 1.1692)\tAcc 57.5195 (Avg-Acc 58.7891)\n",
            "Epoch: [116][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2399 (Avg-Loss 1.1839)\tAcc 55.0781 (Avg-Acc 58.0695)\n",
            "Epoch: [116][27/39]\tTime 0.144 (Avg-Time 0.079)\t Loss 1.2590 (Avg-Loss 1.1873)\tAcc 54.6875 (Avg-Acc 57.9834)\n",
            "Epoch: [116][36/39]\tTime 0.194 (Avg-Time 0.103)\t Loss 1.2086 (Avg-Loss 1.1882)\tAcc 56.2500 (Avg-Acc 57.9128)\n",
            "Epoch: [116][39/39]\tTime 0.023 (Avg-Time 0.101)\t Loss 1.2638 (Avg-Loss 1.1868)\tAcc 62.5000 (Avg-Acc 57.9225)\n",
            "EPOCH: 116 train Results: Acc 57.922 Loss: 1.1868\n",
            "Epoch: [116][0/9]\tTime 0.026 (Avg-Time 0.026)\t Loss 1.2396 (Avg-Loss 1.2396)\tAcc 56.7383 (Avg-Acc 56.7383)\n",
            "Epoch: [116][9/9]\tTime 0.027 (Avg-Time 0.034)\t Loss 1.2337 (Avg-Loss 1.2486)\tAcc 57.0153 (Avg-Acc 55.5800)\n",
            "EPOCH: 116 Validation Results: Acc 55.580 Loss: 1.2486\n",
            "Best Accuracy: 55.7400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [117][0/39]\tTime 0.124 (Avg-Time 0.124)\t Loss 1.1686 (Avg-Loss 1.1686)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [117][9/39]\tTime 0.056 (Avg-Time 0.070)\t Loss 1.2294 (Avg-Loss 1.1820)\tAcc 56.0547 (Avg-Acc 57.7832)\n",
            "Epoch: [117][18/39]\tTime 0.074 (Avg-Time 0.066)\t Loss 1.1529 (Avg-Loss 1.1839)\tAcc 58.5938 (Avg-Acc 57.8845)\n",
            "Epoch: [117][27/39]\tTime 0.057 (Avg-Time 0.064)\t Loss 1.0911 (Avg-Loss 1.1838)\tAcc 61.5234 (Avg-Acc 58.0566)\n",
            "Epoch: [117][36/39]\tTime 0.077 (Avg-Time 0.063)\t Loss 1.1720 (Avg-Loss 1.1842)\tAcc 58.0078 (Avg-Acc 58.0738)\n",
            "Epoch: [117][39/39]\tTime 0.006 (Avg-Time 0.061)\t Loss 1.2471 (Avg-Loss 1.1846)\tAcc 51.5625 (Avg-Acc 58.0250)\n",
            "EPOCH: 117 train Results: Acc 58.025 Loss: 1.1846\n",
            "Epoch: [117][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2449 (Avg-Loss 1.2449)\tAcc 56.5430 (Avg-Acc 56.5430)\n",
            "Epoch: [117][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2424 (Avg-Loss 1.2500)\tAcc 56.6327 (Avg-Acc 55.7600)\n",
            "EPOCH: 117 Validation Results: Acc 55.760 Loss: 1.2500\n",
            "Best Accuracy: 55.7600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [118][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1511 (Avg-Loss 1.1511)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [118][9/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1918 (Avg-Loss 1.1926)\tAcc 58.0078 (Avg-Acc 57.8223)\n",
            "Epoch: [118][18/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.1785 (Avg-Loss 1.1951)\tAcc 58.3008 (Avg-Acc 57.5966)\n",
            "Epoch: [118][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2171 (Avg-Loss 1.1879)\tAcc 57.5195 (Avg-Acc 57.7358)\n",
            "Epoch: [118][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1838 (Avg-Loss 1.1896)\tAcc 55.4688 (Avg-Acc 57.6515)\n",
            "Epoch: [118][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.3119 (Avg-Loss 1.1850)\tAcc 56.2500 (Avg-Acc 57.8275)\n",
            "EPOCH: 118 train Results: Acc 57.828 Loss: 1.1850\n",
            "Epoch: [118][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2253 (Avg-Loss 1.2253)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [118][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2219 (Avg-Loss 1.2393)\tAcc 55.8673 (Avg-Acc 55.8100)\n",
            "EPOCH: 118 Validation Results: Acc 55.810 Loss: 1.2393\n",
            "Best Accuracy: 55.8100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [119][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.0849 (Avg-Loss 1.0849)\tAcc 62.5000 (Avg-Acc 62.5000)\n",
            "Epoch: [119][9/39]\tTime 0.059 (Avg-Time 0.061)\t Loss 1.1961 (Avg-Loss 1.1834)\tAcc 56.9336 (Avg-Acc 57.8809)\n",
            "Epoch: [119][18/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1530 (Avg-Loss 1.1746)\tAcc 57.9102 (Avg-Acc 58.1517)\n",
            "Epoch: [119][27/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.2093 (Avg-Loss 1.1744)\tAcc 56.8359 (Avg-Acc 58.2729)\n",
            "Epoch: [119][36/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.1925 (Avg-Loss 1.1768)\tAcc 57.7148 (Avg-Acc 58.1451)\n",
            "Epoch: [119][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.2150 (Avg-Loss 1.1790)\tAcc 57.8125 (Avg-Acc 58.1125)\n",
            "EPOCH: 119 train Results: Acc 58.112 Loss: 1.1790\n",
            "Epoch: [119][0/9]\tTime 0.016 (Avg-Time 0.016)\t Loss 1.2375 (Avg-Loss 1.2375)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [119][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2321 (Avg-Loss 1.2409)\tAcc 55.8673 (Avg-Acc 55.8300)\n",
            "EPOCH: 119 Validation Results: Acc 55.830 Loss: 1.2409\n",
            "Best Accuracy: 55.8300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [120][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1788 (Avg-Loss 1.1788)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [120][9/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.2322 (Avg-Loss 1.1678)\tAcc 56.0547 (Avg-Acc 59.0723)\n",
            "Epoch: [120][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1310 (Avg-Loss 1.1689)\tAcc 58.3008 (Avg-Acc 58.6040)\n",
            "Epoch: [120][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.2416 (Avg-Loss 1.1695)\tAcc 54.3945 (Avg-Acc 58.5170)\n",
            "Epoch: [120][36/39]\tTime 0.060 (Avg-Time 0.061)\t Loss 1.2057 (Avg-Loss 1.1704)\tAcc 58.7891 (Avg-Acc 58.6413)\n",
            "Epoch: [120][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1110 (Avg-Loss 1.1733)\tAcc 57.8125 (Avg-Acc 58.5900)\n",
            "EPOCH: 120 train Results: Acc 58.590 Loss: 1.1733\n",
            "Epoch: [120][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2141 (Avg-Loss 1.2141)\tAcc 57.3242 (Avg-Acc 57.3242)\n",
            "Epoch: [120][9/9]\tTime 0.025 (Avg-Time 0.020)\t Loss 1.2366 (Avg-Loss 1.2431)\tAcc 55.6122 (Avg-Acc 55.3100)\n",
            "EPOCH: 120 Validation Results: Acc 55.310 Loss: 1.2431\n",
            "Best Accuracy: 55.8300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [121][0/39]\tTime 0.113 (Avg-Time 0.113)\t Loss 1.1573 (Avg-Loss 1.1573)\tAcc 58.1055 (Avg-Acc 58.1055)\n",
            "Epoch: [121][9/39]\tTime 0.322 (Avg-Time 0.158)\t Loss 1.1707 (Avg-Loss 1.1690)\tAcc 57.1289 (Avg-Acc 58.5059)\n",
            "Epoch: [121][18/39]\tTime 0.118 (Avg-Time 0.153)\t Loss 1.2272 (Avg-Loss 1.1728)\tAcc 57.8125 (Avg-Acc 58.3265)\n",
            "Epoch: [121][27/39]\tTime 0.070 (Avg-Time 0.130)\t Loss 1.2232 (Avg-Loss 1.1742)\tAcc 55.6641 (Avg-Acc 58.2485)\n",
            "Epoch: [121][36/39]\tTime 0.056 (Avg-Time 0.113)\t Loss 1.1919 (Avg-Loss 1.1762)\tAcc 56.7383 (Avg-Acc 58.3140)\n",
            "Epoch: [121][39/39]\tTime 0.006 (Avg-Time 0.107)\t Loss 1.0606 (Avg-Loss 1.1766)\tAcc 59.3750 (Avg-Acc 58.3150)\n",
            "EPOCH: 121 train Results: Acc 58.315 Loss: 1.1766\n",
            "Epoch: [121][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2245 (Avg-Loss 1.2245)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [121][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2330 (Avg-Loss 1.2368)\tAcc 57.2704 (Avg-Acc 55.9900)\n",
            "EPOCH: 121 Validation Results: Acc 55.990 Loss: 1.2368\n",
            "Best Accuracy: 55.9900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [122][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1602 (Avg-Loss 1.1602)\tAcc 58.3008 (Avg-Acc 58.3008)\n",
            "Epoch: [122][9/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.1748 (Avg-Loss 1.1772)\tAcc 58.8867 (Avg-Acc 57.9004)\n",
            "Epoch: [122][18/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1891 (Avg-Loss 1.1701)\tAcc 58.0078 (Avg-Acc 58.1980)\n",
            "Epoch: [122][27/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 1.2251 (Avg-Loss 1.1658)\tAcc 56.1523 (Avg-Acc 58.5763)\n",
            "Epoch: [122][36/39]\tTime 0.065 (Avg-Time 0.062)\t Loss 1.1322 (Avg-Loss 1.1674)\tAcc 59.1797 (Avg-Acc 58.4671)\n",
            "Epoch: [122][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.2627 (Avg-Loss 1.1668)\tAcc 53.1250 (Avg-Acc 58.4675)\n",
            "EPOCH: 122 train Results: Acc 58.468 Loss: 1.1668\n",
            "Epoch: [122][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2065 (Avg-Loss 1.2065)\tAcc 59.0820 (Avg-Acc 59.0820)\n",
            "Epoch: [122][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2353 (Avg-Loss 1.2364)\tAcc 55.9949 (Avg-Acc 55.8800)\n",
            "EPOCH: 122 Validation Results: Acc 55.880 Loss: 1.2364\n",
            "Best Accuracy: 55.9900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [123][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1510 (Avg-Loss 1.1510)\tAcc 60.9375 (Avg-Acc 60.9375)\n",
            "Epoch: [123][9/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1828 (Avg-Loss 1.1694)\tAcc 56.9336 (Avg-Acc 58.1055)\n",
            "Epoch: [123][18/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1938 (Avg-Loss 1.1742)\tAcc 56.7383 (Avg-Acc 57.7971)\n",
            "Epoch: [123][27/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1538 (Avg-Loss 1.1731)\tAcc 58.8867 (Avg-Acc 57.9834)\n",
            "Epoch: [123][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.2255 (Avg-Loss 1.1772)\tAcc 56.0547 (Avg-Acc 57.9049)\n",
            "Epoch: [123][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.1859 (Avg-Loss 1.1786)\tAcc 57.8125 (Avg-Acc 57.8850)\n",
            "EPOCH: 123 train Results: Acc 57.885 Loss: 1.1786\n",
            "Epoch: [123][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2268 (Avg-Loss 1.2268)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [123][9/9]\tTime 0.011 (Avg-Time 0.017)\t Loss 1.2323 (Avg-Loss 1.2534)\tAcc 56.3776 (Avg-Acc 55.2100)\n",
            "EPOCH: 123 Validation Results: Acc 55.210 Loss: 1.2534\n",
            "Best Accuracy: 55.9900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [124][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.1605 (Avg-Loss 1.1605)\tAcc 58.1055 (Avg-Acc 58.1055)\n",
            "Epoch: [124][9/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.1578 (Avg-Loss 1.1763)\tAcc 58.2031 (Avg-Acc 57.8613)\n",
            "Epoch: [124][18/39]\tTime 0.059 (Avg-Time 0.061)\t Loss 1.1505 (Avg-Loss 1.1798)\tAcc 58.9844 (Avg-Acc 58.0181)\n",
            "Epoch: [124][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1508 (Avg-Loss 1.1860)\tAcc 58.0078 (Avg-Acc 57.7985)\n",
            "Epoch: [124][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1364 (Avg-Loss 1.1825)\tAcc 59.6680 (Avg-Acc 57.8679)\n",
            "Epoch: [124][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.3012 (Avg-Loss 1.1818)\tAcc 56.2500 (Avg-Acc 57.8300)\n",
            "EPOCH: 124 train Results: Acc 57.830 Loss: 1.1818\n",
            "Epoch: [124][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2240 (Avg-Loss 1.2240)\tAcc 57.0312 (Avg-Acc 57.0312)\n",
            "Epoch: [124][9/9]\tTime 0.011 (Avg-Time 0.018)\t Loss 1.2467 (Avg-Loss 1.2480)\tAcc 56.1224 (Avg-Acc 55.3100)\n",
            "EPOCH: 124 Validation Results: Acc 55.310 Loss: 1.2480\n",
            "Best Accuracy: 55.9900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [125][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1438 (Avg-Loss 1.1438)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [125][9/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.2021 (Avg-Loss 1.1663)\tAcc 57.1289 (Avg-Acc 57.8711)\n",
            "Epoch: [125][18/39]\tTime 0.076 (Avg-Time 0.062)\t Loss 1.1651 (Avg-Loss 1.1723)\tAcc 59.9609 (Avg-Acc 57.7765)\n",
            "Epoch: [125][27/39]\tTime 0.118 (Avg-Time 0.094)\t Loss 1.1442 (Avg-Loss 1.1713)\tAcc 59.1797 (Avg-Acc 57.9729)\n",
            "Epoch: [125][36/39]\tTime 0.105 (Avg-Time 0.107)\t Loss 1.1743 (Avg-Loss 1.1705)\tAcc 60.1562 (Avg-Acc 58.1503)\n",
            "Epoch: [125][39/39]\tTime 0.011 (Avg-Time 0.106)\t Loss 1.3863 (Avg-Loss 1.1724)\tAcc 48.4375 (Avg-Acc 58.0875)\n",
            "EPOCH: 125 train Results: Acc 58.087 Loss: 1.1724\n",
            "Epoch: [125][0/9]\tTime 0.033 (Avg-Time 0.033)\t Loss 1.2245 (Avg-Loss 1.2245)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [125][9/9]\tTime 0.011 (Avg-Time 0.029)\t Loss 1.2469 (Avg-Loss 1.2443)\tAcc 56.2500 (Avg-Acc 55.6800)\n",
            "EPOCH: 125 Validation Results: Acc 55.680 Loss: 1.2443\n",
            "Best Accuracy: 55.9900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [126][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1372 (Avg-Loss 1.1372)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [126][9/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1250 (Avg-Loss 1.1732)\tAcc 60.4492 (Avg-Acc 58.2812)\n",
            "Epoch: [126][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1776 (Avg-Loss 1.1786)\tAcc 59.0820 (Avg-Acc 57.8485)\n",
            "Epoch: [126][27/39]\tTime 0.063 (Avg-Time 0.060)\t Loss 1.1517 (Avg-Loss 1.1710)\tAcc 59.1797 (Avg-Acc 58.1752)\n",
            "Epoch: [126][36/39]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.1720 (Avg-Loss 1.1714)\tAcc 56.6406 (Avg-Acc 58.0870)\n",
            "Epoch: [126][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1906 (Avg-Loss 1.1719)\tAcc 48.4375 (Avg-Acc 58.1200)\n",
            "EPOCH: 126 train Results: Acc 58.120 Loss: 1.1719\n",
            "Epoch: [126][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2410 (Avg-Loss 1.2410)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [126][9/9]\tTime 0.012 (Avg-Time 0.016)\t Loss 1.2592 (Avg-Loss 1.2523)\tAcc 54.4643 (Avg-Acc 55.0100)\n",
            "EPOCH: 126 Validation Results: Acc 55.010 Loss: 1.2523\n",
            "Best Accuracy: 55.9900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [127][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1271 (Avg-Loss 1.1271)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [127][9/39]\tTime 0.059 (Avg-Time 0.061)\t Loss 1.2143 (Avg-Loss 1.1798)\tAcc 58.6914 (Avg-Acc 58.0664)\n",
            "Epoch: [127][18/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1846 (Avg-Loss 1.1684)\tAcc 58.6914 (Avg-Acc 58.9227)\n",
            "Epoch: [127][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1593 (Avg-Loss 1.1682)\tAcc 59.9609 (Avg-Acc 58.8379)\n",
            "Epoch: [127][36/39]\tTime 0.059 (Avg-Time 0.060)\t Loss 1.1299 (Avg-Loss 1.1680)\tAcc 59.0820 (Avg-Acc 58.7785)\n",
            "Epoch: [127][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1464 (Avg-Loss 1.1667)\tAcc 59.3750 (Avg-Acc 58.7925)\n",
            "EPOCH: 127 train Results: Acc 58.792 Loss: 1.1667\n",
            "Epoch: [127][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2225 (Avg-Loss 1.2225)\tAcc 57.5195 (Avg-Acc 57.5195)\n",
            "Epoch: [127][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2485 (Avg-Loss 1.2353)\tAcc 54.9745 (Avg-Acc 55.7600)\n",
            "EPOCH: 127 Validation Results: Acc 55.760 Loss: 1.2353\n",
            "Best Accuracy: 55.9900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [128][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1249 (Avg-Loss 1.1249)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [128][9/39]\tTime 0.058 (Avg-Time 0.059)\t Loss 1.1099 (Avg-Loss 1.1510)\tAcc 59.5703 (Avg-Acc 59.0039)\n",
            "Epoch: [128][18/39]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.1491 (Avg-Loss 1.1599)\tAcc 58.3008 (Avg-Acc 58.7325)\n",
            "Epoch: [128][27/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.0942 (Avg-Loss 1.1581)\tAcc 62.3047 (Avg-Acc 58.7960)\n",
            "Epoch: [128][36/39]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.2027 (Avg-Loss 1.1610)\tAcc 56.6406 (Avg-Acc 58.6069)\n",
            "Epoch: [128][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.3313 (Avg-Loss 1.1615)\tAcc 51.5625 (Avg-Acc 58.5900)\n",
            "EPOCH: 128 train Results: Acc 58.590 Loss: 1.1615\n",
            "Epoch: [128][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2218 (Avg-Loss 1.2218)\tAcc 56.7383 (Avg-Acc 56.7383)\n",
            "Epoch: [128][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2469 (Avg-Loss 1.2481)\tAcc 55.7398 (Avg-Acc 55.2200)\n",
            "EPOCH: 128 Validation Results: Acc 55.220 Loss: 1.2481\n",
            "Best Accuracy: 55.9900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [129][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.0678 (Avg-Loss 1.0678)\tAcc 62.9883 (Avg-Acc 62.9883)\n",
            "Epoch: [129][9/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.2555 (Avg-Loss 1.1922)\tAcc 54.7852 (Avg-Acc 57.8027)\n",
            "Epoch: [129][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2080 (Avg-Loss 1.1880)\tAcc 54.8828 (Avg-Acc 57.7148)\n",
            "Epoch: [129][27/39]\tTime 0.059 (Avg-Time 0.061)\t Loss 1.1898 (Avg-Loss 1.1852)\tAcc 57.3242 (Avg-Acc 57.9206)\n",
            "Epoch: [129][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1695 (Avg-Loss 1.1757)\tAcc 60.0586 (Avg-Acc 58.3008)\n",
            "Epoch: [129][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 0.9634 (Avg-Loss 1.1768)\tAcc 75.0000 (Avg-Acc 58.3000)\n",
            "EPOCH: 129 train Results: Acc 58.300 Loss: 1.1768\n",
            "Epoch: [129][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1997 (Avg-Loss 1.1997)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [129][9/9]\tTime 0.016 (Avg-Time 0.022)\t Loss 1.2346 (Avg-Loss 1.2274)\tAcc 56.8878 (Avg-Acc 56.0500)\n",
            "EPOCH: 129 Validation Results: Acc 56.050 Loss: 1.2274\n",
            "Best Accuracy: 56.0500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [130][0/39]\tTime 0.126 (Avg-Time 0.126)\t Loss 1.1121 (Avg-Loss 1.1121)\tAcc 62.3047 (Avg-Acc 62.3047)\n",
            "Epoch: [130][9/39]\tTime 0.274 (Avg-Time 0.190)\t Loss 1.1686 (Avg-Loss 1.1542)\tAcc 56.8359 (Avg-Acc 59.0820)\n",
            "Epoch: [130][18/39]\tTime 0.067 (Avg-Time 0.163)\t Loss 1.2424 (Avg-Loss 1.1683)\tAcc 56.6406 (Avg-Acc 58.6092)\n",
            "Epoch: [130][27/39]\tTime 0.060 (Avg-Time 0.130)\t Loss 1.1563 (Avg-Loss 1.1612)\tAcc 59.3750 (Avg-Acc 58.7995)\n",
            "Epoch: [130][36/39]\tTime 0.076 (Avg-Time 0.113)\t Loss 1.1262 (Avg-Loss 1.1600)\tAcc 60.1562 (Avg-Acc 58.9105)\n",
            "Epoch: [130][39/39]\tTime 0.006 (Avg-Time 0.108)\t Loss 1.2935 (Avg-Loss 1.1593)\tAcc 51.5625 (Avg-Acc 58.9300)\n",
            "EPOCH: 130 train Results: Acc 58.930 Loss: 1.1593\n",
            "Epoch: [130][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2034 (Avg-Loss 1.2034)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [130][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2419 (Avg-Loss 1.2288)\tAcc 55.2296 (Avg-Acc 55.9600)\n",
            "EPOCH: 130 Validation Results: Acc 55.960 Loss: 1.2288\n",
            "Best Accuracy: 56.0500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [131][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1200 (Avg-Loss 1.1200)\tAcc 59.6680 (Avg-Acc 59.6680)\n",
            "Epoch: [131][9/39]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.1393 (Avg-Loss 1.1518)\tAcc 59.3750 (Avg-Acc 58.8281)\n",
            "Epoch: [131][18/39]\tTime 0.060 (Avg-Time 0.061)\t Loss 1.1843 (Avg-Loss 1.1569)\tAcc 57.3242 (Avg-Acc 58.9278)\n",
            "Epoch: [131][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1495 (Avg-Loss 1.1607)\tAcc 56.3477 (Avg-Acc 58.8100)\n",
            "Epoch: [131][36/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.1988 (Avg-Loss 1.1631)\tAcc 57.7148 (Avg-Acc 58.6835)\n",
            "Epoch: [131][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 0.9749 (Avg-Loss 1.1636)\tAcc 65.6250 (Avg-Acc 58.6775)\n",
            "EPOCH: 131 train Results: Acc 58.678 Loss: 1.1636\n",
            "Epoch: [131][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2135 (Avg-Loss 1.2135)\tAcc 57.9102 (Avg-Acc 57.9102)\n",
            "Epoch: [131][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2285 (Avg-Loss 1.2298)\tAcc 57.7806 (Avg-Acc 56.0500)\n",
            "EPOCH: 131 Validation Results: Acc 56.050 Loss: 1.2298\n",
            "Best Accuracy: 56.0500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [132][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.1229 (Avg-Loss 1.1229)\tAcc 62.1094 (Avg-Acc 62.1094)\n",
            "Epoch: [132][9/39]\tTime 0.060 (Avg-Time 0.062)\t Loss 1.1279 (Avg-Loss 1.1494)\tAcc 60.1562 (Avg-Acc 59.1504)\n",
            "Epoch: [132][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1787 (Avg-Loss 1.1596)\tAcc 58.1055 (Avg-Acc 58.9278)\n",
            "Epoch: [132][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1834 (Avg-Loss 1.1585)\tAcc 57.6172 (Avg-Acc 58.9146)\n",
            "Epoch: [132][36/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.1421 (Avg-Loss 1.1559)\tAcc 59.7656 (Avg-Acc 58.9395)\n",
            "Epoch: [132][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.2725 (Avg-Loss 1.1567)\tAcc 56.2500 (Avg-Acc 58.9050)\n",
            "EPOCH: 132 train Results: Acc 58.905 Loss: 1.1567\n",
            "Epoch: [132][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2068 (Avg-Loss 1.2068)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [132][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2359 (Avg-Loss 1.2395)\tAcc 57.5255 (Avg-Acc 55.9800)\n",
            "EPOCH: 132 Validation Results: Acc 55.980 Loss: 1.2395\n",
            "Best Accuracy: 56.0500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [133][0/39]\tTime 0.065 (Avg-Time 0.065)\t Loss 1.1239 (Avg-Loss 1.1239)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [133][9/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1565 (Avg-Loss 1.1727)\tAcc 58.9844 (Avg-Acc 58.1152)\n",
            "Epoch: [133][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.2063 (Avg-Loss 1.1727)\tAcc 56.3477 (Avg-Acc 58.2699)\n",
            "Epoch: [133][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1060 (Avg-Loss 1.1729)\tAcc 59.3750 (Avg-Acc 58.4159)\n",
            "Epoch: [133][36/39]\tTime 0.059 (Avg-Time 0.060)\t Loss 1.1167 (Avg-Loss 1.1700)\tAcc 59.7656 (Avg-Acc 58.5093)\n",
            "Epoch: [133][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.1405 (Avg-Loss 1.1702)\tAcc 64.0625 (Avg-Acc 58.4900)\n",
            "EPOCH: 133 train Results: Acc 58.490 Loss: 1.1702\n",
            "Epoch: [133][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2070 (Avg-Loss 1.2070)\tAcc 58.3008 (Avg-Acc 58.3008)\n",
            "Epoch: [133][9/9]\tTime 0.014 (Avg-Time 0.016)\t Loss 1.2140 (Avg-Loss 1.2375)\tAcc 57.5255 (Avg-Acc 55.9600)\n",
            "EPOCH: 133 Validation Results: Acc 55.960 Loss: 1.2375\n",
            "Best Accuracy: 56.0500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [134][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1217 (Avg-Loss 1.1217)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [134][9/39]\tTime 0.061 (Avg-Time 0.062)\t Loss 1.1863 (Avg-Loss 1.1625)\tAcc 58.3008 (Avg-Acc 58.9453)\n",
            "Epoch: [134][18/39]\tTime 0.120 (Avg-Time 0.071)\t Loss 1.1507 (Avg-Loss 1.1622)\tAcc 59.0820 (Avg-Acc 58.8559)\n",
            "Epoch: [134][27/39]\tTime 0.157 (Avg-Time 0.115)\t Loss 1.1598 (Avg-Loss 1.1604)\tAcc 59.4727 (Avg-Acc 59.0681)\n",
            "Epoch: [134][36/39]\tTime 0.060 (Avg-Time 0.115)\t Loss 1.1719 (Avg-Loss 1.1568)\tAcc 56.7383 (Avg-Acc 59.0899)\n",
            "Epoch: [134][39/39]\tTime 0.006 (Avg-Time 0.110)\t Loss 1.3357 (Avg-Loss 1.1586)\tAcc 54.6875 (Avg-Acc 59.0700)\n",
            "EPOCH: 134 train Results: Acc 59.070 Loss: 1.1586\n",
            "Epoch: [134][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2164 (Avg-Loss 1.2164)\tAcc 58.8867 (Avg-Acc 58.8867)\n",
            "Epoch: [134][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2324 (Avg-Loss 1.2299)\tAcc 58.6735 (Avg-Acc 55.9000)\n",
            "EPOCH: 134 Validation Results: Acc 55.900 Loss: 1.2299\n",
            "Best Accuracy: 56.0500\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [135][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1092 (Avg-Loss 1.1092)\tAcc 62.0117 (Avg-Acc 62.0117)\n",
            "Epoch: [135][9/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1647 (Avg-Loss 1.1653)\tAcc 58.2031 (Avg-Acc 58.4277)\n",
            "Epoch: [135][18/39]\tTime 0.081 (Avg-Time 0.062)\t Loss 1.1568 (Avg-Loss 1.1665)\tAcc 59.8633 (Avg-Acc 58.6246)\n",
            "Epoch: [135][27/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.1766 (Avg-Loss 1.1616)\tAcc 57.4219 (Avg-Acc 58.7891)\n",
            "Epoch: [135][36/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.1706 (Avg-Loss 1.1624)\tAcc 59.5703 (Avg-Acc 58.8102)\n",
            "Epoch: [135][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.1002 (Avg-Loss 1.1635)\tAcc 59.3750 (Avg-Acc 58.7500)\n",
            "EPOCH: 135 train Results: Acc 58.750 Loss: 1.1635\n",
            "Epoch: [135][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.1960 (Avg-Loss 1.1960)\tAcc 59.6680 (Avg-Acc 59.6680)\n",
            "Epoch: [135][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2237 (Avg-Loss 1.2245)\tAcc 58.1633 (Avg-Acc 56.3300)\n",
            "EPOCH: 135 Validation Results: Acc 56.330 Loss: 1.2245\n",
            "Best Accuracy: 56.3300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [136][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.0814 (Avg-Loss 1.0814)\tAcc 61.4258 (Avg-Acc 61.4258)\n",
            "Epoch: [136][9/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1260 (Avg-Loss 1.1435)\tAcc 58.9844 (Avg-Acc 59.3066)\n",
            "Epoch: [136][18/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1075 (Avg-Loss 1.1503)\tAcc 60.0586 (Avg-Acc 59.0872)\n",
            "Epoch: [136][27/39]\tTime 0.059 (Avg-Time 0.060)\t Loss 1.1762 (Avg-Loss 1.1474)\tAcc 58.2031 (Avg-Acc 59.2006)\n",
            "Epoch: [136][36/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1854 (Avg-Loss 1.1545)\tAcc 58.5938 (Avg-Acc 58.9527)\n",
            "Epoch: [136][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.1737 (Avg-Loss 1.1523)\tAcc 57.8125 (Avg-Acc 59.0625)\n",
            "EPOCH: 136 train Results: Acc 59.062 Loss: 1.1523\n",
            "Epoch: [136][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2203 (Avg-Loss 1.2203)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [136][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2337 (Avg-Loss 1.2322)\tAcc 57.3980 (Avg-Acc 56.6300)\n",
            "EPOCH: 136 Validation Results: Acc 56.630 Loss: 1.2322\n",
            "Best Accuracy: 56.6300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [137][0/39]\tTime 0.062 (Avg-Time 0.062)\t Loss 1.1123 (Avg-Loss 1.1123)\tAcc 60.9375 (Avg-Acc 60.9375)\n",
            "Epoch: [137][9/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1903 (Avg-Loss 1.1611)\tAcc 58.1055 (Avg-Acc 59.0430)\n",
            "Epoch: [137][18/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1005 (Avg-Loss 1.1675)\tAcc 59.4727 (Avg-Acc 58.7325)\n",
            "Epoch: [137][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1759 (Avg-Loss 1.1652)\tAcc 58.0078 (Avg-Acc 58.7472)\n",
            "Epoch: [137][36/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1526 (Avg-Loss 1.1631)\tAcc 60.0586 (Avg-Acc 58.7178)\n",
            "Epoch: [137][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 0.9217 (Avg-Loss 1.1615)\tAcc 76.5625 (Avg-Acc 58.7125)\n",
            "EPOCH: 137 train Results: Acc 58.712 Loss: 1.1615\n",
            "Epoch: [137][0/9]\tTime 0.039 (Avg-Time 0.039)\t Loss 1.2323 (Avg-Loss 1.2323)\tAcc 57.7148 (Avg-Acc 57.7148)\n",
            "Epoch: [137][9/9]\tTime 0.011 (Avg-Time 0.018)\t Loss 1.2544 (Avg-Loss 1.2392)\tAcc 56.3776 (Avg-Acc 55.8200)\n",
            "EPOCH: 137 Validation Results: Acc 55.820 Loss: 1.2392\n",
            "Best Accuracy: 56.6300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [138][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1368 (Avg-Loss 1.1368)\tAcc 59.4727 (Avg-Acc 59.4727)\n",
            "Epoch: [138][9/39]\tTime 0.058 (Avg-Time 0.059)\t Loss 1.1807 (Avg-Loss 1.1698)\tAcc 58.3984 (Avg-Acc 58.6035)\n",
            "Epoch: [138][18/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1308 (Avg-Loss 1.1666)\tAcc 59.4727 (Avg-Acc 58.5681)\n",
            "Epoch: [138][27/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1999 (Avg-Loss 1.1631)\tAcc 57.8125 (Avg-Acc 58.7088)\n",
            "Epoch: [138][36/39]\tTime 0.091 (Avg-Time 0.062)\t Loss 1.1302 (Avg-Loss 1.1592)\tAcc 61.4258 (Avg-Acc 58.8445)\n",
            "Epoch: [138][39/39]\tTime 0.013 (Avg-Time 0.062)\t Loss 1.1007 (Avg-Loss 1.1580)\tAcc 62.5000 (Avg-Acc 58.8800)\n",
            "EPOCH: 138 train Results: Acc 58.880 Loss: 1.1580\n",
            "Epoch: [138][0/9]\tTime 0.043 (Avg-Time 0.043)\t Loss 1.2169 (Avg-Loss 1.2169)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [138][9/9]\tTime 0.040 (Avg-Time 0.036)\t Loss 1.2276 (Avg-Loss 1.2340)\tAcc 56.7602 (Avg-Acc 56.0500)\n",
            "EPOCH: 138 Validation Results: Acc 56.050 Loss: 1.2340\n",
            "Best Accuracy: 56.6300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [139][0/39]\tTime 0.340 (Avg-Time 0.340)\t Loss 1.1212 (Avg-Loss 1.1212)\tAcc 59.6680 (Avg-Acc 59.6680)\n",
            "Epoch: [139][9/39]\tTime 0.128 (Avg-Time 0.215)\t Loss 1.1828 (Avg-Loss 1.1753)\tAcc 58.6914 (Avg-Acc 57.8809)\n",
            "Epoch: [139][18/39]\tTime 0.060 (Avg-Time 0.148)\t Loss 1.1200 (Avg-Loss 1.1677)\tAcc 58.5938 (Avg-Acc 58.2494)\n",
            "Epoch: [139][27/39]\tTime 0.081 (Avg-Time 0.121)\t Loss 1.1801 (Avg-Loss 1.1668)\tAcc 59.6680 (Avg-Acc 58.4124)\n",
            "Epoch: [139][36/39]\tTime 0.057 (Avg-Time 0.106)\t Loss 1.1835 (Avg-Loss 1.1588)\tAcc 57.6172 (Avg-Acc 58.6281)\n",
            "Epoch: [139][39/39]\tTime 0.006 (Avg-Time 0.101)\t Loss 1.1394 (Avg-Loss 1.1578)\tAcc 62.5000 (Avg-Acc 58.7075)\n",
            "EPOCH: 139 train Results: Acc 58.708 Loss: 1.1578\n",
            "Epoch: [139][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2085 (Avg-Loss 1.2085)\tAcc 57.3242 (Avg-Acc 57.3242)\n",
            "Epoch: [139][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2235 (Avg-Loss 1.2425)\tAcc 57.7806 (Avg-Acc 55.7000)\n",
            "EPOCH: 139 Validation Results: Acc 55.700 Loss: 1.2425\n",
            "Best Accuracy: 56.6300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [140][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1252 (Avg-Loss 1.1252)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [140][9/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.2110 (Avg-Loss 1.1324)\tAcc 56.3477 (Avg-Acc 59.9414)\n",
            "Epoch: [140][18/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1312 (Avg-Loss 1.1383)\tAcc 59.5703 (Avg-Acc 59.6166)\n",
            "Epoch: [140][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1494 (Avg-Loss 1.1417)\tAcc 60.2539 (Avg-Acc 59.6017)\n",
            "Epoch: [140][36/39]\tTime 0.084 (Avg-Time 0.061)\t Loss 1.1765 (Avg-Loss 1.1478)\tAcc 58.2031 (Avg-Acc 59.3328)\n",
            "Epoch: [140][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 0.9923 (Avg-Loss 1.1461)\tAcc 62.5000 (Avg-Acc 59.3900)\n",
            "EPOCH: 140 train Results: Acc 59.390 Loss: 1.1461\n",
            "Epoch: [140][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2239 (Avg-Loss 1.2239)\tAcc 57.7148 (Avg-Acc 57.7148)\n",
            "Epoch: [140][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2225 (Avg-Loss 1.2275)\tAcc 56.3776 (Avg-Acc 56.1400)\n",
            "EPOCH: 140 Validation Results: Acc 56.140 Loss: 1.2275\n",
            "Best Accuracy: 56.6300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [141][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1376 (Avg-Loss 1.1376)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [141][9/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1321 (Avg-Loss 1.1530)\tAcc 60.2539 (Avg-Acc 59.3262)\n",
            "Epoch: [141][18/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1770 (Avg-Loss 1.1548)\tAcc 57.2266 (Avg-Acc 58.9073)\n",
            "Epoch: [141][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1300 (Avg-Loss 1.1470)\tAcc 57.7148 (Avg-Acc 59.2948)\n",
            "Epoch: [141][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1210 (Avg-Loss 1.1478)\tAcc 59.1797 (Avg-Acc 59.2905)\n",
            "Epoch: [141][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.0470 (Avg-Loss 1.1486)\tAcc 70.3125 (Avg-Acc 59.2150)\n",
            "EPOCH: 141 train Results: Acc 59.215 Loss: 1.1486\n",
            "Epoch: [141][0/9]\tTime 0.017 (Avg-Time 0.017)\t Loss 1.2276 (Avg-Loss 1.2276)\tAcc 57.8125 (Avg-Acc 57.8125)\n",
            "Epoch: [141][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2399 (Avg-Loss 1.2306)\tAcc 57.5255 (Avg-Acc 56.5100)\n",
            "EPOCH: 141 Validation Results: Acc 56.510 Loss: 1.2306\n",
            "Best Accuracy: 56.6300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [142][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.0646 (Avg-Loss 1.0646)\tAcc 63.3789 (Avg-Acc 63.3789)\n",
            "Epoch: [142][9/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1146 (Avg-Loss 1.1314)\tAcc 59.0820 (Avg-Acc 59.8145)\n",
            "Epoch: [142][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1432 (Avg-Loss 1.1405)\tAcc 59.0820 (Avg-Acc 59.4572)\n",
            "Epoch: [142][27/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.2026 (Avg-Loss 1.1422)\tAcc 56.6406 (Avg-Acc 59.4831)\n",
            "Epoch: [142][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1779 (Avg-Loss 1.1449)\tAcc 59.9609 (Avg-Acc 59.3988)\n",
            "Epoch: [142][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.1543 (Avg-Loss 1.1450)\tAcc 51.5625 (Avg-Acc 59.3875)\n",
            "EPOCH: 142 train Results: Acc 59.388 Loss: 1.1450\n",
            "Epoch: [142][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1993 (Avg-Loss 1.1993)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [142][9/9]\tTime 0.011 (Avg-Time 0.017)\t Loss 1.2336 (Avg-Loss 1.2293)\tAcc 57.2704 (Avg-Acc 56.2900)\n",
            "EPOCH: 142 Validation Results: Acc 56.290 Loss: 1.2293\n",
            "Best Accuracy: 56.6300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [143][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1007 (Avg-Loss 1.1007)\tAcc 62.1094 (Avg-Acc 62.1094)\n",
            "Epoch: [143][9/39]\tTime 0.058 (Avg-Time 0.059)\t Loss 1.1750 (Avg-Loss 1.1403)\tAcc 58.3008 (Avg-Acc 59.6484)\n",
            "Epoch: [143][18/39]\tTime 0.086 (Avg-Time 0.078)\t Loss 1.1264 (Avg-Loss 1.1470)\tAcc 61.2305 (Avg-Acc 59.2876)\n",
            "Epoch: [143][27/39]\tTime 0.126 (Avg-Time 0.119)\t Loss 1.1852 (Avg-Loss 1.1502)\tAcc 58.0078 (Avg-Acc 59.0995)\n",
            "Epoch: [143][36/39]\tTime 0.059 (Avg-Time 0.114)\t Loss 1.1508 (Avg-Loss 1.1469)\tAcc 58.1055 (Avg-Acc 59.3169)\n",
            "Epoch: [143][39/39]\tTime 0.006 (Avg-Time 0.109)\t Loss 1.3109 (Avg-Loss 1.1488)\tAcc 50.0000 (Avg-Acc 59.2575)\n",
            "EPOCH: 143 train Results: Acc 59.258 Loss: 1.1488\n",
            "Epoch: [143][0/9]\tTime 0.019 (Avg-Time 0.019)\t Loss 1.2058 (Avg-Loss 1.2058)\tAcc 56.4453 (Avg-Acc 56.4453)\n",
            "Epoch: [143][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2337 (Avg-Loss 1.2367)\tAcc 56.7602 (Avg-Acc 55.5900)\n",
            "EPOCH: 143 Validation Results: Acc 55.590 Loss: 1.2367\n",
            "Best Accuracy: 56.6300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [144][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1597 (Avg-Loss 1.1597)\tAcc 59.2773 (Avg-Acc 59.2773)\n",
            "Epoch: [144][9/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1859 (Avg-Loss 1.1571)\tAcc 58.3008 (Avg-Acc 59.1406)\n",
            "Epoch: [144][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1373 (Avg-Loss 1.1578)\tAcc 58.3008 (Avg-Acc 58.9535)\n",
            "Epoch: [144][27/39]\tTime 0.067 (Avg-Time 0.061)\t Loss 1.2219 (Avg-Loss 1.1557)\tAcc 56.4453 (Avg-Acc 59.0297)\n",
            "Epoch: [144][36/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.1718 (Avg-Loss 1.1549)\tAcc 57.8125 (Avg-Acc 59.1190)\n",
            "Epoch: [144][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.3219 (Avg-Loss 1.1543)\tAcc 56.2500 (Avg-Acc 59.1300)\n",
            "EPOCH: 144 train Results: Acc 59.130 Loss: 1.1543\n",
            "Epoch: [144][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2140 (Avg-Loss 1.2140)\tAcc 59.6680 (Avg-Acc 59.6680)\n",
            "Epoch: [144][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2315 (Avg-Loss 1.2320)\tAcc 57.6531 (Avg-Acc 56.4700)\n",
            "EPOCH: 144 Validation Results: Acc 56.470 Loss: 1.2320\n",
            "Best Accuracy: 56.6300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [145][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1539 (Avg-Loss 1.1539)\tAcc 60.8398 (Avg-Acc 60.8398)\n",
            "Epoch: [145][9/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1859 (Avg-Loss 1.1534)\tAcc 56.8359 (Avg-Acc 58.8379)\n",
            "Epoch: [145][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1915 (Avg-Loss 1.1547)\tAcc 58.8867 (Avg-Acc 59.0461)\n",
            "Epoch: [145][27/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.1934 (Avg-Loss 1.1512)\tAcc 57.3242 (Avg-Acc 59.2111)\n",
            "Epoch: [145][36/39]\tTime 0.062 (Avg-Time 0.061)\t Loss 1.2074 (Avg-Loss 1.1505)\tAcc 58.3984 (Avg-Acc 59.3380)\n",
            "Epoch: [145][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 0.9975 (Avg-Loss 1.1495)\tAcc 67.1875 (Avg-Acc 59.3125)\n",
            "EPOCH: 145 train Results: Acc 59.312 Loss: 1.1495\n",
            "Epoch: [145][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1983 (Avg-Loss 1.1983)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [145][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2376 (Avg-Loss 1.2250)\tAcc 56.6327 (Avg-Acc 56.8300)\n",
            "EPOCH: 145 Validation Results: Acc 56.830 Loss: 1.2250\n",
            "Best Accuracy: 56.8300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [146][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1425 (Avg-Loss 1.1425)\tAcc 61.2305 (Avg-Acc 61.2305)\n",
            "Epoch: [146][9/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1168 (Avg-Loss 1.1381)\tAcc 61.5234 (Avg-Acc 59.7070)\n",
            "Epoch: [146][18/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1509 (Avg-Loss 1.1426)\tAcc 59.2773 (Avg-Acc 59.4521)\n",
            "Epoch: [146][27/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1131 (Avg-Loss 1.1364)\tAcc 60.5469 (Avg-Acc 59.7412)\n",
            "Epoch: [146][36/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.0928 (Avg-Loss 1.1359)\tAcc 60.9375 (Avg-Acc 59.7498)\n",
            "Epoch: [146][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.3624 (Avg-Loss 1.1374)\tAcc 50.0000 (Avg-Acc 59.6900)\n",
            "EPOCH: 146 train Results: Acc 59.690 Loss: 1.1374\n",
            "Epoch: [146][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.1970 (Avg-Loss 1.1970)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [146][9/9]\tTime 0.012 (Avg-Time 0.016)\t Loss 1.2172 (Avg-Loss 1.2217)\tAcc 57.1429 (Avg-Acc 56.4400)\n",
            "EPOCH: 146 Validation Results: Acc 56.440 Loss: 1.2217\n",
            "Best Accuracy: 56.8300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [147][0/39]\tTime 0.062 (Avg-Time 0.062)\t Loss 1.0847 (Avg-Loss 1.0847)\tAcc 60.1562 (Avg-Acc 60.1562)\n",
            "Epoch: [147][9/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1906 (Avg-Loss 1.1540)\tAcc 58.4961 (Avg-Acc 58.3984)\n",
            "Epoch: [147][18/39]\tTime 0.055 (Avg-Time 0.059)\t Loss 1.1216 (Avg-Loss 1.1465)\tAcc 60.6445 (Avg-Acc 58.8610)\n",
            "Epoch: [147][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1438 (Avg-Loss 1.1521)\tAcc 58.1055 (Avg-Acc 58.5798)\n",
            "Epoch: [147][36/39]\tTime 0.115 (Avg-Time 0.065)\t Loss 1.1210 (Avg-Loss 1.1482)\tAcc 59.7656 (Avg-Acc 59.0240)\n",
            "Epoch: [147][39/39]\tTime 0.025 (Avg-Time 0.066)\t Loss 1.1182 (Avg-Loss 1.1479)\tAcc 64.0625 (Avg-Acc 59.0175)\n",
            "EPOCH: 147 train Results: Acc 59.017 Loss: 1.1479\n",
            "Epoch: [147][0/9]\tTime 0.034 (Avg-Time 0.034)\t Loss 1.2175 (Avg-Loss 1.2175)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [147][9/9]\tTime 0.027 (Avg-Time 0.031)\t Loss 1.2337 (Avg-Loss 1.2325)\tAcc 57.3980 (Avg-Acc 56.1500)\n",
            "EPOCH: 147 Validation Results: Acc 56.150 Loss: 1.2325\n",
            "Best Accuracy: 56.8300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [148][0/39]\tTime 0.254 (Avg-Time 0.254)\t Loss 1.1240 (Avg-Loss 1.1240)\tAcc 59.6680 (Avg-Acc 59.6680)\n",
            "Epoch: [148][9/39]\tTime 0.136 (Avg-Time 0.167)\t Loss 1.1848 (Avg-Loss 1.1459)\tAcc 57.3242 (Avg-Acc 59.4531)\n",
            "Epoch: [148][18/39]\tTime 0.056 (Avg-Time 0.137)\t Loss 1.1292 (Avg-Loss 1.1484)\tAcc 58.9844 (Avg-Acc 59.0820)\n",
            "Epoch: [148][27/39]\tTime 0.057 (Avg-Time 0.112)\t Loss 1.0870 (Avg-Loss 1.1439)\tAcc 59.9609 (Avg-Acc 59.3541)\n",
            "Epoch: [148][36/39]\tTime 0.056 (Avg-Time 0.100)\t Loss 1.1732 (Avg-Loss 1.1427)\tAcc 58.5938 (Avg-Acc 59.5228)\n",
            "Epoch: [148][39/39]\tTime 0.006 (Avg-Time 0.096)\t Loss 1.2321 (Avg-Loss 1.1430)\tAcc 56.2500 (Avg-Acc 59.5375)\n",
            "EPOCH: 148 train Results: Acc 59.538 Loss: 1.1430\n",
            "Epoch: [148][0/9]\tTime 0.018 (Avg-Time 0.018)\t Loss 1.2082 (Avg-Loss 1.2082)\tAcc 58.6914 (Avg-Acc 58.6914)\n",
            "Epoch: [148][9/9]\tTime 0.014 (Avg-Time 0.017)\t Loss 1.2297 (Avg-Loss 1.2187)\tAcc 57.5255 (Avg-Acc 56.8900)\n",
            "EPOCH: 148 Validation Results: Acc 56.890 Loss: 1.2187\n",
            "Best Accuracy: 56.8900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [149][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1175 (Avg-Loss 1.1175)\tAcc 61.4258 (Avg-Acc 61.4258)\n",
            "Epoch: [149][9/39]\tTime 0.057 (Avg-Time 0.058)\t Loss 1.2185 (Avg-Loss 1.1496)\tAcc 58.0078 (Avg-Acc 59.3848)\n",
            "Epoch: [149][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1488 (Avg-Loss 1.1460)\tAcc 60.0586 (Avg-Acc 59.4470)\n",
            "Epoch: [149][27/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1210 (Avg-Loss 1.1452)\tAcc 58.7891 (Avg-Acc 59.5075)\n",
            "Epoch: [149][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1736 (Avg-Loss 1.1457)\tAcc 58.4961 (Avg-Acc 59.4568)\n",
            "Epoch: [149][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.2116 (Avg-Loss 1.1473)\tAcc 57.8125 (Avg-Acc 59.3925)\n",
            "EPOCH: 149 train Results: Acc 59.392 Loss: 1.1473\n",
            "Epoch: [149][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2333 (Avg-Loss 1.2333)\tAcc 58.4961 (Avg-Acc 58.4961)\n",
            "Epoch: [149][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2322 (Avg-Loss 1.2422)\tAcc 55.9949 (Avg-Acc 55.3600)\n",
            "EPOCH: 149 Validation Results: Acc 55.360 Loss: 1.2422\n",
            "Best Accuracy: 56.8900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [150][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1593 (Avg-Loss 1.1593)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [150][9/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 1.1868 (Avg-Loss 1.1581)\tAcc 57.0312 (Avg-Acc 59.1113)\n",
            "Epoch: [150][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1353 (Avg-Loss 1.1549)\tAcc 59.6680 (Avg-Acc 58.8919)\n",
            "Epoch: [150][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1323 (Avg-Loss 1.1520)\tAcc 60.1562 (Avg-Acc 58.9844)\n",
            "Epoch: [150][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1675 (Avg-Loss 1.1497)\tAcc 58.8867 (Avg-Acc 59.0873)\n",
            "Epoch: [150][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 0.9786 (Avg-Loss 1.1488)\tAcc 68.7500 (Avg-Acc 59.1025)\n",
            "EPOCH: 150 train Results: Acc 59.102 Loss: 1.1488\n",
            "Epoch: [150][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2028 (Avg-Loss 1.2028)\tAcc 58.4961 (Avg-Acc 58.4961)\n",
            "Epoch: [150][9/9]\tTime 0.011 (Avg-Time 0.018)\t Loss 1.2262 (Avg-Loss 1.2273)\tAcc 56.6327 (Avg-Acc 56.2500)\n",
            "EPOCH: 150 Validation Results: Acc 56.250 Loss: 1.2273\n",
            "Best Accuracy: 56.8900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [151][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.0923 (Avg-Loss 1.0923)\tAcc 60.5469 (Avg-Acc 60.5469)\n",
            "Epoch: [151][9/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1614 (Avg-Loss 1.1449)\tAcc 59.7656 (Avg-Acc 59.6582)\n",
            "Epoch: [151][18/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.1922 (Avg-Loss 1.1470)\tAcc 56.2500 (Avg-Acc 59.3442)\n",
            "Epoch: [151][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1946 (Avg-Loss 1.1475)\tAcc 57.8125 (Avg-Acc 59.2529)\n",
            "Epoch: [151][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1299 (Avg-Loss 1.1436)\tAcc 59.0820 (Avg-Acc 59.2668)\n",
            "Epoch: [151][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1598 (Avg-Loss 1.1416)\tAcc 54.6875 (Avg-Acc 59.3475)\n",
            "EPOCH: 151 train Results: Acc 59.347 Loss: 1.1416\n",
            "Epoch: [151][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2014 (Avg-Loss 1.2014)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [151][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2122 (Avg-Loss 1.2130)\tAcc 57.6531 (Avg-Acc 57.1400)\n",
            "EPOCH: 151 Validation Results: Acc 57.140 Loss: 1.2130\n",
            "Best Accuracy: 57.1400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [152][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1341 (Avg-Loss 1.1341)\tAcc 60.0586 (Avg-Acc 60.0586)\n",
            "Epoch: [152][9/39]\tTime 0.081 (Avg-Time 0.061)\t Loss 1.1886 (Avg-Loss 1.1484)\tAcc 57.0312 (Avg-Acc 58.8281)\n",
            "Epoch: [152][18/39]\tTime 0.122 (Avg-Time 0.069)\t Loss 1.1853 (Avg-Loss 1.1489)\tAcc 58.7891 (Avg-Acc 58.8045)\n",
            "Epoch: [152][27/39]\tTime 0.161 (Avg-Time 0.114)\t Loss 1.1192 (Avg-Loss 1.1468)\tAcc 60.1562 (Avg-Acc 58.9076)\n",
            "Epoch: [152][36/39]\tTime 0.126 (Avg-Time 0.113)\t Loss 1.1264 (Avg-Loss 1.1461)\tAcc 60.3516 (Avg-Acc 59.0372)\n",
            "Epoch: [152][39/39]\tTime 0.006 (Avg-Time 0.109)\t Loss 1.3529 (Avg-Loss 1.1466)\tAcc 53.1250 (Avg-Acc 59.0700)\n",
            "EPOCH: 152 train Results: Acc 59.070 Loss: 1.1466\n",
            "Epoch: [152][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2037 (Avg-Loss 1.2037)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [152][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2213 (Avg-Loss 1.2208)\tAcc 56.3776 (Avg-Acc 56.5400)\n",
            "EPOCH: 152 Validation Results: Acc 56.540 Loss: 1.2208\n",
            "Best Accuracy: 57.1400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [153][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1798 (Avg-Loss 1.1798)\tAcc 59.8633 (Avg-Acc 59.8633)\n",
            "Epoch: [153][9/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1360 (Avg-Loss 1.1382)\tAcc 60.9375 (Avg-Acc 59.6582)\n",
            "Epoch: [153][18/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1639 (Avg-Loss 1.1470)\tAcc 57.5195 (Avg-Acc 59.1591)\n",
            "Epoch: [153][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1380 (Avg-Loss 1.1454)\tAcc 59.8633 (Avg-Acc 59.4552)\n",
            "Epoch: [153][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1346 (Avg-Loss 1.1395)\tAcc 61.1328 (Avg-Acc 59.5703)\n",
            "Epoch: [153][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.2760 (Avg-Loss 1.1394)\tAcc 50.0000 (Avg-Acc 59.6100)\n",
            "EPOCH: 153 train Results: Acc 59.610 Loss: 1.1394\n",
            "Epoch: [153][0/9]\tTime 0.018 (Avg-Time 0.018)\t Loss 1.2006 (Avg-Loss 1.2006)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [153][9/9]\tTime 0.011 (Avg-Time 0.017)\t Loss 1.2249 (Avg-Loss 1.2234)\tAcc 57.1429 (Avg-Acc 56.9100)\n",
            "EPOCH: 153 Validation Results: Acc 56.910 Loss: 1.2234\n",
            "Best Accuracy: 57.1400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [154][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.1109 (Avg-Loss 1.1109)\tAcc 61.0352 (Avg-Acc 61.0352)\n",
            "Epoch: [154][9/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1884 (Avg-Loss 1.1479)\tAcc 57.7148 (Avg-Acc 59.4531)\n",
            "Epoch: [154][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1320 (Avg-Loss 1.1427)\tAcc 58.8867 (Avg-Acc 59.3596)\n",
            "Epoch: [154][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1450 (Avg-Loss 1.1427)\tAcc 58.3008 (Avg-Acc 59.3924)\n",
            "Epoch: [154][36/39]\tTime 0.059 (Avg-Time 0.060)\t Loss 1.1690 (Avg-Loss 1.1427)\tAcc 57.8125 (Avg-Acc 59.3407)\n",
            "Epoch: [154][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1954 (Avg-Loss 1.1423)\tAcc 57.8125 (Avg-Acc 59.4200)\n",
            "EPOCH: 154 train Results: Acc 59.420 Loss: 1.1423\n",
            "Epoch: [154][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2095 (Avg-Loss 1.2095)\tAcc 59.6680 (Avg-Acc 59.6680)\n",
            "Epoch: [154][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2347 (Avg-Loss 1.2219)\tAcc 56.8878 (Avg-Acc 56.9100)\n",
            "EPOCH: 154 Validation Results: Acc 56.910 Loss: 1.2219\n",
            "Best Accuracy: 57.1400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [155][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1580 (Avg-Loss 1.1580)\tAcc 58.8867 (Avg-Acc 58.8867)\n",
            "Epoch: [155][9/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.1776 (Avg-Loss 1.1568)\tAcc 58.9844 (Avg-Acc 59.1211)\n",
            "Epoch: [155][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1270 (Avg-Loss 1.1602)\tAcc 59.5703 (Avg-Acc 58.9998)\n",
            "Epoch: [155][27/39]\tTime 0.064 (Avg-Time 0.061)\t Loss 1.1317 (Avg-Loss 1.1510)\tAcc 59.7656 (Avg-Acc 59.4761)\n",
            "Epoch: [155][36/39]\tTime 0.060 (Avg-Time 0.061)\t Loss 1.1776 (Avg-Loss 1.1485)\tAcc 57.9102 (Avg-Acc 59.4515)\n",
            "Epoch: [155][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.3387 (Avg-Loss 1.1477)\tAcc 57.8125 (Avg-Acc 59.4575)\n",
            "EPOCH: 155 train Results: Acc 59.458 Loss: 1.1477\n",
            "Epoch: [155][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2031 (Avg-Loss 1.2031)\tAcc 57.2266 (Avg-Acc 57.2266)\n",
            "Epoch: [155][9/9]\tTime 0.012 (Avg-Time 0.016)\t Loss 1.2361 (Avg-Loss 1.2258)\tAcc 56.3776 (Avg-Acc 56.5100)\n",
            "EPOCH: 155 Validation Results: Acc 56.510 Loss: 1.2258\n",
            "Best Accuracy: 57.1400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [156][0/39]\tTime 0.068 (Avg-Time 0.068)\t Loss 1.0720 (Avg-Loss 1.0720)\tAcc 62.2070 (Avg-Acc 62.2070)\n",
            "Epoch: [156][9/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.1082 (Avg-Loss 1.1385)\tAcc 60.4492 (Avg-Acc 59.8145)\n",
            "Epoch: [156][18/39]\tTime 0.070 (Avg-Time 0.061)\t Loss 1.0921 (Avg-Loss 1.1364)\tAcc 62.4023 (Avg-Acc 59.6577)\n",
            "Epoch: [156][27/39]\tTime 0.060 (Avg-Time 0.061)\t Loss 1.1271 (Avg-Loss 1.1375)\tAcc 61.2305 (Avg-Acc 59.6261)\n",
            "Epoch: [156][36/39]\tTime 0.062 (Avg-Time 0.061)\t Loss 1.1683 (Avg-Loss 1.1392)\tAcc 56.0547 (Avg-Acc 59.5571)\n",
            "Epoch: [156][39/39]\tTime 0.012 (Avg-Time 0.063)\t Loss 1.4425 (Avg-Loss 1.1392)\tAcc 50.0000 (Avg-Acc 59.6600)\n",
            "EPOCH: 156 train Results: Acc 59.660 Loss: 1.1392\n",
            "Epoch: [156][0/9]\tTime 0.032 (Avg-Time 0.032)\t Loss 1.2007 (Avg-Loss 1.2007)\tAcc 59.0820 (Avg-Acc 59.0820)\n",
            "Epoch: [156][9/9]\tTime 0.026 (Avg-Time 0.034)\t Loss 1.2132 (Avg-Loss 1.2114)\tAcc 57.3980 (Avg-Acc 56.9200)\n",
            "EPOCH: 156 Validation Results: Acc 56.920 Loss: 1.2114\n",
            "Best Accuracy: 57.1400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [157][0/39]\tTime 0.108 (Avg-Time 0.108)\t Loss 1.1259 (Avg-Loss 1.1259)\tAcc 58.8867 (Avg-Acc 58.8867)\n",
            "Epoch: [157][9/39]\tTime 0.141 (Avg-Time 0.169)\t Loss 1.1530 (Avg-Loss 1.1537)\tAcc 58.3984 (Avg-Acc 58.7402)\n",
            "Epoch: [157][18/39]\tTime 0.120 (Avg-Time 0.148)\t Loss 1.1139 (Avg-Loss 1.1554)\tAcc 59.2773 (Avg-Acc 58.8764)\n",
            "Epoch: [157][27/39]\tTime 0.057 (Avg-Time 0.119)\t Loss 1.1608 (Avg-Loss 1.1525)\tAcc 58.6914 (Avg-Acc 59.0681)\n",
            "Epoch: [157][36/39]\tTime 0.056 (Avg-Time 0.105)\t Loss 1.1123 (Avg-Loss 1.1519)\tAcc 60.7422 (Avg-Acc 58.9712)\n",
            "Epoch: [157][39/39]\tTime 0.006 (Avg-Time 0.100)\t Loss 1.0688 (Avg-Loss 1.1525)\tAcc 62.5000 (Avg-Acc 58.9875)\n",
            "EPOCH: 157 train Results: Acc 58.987 Loss: 1.1525\n",
            "Epoch: [157][0/9]\tTime 0.017 (Avg-Time 0.017)\t Loss 1.1985 (Avg-Loss 1.1985)\tAcc 58.4961 (Avg-Acc 58.4961)\n",
            "Epoch: [157][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2212 (Avg-Loss 1.2246)\tAcc 57.2704 (Avg-Acc 56.7200)\n",
            "EPOCH: 157 Validation Results: Acc 56.720 Loss: 1.2246\n",
            "Best Accuracy: 57.1400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [158][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.0869 (Avg-Loss 1.0869)\tAcc 61.3281 (Avg-Acc 61.3281)\n",
            "Epoch: [158][9/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.1291 (Avg-Loss 1.1446)\tAcc 59.5703 (Avg-Acc 59.6777)\n",
            "Epoch: [158][18/39]\tTime 0.059 (Avg-Time 0.060)\t Loss 1.1762 (Avg-Loss 1.1453)\tAcc 57.8125 (Avg-Acc 59.3544)\n",
            "Epoch: [158][27/39]\tTime 0.060 (Avg-Time 0.061)\t Loss 1.1989 (Avg-Loss 1.1461)\tAcc 55.3711 (Avg-Acc 59.3471)\n",
            "Epoch: [158][36/39]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.1801 (Avg-Loss 1.1440)\tAcc 57.2266 (Avg-Acc 59.3328)\n",
            "Epoch: [158][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.4222 (Avg-Loss 1.1459)\tAcc 48.4375 (Avg-Acc 59.2650)\n",
            "EPOCH: 158 train Results: Acc 59.265 Loss: 1.1459\n",
            "Epoch: [158][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2398 (Avg-Loss 1.2398)\tAcc 57.7148 (Avg-Acc 57.7148)\n",
            "Epoch: [158][9/9]\tTime 0.011 (Avg-Time 0.018)\t Loss 1.2498 (Avg-Loss 1.2365)\tAcc 56.2500 (Avg-Acc 55.5600)\n",
            "EPOCH: 158 Validation Results: Acc 55.560 Loss: 1.2365\n",
            "Best Accuracy: 57.1400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [159][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.1152 (Avg-Loss 1.1152)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [159][9/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1425 (Avg-Loss 1.1436)\tAcc 59.4727 (Avg-Acc 58.7793)\n",
            "Epoch: [159][18/39]\tTime 0.064 (Avg-Time 0.061)\t Loss 1.1645 (Avg-Loss 1.1421)\tAcc 58.2031 (Avg-Acc 59.0049)\n",
            "Epoch: [159][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.0513 (Avg-Loss 1.1446)\tAcc 62.7930 (Avg-Acc 59.0576)\n",
            "Epoch: [159][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1821 (Avg-Loss 1.1453)\tAcc 58.9844 (Avg-Acc 59.1691)\n",
            "Epoch: [159][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.0930 (Avg-Loss 1.1453)\tAcc 62.5000 (Avg-Acc 59.1725)\n",
            "EPOCH: 159 train Results: Acc 59.172 Loss: 1.1453\n",
            "Epoch: [159][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2103 (Avg-Loss 1.2103)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [159][9/9]\tTime 0.012 (Avg-Time 0.016)\t Loss 1.2337 (Avg-Loss 1.2215)\tAcc 55.8673 (Avg-Acc 56.3800)\n",
            "EPOCH: 159 Validation Results: Acc 56.380 Loss: 1.2215\n",
            "Best Accuracy: 57.1400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [160][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.1045 (Avg-Loss 1.1045)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [160][9/39]\tTime 0.080 (Avg-Time 0.060)\t Loss 1.1861 (Avg-Loss 1.1464)\tAcc 57.4219 (Avg-Acc 59.3945)\n",
            "Epoch: [160][18/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1089 (Avg-Loss 1.1402)\tAcc 61.2305 (Avg-Acc 59.7502)\n",
            "Epoch: [160][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1302 (Avg-Loss 1.1380)\tAcc 58.8867 (Avg-Acc 59.7273)\n",
            "Epoch: [160][36/39]\tTime 0.054 (Avg-Time 0.059)\t Loss 1.1229 (Avg-Loss 1.1371)\tAcc 60.3516 (Avg-Acc 59.7049)\n",
            "Epoch: [160][39/39]\tTime 0.005 (Avg-Time 0.058)\t Loss 1.2612 (Avg-Loss 1.1368)\tAcc 54.6875 (Avg-Acc 59.7425)\n",
            "EPOCH: 160 train Results: Acc 59.742 Loss: 1.1368\n",
            "Epoch: [160][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1991 (Avg-Loss 1.1991)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [160][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2391 (Avg-Loss 1.2302)\tAcc 57.0153 (Avg-Acc 55.9500)\n",
            "EPOCH: 160 Validation Results: Acc 55.950 Loss: 1.2302\n",
            "Best Accuracy: 57.1400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [161][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.1528 (Avg-Loss 1.1528)\tAcc 58.0078 (Avg-Acc 58.0078)\n",
            "Epoch: [161][9/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1835 (Avg-Loss 1.1386)\tAcc 58.2031 (Avg-Acc 59.5020)\n",
            "Epoch: [161][18/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1202 (Avg-Loss 1.1420)\tAcc 61.2305 (Avg-Acc 59.4521)\n",
            "Epoch: [161][27/39]\tTime 0.154 (Avg-Time 0.093)\t Loss 1.1633 (Avg-Loss 1.1382)\tAcc 57.6172 (Avg-Acc 59.6784)\n",
            "Epoch: [161][36/39]\tTime 0.141 (Avg-Time 0.108)\t Loss 1.1664 (Avg-Loss 1.1364)\tAcc 59.4727 (Avg-Acc 59.7524)\n",
            "Epoch: [161][39/39]\tTime 0.011 (Avg-Time 0.106)\t Loss 0.9414 (Avg-Loss 1.1370)\tAcc 68.7500 (Avg-Acc 59.7475)\n",
            "EPOCH: 161 train Results: Acc 59.748 Loss: 1.1370\n",
            "Epoch: [161][0/9]\tTime 0.023 (Avg-Time 0.023)\t Loss 1.2030 (Avg-Loss 1.2030)\tAcc 57.5195 (Avg-Acc 57.5195)\n",
            "Epoch: [161][9/9]\tTime 0.026 (Avg-Time 0.027)\t Loss 1.2307 (Avg-Loss 1.2327)\tAcc 55.3571 (Avg-Acc 55.6600)\n",
            "EPOCH: 161 Validation Results: Acc 55.660 Loss: 1.2327\n",
            "Best Accuracy: 57.1400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [162][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1103 (Avg-Loss 1.1103)\tAcc 61.4258 (Avg-Acc 61.4258)\n",
            "Epoch: [162][9/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1740 (Avg-Loss 1.1559)\tAcc 57.9102 (Avg-Acc 58.8477)\n",
            "Epoch: [162][18/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1700 (Avg-Loss 1.1531)\tAcc 58.4961 (Avg-Acc 59.0512)\n",
            "Epoch: [162][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1261 (Avg-Loss 1.1433)\tAcc 60.4492 (Avg-Acc 59.4413)\n",
            "Epoch: [162][36/39]\tTime 0.060 (Avg-Time 0.061)\t Loss 1.1800 (Avg-Loss 1.1429)\tAcc 58.3008 (Avg-Acc 59.4595)\n",
            "Epoch: [162][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1221 (Avg-Loss 1.1426)\tAcc 57.8125 (Avg-Acc 59.4550)\n",
            "EPOCH: 162 train Results: Acc 59.455 Loss: 1.1426\n",
            "Epoch: [162][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2228 (Avg-Loss 1.2228)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [162][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2260 (Avg-Loss 1.2311)\tAcc 56.2500 (Avg-Acc 56.2400)\n",
            "EPOCH: 162 Validation Results: Acc 56.240 Loss: 1.2311\n",
            "Best Accuracy: 57.1400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [163][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1036 (Avg-Loss 1.1036)\tAcc 60.2539 (Avg-Acc 60.2539)\n",
            "Epoch: [163][9/39]\tTime 0.079 (Avg-Time 0.067)\t Loss 1.1018 (Avg-Loss 1.1205)\tAcc 60.1562 (Avg-Acc 60.0781)\n",
            "Epoch: [163][18/39]\tTime 0.058 (Avg-Time 0.064)\t Loss 1.0952 (Avg-Loss 1.1283)\tAcc 60.3516 (Avg-Acc 59.7142)\n",
            "Epoch: [163][27/39]\tTime 0.058 (Avg-Time 0.064)\t Loss 1.1655 (Avg-Loss 1.1260)\tAcc 58.4961 (Avg-Acc 59.8807)\n",
            "Epoch: [163][36/39]\tTime 0.058 (Avg-Time 0.063)\t Loss 1.1266 (Avg-Loss 1.1263)\tAcc 59.2773 (Avg-Acc 59.8791)\n",
            "Epoch: [163][39/39]\tTime 0.006 (Avg-Time 0.061)\t Loss 1.2615 (Avg-Loss 1.1265)\tAcc 54.6875 (Avg-Acc 59.8575)\n",
            "EPOCH: 163 train Results: Acc 59.858 Loss: 1.1265\n",
            "Epoch: [163][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2088 (Avg-Loss 1.2088)\tAcc 59.2773 (Avg-Acc 59.2773)\n",
            "Epoch: [163][9/9]\tTime 0.011 (Avg-Time 0.018)\t Loss 1.2332 (Avg-Loss 1.2194)\tAcc 55.7398 (Avg-Acc 57.1600)\n",
            "EPOCH: 163 Validation Results: Acc 57.160 Loss: 1.2194\n",
            "Best Accuracy: 57.1600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [164][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1195 (Avg-Loss 1.1195)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [164][9/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1323 (Avg-Loss 1.1400)\tAcc 61.6211 (Avg-Acc 59.5117)\n",
            "Epoch: [164][18/39]\tTime 0.062 (Avg-Time 0.060)\t Loss 1.0947 (Avg-Loss 1.1367)\tAcc 61.6211 (Avg-Acc 59.4881)\n",
            "Epoch: [164][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.0581 (Avg-Loss 1.1328)\tAcc 63.6719 (Avg-Acc 59.6540)\n",
            "Epoch: [164][36/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.1464 (Avg-Loss 1.1339)\tAcc 57.9102 (Avg-Acc 59.6416)\n",
            "Epoch: [164][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.2360 (Avg-Loss 1.1356)\tAcc 51.5625 (Avg-Acc 59.5450)\n",
            "EPOCH: 164 train Results: Acc 59.545 Loss: 1.1356\n",
            "Epoch: [164][0/9]\tTime 0.017 (Avg-Time 0.017)\t Loss 1.2053 (Avg-Loss 1.2053)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [164][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2358 (Avg-Loss 1.2198)\tAcc 55.9949 (Avg-Acc 56.8100)\n",
            "EPOCH: 164 Validation Results: Acc 56.810 Loss: 1.2198\n",
            "Best Accuracy: 57.1600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [165][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.0525 (Avg-Loss 1.0525)\tAcc 63.3789 (Avg-Acc 63.3789)\n",
            "Epoch: [165][9/39]\tTime 0.062 (Avg-Time 0.062)\t Loss 1.1853 (Avg-Loss 1.1215)\tAcc 56.2500 (Avg-Acc 59.9414)\n",
            "Epoch: [165][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1663 (Avg-Loss 1.1321)\tAcc 59.8633 (Avg-Acc 59.7502)\n",
            "Epoch: [165][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.0668 (Avg-Loss 1.1335)\tAcc 61.6211 (Avg-Acc 59.4413)\n",
            "Epoch: [165][36/39]\tTime 0.059 (Avg-Time 0.060)\t Loss 1.0990 (Avg-Loss 1.1329)\tAcc 59.0820 (Avg-Acc 59.4990)\n",
            "Epoch: [165][39/39]\tTime 0.014 (Avg-Time 0.060)\t Loss 1.1097 (Avg-Loss 1.1311)\tAcc 56.2500 (Avg-Acc 59.5950)\n",
            "EPOCH: 165 train Results: Acc 59.595 Loss: 1.1311\n",
            "Epoch: [165][0/9]\tTime 0.040 (Avg-Time 0.040)\t Loss 1.1999 (Avg-Loss 1.1999)\tAcc 58.1055 (Avg-Acc 58.1055)\n",
            "Epoch: [165][9/9]\tTime 0.021 (Avg-Time 0.031)\t Loss 1.2220 (Avg-Loss 1.2122)\tAcc 57.7806 (Avg-Acc 56.9700)\n",
            "EPOCH: 165 Validation Results: Acc 56.970 Loss: 1.2122\n",
            "Best Accuracy: 57.1600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [166][0/39]\tTime 0.107 (Avg-Time 0.107)\t Loss 1.1050 (Avg-Loss 1.1050)\tAcc 62.0117 (Avg-Acc 62.0117)\n",
            "Epoch: [166][9/39]\tTime 0.235 (Avg-Time 0.182)\t Loss 1.1404 (Avg-Loss 1.1312)\tAcc 59.3750 (Avg-Acc 59.6387)\n",
            "Epoch: [166][18/39]\tTime 0.130 (Avg-Time 0.151)\t Loss 1.0632 (Avg-Loss 1.1303)\tAcc 63.1836 (Avg-Acc 59.9507)\n",
            "Epoch: [166][27/39]\tTime 0.060 (Avg-Time 0.128)\t Loss 1.1713 (Avg-Loss 1.1290)\tAcc 58.0078 (Avg-Acc 60.0237)\n",
            "Epoch: [166][36/39]\tTime 0.058 (Avg-Time 0.111)\t Loss 1.1183 (Avg-Loss 1.1262)\tAcc 58.4961 (Avg-Acc 60.0771)\n",
            "Epoch: [166][39/39]\tTime 0.006 (Avg-Time 0.107)\t Loss 1.2096 (Avg-Loss 1.1258)\tAcc 64.0625 (Avg-Acc 60.1250)\n",
            "EPOCH: 166 train Results: Acc 60.125 Loss: 1.1258\n",
            "Epoch: [166][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2130 (Avg-Loss 1.2130)\tAcc 59.0820 (Avg-Acc 59.0820)\n",
            "Epoch: [166][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2217 (Avg-Loss 1.2153)\tAcc 58.6735 (Avg-Acc 57.0000)\n",
            "EPOCH: 166 Validation Results: Acc 57.000 Loss: 1.2153\n",
            "Best Accuracy: 57.1600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [167][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1076 (Avg-Loss 1.1076)\tAcc 60.1562 (Avg-Acc 60.1562)\n",
            "Epoch: [167][9/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1300 (Avg-Loss 1.1180)\tAcc 58.6914 (Avg-Acc 59.8926)\n",
            "Epoch: [167][18/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1233 (Avg-Loss 1.1241)\tAcc 60.4492 (Avg-Acc 59.7245)\n",
            "Epoch: [167][27/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1578 (Avg-Loss 1.1287)\tAcc 58.0078 (Avg-Acc 59.6784)\n",
            "Epoch: [167][36/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1457 (Avg-Loss 1.1278)\tAcc 57.7148 (Avg-Acc 59.7392)\n",
            "Epoch: [167][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.3489 (Avg-Loss 1.1283)\tAcc 54.6875 (Avg-Acc 59.7925)\n",
            "EPOCH: 167 train Results: Acc 59.792 Loss: 1.1283\n",
            "Epoch: [167][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2001 (Avg-Loss 1.2001)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [167][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2280 (Avg-Loss 1.2242)\tAcc 56.5051 (Avg-Acc 56.1000)\n",
            "EPOCH: 167 Validation Results: Acc 56.100 Loss: 1.2242\n",
            "Best Accuracy: 57.1600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [168][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1235 (Avg-Loss 1.1235)\tAcc 59.6680 (Avg-Acc 59.6680)\n",
            "Epoch: [168][9/39]\tTime 0.057 (Avg-Time 0.063)\t Loss 1.1559 (Avg-Loss 1.1655)\tAcc 58.2031 (Avg-Acc 58.9355)\n",
            "Epoch: [168][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1341 (Avg-Loss 1.1544)\tAcc 60.3516 (Avg-Acc 59.0409)\n",
            "Epoch: [168][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1165 (Avg-Loss 1.1461)\tAcc 61.4258 (Avg-Acc 59.3506)\n",
            "Epoch: [168][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1437 (Avg-Loss 1.1402)\tAcc 57.3242 (Avg-Acc 59.5017)\n",
            "Epoch: [168][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1260 (Avg-Loss 1.1394)\tAcc 62.5000 (Avg-Acc 59.5475)\n",
            "EPOCH: 168 train Results: Acc 59.547 Loss: 1.1394\n",
            "Epoch: [168][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2076 (Avg-Loss 1.2076)\tAcc 57.4219 (Avg-Acc 57.4219)\n",
            "Epoch: [168][9/9]\tTime 0.011 (Avg-Time 0.018)\t Loss 1.2350 (Avg-Loss 1.2281)\tAcc 56.1224 (Avg-Acc 56.4600)\n",
            "EPOCH: 168 Validation Results: Acc 56.460 Loss: 1.2281\n",
            "Best Accuracy: 57.1600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [169][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.0666 (Avg-Loss 1.0666)\tAcc 63.5742 (Avg-Acc 63.5742)\n",
            "Epoch: [169][9/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1443 (Avg-Loss 1.1348)\tAcc 59.1797 (Avg-Acc 59.7168)\n",
            "Epoch: [169][18/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1044 (Avg-Loss 1.1407)\tAcc 61.3281 (Avg-Acc 59.6063)\n",
            "Epoch: [169][27/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1062 (Avg-Loss 1.1382)\tAcc 61.7188 (Avg-Acc 59.7482)\n",
            "Epoch: [169][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1533 (Avg-Loss 1.1351)\tAcc 60.0586 (Avg-Acc 59.7683)\n",
            "Epoch: [169][39/39]\tTime 0.005 (Avg-Time 0.058)\t Loss 1.2809 (Avg-Loss 1.1347)\tAcc 48.4375 (Avg-Acc 59.7875)\n",
            "EPOCH: 169 train Results: Acc 59.788 Loss: 1.1347\n",
            "Epoch: [169][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1788 (Avg-Loss 1.1788)\tAcc 60.2539 (Avg-Acc 60.2539)\n",
            "Epoch: [169][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2212 (Avg-Loss 1.2091)\tAcc 57.0153 (Avg-Acc 57.3200)\n",
            "EPOCH: 169 Validation Results: Acc 57.320 Loss: 1.2091\n",
            "Best Accuracy: 57.3200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [170][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.0851 (Avg-Loss 1.0851)\tAcc 61.8164 (Avg-Acc 61.8164)\n",
            "Epoch: [170][9/39]\tTime 0.081 (Avg-Time 0.061)\t Loss 1.1330 (Avg-Loss 1.1328)\tAcc 59.6680 (Avg-Acc 60.2344)\n",
            "Epoch: [170][18/39]\tTime 0.059 (Avg-Time 0.060)\t Loss 1.0638 (Avg-Loss 1.1322)\tAcc 62.2070 (Avg-Acc 60.3002)\n",
            "Epoch: [170][27/39]\tTime 0.253 (Avg-Time 0.092)\t Loss 1.1392 (Avg-Loss 1.1335)\tAcc 59.5703 (Avg-Acc 60.0237)\n",
            "Epoch: [170][36/39]\tTime 0.099 (Avg-Time 0.109)\t Loss 1.1423 (Avg-Loss 1.1324)\tAcc 58.9844 (Avg-Acc 59.9319)\n",
            "Epoch: [170][39/39]\tTime 0.006 (Avg-Time 0.109)\t Loss 1.4896 (Avg-Loss 1.1300)\tAcc 40.6250 (Avg-Acc 59.9925)\n",
            "EPOCH: 170 train Results: Acc 59.992 Loss: 1.1300\n",
            "Epoch: [170][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2060 (Avg-Loss 1.2060)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [170][9/9]\tTime 0.011 (Avg-Time 0.014)\t Loss 1.2272 (Avg-Loss 1.2177)\tAcc 55.8673 (Avg-Acc 56.7600)\n",
            "EPOCH: 170 Validation Results: Acc 56.760 Loss: 1.2177\n",
            "Best Accuracy: 57.3200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [171][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1104 (Avg-Loss 1.1104)\tAcc 61.2305 (Avg-Acc 61.2305)\n",
            "Epoch: [171][9/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1721 (Avg-Loss 1.1236)\tAcc 58.2031 (Avg-Acc 60.7910)\n",
            "Epoch: [171][18/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1038 (Avg-Loss 1.1281)\tAcc 61.1328 (Avg-Acc 60.3413)\n",
            "Epoch: [171][27/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1686 (Avg-Loss 1.1267)\tAcc 59.0820 (Avg-Acc 60.2923)\n",
            "Epoch: [171][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1901 (Avg-Loss 1.1294)\tAcc 56.5430 (Avg-Acc 60.0032)\n",
            "Epoch: [171][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.3384 (Avg-Loss 1.1288)\tAcc 48.4375 (Avg-Acc 60.0225)\n",
            "EPOCH: 171 train Results: Acc 60.023 Loss: 1.1288\n",
            "Epoch: [171][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1936 (Avg-Loss 1.1936)\tAcc 59.2773 (Avg-Acc 59.2773)\n",
            "Epoch: [171][9/9]\tTime 0.011 (Avg-Time 0.018)\t Loss 1.2259 (Avg-Loss 1.2156)\tAcc 55.8673 (Avg-Acc 56.7100)\n",
            "EPOCH: 171 Validation Results: Acc 56.710 Loss: 1.2156\n",
            "Best Accuracy: 57.3200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [172][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.0892 (Avg-Loss 1.0892)\tAcc 61.0352 (Avg-Acc 61.0352)\n",
            "Epoch: [172][9/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1498 (Avg-Loss 1.1323)\tAcc 59.0820 (Avg-Acc 60.2539)\n",
            "Epoch: [172][18/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1748 (Avg-Loss 1.1385)\tAcc 58.3008 (Avg-Acc 59.7862)\n",
            "Epoch: [172][27/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1581 (Avg-Loss 1.1370)\tAcc 58.1055 (Avg-Acc 59.6715)\n",
            "Epoch: [172][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1108 (Avg-Loss 1.1331)\tAcc 60.1562 (Avg-Acc 59.7762)\n",
            "Epoch: [172][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.4423 (Avg-Loss 1.1308)\tAcc 48.4375 (Avg-Acc 59.8850)\n",
            "EPOCH: 172 train Results: Acc 59.885 Loss: 1.1308\n",
            "Epoch: [172][0/9]\tTime 0.018 (Avg-Time 0.018)\t Loss 1.2103 (Avg-Loss 1.2103)\tAcc 60.0586 (Avg-Acc 60.0586)\n",
            "Epoch: [172][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2192 (Avg-Loss 1.2158)\tAcc 57.2704 (Avg-Acc 56.6900)\n",
            "EPOCH: 172 Validation Results: Acc 56.690 Loss: 1.2158\n",
            "Best Accuracy: 57.3200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [173][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.0625 (Avg-Loss 1.0625)\tAcc 64.0625 (Avg-Acc 64.0625)\n",
            "Epoch: [173][9/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1720 (Avg-Loss 1.1462)\tAcc 56.4453 (Avg-Acc 59.0332)\n",
            "Epoch: [173][18/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.0935 (Avg-Loss 1.1474)\tAcc 60.6445 (Avg-Acc 59.1386)\n",
            "Epoch: [173][27/39]\tTime 0.061 (Avg-Time 0.068)\t Loss 1.1259 (Avg-Loss 1.1416)\tAcc 60.9375 (Avg-Acc 59.4134)\n",
            "Epoch: [173][36/39]\tTime 0.058 (Avg-Time 0.066)\t Loss 1.1162 (Avg-Loss 1.1387)\tAcc 60.8398 (Avg-Acc 59.4990)\n",
            "Epoch: [173][39/39]\tTime 0.007 (Avg-Time 0.064)\t Loss 1.1549 (Avg-Loss 1.1375)\tAcc 56.2500 (Avg-Acc 59.5100)\n",
            "EPOCH: 173 train Results: Acc 59.510 Loss: 1.1375\n",
            "Epoch: [173][0/9]\tTime 0.032 (Avg-Time 0.032)\t Loss 1.1956 (Avg-Loss 1.1956)\tAcc 59.4727 (Avg-Acc 59.4727)\n",
            "Epoch: [173][9/9]\tTime 0.011 (Avg-Time 0.017)\t Loss 1.2237 (Avg-Loss 1.2161)\tAcc 55.9949 (Avg-Acc 56.6400)\n",
            "EPOCH: 173 Validation Results: Acc 56.640 Loss: 1.2161\n",
            "Best Accuracy: 57.3200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [174][0/39]\tTime 0.062 (Avg-Time 0.062)\t Loss 1.1067 (Avg-Loss 1.1067)\tAcc 60.9375 (Avg-Acc 60.9375)\n",
            "Epoch: [174][9/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1839 (Avg-Loss 1.1164)\tAcc 57.9102 (Avg-Acc 60.2734)\n",
            "Epoch: [174][18/39]\tTime 0.059 (Avg-Time 0.061)\t Loss 1.0986 (Avg-Loss 1.1241)\tAcc 60.7422 (Avg-Acc 59.8736)\n",
            "Epoch: [174][27/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.0887 (Avg-Loss 1.1237)\tAcc 61.6211 (Avg-Acc 59.9505)\n",
            "Epoch: [174][36/39]\tTime 0.124 (Avg-Time 0.066)\t Loss 1.0898 (Avg-Loss 1.1229)\tAcc 62.0117 (Avg-Acc 60.0454)\n",
            "Epoch: [174][39/39]\tTime 0.023 (Avg-Time 0.067)\t Loss 1.3101 (Avg-Loss 1.1238)\tAcc 54.6875 (Avg-Acc 60.0100)\n",
            "EPOCH: 174 train Results: Acc 60.010 Loss: 1.1238\n",
            "Epoch: [174][0/9]\tTime 0.024 (Avg-Time 0.024)\t Loss 1.1937 (Avg-Loss 1.1937)\tAcc 60.3516 (Avg-Acc 60.3516)\n",
            "Epoch: [174][9/9]\tTime 0.059 (Avg-Time 0.055)\t Loss 1.2272 (Avg-Loss 1.2201)\tAcc 57.5255 (Avg-Acc 56.7300)\n",
            "EPOCH: 174 Validation Results: Acc 56.730 Loss: 1.2201\n",
            "Best Accuracy: 57.3200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [175][0/39]\tTime 0.259 (Avg-Time 0.259)\t Loss 1.1121 (Avg-Loss 1.1121)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [175][9/39]\tTime 0.128 (Avg-Time 0.168)\t Loss 1.1895 (Avg-Loss 1.1262)\tAcc 57.2266 (Avg-Acc 59.7168)\n",
            "Epoch: [175][18/39]\tTime 0.056 (Avg-Time 0.130)\t Loss 1.1485 (Avg-Loss 1.1327)\tAcc 59.3750 (Avg-Acc 59.4264)\n",
            "Epoch: [175][27/39]\tTime 0.057 (Avg-Time 0.108)\t Loss 1.1155 (Avg-Loss 1.1302)\tAcc 59.4727 (Avg-Acc 59.5319)\n",
            "Epoch: [175][36/39]\tTime 0.059 (Avg-Time 0.096)\t Loss 1.1095 (Avg-Loss 1.1283)\tAcc 58.9844 (Avg-Acc 59.6310)\n",
            "Epoch: [175][39/39]\tTime 0.006 (Avg-Time 0.092)\t Loss 1.0151 (Avg-Loss 1.1287)\tAcc 64.0625 (Avg-Acc 59.6275)\n",
            "EPOCH: 175 train Results: Acc 59.627 Loss: 1.1287\n",
            "Epoch: [175][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2017 (Avg-Loss 1.2017)\tAcc 59.4727 (Avg-Acc 59.4727)\n",
            "Epoch: [175][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2246 (Avg-Loss 1.2170)\tAcc 57.0153 (Avg-Acc 57.0200)\n",
            "EPOCH: 175 Validation Results: Acc 57.020 Loss: 1.2170\n",
            "Best Accuracy: 57.3200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [176][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.0802 (Avg-Loss 1.0802)\tAcc 62.1094 (Avg-Acc 62.1094)\n",
            "Epoch: [176][9/39]\tTime 0.060 (Avg-Time 0.062)\t Loss 1.1830 (Avg-Loss 1.1173)\tAcc 58.9844 (Avg-Acc 60.2832)\n",
            "Epoch: [176][18/39]\tTime 0.066 (Avg-Time 0.061)\t Loss 1.1771 (Avg-Loss 1.1277)\tAcc 56.7383 (Avg-Acc 59.8376)\n",
            "Epoch: [176][27/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.1532 (Avg-Loss 1.1274)\tAcc 59.4727 (Avg-Acc 59.9679)\n",
            "Epoch: [176][36/39]\tTime 0.077 (Avg-Time 0.062)\t Loss 1.1256 (Avg-Loss 1.1237)\tAcc 59.8633 (Avg-Acc 60.2143)\n",
            "Epoch: [176][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.2246 (Avg-Loss 1.1245)\tAcc 62.5000 (Avg-Acc 60.1975)\n",
            "EPOCH: 176 train Results: Acc 60.197 Loss: 1.1245\n",
            "Epoch: [176][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2055 (Avg-Loss 1.2055)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [176][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2103 (Avg-Loss 1.2131)\tAcc 57.9082 (Avg-Acc 56.8800)\n",
            "EPOCH: 176 Validation Results: Acc 56.880 Loss: 1.2131\n",
            "Best Accuracy: 57.3200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [177][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1313 (Avg-Loss 1.1313)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [177][9/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1772 (Avg-Loss 1.1244)\tAcc 59.0820 (Avg-Acc 59.9512)\n",
            "Epoch: [177][18/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.0833 (Avg-Loss 1.1178)\tAcc 61.0352 (Avg-Acc 60.2590)\n",
            "Epoch: [177][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.0764 (Avg-Loss 1.1188)\tAcc 61.3281 (Avg-Acc 60.3237)\n",
            "Epoch: [177][36/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1366 (Avg-Loss 1.1235)\tAcc 57.6172 (Avg-Acc 60.0560)\n",
            "Epoch: [177][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.1935 (Avg-Loss 1.1244)\tAcc 64.0625 (Avg-Acc 60.1000)\n",
            "EPOCH: 177 train Results: Acc 60.100 Loss: 1.1244\n",
            "Epoch: [177][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2060 (Avg-Loss 1.2060)\tAcc 58.6914 (Avg-Acc 58.6914)\n",
            "Epoch: [177][9/9]\tTime 0.012 (Avg-Time 0.016)\t Loss 1.2229 (Avg-Loss 1.2189)\tAcc 57.0153 (Avg-Acc 57.0600)\n",
            "EPOCH: 177 Validation Results: Acc 57.060 Loss: 1.2189\n",
            "Best Accuracy: 57.3200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [178][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.0631 (Avg-Loss 1.0631)\tAcc 62.3047 (Avg-Acc 62.3047)\n",
            "Epoch: [178][9/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.2016 (Avg-Loss 1.1331)\tAcc 56.7383 (Avg-Acc 60.0098)\n",
            "Epoch: [178][18/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1056 (Avg-Loss 1.1306)\tAcc 62.4023 (Avg-Acc 59.9609)\n",
            "Epoch: [178][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.0941 (Avg-Loss 1.1263)\tAcc 60.9375 (Avg-Acc 60.0167)\n",
            "Epoch: [178][36/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1290 (Avg-Loss 1.1263)\tAcc 60.0586 (Avg-Acc 60.0058)\n",
            "Epoch: [178][39/39]\tTime 0.008 (Avg-Time 0.058)\t Loss 1.2548 (Avg-Loss 1.1265)\tAcc 50.0000 (Avg-Acc 59.9600)\n",
            "EPOCH: 178 train Results: Acc 59.960 Loss: 1.1265\n",
            "Epoch: [178][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2081 (Avg-Loss 1.2081)\tAcc 57.6172 (Avg-Acc 57.6172)\n",
            "Epoch: [178][9/9]\tTime 0.011 (Avg-Time 0.017)\t Loss 1.2295 (Avg-Loss 1.2237)\tAcc 56.1224 (Avg-Acc 56.3500)\n",
            "EPOCH: 178 Validation Results: Acc 56.350 Loss: 1.2237\n",
            "Best Accuracy: 57.3200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [179][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.0785 (Avg-Loss 1.0785)\tAcc 62.3047 (Avg-Acc 62.3047)\n",
            "Epoch: [179][9/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1063 (Avg-Loss 1.1261)\tAcc 62.0117 (Avg-Acc 60.2539)\n",
            "Epoch: [179][18/39]\tTime 0.223 (Avg-Time 0.079)\t Loss 1.1473 (Avg-Loss 1.1309)\tAcc 60.5469 (Avg-Acc 60.0843)\n",
            "Epoch: [179][27/39]\tTime 0.137 (Avg-Time 0.121)\t Loss 1.0708 (Avg-Loss 1.1310)\tAcc 62.1094 (Avg-Acc 60.1039)\n",
            "Epoch: [179][36/39]\tTime 0.057 (Avg-Time 0.116)\t Loss 1.0887 (Avg-Loss 1.1282)\tAcc 61.9141 (Avg-Acc 60.1299)\n",
            "Epoch: [179][39/39]\tTime 0.006 (Avg-Time 0.111)\t Loss 1.0532 (Avg-Loss 1.1297)\tAcc 60.9375 (Avg-Acc 60.0500)\n",
            "EPOCH: 179 train Results: Acc 60.050 Loss: 1.1297\n",
            "Epoch: [179][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.2156 (Avg-Loss 1.2156)\tAcc 58.3984 (Avg-Acc 58.3984)\n",
            "Epoch: [179][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2296 (Avg-Loss 1.2241)\tAcc 54.9745 (Avg-Acc 56.4400)\n",
            "EPOCH: 179 Validation Results: Acc 56.440 Loss: 1.2241\n",
            "Best Accuracy: 57.3200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [180][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.1040 (Avg-Loss 1.1040)\tAcc 60.6445 (Avg-Acc 60.6445)\n",
            "Epoch: [180][9/39]\tTime 0.081 (Avg-Time 0.065)\t Loss 1.1414 (Avg-Loss 1.1047)\tAcc 58.5938 (Avg-Acc 60.7422)\n",
            "Epoch: [180][18/39]\tTime 0.057 (Avg-Time 0.062)\t Loss 1.1116 (Avg-Loss 1.1049)\tAcc 62.5000 (Avg-Acc 61.0609)\n",
            "Epoch: [180][27/39]\tTime 0.055 (Avg-Time 0.062)\t Loss 1.0888 (Avg-Loss 1.1082)\tAcc 62.5000 (Avg-Acc 60.8956)\n",
            "Epoch: [180][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1311 (Avg-Loss 1.1135)\tAcc 59.6680 (Avg-Acc 60.5680)\n",
            "Epoch: [180][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1531 (Avg-Loss 1.1117)\tAcc 56.2500 (Avg-Acc 60.6250)\n",
            "EPOCH: 180 train Results: Acc 60.625 Loss: 1.1117\n",
            "Epoch: [180][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1867 (Avg-Loss 1.1867)\tAcc 58.7891 (Avg-Acc 58.7891)\n",
            "Epoch: [180][9/9]\tTime 0.012 (Avg-Time 0.016)\t Loss 1.2088 (Avg-Loss 1.2156)\tAcc 58.0357 (Avg-Acc 56.7000)\n",
            "EPOCH: 180 Validation Results: Acc 56.700 Loss: 1.2156\n",
            "Best Accuracy: 57.3200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [181][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.1029 (Avg-Loss 1.1029)\tAcc 61.3281 (Avg-Acc 61.3281)\n",
            "Epoch: [181][9/39]\tTime 0.056 (Avg-Time 0.062)\t Loss 1.0780 (Avg-Loss 1.1252)\tAcc 62.0117 (Avg-Acc 60.3711)\n",
            "Epoch: [181][18/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.1170 (Avg-Loss 1.1210)\tAcc 58.4961 (Avg-Acc 60.1357)\n",
            "Epoch: [181][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1477 (Avg-Loss 1.1176)\tAcc 59.9609 (Avg-Acc 60.0481)\n",
            "Epoch: [181][36/39]\tTime 0.078 (Avg-Time 0.061)\t Loss 1.1425 (Avg-Loss 1.1200)\tAcc 60.0586 (Avg-Acc 59.9372)\n",
            "Epoch: [181][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.2678 (Avg-Loss 1.1197)\tAcc 54.6875 (Avg-Acc 60.0050)\n",
            "EPOCH: 181 train Results: Acc 60.005 Loss: 1.1197\n",
            "Epoch: [181][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1900 (Avg-Loss 1.1900)\tAcc 58.3008 (Avg-Acc 58.3008)\n",
            "Epoch: [181][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2110 (Avg-Loss 1.2058)\tAcc 57.9082 (Avg-Acc 56.9600)\n",
            "EPOCH: 181 Validation Results: Acc 56.960 Loss: 1.2058\n",
            "Best Accuracy: 57.3200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [182][0/39]\tTime 0.064 (Avg-Time 0.064)\t Loss 1.0558 (Avg-Loss 1.0558)\tAcc 62.0117 (Avg-Acc 62.0117)\n",
            "Epoch: [182][9/39]\tTime 0.061 (Avg-Time 0.059)\t Loss 1.1629 (Avg-Loss 1.1139)\tAcc 58.1055 (Avg-Acc 60.5957)\n",
            "Epoch: [182][18/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1427 (Avg-Loss 1.1290)\tAcc 60.9375 (Avg-Acc 60.1665)\n",
            "Epoch: [182][27/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1354 (Avg-Loss 1.1254)\tAcc 60.5469 (Avg-Acc 60.0830)\n",
            "Epoch: [182][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1620 (Avg-Loss 1.1261)\tAcc 59.7656 (Avg-Acc 60.1483)\n",
            "Epoch: [182][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.4318 (Avg-Loss 1.1266)\tAcc 51.5625 (Avg-Acc 60.1275)\n",
            "EPOCH: 182 train Results: Acc 60.127 Loss: 1.1266\n",
            "Epoch: [182][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1792 (Avg-Loss 1.1792)\tAcc 60.0586 (Avg-Acc 60.0586)\n",
            "Epoch: [182][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2261 (Avg-Loss 1.2089)\tAcc 57.7806 (Avg-Acc 57.4200)\n",
            "EPOCH: 182 Validation Results: Acc 57.420 Loss: 1.2089\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [183][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.0788 (Avg-Loss 1.0788)\tAcc 60.9375 (Avg-Acc 60.9375)\n",
            "Epoch: [183][9/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1475 (Avg-Loss 1.1401)\tAcc 59.9609 (Avg-Acc 59.9316)\n",
            "Epoch: [183][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1903 (Avg-Loss 1.1399)\tAcc 57.0312 (Avg-Acc 59.5241)\n",
            "Epoch: [183][27/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1668 (Avg-Loss 1.1377)\tAcc 58.9844 (Avg-Acc 59.8040)\n",
            "Epoch: [183][36/39]\tTime 0.136 (Avg-Time 0.075)\t Loss 1.1589 (Avg-Loss 1.1337)\tAcc 56.7383 (Avg-Acc 59.9082)\n",
            "Epoch: [183][39/39]\tTime 0.020 (Avg-Time 0.083)\t Loss 1.3133 (Avg-Loss 1.1319)\tAcc 51.5625 (Avg-Acc 59.9775)\n",
            "EPOCH: 183 train Results: Acc 59.977 Loss: 1.1319\n",
            "Epoch: [183][0/9]\tTime 0.042 (Avg-Time 0.042)\t Loss 1.2013 (Avg-Loss 1.2013)\tAcc 59.3750 (Avg-Acc 59.3750)\n",
            "Epoch: [183][9/9]\tTime 0.076 (Avg-Time 0.055)\t Loss 1.2247 (Avg-Loss 1.2262)\tAcc 56.1224 (Avg-Acc 56.3300)\n",
            "EPOCH: 183 Validation Results: Acc 56.330 Loss: 1.2262\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [184][0/39]\tTime 0.148 (Avg-Time 0.148)\t Loss 1.0395 (Avg-Loss 1.0395)\tAcc 62.6953 (Avg-Acc 62.6953)\n",
            "Epoch: [184][9/39]\tTime 0.138 (Avg-Time 0.122)\t Loss 1.1771 (Avg-Loss 1.1580)\tAcc 58.2031 (Avg-Acc 58.9160)\n",
            "Epoch: [184][18/39]\tTime 0.057 (Avg-Time 0.097)\t Loss 1.1570 (Avg-Loss 1.1489)\tAcc 59.0820 (Avg-Acc 58.9535)\n",
            "Epoch: [184][27/39]\tTime 0.058 (Avg-Time 0.085)\t Loss 1.1013 (Avg-Loss 1.1397)\tAcc 60.9375 (Avg-Acc 59.3541)\n",
            "Epoch: [184][36/39]\tTime 0.060 (Avg-Time 0.079)\t Loss 1.0920 (Avg-Loss 1.1361)\tAcc 61.8164 (Avg-Acc 59.5307)\n",
            "Epoch: [184][39/39]\tTime 0.006 (Avg-Time 0.076)\t Loss 1.0397 (Avg-Loss 1.1336)\tAcc 59.3750 (Avg-Acc 59.6675)\n",
            "EPOCH: 184 train Results: Acc 59.667 Loss: 1.1336\n",
            "Epoch: [184][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1881 (Avg-Loss 1.1881)\tAcc 60.0586 (Avg-Acc 60.0586)\n",
            "Epoch: [184][9/9]\tTime 0.013 (Avg-Time 0.016)\t Loss 1.2188 (Avg-Loss 1.2114)\tAcc 56.6327 (Avg-Acc 57.0300)\n",
            "EPOCH: 184 Validation Results: Acc 57.030 Loss: 1.2114\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [185][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.0697 (Avg-Loss 1.0697)\tAcc 61.7188 (Avg-Acc 61.7188)\n",
            "Epoch: [185][9/39]\tTime 0.070 (Avg-Time 0.061)\t Loss 1.0879 (Avg-Loss 1.1070)\tAcc 62.2070 (Avg-Acc 61.0156)\n",
            "Epoch: [185][18/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1024 (Avg-Loss 1.1127)\tAcc 60.6445 (Avg-Acc 60.8604)\n",
            "Epoch: [185][27/39]\tTime 0.087 (Avg-Time 0.060)\t Loss 1.0718 (Avg-Loss 1.1094)\tAcc 62.0117 (Avg-Acc 60.7806)\n",
            "Epoch: [185][36/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1001 (Avg-Loss 1.1115)\tAcc 63.4766 (Avg-Acc 60.7052)\n",
            "Epoch: [185][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.0405 (Avg-Loss 1.1131)\tAcc 64.0625 (Avg-Acc 60.6375)\n",
            "EPOCH: 185 train Results: Acc 60.638 Loss: 1.1131\n",
            "Epoch: [185][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2151 (Avg-Loss 1.2151)\tAcc 58.5938 (Avg-Acc 58.5938)\n",
            "Epoch: [185][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2293 (Avg-Loss 1.2346)\tAcc 56.2500 (Avg-Acc 56.0700)\n",
            "EPOCH: 185 Validation Results: Acc 56.070 Loss: 1.2346\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [186][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.1230 (Avg-Loss 1.1230)\tAcc 60.8398 (Avg-Acc 60.8398)\n",
            "Epoch: [186][9/39]\tTime 0.058 (Avg-Time 0.062)\t Loss 1.1094 (Avg-Loss 1.1406)\tAcc 58.9844 (Avg-Acc 58.9941)\n",
            "Epoch: [186][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.0871 (Avg-Loss 1.1376)\tAcc 61.0352 (Avg-Acc 59.4110)\n",
            "Epoch: [186][27/39]\tTime 0.061 (Avg-Time 0.060)\t Loss 1.1414 (Avg-Loss 1.1286)\tAcc 56.5430 (Avg-Acc 59.7656)\n",
            "Epoch: [186][36/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.0837 (Avg-Loss 1.1210)\tAcc 62.9883 (Avg-Acc 60.2064)\n",
            "Epoch: [186][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 0.9788 (Avg-Loss 1.1203)\tAcc 64.0625 (Avg-Acc 60.2450)\n",
            "EPOCH: 186 train Results: Acc 60.245 Loss: 1.1203\n",
            "Epoch: [186][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1872 (Avg-Loss 1.1872)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [186][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2180 (Avg-Loss 1.2103)\tAcc 56.5051 (Avg-Acc 56.9900)\n",
            "EPOCH: 186 Validation Results: Acc 56.990 Loss: 1.2103\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [187][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.1181 (Avg-Loss 1.1181)\tAcc 59.9609 (Avg-Acc 59.9609)\n",
            "Epoch: [187][9/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1786 (Avg-Loss 1.1165)\tAcc 55.1758 (Avg-Acc 60.2734)\n",
            "Epoch: [187][18/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.0777 (Avg-Loss 1.1128)\tAcc 61.3281 (Avg-Acc 60.6856)\n",
            "Epoch: [187][27/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.0626 (Avg-Loss 1.1056)\tAcc 61.6211 (Avg-Acc 60.7282)\n",
            "Epoch: [187][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.0873 (Avg-Loss 1.1068)\tAcc 62.9883 (Avg-Acc 60.6788)\n",
            "Epoch: [187][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.0444 (Avg-Loss 1.1063)\tAcc 60.9375 (Avg-Acc 60.7075)\n",
            "EPOCH: 187 train Results: Acc 60.708 Loss: 1.1063\n",
            "Epoch: [187][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1817 (Avg-Loss 1.1817)\tAcc 59.2773 (Avg-Acc 59.2773)\n",
            "Epoch: [187][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2153 (Avg-Loss 1.2094)\tAcc 56.7602 (Avg-Acc 57.1200)\n",
            "EPOCH: 187 Validation Results: Acc 57.120 Loss: 1.2094\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [188][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.0682 (Avg-Loss 1.0682)\tAcc 61.8164 (Avg-Acc 61.8164)\n",
            "Epoch: [188][9/39]\tTime 0.055 (Avg-Time 0.061)\t Loss 1.1023 (Avg-Loss 1.1146)\tAcc 60.0586 (Avg-Acc 60.1953)\n",
            "Epoch: [188][18/39]\tTime 0.204 (Avg-Time 0.104)\t Loss 1.0833 (Avg-Loss 1.1143)\tAcc 61.1328 (Avg-Acc 60.3104)\n",
            "Epoch: [188][27/39]\tTime 0.118 (Avg-Time 0.123)\t Loss 1.1130 (Avg-Loss 1.1187)\tAcc 60.4492 (Avg-Acc 60.3620)\n",
            "Epoch: [188][36/39]\tTime 0.057 (Avg-Time 0.114)\t Loss 1.1421 (Avg-Loss 1.1177)\tAcc 59.4727 (Avg-Acc 60.3516)\n",
            "Epoch: [188][39/39]\tTime 0.006 (Avg-Time 0.109)\t Loss 0.9938 (Avg-Loss 1.1148)\tAcc 59.3750 (Avg-Acc 60.4375)\n",
            "EPOCH: 188 train Results: Acc 60.438 Loss: 1.1148\n",
            "Epoch: [188][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.2042 (Avg-Loss 1.2042)\tAcc 58.3008 (Avg-Acc 58.3008)\n",
            "Epoch: [188][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2239 (Avg-Loss 1.2300)\tAcc 56.8878 (Avg-Acc 56.5900)\n",
            "EPOCH: 188 Validation Results: Acc 56.590 Loss: 1.2300\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [189][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1384 (Avg-Loss 1.1384)\tAcc 60.4492 (Avg-Acc 60.4492)\n",
            "Epoch: [189][9/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1388 (Avg-Loss 1.1232)\tAcc 60.2539 (Avg-Acc 60.5078)\n",
            "Epoch: [189][18/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1160 (Avg-Loss 1.1243)\tAcc 60.8398 (Avg-Acc 60.3361)\n",
            "Epoch: [189][27/39]\tTime 0.058 (Avg-Time 0.060)\t Loss 1.1335 (Avg-Loss 1.1169)\tAcc 59.3750 (Avg-Acc 60.5015)\n",
            "Epoch: [189][36/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.0130 (Avg-Loss 1.1122)\tAcc 63.5742 (Avg-Acc 60.7026)\n",
            "Epoch: [189][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1190 (Avg-Loss 1.1118)\tAcc 54.6875 (Avg-Acc 60.6725)\n",
            "EPOCH: 189 train Results: Acc 60.672 Loss: 1.1118\n",
            "Epoch: [189][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1805 (Avg-Loss 1.1805)\tAcc 58.6914 (Avg-Acc 58.6914)\n",
            "Epoch: [189][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2096 (Avg-Loss 1.2108)\tAcc 56.3776 (Avg-Acc 56.7000)\n",
            "EPOCH: 189 Validation Results: Acc 56.700 Loss: 1.2108\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [190][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.0760 (Avg-Loss 1.0760)\tAcc 60.9375 (Avg-Acc 60.9375)\n",
            "Epoch: [190][9/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.0552 (Avg-Loss 1.1009)\tAcc 64.3555 (Avg-Acc 61.0547)\n",
            "Epoch: [190][18/39]\tTime 0.062 (Avg-Time 0.060)\t Loss 1.0957 (Avg-Loss 1.1040)\tAcc 60.6445 (Avg-Acc 60.8296)\n",
            "Epoch: [190][27/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.1190 (Avg-Loss 1.1062)\tAcc 60.6445 (Avg-Acc 60.8782)\n",
            "Epoch: [190][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.0789 (Avg-Loss 1.1135)\tAcc 58.9844 (Avg-Acc 60.6815)\n",
            "Epoch: [190][39/39]\tTime 0.009 (Avg-Time 0.058)\t Loss 1.0786 (Avg-Loss 1.1148)\tAcc 60.9375 (Avg-Acc 60.6400)\n",
            "EPOCH: 190 train Results: Acc 60.640 Loss: 1.1148\n",
            "Epoch: [190][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1884 (Avg-Loss 1.1884)\tAcc 59.0820 (Avg-Acc 59.0820)\n",
            "Epoch: [190][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2338 (Avg-Loss 1.2199)\tAcc 57.2704 (Avg-Acc 56.8000)\n",
            "EPOCH: 190 Validation Results: Acc 56.800 Loss: 1.2199\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [191][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.1314 (Avg-Loss 1.1314)\tAcc 58.6914 (Avg-Acc 58.6914)\n",
            "Epoch: [191][9/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1042 (Avg-Loss 1.1279)\tAcc 62.0117 (Avg-Acc 60.1270)\n",
            "Epoch: [191][18/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1190 (Avg-Loss 1.1237)\tAcc 60.2539 (Avg-Acc 60.3361)\n",
            "Epoch: [191][27/39]\tTime 0.063 (Avg-Time 0.060)\t Loss 1.1088 (Avg-Loss 1.1233)\tAcc 62.1094 (Avg-Acc 60.2400)\n",
            "Epoch: [191][36/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.1350 (Avg-Loss 1.1219)\tAcc 60.5469 (Avg-Acc 60.2856)\n",
            "Epoch: [191][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.0386 (Avg-Loss 1.1214)\tAcc 64.0625 (Avg-Acc 60.2700)\n",
            "EPOCH: 191 train Results: Acc 60.270 Loss: 1.1214\n",
            "Epoch: [191][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1943 (Avg-Loss 1.1943)\tAcc 59.6680 (Avg-Acc 59.6680)\n",
            "Epoch: [191][9/9]\tTime 0.011 (Avg-Time 0.017)\t Loss 1.2182 (Avg-Loss 1.2117)\tAcc 56.7602 (Avg-Acc 56.9900)\n",
            "EPOCH: 191 Validation Results: Acc 56.990 Loss: 1.2117\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [192][0/39]\tTime 0.057 (Avg-Time 0.057)\t Loss 1.0841 (Avg-Loss 1.0841)\tAcc 60.7422 (Avg-Acc 60.7422)\n",
            "Epoch: [192][9/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1886 (Avg-Loss 1.1242)\tAcc 56.2500 (Avg-Acc 60.2832)\n",
            "Epoch: [192][18/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.1542 (Avg-Loss 1.1207)\tAcc 58.9844 (Avg-Acc 60.6343)\n",
            "Epoch: [192][27/39]\tTime 0.056 (Avg-Time 0.059)\t Loss 1.0724 (Avg-Loss 1.1125)\tAcc 62.2070 (Avg-Acc 60.8224)\n",
            "Epoch: [192][36/39]\tTime 0.092 (Avg-Time 0.066)\t Loss 1.0351 (Avg-Loss 1.1068)\tAcc 62.4023 (Avg-Acc 60.8662)\n",
            "Epoch: [192][39/39]\tTime 0.021 (Avg-Time 0.069)\t Loss 1.0969 (Avg-Loss 1.1073)\tAcc 60.9375 (Avg-Acc 60.8625)\n",
            "EPOCH: 192 train Results: Acc 60.862 Loss: 1.1073\n",
            "Epoch: [192][0/9]\tTime 0.038 (Avg-Time 0.038)\t Loss 1.1798 (Avg-Loss 1.1798)\tAcc 60.5469 (Avg-Acc 60.5469)\n",
            "Epoch: [192][9/9]\tTime 0.071 (Avg-Time 0.060)\t Loss 1.2194 (Avg-Loss 1.2061)\tAcc 56.7602 (Avg-Acc 56.9800)\n",
            "EPOCH: 192 Validation Results: Acc 56.980 Loss: 1.2061\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [193][0/39]\tTime 0.178 (Avg-Time 0.178)\t Loss 1.0750 (Avg-Loss 1.0750)\tAcc 62.7930 (Avg-Acc 62.7930)\n",
            "Epoch: [193][9/39]\tTime 0.115 (Avg-Time 0.158)\t Loss 1.0995 (Avg-Loss 1.0994)\tAcc 61.3281 (Avg-Acc 61.6406)\n",
            "Epoch: [193][18/39]\tTime 0.060 (Avg-Time 0.124)\t Loss 1.1502 (Avg-Loss 1.1084)\tAcc 59.0820 (Avg-Acc 60.7987)\n",
            "Epoch: [193][27/39]\tTime 0.066 (Avg-Time 0.104)\t Loss 1.0582 (Avg-Loss 1.1094)\tAcc 63.2812 (Avg-Acc 60.5504)\n",
            "Epoch: [193][36/39]\tTime 0.056 (Avg-Time 0.093)\t Loss 1.0924 (Avg-Loss 1.1081)\tAcc 61.5234 (Avg-Acc 60.5231)\n",
            "Epoch: [193][39/39]\tTime 0.006 (Avg-Time 0.089)\t Loss 1.0161 (Avg-Loss 1.1066)\tAcc 64.0625 (Avg-Acc 60.5700)\n",
            "EPOCH: 193 train Results: Acc 60.570 Loss: 1.1066\n",
            "Epoch: [193][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1871 (Avg-Loss 1.1871)\tAcc 58.2031 (Avg-Acc 58.2031)\n",
            "Epoch: [193][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2165 (Avg-Loss 1.2128)\tAcc 57.1429 (Avg-Acc 56.6900)\n",
            "EPOCH: 193 Validation Results: Acc 56.690 Loss: 1.2128\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [194][0/39]\tTime 0.059 (Avg-Time 0.059)\t Loss 1.0651 (Avg-Loss 1.0651)\tAcc 60.6445 (Avg-Acc 60.6445)\n",
            "Epoch: [194][9/39]\tTime 0.062 (Avg-Time 0.063)\t Loss 1.0447 (Avg-Loss 1.0945)\tAcc 63.7695 (Avg-Acc 61.4844)\n",
            "Epoch: [194][18/39]\tTime 0.058 (Avg-Time 0.061)\t Loss 1.1243 (Avg-Loss 1.0986)\tAcc 59.3750 (Avg-Acc 61.0917)\n",
            "Epoch: [194][27/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.0630 (Avg-Loss 1.1042)\tAcc 63.4766 (Avg-Acc 60.7422)\n",
            "Epoch: [194][36/39]\tTime 0.060 (Avg-Time 0.061)\t Loss 1.1533 (Avg-Loss 1.1090)\tAcc 60.4492 (Avg-Acc 60.6129)\n",
            "Epoch: [194][39/39]\tTime 0.006 (Avg-Time 0.060)\t Loss 1.1530 (Avg-Loss 1.1091)\tAcc 56.2500 (Avg-Acc 60.5950)\n",
            "EPOCH: 194 train Results: Acc 60.595 Loss: 1.1091\n",
            "Epoch: [194][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.1981 (Avg-Loss 1.1981)\tAcc 60.3516 (Avg-Acc 60.3516)\n",
            "Epoch: [194][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.2194 (Avg-Loss 1.2191)\tAcc 56.7602 (Avg-Acc 56.8400)\n",
            "EPOCH: 194 Validation Results: Acc 56.840 Loss: 1.2191\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [195][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.0841 (Avg-Loss 1.0841)\tAcc 61.9141 (Avg-Acc 61.9141)\n",
            "Epoch: [195][9/39]\tTime 0.063 (Avg-Time 0.060)\t Loss 1.1585 (Avg-Loss 1.1250)\tAcc 58.5938 (Avg-Acc 60.3027)\n",
            "Epoch: [195][18/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.1257 (Avg-Loss 1.1251)\tAcc 58.9844 (Avg-Acc 60.0329)\n",
            "Epoch: [195][27/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.0803 (Avg-Loss 1.1228)\tAcc 60.9375 (Avg-Acc 60.1353)\n",
            "Epoch: [195][36/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1066 (Avg-Loss 1.1191)\tAcc 63.0859 (Avg-Acc 60.3173)\n",
            "Epoch: [195][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.1472 (Avg-Loss 1.1183)\tAcc 60.9375 (Avg-Acc 60.3275)\n",
            "EPOCH: 195 train Results: Acc 60.328 Loss: 1.1183\n",
            "Epoch: [195][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1885 (Avg-Loss 1.1885)\tAcc 59.7656 (Avg-Acc 59.7656)\n",
            "Epoch: [195][9/9]\tTime 0.011 (Avg-Time 0.015)\t Loss 1.1982 (Avg-Loss 1.2124)\tAcc 57.2704 (Avg-Acc 56.7600)\n",
            "EPOCH: 195 Validation Results: Acc 56.760 Loss: 1.2124\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [196][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.0452 (Avg-Loss 1.0452)\tAcc 64.7461 (Avg-Acc 64.7461)\n",
            "Epoch: [196][9/39]\tTime 0.056 (Avg-Time 0.062)\t Loss 1.1242 (Avg-Loss 1.1045)\tAcc 59.2773 (Avg-Acc 60.9668)\n",
            "Epoch: [196][18/39]\tTime 0.062 (Avg-Time 0.061)\t Loss 1.1200 (Avg-Loss 1.1121)\tAcc 60.0586 (Avg-Acc 60.5983)\n",
            "Epoch: [196][27/39]\tTime 0.055 (Avg-Time 0.061)\t Loss 1.1278 (Avg-Loss 1.1112)\tAcc 60.3516 (Avg-Acc 60.5399)\n",
            "Epoch: [196][36/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1287 (Avg-Loss 1.1060)\tAcc 58.3008 (Avg-Acc 60.5838)\n",
            "Epoch: [196][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.2699 (Avg-Loss 1.1041)\tAcc 57.8125 (Avg-Acc 60.6475)\n",
            "EPOCH: 196 train Results: Acc 60.648 Loss: 1.1041\n",
            "Epoch: [196][0/9]\tTime 0.017 (Avg-Time 0.017)\t Loss 1.1935 (Avg-Loss 1.1935)\tAcc 58.1055 (Avg-Acc 58.1055)\n",
            "Epoch: [196][9/9]\tTime 0.011 (Avg-Time 0.018)\t Loss 1.2116 (Avg-Loss 1.2080)\tAcc 57.6531 (Avg-Acc 56.6700)\n",
            "EPOCH: 196 Validation Results: Acc 56.670 Loss: 1.2080\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [197][0/39]\tTime 0.060 (Avg-Time 0.060)\t Loss 1.0734 (Avg-Loss 1.0734)\tAcc 62.6953 (Avg-Acc 62.6953)\n",
            "Epoch: [197][9/39]\tTime 0.060 (Avg-Time 0.059)\t Loss 1.1355 (Avg-Loss 1.1261)\tAcc 60.1562 (Avg-Acc 60.3027)\n",
            "Epoch: [197][18/39]\tTime 0.223 (Avg-Time 0.108)\t Loss 1.1299 (Avg-Loss 1.1341)\tAcc 59.9609 (Avg-Acc 59.6680)\n",
            "Epoch: [197][27/39]\tTime 0.120 (Avg-Time 0.126)\t Loss 1.1185 (Avg-Loss 1.1276)\tAcc 60.0586 (Avg-Acc 59.8737)\n",
            "Epoch: [197][36/39]\tTime 0.057 (Avg-Time 0.115)\t Loss 1.0725 (Avg-Loss 1.1219)\tAcc 60.3516 (Avg-Acc 60.1193)\n",
            "Epoch: [197][39/39]\tTime 0.006 (Avg-Time 0.110)\t Loss 0.9500 (Avg-Loss 1.1208)\tAcc 67.1875 (Avg-Acc 60.1750)\n",
            "EPOCH: 197 train Results: Acc 60.175 Loss: 1.1208\n",
            "Epoch: [197][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.1932 (Avg-Loss 1.1932)\tAcc 58.9844 (Avg-Acc 58.9844)\n",
            "Epoch: [197][9/9]\tTime 0.011 (Avg-Time 0.018)\t Loss 1.2117 (Avg-Loss 1.2123)\tAcc 57.2704 (Avg-Acc 56.6900)\n",
            "EPOCH: 197 Validation Results: Acc 56.690 Loss: 1.2123\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [198][0/39]\tTime 0.061 (Avg-Time 0.061)\t Loss 1.0619 (Avg-Loss 1.0619)\tAcc 60.2539 (Avg-Acc 60.2539)\n",
            "Epoch: [198][9/39]\tTime 0.057 (Avg-Time 0.060)\t Loss 1.1247 (Avg-Loss 1.1157)\tAcc 60.7422 (Avg-Acc 60.7520)\n",
            "Epoch: [198][18/39]\tTime 0.056 (Avg-Time 0.061)\t Loss 1.1527 (Avg-Loss 1.1125)\tAcc 59.6680 (Avg-Acc 60.6086)\n",
            "Epoch: [198][27/39]\tTime 0.079 (Avg-Time 0.061)\t Loss 1.1579 (Avg-Loss 1.1115)\tAcc 57.4219 (Avg-Acc 60.7561)\n",
            "Epoch: [198][36/39]\tTime 0.057 (Avg-Time 0.061)\t Loss 1.0884 (Avg-Loss 1.1094)\tAcc 61.0352 (Avg-Acc 60.6577)\n",
            "Epoch: [198][39/39]\tTime 0.006 (Avg-Time 0.059)\t Loss 1.2789 (Avg-Loss 1.1090)\tAcc 53.1250 (Avg-Acc 60.6800)\n",
            "EPOCH: 198 train Results: Acc 60.680 Loss: 1.1090\n",
            "Epoch: [198][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.1930 (Avg-Loss 1.1930)\tAcc 59.9609 (Avg-Acc 59.9609)\n",
            "Epoch: [198][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2126 (Avg-Loss 1.2094)\tAcc 57.2704 (Avg-Acc 56.8300)\n",
            "EPOCH: 198 Validation Results: Acc 56.830 Loss: 1.2094\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [199][0/39]\tTime 0.058 (Avg-Time 0.058)\t Loss 1.0097 (Avg-Loss 1.0097)\tAcc 63.0859 (Avg-Acc 63.0859)\n",
            "Epoch: [199][9/39]\tTime 0.062 (Avg-Time 0.060)\t Loss 1.1465 (Avg-Loss 1.1110)\tAcc 60.1562 (Avg-Acc 60.6543)\n",
            "Epoch: [199][18/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.0692 (Avg-Loss 1.1192)\tAcc 62.6953 (Avg-Acc 60.5109)\n",
            "Epoch: [199][27/39]\tTime 0.056 (Avg-Time 0.060)\t Loss 1.1253 (Avg-Loss 1.1150)\tAcc 60.9375 (Avg-Acc 60.4632)\n",
            "Epoch: [199][36/39]\tTime 0.055 (Avg-Time 0.059)\t Loss 1.0988 (Avg-Loss 1.1136)\tAcc 59.6680 (Avg-Acc 60.3885)\n",
            "Epoch: [199][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.1655 (Avg-Loss 1.1138)\tAcc 54.6875 (Avg-Acc 60.4075)\n",
            "EPOCH: 199 train Results: Acc 60.407 Loss: 1.1138\n",
            "Epoch: [199][0/9]\tTime 0.015 (Avg-Time 0.015)\t Loss 1.1918 (Avg-Loss 1.1918)\tAcc 59.0820 (Avg-Acc 59.0820)\n",
            "Epoch: [199][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.1951 (Avg-Loss 1.2037)\tAcc 58.5459 (Avg-Acc 57.1500)\n",
            "EPOCH: 199 Validation Results: Acc 57.150 Loss: 1.2037\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [200][0/39]\tTime 0.062 (Avg-Time 0.062)\t Loss 1.0985 (Avg-Loss 1.0985)\tAcc 59.8633 (Avg-Acc 59.8633)\n",
            "Epoch: [200][9/39]\tTime 0.056 (Avg-Time 0.058)\t Loss 1.1216 (Avg-Loss 1.1158)\tAcc 61.8164 (Avg-Acc 59.7754)\n",
            "Epoch: [200][18/39]\tTime 0.058 (Avg-Time 0.059)\t Loss 1.1295 (Avg-Loss 1.1162)\tAcc 58.9844 (Avg-Acc 59.9815)\n",
            "Epoch: [200][27/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1440 (Avg-Loss 1.1131)\tAcc 59.1797 (Avg-Acc 60.1179)\n",
            "Epoch: [200][36/39]\tTime 0.057 (Avg-Time 0.059)\t Loss 1.1509 (Avg-Loss 1.1107)\tAcc 58.3008 (Avg-Acc 60.2354)\n",
            "Epoch: [200][39/39]\tTime 0.006 (Avg-Time 0.058)\t Loss 1.0923 (Avg-Loss 1.1108)\tAcc 53.1250 (Avg-Acc 60.2625)\n",
            "EPOCH: 200 train Results: Acc 60.263 Loss: 1.1108\n",
            "Epoch: [200][0/9]\tTime 0.014 (Avg-Time 0.014)\t Loss 1.1975 (Avg-Loss 1.1975)\tAcc 59.5703 (Avg-Acc 59.5703)\n",
            "Epoch: [200][9/9]\tTime 0.011 (Avg-Time 0.016)\t Loss 1.2147 (Avg-Loss 1.2148)\tAcc 57.5255 (Avg-Acc 56.9700)\n",
            "EPOCH: 200 Validation Results: Acc 56.970 Loss: 1.2148\n",
            "Best Accuracy: 57.4200\n",
            "\n",
            "End time:  Sat Mar 29 19:11:13 2025\n",
            "train executed in 592.9321 seconds\n",
            "\n",
            "Test Accuracy for Config: 57.71%\n",
            "\n",
            "\n",
            "=== Tuning Results ===\n",
            "Best Accuracy: 58.60%\n",
            "Best Configuration:\n",
            "{'lr': 0.02, 'batch_size': 2048, 'hidden_units': [512, 256], 'dropout_rates': [0.4, 0.4], 'pre-process': None, 'epoch': 200, 'weight_decay': 0.0005, 'momentum': 0.9, 'optimizer': 'sgd'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VsVNfYDtEtGC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}