{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:46.802619Z",
     "start_time": "2025-03-26T10:15:46.791029Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import time\n",
    "import seaborn as sns\n",
    "import math\n"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:46.869811Z",
     "start_time": "2025-03-26T10:15:46.866934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('Start time: ', time.ctime())\n",
    "        start_time = time.time()  # start time\n",
    "\n",
    "        result = func(*args, **kwargs)  # run\n",
    "\n",
    "        end_time = time.time()  # end time\n",
    "        print('End time: ', time.ctime())\n",
    "        print(f\"{func.__name__} executed in {(end_time - start_time):.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ],
   "id": "98e36a535e7c2b42",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pre-process\n",
    "\n",
    "Min-max normalization:\n",
    "\n",
    "$$x_{min-max} = {{x-min(x)}\\over{max(x)-min(x)}}$$\n",
    "\n",
    "Standardization:\n",
    "\n",
    "$$x_{norm} = {{x-\\mu}\\over{\\sigma}}$$"
   ],
   "id": "1ea172e08505f1e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:46.874485Z",
     "start_time": "2025-03-26T10:15:46.870687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pre_processing(X, mode=None):\n",
    "    if mode == 'min-max':\n",
    "        print('Pre-process: min-max normalization')\n",
    "        min_each_feature = np.min(X, axis=0)\n",
    "        max_each_feature = np.max(X, axis=0)\n",
    "        scale = max_each_feature - min_each_feature\n",
    "        scale[scale == 0] = 1   # To avoid divided by 0\n",
    "        scaled_train = (X - min_each_feature) / scale\n",
    "        return scaled_train\n",
    "\n",
    "    if mode == 'standardization':\n",
    "        print('Pre-process: standardization')\n",
    "        std_each_feature = np.std(X, axis=0)\n",
    "        mean_each_feature = np.mean(X, axis=0)\n",
    "        std_each_feature[std_each_feature == 0] = 1     # To avoid divided by 0\n",
    "        norm_train = (X - mean_each_feature) / std_each_feature\n",
    "        norm_test = (X - mean_each_feature) / std_each_feature\n",
    "        return norm_train\n",
    "\n",
    "    print('No pre-process')\n",
    "\n",
    "    return X"
   ],
   "id": "2578bdbe5f4e1107",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:46.889891Z",
     "start_time": "2025-03-26T10:15:46.885928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def accuracy(y_hat,y):\n",
    "    '''\n",
    "    y_hat : predicted value\n",
    "    :param y_hat: [batch_size,num_of_class]\n",
    "    :param y: [batch_size,1\n",
    "    :return: \n",
    "    '''\n",
    "    preds=y_hat.argmax(axis=1,keepdims=True)\n",
    "    return np.mean(preds == y)*100"
   ],
   "id": "5f6ecbfb8386710",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:46.905900Z",
     "start_time": "2025-03-26T10:15:46.890577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_gain(nonlinearity, param=None):\n",
    "    if nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    elif nonlinearity == 'leaky_relu':\n",
    "        if param is None:\n",
    "            negative_slope = 0.01\n",
    "        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n",
    "            # True/False are instances of int, hence check above\n",
    "            negative_slope = param\n",
    "        else:\n",
    "            raise ValueError(f\"negative_slope {param} not a valid number\")\n",
    "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
    "    elif nonlinearity == 'selu':\n",
    "        return 3.0 / 4  # Value found empirically (https://github.com/pytorch/pytorch/pull/50664)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported nonlinearity {nonlinearity}\")\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(array):\n",
    "    dimensions = len(array.shape)\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n",
    "\n",
    "    num_input_fmaps = array.shape[1]\n",
    "    num_output_fmaps = array.shape[0]\n",
    "    receptive_field_size = 1\n",
    "    if dimensions > 2:\n",
    "        # math.prod is not always available, accumulate the product manually\n",
    "        # we could use functools.reduce but that is not supported by TorchScript\n",
    "        for s in array.shape[2:]:\n",
    "            receptive_field_size *= s\n",
    "    fan_in = num_input_fmaps * receptive_field_size\n",
    "    fan_out = num_output_fmaps * receptive_field_size\n",
    "\n",
    "    return fan_in, fan_out\n",
    "\n",
    "def _calculate_correct_fan(array, mode):\n",
    "    mode = mode.lower()\n",
    "    valid_modes = ['fan_in', 'fan_out']\n",
    "    if mode not in valid_modes:\n",
    "        raise ValueError(f\"Mode {mode} not supported, please use one of {valid_modes}\")\n",
    "\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(array)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "def kaiming_normal_(array: np.array, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'relu'):\n",
    "    fan = _calculate_correct_fan(array, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    return np.random.normal(0, std, array.shape)"
   ],
   "id": "ac167cc0d89add8",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:46.914463Z",
     "start_time": "2025-03-26T10:15:46.906590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class Parameter(object):\n",
    "#     \"\"\"Parameter class for saving data and gradients\"\"\"\n",
    "#     def __init__(self, data, requires_grad, skip_decay=False):\n",
    "#         self.data = data\n",
    "#         self.grad = None\n",
    "#         self.skip_decay = skip_decay\n",
    "#         self.requires_grad = requires_grad"
   ],
   "id": "785071ee87e7f9d7",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:46.925880Z",
     "start_time": "2025-03-26T10:15:46.917337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        self.name = name\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "    def _forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    def _backward(self, *args):\n",
    "        pass"
   ],
   "id": "767328eb1e97167e",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:46.936144Z",
     "start_time": "2025-03-26T10:15:46.926774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _backward(self, gradient_output):\n",
    "        gradient_output[self.x <= 0] = 0\n",
    "        return gradient_output"
   ],
   "id": "8c6d47201896a516",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Forward:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{xW} + \\mathbf{b}$$\n",
    "\n",
    "Backward:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{W}^T$$\n",
    "\n",
    "Gradient of W:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{x}^T$$\n",
    "\n",
    "Gradient of b:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{b}} = \\frac{\\partial L}{\\partial \\mathbf{y}} $$\n",
    "\n",
    "Gradient of x:\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{W}^T$$"
   ],
   "id": "6d64cedcd4ee0dc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:46.946836Z",
     "start_time": "2025-03-26T10:15:46.936144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FCLayer(Layer):\n",
    "    def __init__(self, name: str, n_in: int, n_out: int, skip_decay=False) -> None:\n",
    "        '''\n",
    "        Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,)\n",
    "        :param n_in: dimensionality of input\n",
    "        :param n_out: number of hidden units\n",
    "        '''\n",
    "        super().__init__(name, requires_grad=True)\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        W = kaiming_normal_(np.array([0] * n_in * n_out).reshape(n_in, n_out), a=math.sqrt(5))\n",
    "        self.W = W\n",
    "        self.b = np.zeros(self.n_out)\n",
    "        self.W_grad = None\n",
    "        self.b_grad = None\n",
    "        self.skip_decay = skip_decay\n",
    "\n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "            x: [batch size, n_in]\n",
    "            W: [n_in, n_out]\n",
    "            b: [n_out]\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        temp = x @ self.W + self.b\n",
    "        # [batch_size,n_in] @ [n_in,n_out] + [n_output] => [batch_size,n_out]\n",
    "        return x @ self.W + self.b\n",
    "\n",
    "    def _backward(self, delta: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        delta: the gradient of the loss function respect to this layer's output 这层损失函数对于这层输出的梯度\n",
    "        :param delta: [batch size, n_out]:\n",
    "        :return:\n",
    "        '''\n",
    "        batch_size = delta.shape[0]\n",
    "        self.W_grad = self.x.T @ delta / batch_size  # [batch_size,n_in]^T @ [batch size, n_out] => [n_in,n_out]\n",
    "        self.b_grad = delta.sum(axis=0) / batch_size  # divide by batch size to get average of gradient\n",
    "        return delta @ self.W.T  # return the gradient of input(x) back to last layer"
   ],
   "id": "e5b00fb819887fa6",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Softmax \n",
    "Formula:\n",
    "$$softmax(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}$$\n",
    "\n",
    "不需要计算 softmax 的完整 Jacobian 矩阵，因为与交叉熵结合后公式极大简化了。\n",
    "只需用 preds - ground_truth 作为梯度，这个计算在 CrossEntropyLoss 里完成了：\n",
    "self.grad = preds - ground_truth\n",
    "因此，在 softmax.backward(gradient_output) 时，不需要额外计算 softmax 的梯度，而是直接返回 gradient_output\n"
   ],
   "id": "86a2cacdcffb889f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:46.958682Z",
     "start_time": "2025-03-26T10:15:46.951863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self,name,requires_grad=False):\n",
    "        super().__init__(name,requires_grad)\n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        x_exp =  np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return x_exp/x_exp.sum(axis=1, keepdims=True)\n",
    "    def _backward(self, gradient_output: np.ndarray) -> np.ndarray:\n",
    "        return gradient_output"
   ],
   "id": "2883c1fd2bf68ed0",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loss Function - Cross Entropy\n",
    "Formula:\n",
    "$$CrossEntropy= - \\sum_{i=1}^{n} y_i log(\\hat {y_i})$$\n",
    "\n",
    "Gradient of softmax:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_k} = \\sum_{i}^{c} \\left( \\frac{\\partial L}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_k} \\right)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y}_i} = - \\frac{y_i}{\\hat{y}_i}, \\qquad \\frac{\\partial \\hat{y}_i}{\\partial z_k} = \\begin{cases}\n",
    "\\hat{y}_i(1 - \\hat{y}_i) & \\text{if } i = k \\\\\n",
    "-\\hat{y}_k\\hat{y}_i & \\text{if } i \\neq k\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_k} = - \\left( (y_k(1 - \\hat{y}_k)) - \\sum_{i \\neq k}^{c} y_i \\hat{y}_k \\right) = -(y_k - \\hat{y}_k \\sum_{i}^{c} y_i) = \\hat{y}_k - y_k\n",
    "$$\n",
    "\n",
    "$$=> \\frac{\\partial L}{\\partial z} = \\hat{y} - y$$\n"
   ],
   "id": "8860ec1e183838c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:46.986355Z",
     "start_time": "2025-03-26T10:15:46.981305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossEntropy(object):\n",
    "    def __init__(self):\n",
    "        self.softmax = Softmax('softmax')\n",
    "\n",
    "    def __call__(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "\n",
    "        :param x:\n",
    "        :param y: [batch_size, 1]\n",
    "        :return:\n",
    "        '''\n",
    "        self.batch_size = x.shape[0]\n",
    "        self.class_num = x.shape[1]\n",
    "\n",
    "        y_hat = self.softmax._forward(x) #[batch_size,num_class]\n",
    "\n",
    "        y=self.one_hot_encoding(y)\n",
    "        self.grad = y_hat - y\n",
    "\n",
    "        loss = -1 * (y * np.log(y_hat + 1e-8)).sum() / self.batch_size  # to avoid divided by 0\n",
    "        return loss\n",
    "\n",
    "    def one_hot_encoding(self, x):\n",
    "        one_hot_encoded = np.zeros((self.batch_size, self.class_num))\n",
    "        one_hot_encoded[np.arange(x.shape[0]), x.flatten()] = 1\n",
    "        return one_hot_encoded"
   ],
   "id": "bdb959962b6db01c",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:47.014208Z",
     "start_time": "2025-03-26T10:15:46.997108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP_V2():\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            FCLayer('fc1', n_in=128, n_out=512),\n",
    "            # Dropout('dropout1', 0.6),\n",
    "            # ReLU('relu1'),\n",
    "            # FCLayer('fc2', n_in=512, n_out=256),\n",
    "            # Dropout('dropout2', 0.4),\n",
    "            BatchNormalization(\"batchnorm1\",feature_num=512),\n",
    "            # ReLU('relu2'),\n",
    "            FCLayer('fc2', n_in=512, n_out=128),\n",
    "            ReLU('relu2'),\n",
    "            FCLayer('fc3', n_in=128, n_out=10)\n",
    "        ]\n",
    "        self.parameters = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, \"W\"):\n",
    "                # 将每个参数及其梯度、skip_decay 作为引用添加到 parameters 中\n",
    "                self.parameters.append([layer.W, layer.W_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"b\"):\n",
    "                self.parameters.append([layer.b, layer.b_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"gamma\"):\n",
    "                self.parameters.append([layer.gamma, layer.gamma_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"beta\"):\n",
    "                self.parameters.append([layer.beta, layer.beta_grad, layer.skip_decay])\n",
    "\n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        for layer in self.layers:\n",
    "            x= layer._forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _backward(self, gradient_output: np.ndarray) -> np.ndarray:\n",
    "        for layer in self.layers[::-1]:\n",
    "            gradient_output= layer._backward(gradient_output)\n",
    "        return gradient_output\n",
    "\n",
    "    def _fit(self,mode='train'):\n",
    "        if mode=='train':\n",
    "            for layer in self.layers:\n",
    "                layer.train=True\n",
    "        elif mode=='eval':\n",
    "            for layer in self.layers:\n",
    "                layer.train=False"
   ],
   "id": "6dc7ad6e3067e7bd",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### AdamW\n",
    "Formula:\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n",
    "\n",
    "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n",
    "\n",
    "$$ \\text{bias correction: } \\ \\   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    "use decoupled weight decay:\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\eta \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}} + \\epsilon}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_t (1 - \\eta \\lambda)\n",
    "$$"
   ],
   "id": "cffbfe5b86a86349"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:47.020857Z",
     "start_time": "2025-03-26T10:15:47.014208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AdamW(object):\n",
    "    '''\n",
    "    the parameters have all the layers W and b\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model, lr=1e-3, decoupled_weight_decay=0, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.decoupled_weight_decay = decoupled_weight_decay\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "        self.m = [np.zeros(p[0].shape) for p in self.get_parameters()]\n",
    "        self.v = [np.zeros(p[0].shape) for p in self.get_parameters()]\n",
    "\n",
    "    def get_parameters(self):\n",
    "        parameters = []\n",
    "        for layer in self.model.layers:\n",
    "            if hasattr(layer, \"W\"):\n",
    "                parameters.append([layer.W, layer.W_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"b\"):\n",
    "                parameters.append([layer.b, layer.b_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"gamma\"):\n",
    "                parameters.append([layer.gamma, layer.gamma_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"beta\"):\n",
    "                parameters.append([layer.beta, layer.beta_grad, layer.skip_decay])\n",
    "        return parameters\n",
    "\n",
    "    def step(self):\n",
    "        parameters = self.get_parameters()  # 动态获取最新的参数\n",
    "        for i, (param_list, m, v) in enumerate(zip(parameters, self.m, self.v)):\n",
    "            param, param_grad, skip_decay = param_list\n",
    "            self.t += 1\n",
    "\n",
    "            # 计算动量和方差\n",
    "            m = self.beta1 * m + (1 - self.beta1) * param_grad\n",
    "            v = self.beta2 * v + (1 - self.beta2) * np.power(param_grad, 2)\n",
    "\n",
    "            # 更新 m 和 v\n",
    "            self.m[i] = m\n",
    "            self.v[i] = v\n",
    "\n",
    "            # 计算偏差修正后的 m 和 v\n",
    "            m_hat = m / (1 - np.power(self.beta1, self.t))\n",
    "            v_hat = v / (1 - np.power(self.beta2, self.t))\n",
    "\n",
    "            # 更新参数\n",
    "            update = self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "            # 如果 skip_decay 为 True, 不应用权重衰减\n",
    "            if not skip_decay:\n",
    "                param -= update\n",
    "                param *= (1 - self.lr * self.decoupled_weight_decay)  # 权重衰减\n",
    "            else:\n",
    "                param -= update  # 仅应用 Adam 更新步骤"
   ],
   "id": "8f0ce4bcccfc5f10",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### SGD with Momentum\n",
    "SGD Formula:\n",
    "$$θ_{t+1}=θ_t−\\eta  \\cdot ∇L(θ_t)$$\n",
    "\n",
    "Momentum 梯度下降的公式如下：\n",
    "$$\n",
    "\\begin{equation}\n",
    "v_t = \\beta v_{t-1} - \\eta \\nabla L(\\theta_t)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t + v_t\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$$\n",
    "    \\( v_t \\) 是当前动量\\\\\n",
    "    \\( \\beta \\) 是动量系数（通常取 0.9）\\\\\n",
    "    \\( \\eta \\) 是学习率\\\\\n",
    "    \\( \\nabla L(\\theta_t) \\) 是损失函数对参数的梯度\n",
    "$$"
   ],
   "id": "d37f07c8cd64eb59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:47.028931Z",
     "start_time": "2025-03-26T10:15:47.022218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGDMomentum:\n",
    "    def __init__(self, model, lr=0.01, momentum=0.9, weight_decay=0.0001):\n",
    "        self.model = model  # 引用模型\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.v = [np.zeros(param[0].shape) for param in self.model.parameters]\n",
    "    def get_parameters(self):\n",
    "        # 直接从模型获取最新的参数\n",
    "        parameters = []\n",
    "        for layer in self.model.layers:\n",
    "            if hasattr(layer, \"W\"):\n",
    "                parameters.append([layer.W, layer.W_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"b\"):\n",
    "                parameters.append([layer.b, layer.b_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"gamma\"):\n",
    "                parameters.append([layer.gamma, layer.gamma_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"beta\"):\n",
    "                parameters.append([layer.beta, layer.beta_grad, layer.skip_decay])\n",
    "        return parameters\n",
    "\n",
    "    def step(self):\n",
    "        # 动态获取最新的模型参数\n",
    "        self.parameters = self.get_parameters()\n",
    "        # 直接从 model 中获取 parameters 和 gradients\n",
    "        for i, (v, param_list) in enumerate(zip(self.v, self.parameters)):\n",
    "            param, param_grad, skip_decay = param_list\n",
    "            if param_grad is not None:\n",
    "                if not skip_decay:\n",
    "                    param -= self.weight_decay * param  # 应用权重衰减\n",
    "                v[:] = self.momentum * v + self.lr * param_grad  # 更新动量\n",
    "                self.v[i] = v\n",
    "                param -= v  # 更新参数"
   ],
   "id": "f414f1bd779f820b",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Normalization\n",
    "核心思想是：在每一层输入神经元的激活值进行归一化（normalization）然后进行缩放和平移，以保持模型的表达能力\n",
    "Forward:\n",
    "$$\\hat x_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "$$\\mathbf{y}=\\gamma\\frac{\\mathbf{x}-E(\\mathbf{x})}{\\sqrt{\\sigma^2_B+\\epsilon}}+\\beta = \\gamma \\hat {\\mathbf{x}}+\\beta$$\n",
    "\n",
    "$E(\\mathbf{x})$ is the mean of the current mini-batch, $\\sigma^2_B$ is the variance of the current mini-batch\n",
    "\n",
    "Backward:\n",
    "\n",
    "m is the number of batch size\n",
    "\n",
    "\n",
    "\n",
    "Gradient of $\\gamma$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\gamma} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial \\gamma} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i} \\hat{x}_i\n",
    ", \\qquad where \\ \\ \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{(\\sigma^2)+\\epsilon}}\n",
    "$$\n",
    "Gradient of $\\beta$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\beta} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial \\beta} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i}=\\frac{1}{m}\\sum_{i=1}^{m} gradient\\_output$$\n",
    "\n",
    "Gradient of $x_i$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\sigma^2} = \\frac{\\partial L}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial \\hat {x_i}} \\cdot \\frac{\\partial \\hat {x_i}}{\\partial \\sigma^2}\\\\=\\frac{\\partial L}{\\partial y_i} \\cdot \\gamma \\cdot -\\frac{1}{2} \\hat{x_i} (\\sigma^2+\\epsilon)^{-1}\\\\$$\n",
    "\n",
    "to simplify the formula let $\\sigma = \\sqrt{\\sigma^2+\\epsilon}$ so the final formula is :\n",
    "$$\\frac{\\partial L}{\\partial \\sigma^2} = -\\frac{\\gamma}{\\sigma} \\sum_{i=1}^{m} \\hat {x_i}\\frac{\\partial L}{\\partial \\gamma} $$\n",
    "$$\\frac{\\partial \\mu}{\\partial x_i}=\\frac{1}{m}$$\n",
    "$$\\frac{\\partial \\sigma^2}{\\partial x_i}=\\frac{2}{m}(x_i-\\mu)$$\n",
    "$$\\frac{\\partial \\hat {x_i}}{\\partial x_i}=\\frac{1}{\\sigma} - \\frac{1}{m}\\sum_{j=1}^{m}\\frac{1}{\\sigma}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\hat{x_i}}= \\gamma \\cdot  gradient\\_output_i$$\n",
    "$$\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial \\hat x_i} \\cdot \\frac{\\partial \\hat x_i}{\\partial x_i} + \\frac{\\partial L}{\\partial \\sigma^2} \\cdot \\frac{\\partial \\sigma^2}{\\partial x_i} + \\frac{\\partial L}{\\partial \\mu} \\cdot \\frac{\\partial \\mu}{\\partial x_i} \\\\=\\frac{\\gamma}{\\sigma} \\left( \\frac{\\partial L}{\\partial y_i} - \\frac{1}{m} \\hat{x}_i \\frac{\\partial L}{\\partial \\gamma} - \\frac{\\partial L}{\\partial \\beta}  \\right)$$"
   ],
   "id": "2eecd3ec6ec4360c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:47.040622Z",
     "start_time": "2025-03-26T10:15:47.035806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BatchNormalization(Layer):\n",
    "    def __init__(self, name, feature_num,skip_decay=True, epsilon=1e-5, requires_grad=True):\n",
    "        super().__init__(name)\n",
    "        self.epsilon = epsilon\n",
    "        self.requires_grad = requires_grad\n",
    "        self.skip_decay = skip_decay\n",
    "        self.gamma = np.ones(feature_num)\n",
    "        self.beta = np.zeros(feature_num)\n",
    "\n",
    "        self.gamma_grad = None\n",
    "        self.beta_grad = None\n",
    "\n",
    "        self.ema = np.zeros(feature_num)\n",
    "        self.emv = np.zeros(feature_num)\n",
    "\n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        x: [batch_size,feature number]\n",
    "        gamma: [feature number]\n",
    "        beta: [feature number]\n",
    "        :param x:\n",
    "        :return:\n",
    "        '''\n",
    "        if self.train:\n",
    "            batch_mean = x.mean(axis=0)\n",
    "            batch_variance = x.var(axis=0)\n",
    "            batch_std = np.sqrt(batch_variance + self.epsilon)\n",
    "            # record exponential moving average and variance to\n",
    "            momentum = 0.9\n",
    "            self.ema = momentum * self.ema + (1 - momentum) * batch_mean\n",
    "            self.emv = momentum * self.emv + (1 - momentum) * batch_variance\n",
    "        else:\n",
    "            batch_mean = self.ema.data\n",
    "            batch_std = np.sqrt(self.emv + self.epsilon)\n",
    "        self.norm = (x - batch_mean) / batch_std\n",
    "        self.gamma_norm = self.gamma / batch_std\n",
    "\n",
    "        return self.gamma * self.norm + self.beta\n",
    "\n",
    "    def _backward(self, gradient_output: np.ndarray) -> np.ndarray:\n",
    "        # make sure that gradient_output is the gradient of next layer, indicating that the gradient of loss about y\n",
    "        batch_size = gradient_output.shape[0]\n",
    "        self.gamma_grad = (gradient_output * self.norm).sum(axis=0) / batch_size\n",
    "        self.beta_grad = gradient_output.sum(axis=0) / batch_size\n",
    "        dLdx = self.gamma_norm * (gradient_output - self.norm * self.gamma_grad - self.beta_grad)\n",
    "        return dLdx"
   ],
   "id": "a471a89cc91ed1b",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:47.046659Z",
     "start_time": "2025-03-26T10:15:47.040622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AverageMeterics(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ],
   "id": "9e82f74a1f63aea1",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:47.076957Z",
     "start_time": "2025-03-26T10:15:47.067384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adam:\n",
    "    pass\n",
    "\n",
    "\n",
    "class CosineLR:\n",
    "    pass\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self,config,model=None,train_loader=None,valid_loader=None):\n",
    "        self.config=config\n",
    "        self.epochs = self.config['epoch']\n",
    "        self.lr=self.config['lr']\n",
    "        self.model=model\n",
    "        self.train_loader=train_loader\n",
    "        self.valid_loader=valid_loader\n",
    "        self.print_freq=self.config['print_freq']\n",
    "        # self.scheduler= self.config['scheduler']\n",
    "        self.train_precision=[]\n",
    "        self.valid_precision=[]\n",
    "        self.train_loss=[]\n",
    "        self.valid_loss=[]\n",
    "        self.criterion=CrossEntropy()\n",
    "        if self.config['optimizer'] == 'sgd':\n",
    "            self.optimizer = SGDMomentum(self.model, self.lr, self.config['momentum'],\n",
    "                                         self.config['weight_decay'])\n",
    "        elif self.config['optimizer'] == 'adamw':\n",
    "            self.optimizer = AdamW(self.model, self.lr, self.config['weight_decay'])\n",
    "        # if self.scheduler == 'cos':\n",
    "        #     self.train_scheduler = CosineLR(self.optimizer, T_max=self.epochs)\n",
    "    @timer\n",
    "    def train(self):\n",
    "        best_accuracy=0\n",
    "        for epoch in range(self.epochs):\n",
    "            print('current lr {:.5e}'.format(self.optimizer.lr))\n",
    "            self.train_per_epoch(epoch)\n",
    "            acc1 = self.validate(epoch)\n",
    "\n",
    "            # remember best prec@1\n",
    "            best_acc1 = max(acc1, best_accuracy)\n",
    "            output_best = 'Best Prec@1: %.3f\\n' % (best_acc1)\n",
    "            print(output_best)\n",
    "    def train_per_epoch(self,epoch):\n",
    "        batch_time=AverageMeterics()\n",
    "        losses=AverageMeterics()\n",
    "        top1=AverageMeterics()        \n",
    "        self.model._fit()\n",
    "        end_time = time.time()\n",
    "        for i,(X,y) in enumerate(self.train_loader):\n",
    "            y_hat=self.model._forward(X)\n",
    "            loss=self.criterion(y_hat,y)\n",
    "            \n",
    "            self.model._backward(self.criterion.grad)\n",
    "            self.optimizer.step()\n",
    "            precision=accuracy(y_hat,y)\n",
    "            losses.update(loss,X.shape[0])\n",
    "            top1.update(precision,X.shape[0])\n",
    "            \n",
    "            batch_time.update(time.time() - end_time)\n",
    "            end_time = time.time()\n",
    "            if (i%self.print_freq ==0) or (i==len(self.train_loader)-1):\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        epoch + 1, i, len(self.train_loader) - 1, batch_time=batch_time,\n",
    "                        loss=losses, top1=top1))\n",
    "        print('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} Loss: {losses.avg:.4f}'.format(epoch=epoch + 1 , flag='train', top1=top1, losses=losses))\n",
    "        self.train_loss.append(losses.avg)\n",
    "        self.train_precision.append(top1.avg)\n",
    "    def validate(self, epoch):\n",
    "        batch_time = AverageMeterics()\n",
    "        losses = AverageMeterics()\n",
    "        top1 = AverageMeterics()\n",
    "\n",
    "        self.model._fit(mode='test')\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (X, y) in enumerate(self.valid_loader):\n",
    "            # compute output\n",
    "            y_hat = self.model._forward(X)\n",
    "            loss = self.criterion(y_hat, y)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            precision = accuracy(y_hat, y)\n",
    "            losses.update(loss, X.shape[0])\n",
    "            top1.update(precision, X.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if (i % self.print_freq == 0) or (i == len(self.valid_loader) - 1):\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Accuracy {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                    i, len(self.valid_loader) - 1, batch_time=batch_time, loss=losses,\n",
    "                    top1=top1))\n",
    "\n",
    "        print('EPOCH: {epoch} {flag} Results: Accuracy {top1.avg:.3f} Loss: {losses.avg:.4f}'.format(epoch=epoch + 1,\n",
    "                                                                                                   flag='val',\n",
    "                                                                                                   top1=top1,\n",
    "                                                                                                   losses=losses))\n",
    "        self.valid_loss.append(losses.avg)\n",
    "        self.valid_precision.append(top1.avg)\n",
    "\n",
    "        return top1.avg"
   ],
   "id": "bf29c48b65013fe3",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:47.083435Z",
     "start_time": "2025-03-26T10:15:47.078185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, name, drop_rate=0.5, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.fix_value = 1 / (1 - self.drop_rate)   # to keep average fixed\n",
    "\n",
    "    def _forward(self, x):\n",
    "        if self.train:\n",
    "            self.mask = np.random.uniform(0, 1, x.shape) > self.drop_rate\n",
    "            return x * self.mask * self.fix_value\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def _backward(self, grad_output):\n",
    "        if self.train:\n",
    "            return grad_output * self.mask\n",
    "        else:\n",
    "            return grad_output"
   ],
   "id": "66ffb78d9831ae51",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:47.088500Z",
     "start_time": "2025-03-26T10:15:47.084255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dataloader(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=True, seed=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.index = np.arange(X.shape[0])\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            if self.seed is not None:\n",
    "                np.random.seed(self.seed)\n",
    "            np.random.shuffle(self.index)\n",
    "        self.n = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.n >= len(self.index):\n",
    "            raise StopIteration\n",
    "\n",
    "        index = self.index[self.n:self.n + self.batch_size]\n",
    "        batch_X = self.X[index]\n",
    "        batch_y = self.y[index]\n",
    "        self.n += self.batch_size\n",
    "\n",
    "        return batch_X, batch_y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            num of batch\n",
    "        \"\"\"\n",
    "        return (len(self.index) + self.batch_size - 1) // self.batch_size  # ceiling"
   ],
   "id": "66f87db9952bdaef",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:47.103420Z",
     "start_time": "2025-03-26T10:15:47.098894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "batch_size=1024\n",
    "config={'lr': 0.01,'batch_size': batch_size,'momentum': 0.9,'weight_decay': 5e-4,'seed': 0,'epoch': 200,\n",
    "    'optimizer': 'adamw',     # adam, sgd\n",
    "    'scheduler': None,      # cos, None\n",
    "    'pre-process': 'standardization',      # min-max, standardization, None\n",
    "    'print_freq': 50000 // batch_size // 5\n",
    "}\n",
    "np.random.seed(config['seed'])"
   ],
   "id": "d76807819d7f07c2",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:47.132736Z",
     "start_time": "2025-03-26T10:15:47.115573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir_path='E:\\\\Postgraduate\\\\25S1\\\\COMP5329\\\\Assignment\\\\Assignment1\\\\Assignment1-Dataset\\\\'\n",
    "train_file='train_data.npy'\n",
    "train_label_file='train_label.npy'\n",
    "train_data=np.load(dir_path+train_file)\n",
    "train_label=np.load(dir_path+train_label_file)\n",
    "test_file='test_data.npy'\n",
    "test_label_file='test_label.npy'"
   ],
   "id": "649ce56cf5d9e69a",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:15:47.135331Z",
     "start_time": "2025-03-26T10:15:47.133326Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b000da5de410dfaa",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-26T10:15:47.135331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_X=pre_processing(train_data,config['pre-process'])\n",
    "train_dataloader=Dataloader(train_X, train_label, config['batch_size'], shuffle=True, seed=config['seed'])\n",
    "test_X=np.load(dir_path+test_file)\n",
    "test_label=np.load(dir_path+test_label_file)\n",
    "test_X=pre_processing(test_X,config['pre-process'])\n",
    "test_dataloader=Dataloader(test_X, test_label, config['batch_size'], shuffle=False, seed=config['seed'])\n",
    "\n",
    "model = MLP_V2()\n",
    "trainer=Trainer(config,model,train_dataloader,test_dataloader)\n",
    "trainer.train()\n"
   ],
   "id": "d376e649804a297d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process: standardization\n",
      "Pre-process: standardization\n",
      "Start time:  Wed Mar 26 21:15:47 2025\n",
      "current lr 1.00000e-02\n",
      "Epoch: [1][0/48]\tTime 0.047 (0.047)\tLoss 12.2224 (12.2224)\tPrec@1 11.426 (11.426)\n",
      "Epoch: [1][9/48]\tTime 0.032 (0.027)\tLoss 6.9798 (7.9511)\tPrec@1 26.855 (25.039)\n",
      "Epoch: [1][18/48]\tTime 0.016 (0.026)\tLoss 4.7496 (6.8890)\tPrec@1 28.906 (26.336)\n",
      "Epoch: [1][27/48]\tTime 0.027 (0.026)\tLoss 2.6217 (5.8386)\tPrec@1 32.324 (27.560)\n",
      "Epoch: [1][36/48]\tTime 0.016 (0.025)\tLoss 1.9379 (4.9444)\tPrec@1 31.543 (27.972)\n",
      "Epoch: [1][45/48]\tTime 0.035 (0.025)\tLoss 1.9493 (4.3600)\tPrec@1 30.762 (28.072)\n",
      "Epoch: [1][48/48]\tTime 0.028 (0.025)\tLoss 1.9407 (4.2170)\tPrec@1 31.486 (28.288)\n",
      "EPOCH: 1 train Results: Prec@1 28.288 Loss: 4.2170\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.8065 (1.8065)\tAccuracy 34.961 (34.961)\n",
      "Test: [9/9]\tTime 0.000 (0.010)\tLoss 1.8088 (1.8244)\tAccuracy 33.673 (33.910)\n",
      "EPOCH: 1 val Results: Accuracy 33.910 Loss: 1.8244\n",
      "Best Prec@1: 33.910\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [2][0/48]\tTime 0.031 (0.031)\tLoss 1.7710 (1.7710)\tPrec@1 36.133 (36.133)\n",
      "Epoch: [2][9/48]\tTime 0.031 (0.025)\tLoss 1.7192 (1.7738)\tPrec@1 37.598 (36.318)\n",
      "Epoch: [2][18/48]\tTime 0.016 (0.026)\tLoss 1.6704 (1.7409)\tPrec@1 39.355 (37.397)\n",
      "Epoch: [2][27/48]\tTime 0.032 (0.026)\tLoss 1.6723 (1.7165)\tPrec@1 39.648 (38.236)\n",
      "Epoch: [2][36/48]\tTime 0.016 (0.025)\tLoss 1.5929 (1.6993)\tPrec@1 42.285 (39.039)\n",
      "Epoch: [2][45/48]\tTime 0.016 (0.025)\tLoss 1.5663 (1.6806)\tPrec@1 42.480 (39.695)\n",
      "Epoch: [2][48/48]\tTime 0.016 (0.025)\tLoss 1.6076 (1.6760)\tPrec@1 41.981 (39.840)\n",
      "EPOCH: 2 train Results: Prec@1 39.840 Loss: 1.6760\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.5843 (1.5843)\tAccuracy 43.555 (43.555)\n",
      "Test: [9/9]\tTime 0.004 (0.012)\tLoss 1.5680 (1.6044)\tAccuracy 45.663 (43.100)\n",
      "EPOCH: 2 val Results: Accuracy 43.100 Loss: 1.6044\n",
      "Best Prec@1: 43.100\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [3][0/48]\tTime 0.031 (0.031)\tLoss 1.5705 (1.5705)\tPrec@1 44.336 (44.336)\n",
      "Epoch: [3][9/48]\tTime 0.032 (0.025)\tLoss 1.5661 (1.5416)\tPrec@1 44.824 (45.361)\n",
      "Epoch: [3][18/48]\tTime 0.031 (0.025)\tLoss 1.4687 (1.5220)\tPrec@1 47.754 (45.652)\n",
      "Epoch: [3][27/48]\tTime 0.031 (0.025)\tLoss 1.5187 (1.5200)\tPrec@1 45.020 (45.961)\n",
      "Epoch: [3][36/48]\tTime 0.031 (0.025)\tLoss 1.5060 (1.5141)\tPrec@1 45.020 (46.102)\n",
      "Epoch: [3][45/48]\tTime 0.032 (0.025)\tLoss 1.4290 (1.5133)\tPrec@1 46.973 (46.111)\n",
      "Epoch: [3][48/48]\tTime 0.020 (0.025)\tLoss 1.4724 (1.5125)\tPrec@1 44.458 (46.090)\n",
      "EPOCH: 3 train Results: Prec@1 46.090 Loss: 1.5125\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.5280 (1.5280)\tAccuracy 44.922 (44.922)\n",
      "Test: [9/9]\tTime 0.010 (0.012)\tLoss 1.5567 (1.5534)\tAccuracy 45.536 (45.330)\n",
      "EPOCH: 3 val Results: Accuracy 45.330 Loss: 1.5534\n",
      "Best Prec@1: 45.330\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [4][0/48]\tTime 0.015 (0.015)\tLoss 1.4268 (1.4268)\tPrec@1 46.777 (46.777)\n",
      "Epoch: [4][9/48]\tTime 0.032 (0.025)\tLoss 1.4223 (1.4348)\tPrec@1 48.438 (48.477)\n",
      "Epoch: [4][18/48]\tTime 0.033 (0.025)\tLoss 1.4394 (1.4340)\tPrec@1 46.582 (48.479)\n",
      "Epoch: [4][27/48]\tTime 0.019 (0.025)\tLoss 1.4424 (1.4355)\tPrec@1 47.852 (48.389)\n",
      "Epoch: [4][36/48]\tTime 0.016 (0.025)\tLoss 1.4826 (1.4389)\tPrec@1 47.266 (48.274)\n",
      "Epoch: [4][45/48]\tTime 0.031 (0.025)\tLoss 1.4523 (1.4447)\tPrec@1 49.414 (48.270)\n",
      "Epoch: [4][48/48]\tTime 0.016 (0.025)\tLoss 1.4612 (1.4463)\tPrec@1 48.821 (48.268)\n",
      "EPOCH: 4 train Results: Prec@1 48.268 Loss: 1.4463\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.4799 (1.4799)\tAccuracy 47.559 (47.559)\n",
      "Test: [9/9]\tTime 0.012 (0.012)\tLoss 1.4983 (1.5093)\tAccuracy 46.173 (46.840)\n",
      "EPOCH: 4 val Results: Accuracy 46.840 Loss: 1.5093\n",
      "Best Prec@1: 46.840\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [5][0/48]\tTime 0.020 (0.020)\tLoss 1.3679 (1.3679)\tPrec@1 51.074 (51.074)\n",
      "Epoch: [5][9/48]\tTime 0.031 (0.025)\tLoss 1.4467 (1.3656)\tPrec@1 49.316 (51.230)\n",
      "Epoch: [5][18/48]\tTime 0.035 (0.025)\tLoss 1.4031 (1.3855)\tPrec@1 50.000 (50.298)\n",
      "Epoch: [5][27/48]\tTime 0.033 (0.025)\tLoss 1.4149 (1.3847)\tPrec@1 49.023 (50.610)\n",
      "Epoch: [5][36/48]\tTime 0.020 (0.025)\tLoss 1.3941 (1.3876)\tPrec@1 50.000 (50.631)\n",
      "Epoch: [5][45/48]\tTime 0.016 (0.025)\tLoss 1.4253 (1.3923)\tPrec@1 46.777 (50.391)\n",
      "Epoch: [5][48/48]\tTime 0.019 (0.024)\tLoss 1.4765 (1.3968)\tPrec@1 46.698 (50.248)\n",
      "EPOCH: 5 train Results: Prec@1 50.248 Loss: 1.3968\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.4565 (1.4565)\tAccuracy 48.242 (48.242)\n",
      "Test: [9/9]\tTime 0.016 (0.013)\tLoss 1.4811 (1.4889)\tAccuracy 48.724 (47.380)\n",
      "EPOCH: 5 val Results: Accuracy 47.380 Loss: 1.4889\n",
      "Best Prec@1: 47.380\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [6][0/48]\tTime 0.016 (0.016)\tLoss 1.3487 (1.3487)\tPrec@1 51.660 (51.660)\n",
      "Epoch: [6][9/48]\tTime 0.031 (0.024)\tLoss 1.3293 (1.3357)\tPrec@1 53.125 (52.139)\n",
      "Epoch: [6][18/48]\tTime 0.024 (0.024)\tLoss 1.3018 (1.3340)\tPrec@1 52.246 (52.138)\n",
      "Epoch: [6][27/48]\tTime 0.017 (0.023)\tLoss 1.3300 (1.3420)\tPrec@1 52.246 (52.082)\n",
      "Epoch: [6][36/48]\tTime 0.015 (0.024)\tLoss 1.4012 (1.3482)\tPrec@1 50.000 (51.813)\n",
      "Epoch: [6][45/48]\tTime 0.016 (0.024)\tLoss 1.3512 (1.3502)\tPrec@1 48.828 (51.654)\n",
      "Epoch: [6][48/48]\tTime 0.032 (0.024)\tLoss 1.3991 (1.3535)\tPrec@1 49.528 (51.540)\n",
      "EPOCH: 6 train Results: Prec@1 51.540 Loss: 1.3535\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.4702 (1.4702)\tAccuracy 48.438 (48.438)\n",
      "Test: [9/9]\tTime 0.009 (0.011)\tLoss 1.4791 (1.4782)\tAccuracy 49.107 (48.140)\n",
      "EPOCH: 6 val Results: Accuracy 48.140 Loss: 1.4782\n",
      "Best Prec@1: 48.140\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [7][0/48]\tTime 0.022 (0.022)\tLoss 1.3168 (1.3168)\tPrec@1 52.246 (52.246)\n",
      "Epoch: [7][9/48]\tTime 0.013 (0.023)\tLoss 1.2966 (1.2924)\tPrec@1 54.590 (53.750)\n",
      "Epoch: [7][18/48]\tTime 0.035 (0.024)\tLoss 1.3382 (1.2955)\tPrec@1 52.539 (53.773)\n",
      "Epoch: [7][27/48]\tTime 0.031 (0.024)\tLoss 1.2917 (1.3012)\tPrec@1 52.734 (53.453)\n",
      "Epoch: [7][36/48]\tTime 0.016 (0.024)\tLoss 1.3400 (1.3070)\tPrec@1 50.586 (53.170)\n",
      "Epoch: [7][45/48]\tTime 0.031 (0.024)\tLoss 1.3098 (1.3095)\tPrec@1 53.125 (53.127)\n",
      "Epoch: [7][48/48]\tTime 0.016 (0.024)\tLoss 1.3865 (1.3132)\tPrec@1 50.354 (53.016)\n",
      "EPOCH: 7 train Results: Prec@1 53.016 Loss: 1.3132\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.4568 (1.4568)\tAccuracy 48.438 (48.438)\n",
      "Test: [9/9]\tTime 0.000 (0.011)\tLoss 1.4677 (1.4805)\tAccuracy 49.745 (48.260)\n",
      "EPOCH: 7 val Results: Accuracy 48.260 Loss: 1.4805\n",
      "Best Prec@1: 48.260\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [8][0/48]\tTime 0.031 (0.031)\tLoss 1.2237 (1.2237)\tPrec@1 57.617 (57.617)\n",
      "Epoch: [8][9/48]\tTime 0.031 (0.025)\tLoss 1.2379 (1.2509)\tPrec@1 55.078 (55.322)\n",
      "Epoch: [8][18/48]\tTime 0.031 (0.025)\tLoss 1.3024 (1.2642)\tPrec@1 54.004 (54.878)\n",
      "Epoch: [8][27/48]\tTime 0.031 (0.025)\tLoss 1.2908 (1.2678)\tPrec@1 53.516 (54.572)\n",
      "Epoch: [8][36/48]\tTime 0.031 (0.025)\tLoss 1.2765 (1.2739)\tPrec@1 53.418 (54.329)\n",
      "Epoch: [8][45/48]\tTime 0.016 (0.025)\tLoss 1.3323 (1.2840)\tPrec@1 49.414 (54.044)\n",
      "Epoch: [8][48/48]\tTime 0.032 (0.025)\tLoss 1.2861 (1.2863)\tPrec@1 55.425 (53.984)\n",
      "EPOCH: 8 train Results: Prec@1 53.984 Loss: 1.2863\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.4343 (1.4343)\tAccuracy 49.023 (49.023)\n",
      "Test: [9/9]\tTime 0.016 (0.011)\tLoss 1.4449 (1.4683)\tAccuracy 48.214 (48.790)\n",
      "EPOCH: 8 val Results: Accuracy 48.790 Loss: 1.4683\n",
      "Best Prec@1: 48.790\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [9][0/48]\tTime 0.031 (0.031)\tLoss 1.2602 (1.2602)\tPrec@1 55.273 (55.273)\n",
      "Epoch: [9][9/48]\tTime 0.016 (0.024)\tLoss 1.2541 (1.2240)\tPrec@1 56.152 (56.309)\n",
      "Epoch: [9][18/48]\tTime 0.016 (0.024)\tLoss 1.2265 (1.2323)\tPrec@1 54.883 (55.916)\n",
      "Epoch: [9][27/48]\tTime 0.016 (0.024)\tLoss 1.2741 (1.2436)\tPrec@1 53.906 (55.399)\n",
      "Epoch: [9][36/48]\tTime 0.031 (0.024)\tLoss 1.2763 (1.2517)\tPrec@1 56.152 (55.189)\n",
      "Epoch: [9][45/48]\tTime 0.024 (0.024)\tLoss 1.3336 (1.2589)\tPrec@1 52.930 (54.974)\n",
      "Epoch: [9][48/48]\tTime 0.032 (0.024)\tLoss 1.3039 (1.2614)\tPrec@1 52.476 (54.856)\n",
      "EPOCH: 9 train Results: Prec@1 54.856 Loss: 1.2614\n",
      "Test: [0/9]\tTime 0.000 (0.000)\tLoss 1.4668 (1.4668)\tAccuracy 49.512 (49.512)\n",
      "Test: [9/9]\tTime 0.017 (0.011)\tLoss 1.4580 (1.4795)\tAccuracy 49.235 (48.860)\n",
      "EPOCH: 9 val Results: Accuracy 48.860 Loss: 1.4795\n",
      "Best Prec@1: 48.860\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [10][0/48]\tTime 0.015 (0.015)\tLoss 1.1295 (1.1295)\tPrec@1 59.961 (59.961)\n",
      "Epoch: [10][9/48]\tTime 0.036 (0.024)\tLoss 1.2056 (1.1941)\tPrec@1 56.641 (56.934)\n",
      "Epoch: [10][18/48]\tTime 0.031 (0.026)\tLoss 1.2472 (1.2131)\tPrec@1 58.008 (56.728)\n",
      "Epoch: [10][27/48]\tTime 0.031 (0.025)\tLoss 1.3236 (1.2269)\tPrec@1 52.246 (56.198)\n",
      "Epoch: [10][36/48]\tTime 0.016 (0.025)\tLoss 1.2569 (1.2302)\tPrec@1 55.566 (56.155)\n",
      "Epoch: [10][45/48]\tTime 0.016 (0.025)\tLoss 1.3570 (1.2375)\tPrec@1 52.539 (55.927)\n",
      "Epoch: [10][48/48]\tTime 0.031 (0.025)\tLoss 1.2635 (1.2390)\tPrec@1 53.892 (55.818)\n",
      "EPOCH: 10 train Results: Prec@1 55.818 Loss: 1.2390\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.4306 (1.4306)\tAccuracy 50.293 (50.293)\n",
      "Test: [9/9]\tTime 0.000 (0.011)\tLoss 1.4552 (1.4619)\tAccuracy 48.469 (49.370)\n",
      "EPOCH: 10 val Results: Accuracy 49.370 Loss: 1.4619\n",
      "Best Prec@1: 49.370\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [11][0/48]\tTime 0.031 (0.031)\tLoss 1.1432 (1.1432)\tPrec@1 59.277 (59.277)\n",
      "Epoch: [11][9/48]\tTime 0.031 (0.025)\tLoss 1.1768 (1.1756)\tPrec@1 56.055 (58.232)\n",
      "Epoch: [11][18/48]\tTime 0.031 (0.025)\tLoss 1.2249 (1.1827)\tPrec@1 57.910 (57.756)\n",
      "Epoch: [11][27/48]\tTime 0.026 (0.025)\tLoss 1.2215 (1.1942)\tPrec@1 57.715 (57.471)\n",
      "Epoch: [11][36/48]\tTime 0.032 (0.024)\tLoss 1.2295 (1.2027)\tPrec@1 56.445 (57.256)\n",
      "Epoch: [11][45/48]\tTime 0.027 (0.024)\tLoss 1.2412 (1.2124)\tPrec@1 56.738 (56.912)\n",
      "Epoch: [11][48/48]\tTime 0.032 (0.024)\tLoss 1.1922 (1.2159)\tPrec@1 57.901 (56.840)\n",
      "EPOCH: 11 train Results: Prec@1 56.840 Loss: 1.2159\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.4441 (1.4441)\tAccuracy 49.609 (49.609)\n",
      "Test: [9/9]\tTime 0.004 (0.011)\tLoss 1.4647 (1.4812)\tAccuracy 49.362 (48.900)\n",
      "EPOCH: 11 val Results: Accuracy 48.900 Loss: 1.4812\n",
      "Best Prec@1: 48.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [12][0/48]\tTime 0.031 (0.031)\tLoss 1.1910 (1.1910)\tPrec@1 56.641 (56.641)\n",
      "Epoch: [12][9/48]\tTime 0.022 (0.025)\tLoss 1.1329 (1.1559)\tPrec@1 59.766 (58.223)\n",
      "Epoch: [12][18/48]\tTime 0.021 (0.024)\tLoss 1.1431 (1.1659)\tPrec@1 58.398 (57.951)\n",
      "Epoch: [12][27/48]\tTime 0.025 (0.024)\tLoss 1.1688 (1.1761)\tPrec@1 57.324 (57.607)\n",
      "Epoch: [12][36/48]\tTime 0.016 (0.024)\tLoss 1.2467 (1.1883)\tPrec@1 56.152 (57.324)\n",
      "Epoch: [12][45/48]\tTime 0.031 (0.024)\tLoss 1.2312 (1.1960)\tPrec@1 56.348 (57.239)\n",
      "Epoch: [12][48/48]\tTime 0.016 (0.024)\tLoss 1.2646 (1.2003)\tPrec@1 53.892 (57.076)\n",
      "EPOCH: 12 train Results: Prec@1 57.076 Loss: 1.2003\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.4337 (1.4337)\tAccuracy 50.000 (50.000)\n",
      "Test: [9/9]\tTime 0.016 (0.013)\tLoss 1.4953 (1.4806)\tAccuracy 47.832 (49.280)\n",
      "EPOCH: 12 val Results: Accuracy 49.280 Loss: 1.4806\n",
      "Best Prec@1: 49.280\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [13][0/48]\tTime 0.016 (0.016)\tLoss 1.1606 (1.1606)\tPrec@1 56.445 (56.445)\n",
      "Epoch: [13][9/48]\tTime 0.026 (0.025)\tLoss 1.1324 (1.1366)\tPrec@1 60.742 (59.463)\n",
      "Epoch: [13][18/48]\tTime 0.031 (0.024)\tLoss 1.1844 (1.1385)\tPrec@1 55.371 (59.226)\n",
      "Epoch: [13][27/48]\tTime 0.023 (0.024)\tLoss 1.1899 (1.1588)\tPrec@1 57.227 (58.650)\n",
      "Epoch: [13][36/48]\tTime 0.031 (0.024)\tLoss 1.1914 (1.1716)\tPrec@1 55.957 (58.026)\n",
      "Epoch: [13][45/48]\tTime 0.025 (0.024)\tLoss 1.2778 (1.1806)\tPrec@1 55.664 (57.706)\n",
      "Epoch: [13][48/48]\tTime 0.018 (0.024)\tLoss 1.1783 (1.1820)\tPrec@1 59.080 (57.708)\n",
      "EPOCH: 13 train Results: Prec@1 57.708 Loss: 1.1820\n",
      "Test: [0/9]\tTime 0.014 (0.014)\tLoss 1.4336 (1.4336)\tAccuracy 50.293 (50.293)\n",
      "Test: [9/9]\tTime 0.009 (0.012)\tLoss 1.4980 (1.4903)\tAccuracy 48.087 (49.320)\n",
      "EPOCH: 13 val Results: Accuracy 49.320 Loss: 1.4903\n",
      "Best Prec@1: 49.320\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [14][0/48]\tTime 0.026 (0.026)\tLoss 1.0854 (1.0854)\tPrec@1 61.230 (61.230)\n",
      "Epoch: [14][9/48]\tTime 0.023 (0.025)\tLoss 1.1503 (1.1229)\tPrec@1 59.082 (60.098)\n",
      "Epoch: [14][18/48]\tTime 0.033 (0.024)\tLoss 1.1685 (1.1296)\tPrec@1 57.812 (59.709)\n",
      "Epoch: [14][27/48]\tTime 0.014 (0.024)\tLoss 1.1951 (1.1461)\tPrec@1 56.934 (59.162)\n",
      "Epoch: [14][36/48]\tTime 0.026 (0.024)\tLoss 1.1938 (1.1518)\tPrec@1 56.738 (59.021)\n",
      "Epoch: [14][45/48]\tTime 0.021 (0.024)\tLoss 1.2379 (1.1637)\tPrec@1 54.004 (58.536)\n",
      "Epoch: [14][48/48]\tTime 0.025 (0.024)\tLoss 1.2346 (1.1663)\tPrec@1 53.774 (58.402)\n",
      "EPOCH: 14 train Results: Prec@1 58.402 Loss: 1.1663\n",
      "Test: [0/9]\tTime 0.000 (0.000)\tLoss 1.4254 (1.4254)\tAccuracy 51.367 (51.367)\n",
      "Test: [9/9]\tTime 0.010 (0.012)\tLoss 1.4755 (1.4719)\tAccuracy 49.107 (49.690)\n",
      "EPOCH: 14 val Results: Accuracy 49.690 Loss: 1.4719\n",
      "Best Prec@1: 49.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [15][0/48]\tTime 0.023 (0.023)\tLoss 1.0804 (1.0804)\tPrec@1 60.742 (60.742)\n",
      "Epoch: [15][9/48]\tTime 0.027 (0.025)\tLoss 1.1384 (1.1049)\tPrec@1 58.789 (60.742)\n",
      "Epoch: [15][18/48]\tTime 0.016 (0.024)\tLoss 1.1820 (1.1171)\tPrec@1 56.934 (60.131)\n",
      "Epoch: [15][27/48]\tTime 0.016 (0.024)\tLoss 1.1707 (1.1299)\tPrec@1 58.398 (59.741)\n",
      "Epoch: [15][36/48]\tTime 0.018 (0.024)\tLoss 1.1323 (1.1448)\tPrec@1 60.352 (59.156)\n",
      "Epoch: [15][45/48]\tTime 0.016 (0.025)\tLoss 1.1896 (1.1520)\tPrec@1 58.203 (58.878)\n",
      "Epoch: [15][48/48]\tTime 0.016 (0.025)\tLoss 1.2092 (1.1545)\tPrec@1 54.363 (58.712)\n",
      "EPOCH: 15 train Results: Prec@1 58.712 Loss: 1.1545\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.4277 (1.4277)\tAccuracy 50.098 (50.098)\n",
      "Test: [9/9]\tTime 0.000 (0.011)\tLoss 1.4710 (1.4834)\tAccuracy 48.980 (49.330)\n",
      "EPOCH: 15 val Results: Accuracy 49.330 Loss: 1.4834\n",
      "Best Prec@1: 49.330\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [16][0/48]\tTime 0.031 (0.031)\tLoss 1.0803 (1.0803)\tPrec@1 62.012 (62.012)\n",
      "Epoch: [16][9/48]\tTime 0.031 (0.025)\tLoss 1.0939 (1.0817)\tPrec@1 60.059 (61.260)\n",
      "Epoch: [16][18/48]\tTime 0.020 (0.025)\tLoss 1.1432 (1.1013)\tPrec@1 59.180 (60.578)\n",
      "Epoch: [16][27/48]\tTime 0.016 (0.024)\tLoss 1.1406 (1.1092)\tPrec@1 59.473 (60.442)\n",
      "Epoch: [16][36/48]\tTime 0.014 (0.024)\tLoss 1.1587 (1.1228)\tPrec@1 58.008 (60.074)\n",
      "Epoch: [16][45/48]\tTime 0.025 (0.025)\tLoss 1.1969 (1.1344)\tPrec@1 58.887 (59.674)\n",
      "Epoch: [16][48/48]\tTime 0.020 (0.025)\tLoss 1.1669 (1.1373)\tPrec@1 59.670 (59.598)\n",
      "EPOCH: 16 train Results: Prec@1 59.598 Loss: 1.1373\n",
      "Test: [0/9]\tTime 0.013 (0.013)\tLoss 1.4269 (1.4269)\tAccuracy 51.367 (51.367)\n",
      "Test: [9/9]\tTime 0.010 (0.011)\tLoss 1.4949 (1.4897)\tAccuracy 50.000 (49.710)\n",
      "EPOCH: 16 val Results: Accuracy 49.710 Loss: 1.4897\n",
      "Best Prec@1: 49.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [17][0/48]\tTime 0.019 (0.019)\tLoss 1.0529 (1.0529)\tPrec@1 62.598 (62.598)\n",
      "Epoch: [17][9/48]\tTime 0.024 (0.024)\tLoss 1.0205 (1.0758)\tPrec@1 64.551 (61.885)\n",
      "Epoch: [17][18/48]\tTime 0.026 (0.025)\tLoss 1.0985 (1.0844)\tPrec@1 61.523 (61.554)\n",
      "Epoch: [17][27/48]\tTime 0.023 (0.025)\tLoss 1.1489 (1.1057)\tPrec@1 58.691 (60.955)\n",
      "Epoch: [17][36/48]\tTime 0.016 (0.025)\tLoss 1.1571 (1.1118)\tPrec@1 58.105 (60.573)\n",
      "Epoch: [17][45/48]\tTime 0.016 (0.025)\tLoss 1.2514 (1.1263)\tPrec@1 54.492 (59.910)\n",
      "Epoch: [17][48/48]\tTime 0.032 (0.025)\tLoss 1.2084 (1.1298)\tPrec@1 58.491 (59.802)\n",
      "EPOCH: 17 train Results: Prec@1 59.802 Loss: 1.1298\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.4191 (1.4191)\tAccuracy 51.367 (51.367)\n",
      "Test: [9/9]\tTime 0.012 (0.012)\tLoss 1.4867 (1.4782)\tAccuracy 49.490 (49.430)\n",
      "EPOCH: 17 val Results: Accuracy 49.430 Loss: 1.4782\n",
      "Best Prec@1: 49.430\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [18][0/48]\tTime 0.032 (0.032)\tLoss 1.0550 (1.0550)\tPrec@1 61.914 (61.914)\n",
      "Epoch: [18][9/48]\tTime 0.026 (0.025)\tLoss 1.1220 (1.0785)\tPrec@1 56.543 (61.338)\n",
      "Epoch: [18][18/48]\tTime 0.016 (0.024)\tLoss 1.1522 (1.0812)\tPrec@1 58.301 (61.133)\n",
      "Epoch: [18][27/48]\tTime 0.024 (0.025)\tLoss 1.1565 (1.1042)\tPrec@1 58.594 (60.638)\n",
      "Epoch: [18][36/48]\tTime 0.021 (0.024)\tLoss 1.0745 (1.1116)\tPrec@1 63.281 (60.246)\n",
      "Epoch: [18][45/48]\tTime 0.033 (0.025)\tLoss 1.1269 (1.1186)\tPrec@1 59.863 (59.933)\n",
      "Epoch: [18][48/48]\tTime 0.013 (0.024)\tLoss 1.1844 (1.1214)\tPrec@1 58.608 (59.798)\n",
      "EPOCH: 18 train Results: Prec@1 59.798 Loss: 1.1214\n",
      "Test: [0/9]\tTime 0.023 (0.023)\tLoss 1.4508 (1.4508)\tAccuracy 51.855 (51.855)\n",
      "Test: [9/9]\tTime 0.018 (0.013)\tLoss 1.5329 (1.4955)\tAccuracy 49.872 (50.370)\n",
      "EPOCH: 18 val Results: Accuracy 50.370 Loss: 1.4955\n",
      "Best Prec@1: 50.370\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [19][0/48]\tTime 0.026 (0.026)\tLoss 1.0468 (1.0468)\tPrec@1 62.891 (62.891)\n",
      "Epoch: [19][9/48]\tTime 0.032 (0.024)\tLoss 1.1099 (1.0492)\tPrec@1 60.156 (63.066)\n",
      "Epoch: [19][18/48]\tTime 0.019 (0.024)\tLoss 1.1132 (1.0679)\tPrec@1 60.254 (62.310)\n",
      "Epoch: [19][27/48]\tTime 0.032 (0.024)\tLoss 1.1032 (1.0816)\tPrec@1 61.133 (61.635)\n",
      "Epoch: [19][36/48]\tTime 0.031 (0.024)\tLoss 1.1926 (1.0990)\tPrec@1 58.008 (61.022)\n",
      "Epoch: [19][45/48]\tTime 0.019 (0.024)\tLoss 1.1432 (1.1073)\tPrec@1 58.789 (60.657)\n",
      "Epoch: [19][48/48]\tTime 0.011 (0.024)\tLoss 1.1943 (1.1091)\tPrec@1 59.198 (60.604)\n",
      "EPOCH: 19 train Results: Prec@1 60.604 Loss: 1.1091\n",
      "Test: [0/9]\tTime 0.017 (0.017)\tLoss 1.4545 (1.4545)\tAccuracy 50.684 (50.684)\n",
      "Test: [9/9]\tTime 0.000 (0.011)\tLoss 1.4966 (1.5010)\tAccuracy 49.617 (49.710)\n",
      "EPOCH: 19 val Results: Accuracy 49.710 Loss: 1.5010\n",
      "Best Prec@1: 49.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [20][0/48]\tTime 0.031 (0.031)\tLoss 1.0687 (1.0687)\tPrec@1 61.914 (61.914)\n",
      "Epoch: [20][9/48]\tTime 0.032 (0.025)\tLoss 1.0255 (1.0545)\tPrec@1 63.184 (62.363)\n",
      "Epoch: [20][18/48]\tTime 0.031 (0.025)\tLoss 1.0550 (1.0647)\tPrec@1 64.062 (62.387)\n",
      "Epoch: [20][27/48]\tTime 0.031 (0.025)\tLoss 1.1884 (1.0803)\tPrec@1 60.059 (61.572)\n",
      "Epoch: [20][36/48]\tTime 0.031 (0.025)\tLoss 1.1330 (1.0897)\tPrec@1 59.082 (61.077)\n",
      "Epoch: [20][45/48]\tTime 0.032 (0.024)\tLoss 1.1247 (1.0981)\tPrec@1 60.449 (60.744)\n",
      "Epoch: [20][48/48]\tTime 0.016 (0.024)\tLoss 1.1706 (1.1000)\tPrec@1 55.071 (60.608)\n",
      "EPOCH: 20 train Results: Prec@1 60.608 Loss: 1.1000\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.4792 (1.4792)\tAccuracy 51.270 (51.270)\n",
      "Test: [9/9]\tTime 0.010 (0.012)\tLoss 1.5018 (1.5065)\tAccuracy 50.000 (50.210)\n",
      "EPOCH: 20 val Results: Accuracy 50.210 Loss: 1.5065\n",
      "Best Prec@1: 50.210\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [21][0/48]\tTime 0.021 (0.021)\tLoss 1.0031 (1.0031)\tPrec@1 63.184 (63.184)\n",
      "Epoch: [21][9/48]\tTime 0.026 (0.025)\tLoss 1.0674 (1.0276)\tPrec@1 62.402 (63.379)\n",
      "Epoch: [21][18/48]\tTime 0.016 (0.024)\tLoss 1.1341 (1.0492)\tPrec@1 60.059 (62.567)\n",
      "Epoch: [21][27/48]\tTime 0.032 (0.024)\tLoss 1.0761 (1.0613)\tPrec@1 60.742 (62.151)\n",
      "Epoch: [21][36/48]\tTime 0.020 (0.024)\tLoss 1.1515 (1.0781)\tPrec@1 58.887 (61.500)\n",
      "Epoch: [21][45/48]\tTime 0.013 (0.024)\tLoss 1.1314 (1.0881)\tPrec@1 59.961 (61.194)\n",
      "Epoch: [21][48/48]\tTime 0.012 (0.024)\tLoss 1.1535 (1.0909)\tPrec@1 58.137 (61.040)\n",
      "EPOCH: 21 train Results: Prec@1 61.040 Loss: 1.0909\n",
      "Test: [0/9]\tTime 0.014 (0.014)\tLoss 1.5294 (1.5294)\tAccuracy 52.344 (52.344)\n",
      "Test: [9/9]\tTime 0.016 (0.012)\tLoss 1.5123 (1.5398)\tAccuracy 48.980 (50.100)\n",
      "EPOCH: 21 val Results: Accuracy 50.100 Loss: 1.5398\n",
      "Best Prec@1: 50.100\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [22][0/48]\tTime 0.016 (0.016)\tLoss 1.0371 (1.0371)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [22][9/48]\tTime 0.020 (0.023)\tLoss 1.0668 (1.0396)\tPrec@1 61.523 (63.281)\n",
      "Epoch: [22][18/48]\tTime 0.016 (0.023)\tLoss 1.0561 (1.0508)\tPrec@1 60.156 (62.855)\n",
      "Epoch: [22][27/48]\tTime 0.031 (0.024)\tLoss 1.0985 (1.0564)\tPrec@1 60.059 (62.486)\n",
      "Epoch: [22][36/48]\tTime 0.033 (0.024)\tLoss 1.1435 (1.0688)\tPrec@1 58.105 (61.851)\n",
      "Epoch: [22][45/48]\tTime 0.016 (0.024)\tLoss 1.1334 (1.0783)\tPrec@1 58.398 (61.492)\n",
      "Epoch: [22][48/48]\tTime 0.016 (0.024)\tLoss 1.1215 (1.0838)\tPrec@1 58.373 (61.272)\n",
      "EPOCH: 22 train Results: Prec@1 61.272 Loss: 1.0838\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.5081 (1.5081)\tAccuracy 50.293 (50.293)\n",
      "Test: [9/9]\tTime 0.000 (0.011)\tLoss 1.5609 (1.5473)\tAccuracy 48.469 (49.350)\n",
      "EPOCH: 22 val Results: Accuracy 49.350 Loss: 1.5473\n",
      "Best Prec@1: 49.350\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [23][0/48]\tTime 0.031 (0.031)\tLoss 0.9907 (0.9907)\tPrec@1 64.746 (64.746)\n",
      "Epoch: [23][9/48]\tTime 0.016 (0.024)\tLoss 0.9875 (1.0129)\tPrec@1 65.332 (63.535)\n",
      "Epoch: [23][18/48]\tTime 0.016 (0.024)\tLoss 1.0827 (1.0377)\tPrec@1 61.914 (62.819)\n",
      "Epoch: [23][27/48]\tTime 0.031 (0.024)\tLoss 1.0629 (1.0470)\tPrec@1 61.230 (62.605)\n",
      "Epoch: [23][36/48]\tTime 0.016 (0.024)\tLoss 1.0591 (1.0631)\tPrec@1 62.793 (62.062)\n",
      "Epoch: [23][45/48]\tTime 0.016 (0.024)\tLoss 1.0670 (1.0755)\tPrec@1 61.719 (61.649)\n",
      "Epoch: [23][48/48]\tTime 0.016 (0.024)\tLoss 1.1661 (1.0804)\tPrec@1 58.608 (61.534)\n",
      "EPOCH: 23 train Results: Prec@1 61.534 Loss: 1.0804\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.4908 (1.4908)\tAccuracy 52.344 (52.344)\n",
      "Test: [9/9]\tTime 0.000 (0.011)\tLoss 1.5684 (1.5539)\tAccuracy 49.235 (49.700)\n",
      "EPOCH: 23 val Results: Accuracy 49.700 Loss: 1.5539\n",
      "Best Prec@1: 49.700\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [24][0/48]\tTime 0.032 (0.032)\tLoss 1.0176 (1.0176)\tPrec@1 62.305 (62.305)\n",
      "Epoch: [24][9/48]\tTime 0.019 (0.024)\tLoss 1.0508 (0.9988)\tPrec@1 60.938 (65.059)\n",
      "Epoch: [24][18/48]\tTime 0.016 (0.024)\tLoss 1.1053 (1.0245)\tPrec@1 61.621 (63.723)\n",
      "Epoch: [24][27/48]\tTime 0.031 (0.024)\tLoss 1.0864 (1.0437)\tPrec@1 61.914 (62.842)\n",
      "Epoch: [24][36/48]\tTime 0.031 (0.024)\tLoss 1.1063 (1.0575)\tPrec@1 58.496 (62.239)\n",
      "Epoch: [24][45/48]\tTime 0.029 (0.024)\tLoss 1.0947 (1.0652)\tPrec@1 60.352 (61.980)\n",
      "Epoch: [24][48/48]\tTime 0.031 (0.024)\tLoss 1.1654 (1.0715)\tPrec@1 56.722 (61.706)\n",
      "EPOCH: 24 train Results: Prec@1 61.706 Loss: 1.0715\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.5336 (1.5336)\tAccuracy 50.098 (50.098)\n",
      "Test: [9/9]\tTime 0.009 (0.012)\tLoss 1.5668 (1.5636)\tAccuracy 48.214 (49.530)\n",
      "EPOCH: 24 val Results: Accuracy 49.530 Loss: 1.5636\n",
      "Best Prec@1: 49.530\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [25][0/48]\tTime 0.023 (0.023)\tLoss 0.9607 (0.9607)\tPrec@1 65.820 (65.820)\n",
      "Epoch: [25][9/48]\tTime 0.024 (0.024)\tLoss 0.9818 (0.9948)\tPrec@1 64.453 (64.248)\n",
      "Epoch: [25][18/48]\tTime 0.032 (0.024)\tLoss 1.0212 (1.0142)\tPrec@1 63.477 (63.841)\n",
      "Epoch: [25][27/48]\tTime 0.031 (0.024)\tLoss 1.0454 (1.0282)\tPrec@1 62.012 (63.243)\n",
      "Epoch: [25][36/48]\tTime 0.031 (0.024)\tLoss 1.0724 (1.0428)\tPrec@1 62.402 (62.827)\n",
      "Epoch: [25][45/48]\tTime 0.031 (0.024)\tLoss 1.1280 (1.0547)\tPrec@1 59.668 (62.375)\n",
      "Epoch: [25][48/48]\tTime 0.016 (0.024)\tLoss 1.1455 (1.0594)\tPrec@1 58.844 (62.182)\n",
      "EPOCH: 25 train Results: Prec@1 62.182 Loss: 1.0594\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.5083 (1.5083)\tAccuracy 49.609 (49.609)\n",
      "Test: [9/9]\tTime 0.016 (0.012)\tLoss 1.5432 (1.5524)\tAccuracy 49.235 (49.370)\n",
      "EPOCH: 25 val Results: Accuracy 49.370 Loss: 1.5524\n",
      "Best Prec@1: 49.370\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [26][0/48]\tTime 0.031 (0.031)\tLoss 1.0118 (1.0118)\tPrec@1 62.988 (62.988)\n",
      "Epoch: [26][9/48]\tTime 0.019 (0.025)\tLoss 1.0423 (1.0077)\tPrec@1 62.012 (63.838)\n",
      "Epoch: [26][18/48]\tTime 0.016 (0.025)\tLoss 1.0477 (1.0208)\tPrec@1 64.258 (63.883)\n",
      "Epoch: [26][27/48]\tTime 0.023 (0.025)\tLoss 1.0346 (1.0293)\tPrec@1 63.184 (63.539)\n",
      "Epoch: [26][36/48]\tTime 0.032 (0.025)\tLoss 1.0645 (1.0408)\tPrec@1 60.840 (62.875)\n",
      "Epoch: [26][45/48]\tTime 0.016 (0.025)\tLoss 1.1493 (1.0505)\tPrec@1 58.887 (62.555)\n",
      "Epoch: [26][48/48]\tTime 0.032 (0.025)\tLoss 1.1195 (1.0540)\tPrec@1 59.906 (62.420)\n",
      "EPOCH: 26 train Results: Prec@1 62.420 Loss: 1.0540\n",
      "Test: [0/9]\tTime 0.013 (0.013)\tLoss 1.5049 (1.5049)\tAccuracy 50.488 (50.488)\n",
      "Test: [9/9]\tTime 0.023 (0.012)\tLoss 1.5669 (1.5496)\tAccuracy 47.704 (49.500)\n",
      "EPOCH: 26 val Results: Accuracy 49.500 Loss: 1.5496\n",
      "Best Prec@1: 49.500\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [27][0/48]\tTime 0.025 (0.025)\tLoss 1.0081 (1.0081)\tPrec@1 63.574 (63.574)\n",
      "Epoch: [27][9/48]\tTime 0.031 (0.025)\tLoss 0.9840 (0.9843)\tPrec@1 63.672 (65.039)\n",
      "Epoch: [27][18/48]\tTime 0.031 (0.025)\tLoss 1.1373 (1.0045)\tPrec@1 61.426 (64.535)\n",
      "Epoch: [27][27/48]\tTime 0.016 (0.024)\tLoss 1.0208 (1.0120)\tPrec@1 62.207 (63.888)\n",
      "Epoch: [27][36/48]\tTime 0.016 (0.024)\tLoss 1.1072 (1.0300)\tPrec@1 62.109 (63.168)\n",
      "Epoch: [27][45/48]\tTime 0.031 (0.024)\tLoss 1.1130 (1.0461)\tPrec@1 61.523 (62.572)\n",
      "Epoch: [27][48/48]\tTime 0.016 (0.024)\tLoss 1.0991 (1.0495)\tPrec@1 58.373 (62.424)\n",
      "EPOCH: 27 train Results: Prec@1 62.424 Loss: 1.0495\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.5429 (1.5429)\tAccuracy 50.684 (50.684)\n",
      "Test: [9/9]\tTime 0.000 (0.011)\tLoss 1.5854 (1.5946)\tAccuracy 49.872 (49.530)\n",
      "EPOCH: 27 val Results: Accuracy 49.530 Loss: 1.5946\n",
      "Best Prec@1: 49.530\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [28][0/48]\tTime 0.031 (0.031)\tLoss 0.9826 (0.9826)\tPrec@1 65.430 (65.430)\n",
      "Epoch: [28][9/48]\tTime 0.016 (0.024)\tLoss 0.9712 (0.9920)\tPrec@1 64.453 (64.258)\n",
      "Epoch: [28][18/48]\tTime 0.016 (0.024)\tLoss 1.0642 (1.0032)\tPrec@1 59.766 (64.032)\n",
      "Epoch: [28][27/48]\tTime 0.020 (0.024)\tLoss 0.9780 (1.0094)\tPrec@1 64.453 (63.860)\n",
      "Epoch: [28][36/48]\tTime 0.031 (0.024)\tLoss 1.1047 (1.0229)\tPrec@1 60.449 (63.379)\n",
      "Epoch: [28][45/48]\tTime 0.027 (0.025)\tLoss 1.1098 (1.0362)\tPrec@1 60.254 (62.876)\n",
      "Epoch: [28][48/48]\tTime 0.016 (0.025)\tLoss 1.1668 (1.0414)\tPrec@1 57.429 (62.718)\n",
      "EPOCH: 28 train Results: Prec@1 62.718 Loss: 1.0414\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.5212 (1.5212)\tAccuracy 51.758 (51.758)\n",
      "Test: [9/9]\tTime 0.016 (0.013)\tLoss 1.5652 (1.5724)\tAccuracy 48.852 (49.260)\n",
      "EPOCH: 28 val Results: Accuracy 49.260 Loss: 1.5724\n",
      "Best Prec@1: 49.260\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [29][0/48]\tTime 0.016 (0.016)\tLoss 0.9283 (0.9283)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [29][9/48]\tTime 0.031 (0.024)\tLoss 0.9585 (0.9771)\tPrec@1 65.527 (65.156)\n",
      "Epoch: [29][18/48]\tTime 0.016 (0.023)\tLoss 1.0247 (0.9939)\tPrec@1 64.258 (64.412)\n",
      "Epoch: [29][27/48]\tTime 0.026 (0.024)\tLoss 0.9991 (1.0062)\tPrec@1 64.551 (64.150)\n",
      "Epoch: [29][36/48]\tTime 0.020 (0.024)\tLoss 1.0446 (1.0254)\tPrec@1 61.719 (63.345)\n",
      "Epoch: [29][45/48]\tTime 0.031 (0.024)\tLoss 1.0562 (1.0361)\tPrec@1 61.523 (62.927)\n",
      "Epoch: [29][48/48]\tTime 0.016 (0.024)\tLoss 1.1129 (1.0398)\tPrec@1 59.788 (62.788)\n",
      "EPOCH: 29 train Results: Prec@1 62.788 Loss: 1.0398\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.5077 (1.5077)\tAccuracy 52.344 (52.344)\n",
      "Test: [9/9]\tTime 0.000 (0.011)\tLoss 1.5833 (1.5874)\tAccuracy 49.490 (49.770)\n",
      "EPOCH: 29 val Results: Accuracy 49.770 Loss: 1.5874\n",
      "Best Prec@1: 49.770\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [30][0/48]\tTime 0.032 (0.032)\tLoss 1.0171 (1.0171)\tPrec@1 62.988 (62.988)\n",
      "Epoch: [30][9/48]\tTime 0.031 (0.026)\tLoss 1.0012 (0.9870)\tPrec@1 65.137 (64.248)\n",
      "Epoch: [30][18/48]\tTime 0.016 (0.025)\tLoss 1.0127 (0.9983)\tPrec@1 65.430 (64.150)\n",
      "Epoch: [30][27/48]\tTime 0.025 (0.025)\tLoss 1.0016 (1.0072)\tPrec@1 62.891 (63.818)\n",
      "Epoch: [30][36/48]\tTime 0.032 (0.025)\tLoss 1.0847 (1.0188)\tPrec@1 61.035 (63.379)\n",
      "Epoch: [30][45/48]\tTime 0.010 (0.025)\tLoss 1.0795 (1.0313)\tPrec@1 61.523 (62.993)\n",
      "Epoch: [30][48/48]\tTime 0.016 (0.025)\tLoss 1.1345 (1.0350)\tPrec@1 60.377 (62.864)\n",
      "EPOCH: 30 train Results: Prec@1 62.864 Loss: 1.0350\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.5577 (1.5577)\tAccuracy 51.465 (51.465)\n",
      "Test: [9/9]\tTime 0.000 (0.011)\tLoss 1.6114 (1.5938)\tAccuracy 47.959 (49.430)\n",
      "EPOCH: 30 val Results: Accuracy 49.430 Loss: 1.5938\n",
      "Best Prec@1: 49.430\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [31][0/48]\tTime 0.038 (0.038)\tLoss 0.9230 (0.9230)\tPrec@1 68.359 (68.359)\n",
      "Epoch: [31][9/48]\tTime 0.032 (0.025)\tLoss 0.9696 (0.9681)\tPrec@1 65.430 (65.869)\n",
      "Epoch: [31][18/48]\tTime 0.016 (0.024)\tLoss 0.9916 (0.9861)\tPrec@1 66.504 (65.296)\n",
      "Epoch: [31][27/48]\tTime 0.031 (0.024)\tLoss 1.0516 (1.0004)\tPrec@1 62.793 (64.655)\n",
      "Epoch: [31][36/48]\tTime 0.031 (0.024)\tLoss 1.0671 (1.0155)\tPrec@1 61.426 (63.875)\n",
      "Epoch: [31][45/48]\tTime 0.032 (0.024)\tLoss 1.1445 (1.0306)\tPrec@1 58.594 (63.215)\n",
      "Epoch: [31][48/48]\tTime 0.016 (0.024)\tLoss 1.1738 (1.0344)\tPrec@1 57.547 (63.080)\n",
      "EPOCH: 31 train Results: Prec@1 63.080 Loss: 1.0344\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.5764 (1.5764)\tAccuracy 50.293 (50.293)\n",
      "Test: [9/9]\tTime 0.012 (0.012)\tLoss 1.6062 (1.6026)\tAccuracy 49.872 (49.320)\n",
      "EPOCH: 31 val Results: Accuracy 49.320 Loss: 1.6026\n",
      "Best Prec@1: 49.320\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [32][0/48]\tTime 0.028 (0.028)\tLoss 0.9417 (0.9417)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [32][9/48]\tTime 0.034 (0.025)\tLoss 0.9779 (0.9768)\tPrec@1 62.695 (64.668)\n",
      "Epoch: [32][18/48]\tTime 0.024 (0.025)\tLoss 0.9599 (0.9895)\tPrec@1 64.941 (64.412)\n",
      "Epoch: [32][27/48]\tTime 0.036 (0.025)\tLoss 1.0243 (0.9965)\tPrec@1 61.816 (64.073)\n",
      "Epoch: [32][36/48]\tTime 0.032 (0.025)\tLoss 0.9784 (1.0112)\tPrec@1 67.090 (63.569)\n",
      "Epoch: [32][45/48]\tTime 0.016 (0.025)\tLoss 1.1034 (1.0249)\tPrec@1 60.352 (63.128)\n",
      "Epoch: [32][48/48]\tTime 0.016 (0.025)\tLoss 1.0961 (1.0289)\tPrec@1 62.736 (63.056)\n",
      "EPOCH: 32 train Results: Prec@1 63.056 Loss: 1.0289\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.5906 (1.5906)\tAccuracy 50.586 (50.586)\n",
      "Test: [9/9]\tTime 0.000 (0.011)\tLoss 1.6099 (1.5994)\tAccuracy 48.724 (49.430)\n",
      "EPOCH: 32 val Results: Accuracy 49.430 Loss: 1.5994\n",
      "Best Prec@1: 49.430\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [33][0/48]\tTime 0.031 (0.031)\tLoss 0.9746 (0.9746)\tPrec@1 62.598 (62.598)\n",
      "Epoch: [33][9/48]\tTime 0.016 (0.024)\tLoss 0.9953 (0.9756)\tPrec@1 63.184 (64.678)\n",
      "Epoch: [33][18/48]\tTime 0.024 (0.024)\tLoss 1.0022 (0.9873)\tPrec@1 65.039 (64.479)\n",
      "Epoch: [33][27/48]\tTime 0.031 (0.024)\tLoss 0.9834 (0.9943)\tPrec@1 63.379 (64.202)\n",
      "Epoch: [33][36/48]\tTime 0.016 (0.024)\tLoss 1.0765 (1.0131)\tPrec@1 62.207 (63.645)\n",
      "Epoch: [33][45/48]\tTime 0.016 (0.024)\tLoss 1.1333 (1.0244)\tPrec@1 58.887 (63.317)\n",
      "Epoch: [33][48/48]\tTime 0.015 (0.024)\tLoss 1.1539 (1.0264)\tPrec@1 58.255 (63.216)\n",
      "EPOCH: 33 train Results: Prec@1 63.216 Loss: 1.0264\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.5684 (1.5684)\tAccuracy 50.879 (50.879)\n",
      "Test: [9/9]\tTime 0.000 (0.012)\tLoss 1.6522 (1.6183)\tAccuracy 49.490 (49.990)\n",
      "EPOCH: 33 val Results: Accuracy 49.990 Loss: 1.6183\n",
      "Best Prec@1: 49.990\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [34][0/48]\tTime 0.031 (0.031)\tLoss 0.9392 (0.9392)\tPrec@1 66.504 (66.504)\n",
      "Epoch: [34][9/48]\tTime 0.019 (0.025)\tLoss 0.9763 (0.9607)\tPrec@1 64.746 (65.957)\n",
      "Epoch: [34][18/48]\tTime 0.025 (0.025)\tLoss 1.0224 (0.9789)\tPrec@1 65.625 (65.291)\n",
      "Epoch: [34][27/48]\tTime 0.031 (0.025)\tLoss 1.0256 (0.9891)\tPrec@1 63.867 (64.718)\n",
      "Epoch: [34][36/48]\tTime 0.023 (0.024)\tLoss 1.1185 (1.0030)\tPrec@1 59.766 (64.258)\n",
      "Epoch: [34][45/48]\tTime 0.026 (0.025)\tLoss 1.1142 (1.0175)\tPrec@1 59.473 (63.655)\n",
      "Epoch: [34][48/48]\tTime 0.018 (0.025)\tLoss 1.0964 (1.0210)\tPrec@1 62.264 (63.544)\n",
      "EPOCH: 34 train Results: Prec@1 63.544 Loss: 1.0210\n",
      "Test: [0/9]\tTime 0.014 (0.014)\tLoss 1.5706 (1.5706)\tAccuracy 51.465 (51.465)\n",
      "Test: [9/9]\tTime 0.013 (0.012)\tLoss 1.5759 (1.6088)\tAccuracy 48.980 (49.300)\n",
      "EPOCH: 34 val Results: Accuracy 49.300 Loss: 1.6088\n",
      "Best Prec@1: 49.300\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [35][0/48]\tTime 0.019 (0.019)\tLoss 0.9286 (0.9286)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [35][9/48]\tTime 0.034 (0.024)\tLoss 0.9470 (0.9597)\tPrec@1 65.039 (65.820)\n",
      "Epoch: [35][18/48]\tTime 0.027 (0.024)\tLoss 0.9805 (0.9711)\tPrec@1 64.062 (65.106)\n",
      "Epoch: [35][27/48]\tTime 0.023 (0.024)\tLoss 1.0404 (0.9895)\tPrec@1 64.258 (64.495)\n",
      "Epoch: [35][36/48]\tTime 0.023 (0.024)\tLoss 1.1399 (1.0026)\tPrec@1 59.082 (64.010)\n",
      "Epoch: [35][45/48]\tTime 0.032 (0.025)\tLoss 1.1629 (1.0175)\tPrec@1 59.180 (63.489)\n",
      "Epoch: [35][48/48]\tTime 0.027 (0.025)\tLoss 1.0318 (1.0206)\tPrec@1 63.915 (63.434)\n",
      "EPOCH: 35 train Results: Prec@1 63.434 Loss: 1.0206\n",
      "Test: [0/9]\tTime 0.000 (0.000)\tLoss 1.5689 (1.5689)\tAccuracy 50.781 (50.781)\n",
      "Test: [9/9]\tTime 0.000 (0.011)\tLoss 1.6080 (1.6183)\tAccuracy 49.617 (48.970)\n",
      "EPOCH: 35 val Results: Accuracy 48.970 Loss: 1.6183\n",
      "Best Prec@1: 48.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [36][0/48]\tTime 0.031 (0.031)\tLoss 0.9559 (0.9559)\tPrec@1 67.578 (67.578)\n",
      "Epoch: [36][9/48]\tTime 0.016 (0.023)\tLoss 0.9898 (0.9638)\tPrec@1 63.379 (65.703)\n",
      "Epoch: [36][18/48]\tTime 0.031 (0.024)\tLoss 1.0122 (0.9777)\tPrec@1 64.941 (65.224)\n",
      "Epoch: [36][27/48]\tTime 0.027 (0.024)\tLoss 1.0568 (0.9875)\tPrec@1 61.426 (64.648)\n",
      "Epoch: [36][36/48]\tTime 0.016 (0.024)\tLoss 1.0433 (1.0004)\tPrec@1 63.867 (64.134)\n",
      "Epoch: [36][45/48]\tTime 0.039 (0.024)\tLoss 1.0946 (1.0161)\tPrec@1 61.523 (63.615)\n",
      "Epoch: [36][48/48]\tTime 0.016 (0.024)\tLoss 1.1091 (1.0189)\tPrec@1 60.024 (63.474)\n",
      "EPOCH: 36 train Results: Prec@1 63.474 Loss: 1.0189\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.5915 (1.5915)\tAccuracy 49.121 (49.121)\n",
      "Test: [9/9]\tTime 0.016 (0.013)\tLoss 1.6174 (1.6075)\tAccuracy 49.617 (48.920)\n",
      "EPOCH: 36 val Results: Accuracy 48.920 Loss: 1.6075\n",
      "Best Prec@1: 48.920\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [37][0/48]\tTime 0.016 (0.016)\tLoss 0.9111 (0.9111)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [37][9/48]\tTime 0.031 (0.023)\tLoss 0.9648 (0.9459)\tPrec@1 63.477 (66.182)\n",
      "Epoch: [37][18/48]\tTime 0.016 (0.023)\tLoss 1.0163 (0.9624)\tPrec@1 65.039 (65.856)\n",
      "Epoch: [37][27/48]\tTime 0.031 (0.024)\tLoss 1.0426 (0.9759)\tPrec@1 63.770 (65.224)\n",
      "Epoch: [37][36/48]\tTime 0.016 (0.023)\tLoss 1.0304 (0.9938)\tPrec@1 63.184 (64.535)\n",
      "Epoch: [37][45/48]\tTime 0.035 (0.024)\tLoss 1.0416 (1.0046)\tPrec@1 61.621 (64.152)\n",
      "Epoch: [37][48/48]\tTime 0.016 (0.023)\tLoss 1.1119 (1.0082)\tPrec@1 60.849 (64.028)\n",
      "EPOCH: 37 train Results: Prec@1 64.028 Loss: 1.0082\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.6187 (1.6187)\tAccuracy 52.637 (52.637)\n",
      "Test: [9/9]\tTime 0.016 (0.013)\tLoss 1.6691 (1.6650)\tAccuracy 49.362 (49.100)\n",
      "EPOCH: 37 val Results: Accuracy 49.100 Loss: 1.6650\n",
      "Best Prec@1: 49.100\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [38][0/48]\tTime 0.031 (0.031)\tLoss 0.9495 (0.9495)\tPrec@1 65.820 (65.820)\n",
      "Epoch: [38][9/48]\tTime 0.031 (0.027)\tLoss 0.9840 (0.9579)\tPrec@1 63.770 (65.381)\n",
      "Epoch: [38][18/48]\tTime 0.032 (0.029)\tLoss 0.9358 (0.9679)\tPrec@1 67.676 (65.193)\n",
      "Epoch: [38][27/48]\tTime 0.032 (0.028)\tLoss 0.9671 (0.9814)\tPrec@1 64.551 (64.617)\n",
      "Epoch: [38][36/48]\tTime 0.047 (0.029)\tLoss 1.0166 (0.9918)\tPrec@1 62.891 (64.237)\n",
      "Epoch: [38][45/48]\tTime 0.032 (0.031)\tLoss 1.1123 (1.0068)\tPrec@1 60.059 (63.706)\n",
      "Epoch: [38][48/48]\tTime 0.033 (0.030)\tLoss 1.0631 (1.0107)\tPrec@1 62.382 (63.592)\n",
      "EPOCH: 38 train Results: Prec@1 63.592 Loss: 1.0107\n",
      "Test: [0/9]\tTime 0.015 (0.015)\tLoss 1.6105 (1.6105)\tAccuracy 50.195 (50.195)\n",
      "Test: [9/9]\tTime 0.014 (0.016)\tLoss 1.6547 (1.6420)\tAccuracy 48.980 (48.750)\n",
      "EPOCH: 38 val Results: Accuracy 48.750 Loss: 1.6420\n",
      "Best Prec@1: 48.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [39][0/48]\tTime 0.047 (0.047)\tLoss 0.9538 (0.9538)\tPrec@1 66.211 (66.211)\n",
      "Epoch: [39][9/48]\tTime 0.026 (0.029)\tLoss 0.9647 (0.9543)\tPrec@1 65.430 (66.074)\n",
      "Epoch: [39][18/48]\tTime 0.035 (0.033)\tLoss 1.0574 (0.9664)\tPrec@1 61.328 (65.234)\n",
      "Epoch: [39][27/48]\tTime 0.031 (0.032)\tLoss 1.0516 (0.9824)\tPrec@1 62.598 (64.607)\n",
      "Epoch: [39][36/48]\tTime 0.043 (0.032)\tLoss 1.0120 (0.9884)\tPrec@1 63.477 (64.414)\n",
      "Epoch: [39][45/48]\tTime 0.016 (0.031)\tLoss 1.0798 (1.0011)\tPrec@1 60.449 (63.973)\n",
      "Epoch: [39][48/48]\tTime 0.031 (0.031)\tLoss 1.0980 (1.0063)\tPrec@1 60.731 (63.808)\n",
      "EPOCH: 39 train Results: Prec@1 63.808 Loss: 1.0063\n",
      "Test: [0/9]\tTime 0.013 (0.013)\tLoss 1.6189 (1.6189)\tAccuracy 50.195 (50.195)\n",
      "Test: [9/9]\tTime 0.016 (0.014)\tLoss 1.6722 (1.6365)\tAccuracy 48.724 (49.310)\n",
      "EPOCH: 39 val Results: Accuracy 49.310 Loss: 1.6365\n",
      "Best Prec@1: 49.310\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [40][0/48]\tTime 0.045 (0.045)\tLoss 0.9346 (0.9346)\tPrec@1 66.504 (66.504)\n",
      "Epoch: [40][9/48]\tTime 0.034 (0.033)\tLoss 0.9677 (0.9483)\tPrec@1 64.062 (66.191)\n",
      "Epoch: [40][18/48]\tTime 0.029 (0.032)\tLoss 1.0154 (0.9542)\tPrec@1 62.891 (65.800)\n",
      "Epoch: [40][27/48]\tTime 0.028 (0.031)\tLoss 1.0114 (0.9606)\tPrec@1 64.258 (65.604)\n",
      "Epoch: [40][36/48]\tTime 0.030 (0.030)\tLoss 1.0335 (0.9772)\tPrec@1 62.695 (64.981)\n",
      "Epoch: [40][45/48]\tTime 0.016 (0.029)\tLoss 1.0655 (0.9965)\tPrec@1 61.621 (64.249)\n",
      "Epoch: [40][48/48]\tTime 0.031 (0.029)\tLoss 1.0574 (1.0006)\tPrec@1 62.736 (64.128)\n",
      "EPOCH: 40 train Results: Prec@1 64.128 Loss: 1.0006\n",
      "Test: [0/9]\tTime 0.000 (0.000)\tLoss 1.6230 (1.6230)\tAccuracy 49.609 (49.609)\n",
      "Test: [9/9]\tTime 0.016 (0.011)\tLoss 1.6745 (1.6628)\tAccuracy 48.597 (49.150)\n",
      "EPOCH: 40 val Results: Accuracy 49.150 Loss: 1.6628\n",
      "Best Prec@1: 49.150\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [41][0/48]\tTime 0.031 (0.031)\tLoss 0.9611 (0.9611)\tPrec@1 65.137 (65.137)\n",
      "Epoch: [41][9/48]\tTime 0.031 (0.025)\tLoss 0.9333 (0.9353)\tPrec@1 67.773 (66.328)\n",
      "Epoch: [41][18/48]\tTime 0.027 (0.025)\tLoss 1.0755 (0.9574)\tPrec@1 61.621 (65.527)\n",
      "Epoch: [41][27/48]\tTime 0.013 (0.025)\tLoss 0.9384 (0.9723)\tPrec@1 66.211 (65.091)\n",
      "Epoch: [41][36/48]\tTime 0.031 (0.025)\tLoss 0.9812 (0.9821)\tPrec@1 63.965 (64.741)\n",
      "Epoch: [41][45/48]\tTime 0.025 (0.025)\tLoss 1.0649 (0.9928)\tPrec@1 61.523 (64.305)\n",
      "Epoch: [41][48/48]\tTime 0.023 (0.025)\tLoss 1.1017 (0.9977)\tPrec@1 61.321 (64.164)\n",
      "EPOCH: 41 train Results: Prec@1 64.164 Loss: 0.9977\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.6115 (1.6115)\tAccuracy 50.781 (50.781)\n",
      "Test: [9/9]\tTime 0.010 (0.012)\tLoss 1.6458 (1.6453)\tAccuracy 48.597 (49.530)\n",
      "EPOCH: 41 val Results: Accuracy 49.530 Loss: 1.6453\n",
      "Best Prec@1: 49.530\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [42][0/48]\tTime 0.026 (0.026)\tLoss 0.9201 (0.9201)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [42][9/48]\tTime 0.026 (0.026)\tLoss 0.9638 (0.9435)\tPrec@1 63.770 (66.123)\n",
      "Epoch: [42][18/48]\tTime 0.032 (0.028)\tLoss 0.9472 (0.9496)\tPrec@1 65.723 (65.861)\n",
      "Epoch: [42][27/48]\tTime 0.030 (0.029)\tLoss 0.9857 (0.9638)\tPrec@1 62.988 (65.318)\n",
      "Epoch: [42][36/48]\tTime 0.031 (0.030)\tLoss 1.0473 (0.9775)\tPrec@1 60.742 (64.762)\n",
      "Epoch: [42][45/48]\tTime 0.032 (0.030)\tLoss 1.0523 (0.9946)\tPrec@1 62.402 (64.173)\n",
      "Epoch: [42][48/48]\tTime 0.026 (0.030)\tLoss 1.0480 (0.9989)\tPrec@1 63.208 (64.058)\n",
      "EPOCH: 42 train Results: Prec@1 64.058 Loss: 0.9989\n",
      "Test: [0/9]\tTime 0.014 (0.014)\tLoss 1.6104 (1.6104)\tAccuracy 50.293 (50.293)\n",
      "Test: [9/9]\tTime 0.013 (0.015)\tLoss 1.6559 (1.6583)\tAccuracy 48.980 (48.520)\n",
      "EPOCH: 42 val Results: Accuracy 48.520 Loss: 1.6583\n",
      "Best Prec@1: 48.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [43][0/48]\tTime 0.035 (0.035)\tLoss 0.9090 (0.9090)\tPrec@1 66.602 (66.602)\n",
      "Epoch: [43][9/48]\tTime 0.036 (0.036)\tLoss 0.9378 (0.9375)\tPrec@1 67.188 (66.455)\n",
      "Epoch: [43][18/48]\tTime 0.029 (0.034)\tLoss 0.9351 (0.9592)\tPrec@1 66.406 (65.450)\n",
      "Epoch: [43][27/48]\tTime 0.027 (0.033)\tLoss 0.9697 (0.9689)\tPrec@1 65.039 (65.077)\n",
      "Epoch: [43][36/48]\tTime 0.028 (0.032)\tLoss 1.0471 (0.9876)\tPrec@1 61.035 (64.398)\n",
      "Epoch: [43][45/48]\tTime 0.027 (0.031)\tLoss 1.0839 (1.0019)\tPrec@1 60.840 (63.842)\n",
      "Epoch: [43][48/48]\tTime 0.024 (0.031)\tLoss 1.0178 (1.0049)\tPrec@1 61.910 (63.798)\n",
      "EPOCH: 43 train Results: Prec@1 63.798 Loss: 1.0049\n",
      "Test: [0/9]\tTime 0.015 (0.015)\tLoss 1.6190 (1.6190)\tAccuracy 50.977 (50.977)\n",
      "Test: [9/9]\tTime 0.011 (0.013)\tLoss 1.6880 (1.6578)\tAccuracy 47.066 (49.090)\n",
      "EPOCH: 43 val Results: Accuracy 49.090 Loss: 1.6578\n",
      "Best Prec@1: 49.090\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [44][0/48]\tTime 0.026 (0.026)\tLoss 0.9645 (0.9645)\tPrec@1 66.309 (66.309)\n",
      "Epoch: [44][9/48]\tTime 0.029 (0.027)\tLoss 0.8950 (0.9472)\tPrec@1 66.797 (66.270)\n",
      "Epoch: [44][18/48]\tTime 0.026 (0.027)\tLoss 1.0427 (0.9616)\tPrec@1 62.402 (65.666)\n",
      "Epoch: [44][27/48]\tTime 0.026 (0.027)\tLoss 0.9928 (0.9747)\tPrec@1 63.867 (65.060)\n",
      "Epoch: [44][36/48]\tTime 0.029 (0.027)\tLoss 0.9398 (0.9859)\tPrec@1 66.113 (64.501)\n",
      "Epoch: [44][45/48]\tTime 0.028 (0.027)\tLoss 1.0726 (0.9978)\tPrec@1 61.914 (64.139)\n",
      "Epoch: [44][48/48]\tTime 0.023 (0.027)\tLoss 1.0611 (1.0005)\tPrec@1 62.264 (64.088)\n",
      "EPOCH: 44 train Results: Prec@1 64.088 Loss: 1.0005\n",
      "Test: [0/9]\tTime 0.013 (0.013)\tLoss 1.6385 (1.6385)\tAccuracy 50.391 (50.391)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "fa5126bd65ef1423",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
