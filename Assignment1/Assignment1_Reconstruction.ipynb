{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.546086Z",
     "start_time": "2025-03-28T05:03:11.542677Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import time\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.625321Z",
     "start_time": "2025-03-28T05:03:11.621910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('Start time: ', time.ctime())\n",
    "        start_time = time.time()  # start time\n",
    "\n",
    "        result = func(*args, **kwargs)  # run\n",
    "\n",
    "        end_time = time.time()  # end time\n",
    "        print('End time: ', time.ctime())\n",
    "        print(f\"{func.__name__} executed in {(end_time - start_time):.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ],
   "id": "98e36a535e7c2b42",
   "outputs": [],
   "execution_count": 164
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pre-process\n",
    "\n",
    "Min-max normalization:\n",
    "\n",
    "$$x_{min-max} = {{x-min(x)}\\over{max(x)-min(x)}}$$\n",
    "\n",
    "Standardization:\n",
    "\n",
    "$$x_{norm} = {{x-\\mu}\\over{\\sigma}}$$"
   ],
   "id": "1ea172e08505f1e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.630439Z",
     "start_time": "2025-03-28T05:03:11.626322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pre_processing(X, mode=None):\n",
    "    if mode == 'min-max':\n",
    "        print('Pre-process: min-max normalization')\n",
    "        min_each_feature = np.min(X, axis=0)\n",
    "        max_each_feature = np.max(X, axis=0)\n",
    "        scale = max_each_feature - min_each_feature\n",
    "        scale[scale == 0] = 1   # To avoid divided by 0\n",
    "        scaled_train = (X - min_each_feature) / scale\n",
    "        return scaled_train\n",
    "\n",
    "    if mode == 'standardization':\n",
    "        print('Pre-process: standardization')\n",
    "        std_each_feature = np.std(X, axis=0)\n",
    "        mean_each_feature = np.mean(X, axis=0)\n",
    "        std_each_feature[std_each_feature == 0] = 1     # To avoid divided by 0\n",
    "        norm_train = (X - mean_each_feature) / std_each_feature\n",
    "        norm_test = (X - mean_each_feature) / std_each_feature\n",
    "        return norm_train\n",
    "\n",
    "    print('No pre-process')\n",
    "\n",
    "    return X"
   ],
   "id": "2578bdbe5f4e1107",
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.637100Z",
     "start_time": "2025-03-28T05:03:11.634433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def accuracy(y_hat,y):\n",
    "    '''\n",
    "    y_hat : predicted value\n",
    "    :param y_hat: [batch_size,num_of_class]\n",
    "    :param y: [batch_size,1\n",
    "    :return: \n",
    "    '''\n",
    "    preds=y_hat.argmax(axis=1,keepdims=True)\n",
    "    return np.mean(preds == y)*100"
   ],
   "id": "5f6ecbfb8386710",
   "outputs": [],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.651076Z",
     "start_time": "2025-03-28T05:03:11.644188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_gain(nonlinearity, param=None):\n",
    "    gains = {\n",
    "        'sigmoid': 1.0,\n",
    "        'tanh': 5.0 / 3,\n",
    "        'relu': math.sqrt(2.0),\n",
    "        'selu': 3.0 / 4\n",
    "    }\n",
    "    \n",
    "    if nonlinearity in gains:\n",
    "        return gains[nonlinearity]\n",
    "    \n",
    "    if nonlinearity == 'leaky_relu':\n",
    "        negative_slope = param if isinstance(param, (int, float)) and not isinstance(param, bool) else 0.01\n",
    "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
    "    \n",
    "    raise ValueError(f\"Unsupported nonlinearity: {nonlinearity}\")\n",
    "\n",
    "def calculate_fan(array):\n",
    "    if array.ndim < 2:\n",
    "        raise ValueError(\"Fan in and fan out require at least 2D tensors\")\n",
    "    \n",
    "    fan_in = array.shape[1] * np.prod(array.shape[2:]) if array.ndim > 2 else array.shape[1]\n",
    "    fan_out = array.shape[0] * np.prod(array.shape[2:]) if array.ndim > 2 else array.shape[0]\n",
    "    \n",
    "    return fan_in, fan_out\n",
    "\n",
    "def get_correct_fan(array, mode):\n",
    "    mode = mode.lower()\n",
    "    if mode not in {'fan_in', 'fan_out'}:\n",
    "        raise ValueError(\"Mode must be 'fan_in' or 'fan_out'\")\n",
    "    \n",
    "    fan_in, fan_out = calculate_fan(array)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "def kaiming_normal(array: np.ndarray, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'relu'):\n",
    "    fan = get_correct_fan(array, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    return np.random.normal(0, std, array.shape)"
   ],
   "id": "ac167cc0d89add8",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.656919Z",
     "start_time": "2025-03-28T05:03:11.652879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        self.name = name\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "    def _forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    def _backward(self, *args):\n",
    "        pass"
   ],
   "id": "767328eb1e97167e",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.661280Z",
     "start_time": "2025-03-28T05:03:11.657927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _backward(self, gradient_output):\n",
    "        gradient_output[self.x <= 0] = 0\n",
    "        return gradient_output"
   ],
   "id": "8c6d47201896a516",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Forward:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{xW} + \\mathbf{b}$$\n",
    "\n",
    "Backward:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{W}^T$$\n",
    "\n",
    "Gradient of W:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{x}^T$$\n",
    "\n",
    "Gradient of b:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{b}} = \\frac{\\partial L}{\\partial \\mathbf{y}} $$\n",
    "\n",
    "Gradient of x:\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{W}^T$$"
   ],
   "id": "6d64cedcd4ee0dc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.667997Z",
     "start_time": "2025-03-28T05:03:11.662278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FCLayer(Layer):\n",
    "    def __init__(self, name: str, n_in: int, n_out: int, skip_decay=False) -> None:\n",
    "        '''\n",
    "        Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,)\n",
    "        :param n_in: dimensionality of input\n",
    "        :param n_out: number of hidden units\n",
    "        '''\n",
    "        super().__init__(name, requires_grad=True)\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        W = kaiming_normal(np.array([0] * n_in * n_out).reshape(n_in, n_out), a=math.sqrt(5))\n",
    "        self.W = W\n",
    "        self.b = np.zeros(self.n_out)\n",
    "        self.W_grad = None\n",
    "        self.b_grad = None\n",
    "        self.skip_decay = skip_decay\n",
    "\n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "            x: [batch size, n_in]\n",
    "            W: [n_in, n_out]\n",
    "            b: [n_out]\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        temp = x @ self.W + self.b\n",
    "        # [batch_size,n_in] @ [n_in,n_out] + [n_output] => [batch_size,n_out]\n",
    "        # return x @ self.W + self.b\n",
    "        return temp\n",
    "\n",
    "    def _backward(self, delta: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        delta: the gradient of the loss function respect to this layer's output 这层损失函数对于这层输出的梯度\n",
    "        :param delta: [batch size, n_out]:\n",
    "        :return:\n",
    "        '''\n",
    "        batch_size = delta.shape[0]\n",
    "        self.W_grad = self.x.T @ delta / batch_size  # [batch_size,n_in]^T @ [batch size, n_out] => [n_in,n_out]\n",
    "        self.b_grad = delta.sum(axis=0) / batch_size  # divide by batch size to get average of gradient\n",
    "        return delta @ self.W.T  # return the gradient of input(x) back to last layer"
   ],
   "id": "e5b00fb819887fa6",
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Softmax \n",
    "Formula:\n",
    "$$softmax(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}$$\n",
    "\n",
    "不需要计算 softmax 的完整 Jacobian 矩阵，因为与交叉熵结合后公式极大简化了。\n",
    "只需用 preds - ground_truth 作为梯度，这个计算在 CrossEntropyLoss 里完成了：\n",
    "self.grad = preds - ground_truth\n",
    "因此，在 softmax.backward(gradient_output) 时，不需要额外计算 softmax 的梯度，而是直接返回 gradient_output\n"
   ],
   "id": "86a2cacdcffb889f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.683516Z",
     "start_time": "2025-03-28T05:03:11.678204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self,name,requires_grad=False):\n",
    "        super().__init__(name,requires_grad)\n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        x_exp =  np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return x_exp/x_exp.sum(axis=1, keepdims=True)\n",
    "    def _backward(self, gradient_output: np.ndarray) -> np.ndarray:\n",
    "        return gradient_output"
   ],
   "id": "2883c1fd2bf68ed0",
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loss Function - Cross Entropy\n",
    "Formula:\n",
    "$$CrossEntropy= - \\sum_{i=1}^{n} y_i log(\\hat {y_i})$$\n",
    "\n",
    "Gradient of softmax:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_k} = \\sum_{i}^{c} \\left( \\frac{\\partial L}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_k} \\right)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y}_i} = - \\frac{y_i}{\\hat{y}_i}, \\qquad \\frac{\\partial \\hat{y}_i}{\\partial z_k} = \\begin{cases}\n",
    "\\hat{y}_i(1 - \\hat{y}_i) & \\text{if } i = k \\\\\n",
    "-\\hat{y}_k\\hat{y}_i & \\text{if } i \\neq k\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_k} = - \\left( (y_k(1 - \\hat{y}_k)) - \\sum_{i \\neq k}^{c} y_i \\hat{y}_k \\right) = -(y_k - \\hat{y}_k \\sum_{i}^{c} y_i) = \\hat{y}_k - y_k\n",
    "$$\n",
    "\n",
    "$$=> \\frac{\\partial L}{\\partial z} = \\hat{y} - y$$\n"
   ],
   "id": "8860ec1e183838c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.707713Z",
     "start_time": "2025-03-28T05:03:11.703008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossEntropy(object):\n",
    "    def __init__(self):\n",
    "        self.softmax = Softmax('softmax')\n",
    "\n",
    "    def __call__(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "\n",
    "        :param x:\n",
    "        :param y: [batch_size, 1]\n",
    "        :return:\n",
    "        '''\n",
    "        self.batch_size = x.shape[0]\n",
    "        self.class_num = x.shape[1]\n",
    "        y_hat = self.softmax._forward(x) #[batch_size,num_class]\n",
    "        y=self.one_hot_encoding(y)\n",
    "        self.grad = y_hat - y\n",
    "        loss = -1 * (y * np.log(y_hat + 1e-8)).sum() / self.batch_size  # to avoid divided by 0\n",
    "        return loss\n",
    "\n",
    "    def one_hot_encoding(self, x):\n",
    "        one_hot_encoded = np.zeros((self.batch_size, self.class_num))\n",
    "        one_hot_encoded[np.arange(x.shape[0]), x.flatten()] = 1\n",
    "        return one_hot_encoded"
   ],
   "id": "bdb959962b6db01c",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.747594Z",
     "start_time": "2025-03-28T05:03:11.739966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP():\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            FCLayer('fc1', n_in=128, n_out=256),\n",
    "            # Dropout('dropout1', 0.6),\n",
    "            # ReLU('relu1'),\n",
    "            # FCLayer('fc2', n_in=512, n_out=256),\n",
    "            # Dropout('dropout2', 0.4),\n",
    "            BatchNormalization(\"batchnorm1\",feature_num=256),\n",
    "            \n",
    "            Dropout('dropout1',drop_rate=0.3),\n",
    "            ReLU('relu1'),\n",
    "            # ReLU('relu2'),\n",
    "            FCLayer('fc2', n_in=256, n_out=128),\n",
    "            BatchNormalization(\"batchnorm2\",feature_num=128),\n",
    "            Dropout('dropout2',drop_rate=0.3),\n",
    "            ReLU('relu2'),\n",
    "            \n",
    "            FCLayer('fc3', n_in=128, n_out=10)\n",
    "        ]\n",
    "        self.parameters = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, \"W\"):\n",
    "                # 将每个参数及其梯度、skip_decay 作为引用添加到 parameters 中\n",
    "                self.parameters.append([layer.W, layer.W_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"b\"):\n",
    "                self.parameters.append([layer.b, layer.b_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"gamma\"):\n",
    "                self.parameters.append([layer.gamma, layer.gamma_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"beta\"):\n",
    "                self.parameters.append([layer.beta, layer.beta_grad, layer.skip_decay])\n",
    "\n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        for layer in self.layers:\n",
    "            x= layer._forward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _backward(self, gradient_output: np.ndarray) -> np.ndarray:\n",
    "        for layer in self.layers[::-1]:\n",
    "            gradient_output= layer._backward(gradient_output)\n",
    "        return gradient_output\n",
    "\n",
    "    def _fit(self,mode='train'):\n",
    "        if mode=='train':\n",
    "            for layer in self.layers:\n",
    "                layer.train=True\n",
    "        elif mode=='eval':\n",
    "            for layer in self.layers:\n",
    "                layer.train=False\n",
    "    def _predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        self._fit('eval')\n",
    "        y_hat = self._forward(x)\n",
    "        return y_hat"
   ],
   "id": "6dc7ad6e3067e7bd",
   "outputs": [],
   "execution_count": 173
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### AdamW\n",
    "Formula:\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n",
    "\n",
    "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n",
    "\n",
    "$$ \\text{bias correction: } \\ \\   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    "use decoupled weight decay:\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\eta \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}} + \\epsilon}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_t (1 - \\eta \\lambda)\n",
    "$$"
   ],
   "id": "cffbfe5b86a86349"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.762317Z",
     "start_time": "2025-03-28T05:03:11.756261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AdamW(object):\n",
    "    '''\n",
    "    the parameters have all the layers W and b\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model, lr=1e-3, decoupled_weight_decay=0, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.decoupled_weight_decay = decoupled_weight_decay\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "        self.m = [np.zeros(p[0].shape) for p in self.get_parameters()]\n",
    "        self.v = [np.zeros(p[0].shape) for p in self.get_parameters()]\n",
    "\n",
    "    def get_parameters(self):\n",
    "        parameters = []\n",
    "        for layer in self.model.layers:\n",
    "            if hasattr(layer, \"W\"):\n",
    "                parameters.append([layer.W, layer.W_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"b\"):\n",
    "                parameters.append([layer.b, layer.b_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"gamma\"):\n",
    "                parameters.append([layer.gamma, layer.gamma_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"beta\"):\n",
    "                parameters.append([layer.beta, layer.beta_grad, layer.skip_decay])\n",
    "        return parameters\n",
    "\n",
    "    def step(self):\n",
    "        parameters = self.get_parameters() \n",
    "        for i, (param_list, m, v) in enumerate(zip(parameters, self.m, self.v)):\n",
    "            param, param_grad, skip_decay = param_list\n",
    "            self.t += 1\n",
    "            m = self.beta1 * m + (1 - self.beta1) * param_grad\n",
    "            v = self.beta2 * v + (1 - self.beta2) * np.power(param_grad, 2)\n",
    "            self.m[i] = m\n",
    "            self.v[i] = v\n",
    "            m_hat = m / (1 - np.power(self.beta1, self.t))\n",
    "            v_hat = v / (1 - np.power(self.beta2, self.t))\n",
    "\n",
    "            update = self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "            if not skip_decay:\n",
    "                param -= update\n",
    "                param *= (1 - self.lr * self.decoupled_weight_decay)\n",
    "            else:\n",
    "                param -= update"
   ],
   "id": "8f0ce4bcccfc5f10",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### SGD with Momentum\n",
    "SGD Formula:\n",
    "$$θ_{t+1}=θ_t−\\eta  \\cdot ∇L(θ_t)$$\n",
    "\n",
    "Momentum 梯度下降的公式如下：\n",
    "$$\n",
    "\\begin{equation}\n",
    "v_t = \\beta v_{t-1} - \\eta \\nabla L(\\theta_t)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t + v_t\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "$$\n",
    "    \\( v_t \\) 是当前动量\\\\\n",
    "    \\( \\beta \\) 是动量系数（通常取 0.9）\\\\\n",
    "    \\( \\eta \\) 是学习率\\\\\n",
    "    \\( \\nabla L(\\theta_t) \\) 是损失函数对参数的梯度\n",
    "$$"
   ],
   "id": "d37f07c8cd64eb59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.768552Z",
     "start_time": "2025-03-28T05:03:11.763325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGDMomentum:\n",
    "    def __init__(self, model, lr=0.01, momentum=0.9, weight_decay=0.0001):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.v = [np.zeros(param[0].shape) for param in self.model.parameters]\n",
    "    def get_parameters(self):\n",
    "        parameters = []\n",
    "        for layer in self.model.layers:\n",
    "            if hasattr(layer, \"W\"):\n",
    "                parameters.append([layer.W, layer.W_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"b\"):\n",
    "                parameters.append([layer.b, layer.b_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"gamma\"):\n",
    "                parameters.append([layer.gamma, layer.gamma_grad, layer.skip_decay])\n",
    "            if hasattr(layer, \"beta\"):\n",
    "                parameters.append([layer.beta, layer.beta_grad, layer.skip_decay])\n",
    "        return parameters\n",
    "\n",
    "    def step(self):\n",
    "        self.parameters = self.get_parameters()\n",
    "        # 直接从 model 中获取 parameters 和 gradients\n",
    "        for i, (v, param_list) in enumerate(zip(self.v, self.parameters)):\n",
    "            param, param_grad, skip_decay = param_list\n",
    "            if param_grad is not None:\n",
    "                if not skip_decay:\n",
    "                    param -= self.weight_decay * param  # 应用权重衰减\n",
    "                v[:] = self.momentum * v + self.lr * param_grad  # 更新动量\n",
    "                self.v[i] = v\n",
    "                param -= v  # 更新参数"
   ],
   "id": "f414f1bd779f820b",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Normalization\n",
    "核心思想是：在每一层输入神经元的激活值进行归一化（normalization）然后进行缩放和平移，以保持模型的表达能力\n",
    "Forward:\n",
    "$$\\hat x_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "$$\\mathbf{y}=\\gamma\\frac{\\mathbf{x}-E(\\mathbf{x})}{\\sqrt{\\sigma^2_B+\\epsilon}}+\\beta = \\gamma \\hat {\\mathbf{x}}+\\beta$$\n",
    "\n",
    "$E(\\mathbf{x})$ is the mean of the current mini-batch, $\\sigma^2_B$ is the variance of the current mini-batch\n",
    "\n",
    "Backward:\n",
    "\n",
    "m is the number of batch size\n",
    "\n",
    "\n",
    "\n",
    "Gradient of $\\gamma$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\gamma} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial \\gamma} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i} \\hat{x}_i\n",
    ", \\qquad where \\ \\ \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{(\\sigma^2)+\\epsilon}}\n",
    "$$\n",
    "Gradient of $\\beta$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\beta} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial \\beta} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i}=\\frac{1}{m}\\sum_{i=1}^{m} gradient\\_output$$\n",
    "\n",
    "Gradient of $x_i$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\sigma^2} = \\frac{\\partial L}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial \\hat {x_i}} \\cdot \\frac{\\partial \\hat {x_i}}{\\partial \\sigma^2}\\\\=\\frac{\\partial L}{\\partial y_i} \\cdot \\gamma \\cdot -\\frac{1}{2} \\hat{x_i} (\\sigma^2+\\epsilon)^{-1}\\\\$$\n",
    "\n",
    "to simplify the formula let $\\sigma = \\sqrt{\\sigma^2+\\epsilon}$ so the final formula is :\n",
    "$$\\frac{\\partial L}{\\partial \\sigma^2} = -\\frac{\\gamma}{\\sigma} \\sum_{i=1}^{m} \\hat {x_i}\\frac{\\partial L}{\\partial \\gamma} $$\n",
    "$$\\frac{\\partial \\mu}{\\partial x_i}=\\frac{1}{m}$$\n",
    "$$\\frac{\\partial \\sigma^2}{\\partial x_i}=\\frac{2}{m}(x_i-\\mu)$$\n",
    "$$\\frac{\\partial \\hat {x_i}}{\\partial x_i}=\\frac{1}{\\sigma} - \\frac{1}{m}\\sum_{j=1}^{m}\\frac{1}{\\sigma}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\hat{x_i}}= \\gamma \\cdot  gradient\\_output_i$$\n",
    "$$\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial \\hat x_i} \\cdot \\frac{\\partial \\hat x_i}{\\partial x_i} + \\frac{\\partial L}{\\partial \\sigma^2} \\cdot \\frac{\\partial \\sigma^2}{\\partial x_i} + \\frac{\\partial L}{\\partial \\mu} \\cdot \\frac{\\partial \\mu}{\\partial x_i} \\\\=\\frac{\\gamma}{\\sigma} \\left( \\frac{\\partial L}{\\partial y_i} - \\frac{1}{m} \\hat{x}_i \\frac{\\partial L}{\\partial \\gamma} - \\frac{\\partial L}{\\partial \\beta}  \\right)$$"
   ],
   "id": "2eecd3ec6ec4360c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.775751Z",
     "start_time": "2025-03-28T05:03:11.769900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BatchNormalization(Layer):\n",
    "    def __init__(self, name, feature_num,skip_decay=True, epsilon=1e-5, requires_grad=True):\n",
    "        super().__init__(name)\n",
    "        self.epsilon = epsilon\n",
    "        self.requires_grad = requires_grad\n",
    "        self.skip_decay = skip_decay\n",
    "        self.gamma = np.ones(feature_num)\n",
    "        self.beta = np.zeros(feature_num)\n",
    "\n",
    "        self.gamma_grad = None\n",
    "        self.beta_grad = None\n",
    "\n",
    "        self.ema = np.zeros(feature_num)\n",
    "        self.emv = np.zeros(feature_num)\n",
    "\n",
    "    def _forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        x: [batch_size,feature number]\n",
    "        gamma: [feature number]\n",
    "        beta: [feature number]\n",
    "        :param x:\n",
    "        :return:\n",
    "        '''\n",
    "        if self.train:\n",
    "            batch_mean = x.mean(axis=0)\n",
    "            batch_variance = x.var(axis=0)\n",
    "            batch_std = np.sqrt(batch_variance + self.epsilon)\n",
    "            # record exponential moving average and variance to\n",
    "            momentum = 0.9\n",
    "            self.ema = momentum * self.ema + (1 - momentum) * batch_mean\n",
    "            self.emv = momentum * self.emv + (1 - momentum) * batch_variance\n",
    "        else:\n",
    "            batch_mean = self.ema.data\n",
    "            batch_std = np.sqrt(self.emv + self.epsilon)\n",
    "        self.norm = (x - batch_mean) / batch_std\n",
    "        self.gamma_norm = self.gamma / batch_std\n",
    "\n",
    "        return self.gamma * self.norm + self.beta\n",
    "\n",
    "    def _backward(self, gradient_output: np.ndarray) -> np.ndarray:\n",
    "        # make sure that gradient_output is the gradient of next layer, indicating that the gradient of loss about y\n",
    "        batch_size = gradient_output.shape[0]\n",
    "        self.gamma_grad = (gradient_output * self.norm).sum(axis=0) / batch_size\n",
    "        self.beta_grad = gradient_output.sum(axis=0) / batch_size\n",
    "        dLdx = self.gamma_norm * (gradient_output - self.norm * self.gamma_grad - self.beta_grad)\n",
    "        return dLdx"
   ],
   "id": "a471a89cc91ed1b",
   "outputs": [],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.808217Z",
     "start_time": "2025-03-28T05:03:11.804216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AverageMeterics(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ],
   "id": "9e82f74a1f63aea1",
   "outputs": [],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.827978Z",
     "start_time": "2025-03-28T05:03:11.816868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,config,model=None,train_loader=None,valid_loader=None):\n",
    "        self.config=config\n",
    "        self.epochs = self.config['epoch']\n",
    "        self.lr=self.config['lr']\n",
    "        self.model=model\n",
    "        self.train_loader=train_loader\n",
    "        self.valid_loader=valid_loader\n",
    "        self.print_freq=self.config['print_freq']\n",
    "        # self.scheduler= self.config['scheduler']\n",
    "        self.train_accuracy=[]\n",
    "        self.valid_accuracy=[]\n",
    "        self.train_loss=[]\n",
    "        self.valid_loss=[]\n",
    "        self.criterion=CrossEntropy()\n",
    "        if self.config['optimizer'] == 'sgd':\n",
    "            self.optimizer = SGDMomentum(self.model, self.lr, self.config['momentum'],\n",
    "                                         self.config['weight_decay'])\n",
    "        elif self.config['optimizer'] == 'adamw':\n",
    "            self.optimizer = AdamW(self.model, self.lr, self.config['weight_decay'])\n",
    "        # if self.scheduler == 'cos':\n",
    "        #     self.train_scheduler = CosineLR(self.optimizer, T_max=self.epochs)\n",
    "    @timer\n",
    "    def train(self):\n",
    "        best_accuracy=0\n",
    "        for epoch in range(self.epochs):\n",
    "            print('current lr {:.5e}'.format(self.optimizer.lr))\n",
    "            self.train_per_epoch(epoch)\n",
    "            acc1 = self.validate(epoch)\n",
    "\n",
    "            best_acc1 = max(acc1, best_accuracy)\n",
    "            output_best = f'Best Accuracy: {best_acc1}.4f\\n'\n",
    "            print(output_best)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def train_per_epoch(self,epoch):\n",
    "        batch_time=AverageMeterics()\n",
    "        losses=AverageMeterics()\n",
    "        best_acc=AverageMeterics()        \n",
    "        self.model._fit()\n",
    "        end_time = time.time()\n",
    "        for i,(X,y) in enumerate(self.train_loader):\n",
    "            y_hat=self.model._forward(X)\n",
    "            loss=self.criterion(y_hat,y)\n",
    "            \n",
    "            self.model._backward(self.criterion.grad)\n",
    "            self.optimizer.step()\n",
    "            acc=accuracy(y_hat,y)\n",
    "            losses.update(loss,X.shape[0])\n",
    "            best_acc.update(acc,X.shape[0])\n",
    "            \n",
    "            batch_time.update(time.time() - end_time)\n",
    "            end_time = time.time()\n",
    "            if (i%self.print_freq ==0) or (i==len(self.train_loader)-1):\n",
    "                print(f'Epoch: [{epoch + 1}][{i}/{len(self.train_loader) - 1}]\\tTime {batch_time.val:.3f} (Avg-Time {batch_time.avg:.3f})\\t '\n",
    "                      f'Loss {losses.val:.4f} (Avg-Loss {losses.avg:.4f})\\t'\n",
    "                      f'Acc {best_acc.val:.4f} (Avg-Acc {best_acc.avg:.4f})')\n",
    "        print(f'EPOCH: {epoch+1} train Results: Acc {best_acc.avg:.3f} Loss: {losses.avg:.4f}')\n",
    "        self.train_loss.append(losses.avg)\n",
    "        self.train_accuracy.append(best_acc.avg)\n",
    "    def validate(self, epoch):\n",
    "        batch_time = AverageMeterics()\n",
    "        losses = AverageMeterics()\n",
    "        best_acc = AverageMeterics()\n",
    "\n",
    "        self.model._fit(mode='eval')\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (X, y) in enumerate(self.valid_loader):\n",
    "            # compute output\n",
    "            y_hat = self.model._forward(X)\n",
    "            loss = self.criterion(y_hat, y)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc = accuracy(y_hat, y)\n",
    "            losses.update(loss, X.shape[0])\n",
    "            best_acc.update(acc, X.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if (i % self.print_freq == 0) or (i == len(self.valid_loader) - 1):\n",
    "                print(f'Epoch: [{epoch + 1}][{i}/{len(self.valid_loader) - 1}]\\tTime {batch_time.val:.3f} (Avg-Time {batch_time.avg:.3f})\\t '\n",
    "                      f'Loss {losses.val:.4f} (Avg-Loss {losses.avg:.4f})\\t'\n",
    "                      f'Acc {best_acc.val:.4f} (Avg-Acc {best_acc.avg:.4f})')\n",
    "\n",
    "        print(f'EPOCH: {epoch+1} Validation Results: Acc {best_acc.avg:.3f} Loss: {losses.avg:.4f}')\n",
    "        self.valid_loss.append(losses.avg)\n",
    "        self.valid_accuracy.append(best_acc.avg)\n",
    "\n",
    "        return best_acc.avg"
   ],
   "id": "bf29c48b65013fe3",
   "outputs": [],
   "execution_count": 178
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.834371Z",
     "start_time": "2025-03-28T05:03:11.830429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, name, drop_rate=0.5, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.fix_value = 1 / (1 - self.drop_rate)   # to keep average fixed\n",
    "\n",
    "    def _forward(self, x):\n",
    "        if self.train:\n",
    "            self.mask = np.random.uniform(0, 1, x.shape) > self.drop_rate\n",
    "            return x * self.mask * self.fix_value\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def _backward(self, grad_output):\n",
    "        if self.train:\n",
    "            return grad_output * self.mask\n",
    "        else:\n",
    "            return grad_output"
   ],
   "id": "66ffb78d9831ae51",
   "outputs": [],
   "execution_count": 179
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.840244Z",
     "start_time": "2025-03-28T05:03:11.836029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dataloader(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=True, seed=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.index = np.arange(X.shape[0])\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            if self.seed is not None:\n",
    "                np.random.seed(self.seed)\n",
    "            np.random.shuffle(self.index)\n",
    "        self.n = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.n >= len(self.index):\n",
    "            raise StopIteration\n",
    "        index = self.index[self.n:self.n + self.batch_size]\n",
    "        batch_X = self.X[index]\n",
    "        batch_y = self.y[index]\n",
    "        self.n += self.batch_size\n",
    "\n",
    "        return batch_X, batch_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.index) + self.batch_size - 1) // self.batch_size"
   ],
   "id": "66f87db9952bdaef",
   "outputs": [],
   "execution_count": 180
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.862474Z",
     "start_time": "2025-03-28T05:03:11.858354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "batch_size=1024\n",
    "config={'lr': 0.01,\n",
    "        'batch_size': batch_size,\n",
    "        'momentum': 0.9,\n",
    "        'weight_decay': 5e-4,\n",
    "        'seed': 0,\n",
    "        'epoch': 200,\n",
    "        'optimizer': 'sgd',     # adam, sgd\n",
    "        'scheduler': None,      # cos, None\n",
    "        'pre-process': 'standardization',      # min-max, standardization, None\n",
    "        'print_freq': 50000 // batch_size // 5\n",
    "}\n",
    "np.random.seed(config['seed'])"
   ],
   "id": "d76807819d7f07c2",
   "outputs": [],
   "execution_count": 181
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:03:11.910945Z",
     "start_time": "2025-03-28T05:03:11.870700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir_path='E:\\\\Postgraduate\\\\25S1\\\\COMP5329\\\\Assignment\\\\Assignment1\\\\Assignment1-Dataset\\\\'\n",
    "train_file='train_data.npy'\n",
    "train_label_file='train_label.npy'\n",
    "train_data=np.load(dir_path+train_file)\n",
    "train_label=np.load(dir_path+train_label_file)\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_data, train_label, test_size=0.2, random_state=5329, shuffle=True)\n",
    "test_file='test_data.npy'\n",
    "test_label_file='test_label.npy'"
   ],
   "id": "649ce56cf5d9e69a",
   "outputs": [],
   "execution_count": 182
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:06:22.053889Z",
     "start_time": "2025-03-28T05:03:11.912363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_X=pre_processing(train_X,config['pre-process'])\n",
    "val_X=pre_processing(val_X,config['pre-process'])\n",
    "train_dataloader=Dataloader(train_X, train_y, config['batch_size'], shuffle=True, seed=config['seed'])\n",
    "validation_dataloader=Dataloader(val_X, val_y, config['batch_size'], shuffle=False, seed=config['seed'])\n",
    "test_X=np.load(dir_path+test_file)\n",
    "test_label=np.load(dir_path+test_label_file)\n",
    "test_X=pre_processing(test_X,config['pre-process'])\n",
    "test_dataloader=Dataloader(test_X, test_label, config['batch_size'], shuffle=False, seed=config['seed'])\n",
    "\n",
    "model = MLP()\n",
    "trainer=Trainer(config,model,train_dataloader,validation_dataloader)\n",
    "trainer.train()\n"
   ],
   "id": "d376e649804a297d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process: standardization\n",
      "Pre-process: standardization\n",
      "Pre-process: standardization\n",
      "Start time:  Fri Mar 28 16:03:12 2025\n",
      "current lr 1.00000e-02\n",
      "Epoch: [1][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 6.3208 (Avg-Loss6.3208)\tAcc 10.2539 (Avg-Acc10.2539)\n",
      "Epoch: [1][9/39]\tTime 0.022 (Avg-Time0.023)\t Loss 5.8828 (Avg-Loss6.0885)\tAcc 10.8398 (Avg-Acc10.6055)\n",
      "Epoch: [1][18/39]\tTime 0.023 (Avg-Time0.023)\t Loss 5.2248 (Avg-Loss5.7548)\tAcc 12.0117 (Avg-Acc11.1585)\n",
      "Epoch: [1][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 4.5546 (Avg-Loss5.4643)\tAcc 14.1602 (Avg-Acc11.9001)\n",
      "Epoch: [1][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 4.1245 (Avg-Loss5.1965)\tAcc 15.8203 (Avg-Acc12.7639)\n",
      "Epoch: [1][39/39]\tTime 0.007 (Avg-Time0.023)\t Loss 3.9389 (Avg-Loss5.1404)\tAcc 14.0625 (Avg-Acc12.9225)\n",
      "EPOCH: 1 train Results: Acc 12.922 Loss: 5.1404\n",
      "Epoch: [1][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 2.6875 (Avg-Loss2.6875)\tAcc 22.1680 (Avg-Acc22.1680)\n",
      "Epoch: [1][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 2.5862 (Avg-Loss2.6796)\tAcc 21.6837 (Avg-Acc21.4900)\n",
      "EPOCH: 1 Validation Results: Acc 21.490 Loss: 2.6796\n",
      "Best Accuracy: 21.49.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [2][0/39]\tTime 0.046 (Avg-Time0.046)\t Loss 4.0362 (Avg-Loss4.0362)\tAcc 15.0391 (Avg-Acc15.0391)\n",
      "Epoch: [2][9/39]\tTime 0.021 (Avg-Time0.026)\t Loss 3.6889 (Avg-Loss3.8573)\tAcc 19.5312 (Avg-Acc18.1543)\n",
      "Epoch: [2][18/39]\tTime 0.020 (Avg-Time0.029)\t Loss 3.5587 (Avg-Loss3.7079)\tAcc 17.4805 (Avg-Acc18.5290)\n",
      "Epoch: [2][27/39]\tTime 0.024 (Avg-Time0.027)\t Loss 3.3525 (Avg-Loss3.6001)\tAcc 19.1406 (Avg-Acc18.9872)\n",
      "Epoch: [2][36/39]\tTime 0.022 (Avg-Time0.026)\t Loss 3.3175 (Avg-Loss3.5139)\tAcc 18.7500 (Avg-Acc19.3702)\n",
      "Epoch: [2][39/39]\tTime 0.005 (Avg-Time0.026)\t Loss 3.0521 (Avg-Loss3.4988)\tAcc 23.4375 (Avg-Acc19.4650)\n",
      "EPOCH: 2 train Results: Acc 19.465 Loss: 3.4988\n",
      "Epoch: [2][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 2.1583 (Avg-Loss2.1583)\tAcc 30.6641 (Avg-Acc30.6641)\n",
      "Epoch: [2][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 2.0943 (Avg-Loss2.1760)\tAcc 30.1020 (Avg-Acc28.4200)\n",
      "EPOCH: 2 Validation Results: Acc 28.420 Loss: 2.1760\n",
      "Best Accuracy: 28.42.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [3][0/39]\tTime 0.026 (Avg-Time0.026)\t Loss 3.1064 (Avg-Loss3.1064)\tAcc 21.0938 (Avg-Acc21.0938)\n",
      "Epoch: [3][9/39]\tTime 0.026 (Avg-Time0.023)\t Loss 3.0378 (Avg-Loss3.0316)\tAcc 22.4609 (Avg-Acc21.8945)\n",
      "Epoch: [3][18/39]\tTime 0.024 (Avg-Time0.023)\t Loss 2.8985 (Avg-Loss2.9871)\tAcc 22.9492 (Avg-Acc22.0343)\n",
      "Epoch: [3][27/39]\tTime 0.023 (Avg-Time0.024)\t Loss 2.8994 (Avg-Loss2.9347)\tAcc 23.3398 (Avg-Acc22.4156)\n",
      "Epoch: [3][36/39]\tTime 0.023 (Avg-Time0.024)\t Loss 2.7110 (Avg-Loss2.8889)\tAcc 23.0469 (Avg-Acc22.7645)\n",
      "Epoch: [3][39/39]\tTime 0.004 (Avg-Time0.023)\t Loss 2.8925 (Avg-Loss2.8801)\tAcc 28.1250 (Avg-Acc22.7725)\n",
      "EPOCH: 3 train Results: Acc 22.773 Loss: 2.8801\n",
      "Epoch: [3][0/9]\tTime 0.008 (Avg-Time0.008)\t Loss 1.9748 (Avg-Loss1.9748)\tAcc 34.0820 (Avg-Acc34.0820)\n",
      "Epoch: [3][9/9]\tTime 0.006 (Avg-Time0.008)\t Loss 1.9337 (Avg-Loss1.9928)\tAcc 32.9082 (Avg-Acc31.1400)\n",
      "EPOCH: 3 Validation Results: Acc 31.140 Loss: 1.9928\n",
      "Best Accuracy: 31.14.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [4][0/39]\tTime 0.026 (Avg-Time0.026)\t Loss 2.5870 (Avg-Loss2.5870)\tAcc 26.2695 (Avg-Acc26.2695)\n",
      "Epoch: [4][9/39]\tTime 0.024 (Avg-Time0.024)\t Loss 2.5799 (Avg-Loss2.6235)\tAcc 23.6328 (Avg-Acc24.2773)\n",
      "Epoch: [4][18/39]\tTime 0.031 (Avg-Time0.023)\t Loss 2.5075 (Avg-Loss2.6087)\tAcc 26.7578 (Avg-Acc24.4552)\n",
      "Epoch: [4][27/39]\tTime 0.023 (Avg-Time0.023)\t Loss 2.4721 (Avg-Loss2.5814)\tAcc 25.5859 (Avg-Acc24.5919)\n",
      "Epoch: [4][36/39]\tTime 0.024 (Avg-Time0.023)\t Loss 2.4357 (Avg-Loss2.5518)\tAcc 26.3672 (Avg-Acc24.9419)\n",
      "Epoch: [4][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 2.1654 (Avg-Loss2.5465)\tAcc 39.0625 (Avg-Acc24.9400)\n",
      "EPOCH: 4 train Results: Acc 24.940 Loss: 2.5465\n",
      "Epoch: [4][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.8856 (Avg-Loss1.8856)\tAcc 36.3281 (Avg-Acc36.3281)\n",
      "Epoch: [4][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.8557 (Avg-Loss1.9061)\tAcc 35.3316 (Avg-Acc33.3600)\n",
      "EPOCH: 4 Validation Results: Acc 33.360 Loss: 1.9061\n",
      "Best Accuracy: 33.36.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [5][0/39]\tTime 0.026 (Avg-Time0.026)\t Loss 2.3943 (Avg-Loss2.3943)\tAcc 27.4414 (Avg-Acc27.4414)\n",
      "Epoch: [5][9/39]\tTime 0.025 (Avg-Time0.024)\t Loss 2.2982 (Avg-Loss2.3605)\tAcc 29.8828 (Avg-Acc27.1875)\n",
      "Epoch: [5][18/39]\tTime 0.023 (Avg-Time0.024)\t Loss 2.3719 (Avg-Loss2.3517)\tAcc 25.0977 (Avg-Acc26.8606)\n",
      "Epoch: [5][27/39]\tTime 0.024 (Avg-Time0.024)\t Loss 2.2930 (Avg-Loss2.3387)\tAcc 26.6602 (Avg-Acc26.7997)\n",
      "Epoch: [5][36/39]\tTime 0.023 (Avg-Time0.024)\t Loss 2.3022 (Avg-Loss2.3237)\tAcc 23.8281 (Avg-Acc26.8713)\n",
      "Epoch: [5][39/39]\tTime 0.003 (Avg-Time0.024)\t Loss 2.1733 (Avg-Loss2.3200)\tAcc 29.6875 (Avg-Acc26.8925)\n",
      "EPOCH: 5 train Results: Acc 26.892 Loss: 2.3200\n",
      "Epoch: [5][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.8342 (Avg-Loss1.8342)\tAcc 37.4023 (Avg-Acc37.4023)\n",
      "Epoch: [5][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.8102 (Avg-Loss1.8529)\tAcc 37.5000 (Avg-Acc34.8400)\n",
      "EPOCH: 5 Validation Results: Acc 34.840 Loss: 1.8529\n",
      "Best Accuracy: 34.84.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [6][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 2.1825 (Avg-Loss2.1825)\tAcc 29.7852 (Avg-Acc29.7852)\n",
      "Epoch: [6][9/39]\tTime 0.022 (Avg-Time0.023)\t Loss 2.1692 (Avg-Loss2.2221)\tAcc 28.5156 (Avg-Acc27.7539)\n",
      "Epoch: [6][18/39]\tTime 0.027 (Avg-Time0.022)\t Loss 2.2600 (Avg-Loss2.2163)\tAcc 26.7578 (Avg-Acc27.4517)\n",
      "Epoch: [6][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 2.1352 (Avg-Loss2.2060)\tAcc 30.0781 (Avg-Acc27.8251)\n",
      "Epoch: [6][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 2.1687 (Avg-Loss2.1941)\tAcc 28.9062 (Avg-Acc28.0432)\n",
      "Epoch: [6][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 2.3829 (Avg-Loss2.1897)\tAcc 18.7500 (Avg-Acc28.1125)\n",
      "EPOCH: 6 train Results: Acc 28.113 Loss: 2.1897\n",
      "Epoch: [6][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.7959 (Avg-Loss1.7959)\tAcc 38.8672 (Avg-Acc38.8672)\n",
      "Epoch: [6][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.7741 (Avg-Loss1.8160)\tAcc 38.1378 (Avg-Acc35.8200)\n",
      "EPOCH: 6 Validation Results: Acc 35.820 Loss: 1.8160\n",
      "Best Accuracy: 35.82.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [7][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 2.1163 (Avg-Loss2.1163)\tAcc 29.1992 (Avg-Acc29.1992)\n",
      "Epoch: [7][9/39]\tTime 0.025 (Avg-Time0.022)\t Loss 2.1151 (Avg-Loss2.1094)\tAcc 29.1992 (Avg-Acc29.0625)\n",
      "Epoch: [7][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 2.1146 (Avg-Loss2.1079)\tAcc 29.0039 (Avg-Acc29.2403)\n",
      "Epoch: [7][27/39]\tTime 0.025 (Avg-Time0.022)\t Loss 2.1178 (Avg-Loss2.1018)\tAcc 27.3438 (Avg-Acc29.0876)\n",
      "Epoch: [7][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 2.0040 (Avg-Loss2.0881)\tAcc 29.8828 (Avg-Acc29.2916)\n",
      "Epoch: [7][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.9713 (Avg-Loss2.0854)\tAcc 28.1250 (Avg-Acc29.3300)\n",
      "EPOCH: 7 train Results: Acc 29.330 Loss: 2.0854\n",
      "Epoch: [7][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.7731 (Avg-Loss1.7731)\tAcc 39.5508 (Avg-Acc39.5508)\n",
      "Epoch: [7][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.7520 (Avg-Loss1.7904)\tAcc 39.1582 (Avg-Acc36.7500)\n",
      "EPOCH: 7 Validation Results: Acc 36.750 Loss: 1.7904\n",
      "Best Accuracy: 36.75.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [8][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 2.0726 (Avg-Loss2.0726)\tAcc 30.9570 (Avg-Acc30.9570)\n",
      "Epoch: [8][9/39]\tTime 0.022 (Avg-Time0.021)\t Loss 1.9620 (Avg-Loss2.0080)\tAcc 31.9336 (Avg-Acc30.8691)\n",
      "Epoch: [8][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 2.0450 (Avg-Loss2.0130)\tAcc 31.7383 (Avg-Acc30.4996)\n",
      "Epoch: [8][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 2.0123 (Avg-Loss2.0073)\tAcc 29.7852 (Avg-Acc30.6989)\n",
      "Epoch: [8][36/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.9604 (Avg-Loss2.0061)\tAcc 30.8594 (Avg-Acc30.6218)\n",
      "Epoch: [8][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.8770 (Avg-Loss2.0042)\tAcc 28.1250 (Avg-Acc30.6450)\n",
      "EPOCH: 8 train Results: Acc 30.645 Loss: 2.0042\n",
      "Epoch: [8][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.7518 (Avg-Loss1.7518)\tAcc 40.4297 (Avg-Acc40.4297)\n",
      "Epoch: [8][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.7332 (Avg-Loss1.7700)\tAcc 40.6888 (Avg-Acc37.5100)\n",
      "EPOCH: 8 Validation Results: Acc 37.510 Loss: 1.7700\n",
      "Best Accuracy: 37.51.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [9][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 2.0809 (Avg-Loss2.0809)\tAcc 28.0273 (Avg-Acc28.0273)\n",
      "Epoch: [9][9/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.9436 (Avg-Loss1.9791)\tAcc 30.0781 (Avg-Acc30.7324)\n",
      "Epoch: [9][18/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.9485 (Avg-Loss1.9607)\tAcc 32.0312 (Avg-Acc31.3014)\n",
      "Epoch: [9][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.8866 (Avg-Loss1.9576)\tAcc 32.7148 (Avg-Acc31.4558)\n",
      "Epoch: [9][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.9342 (Avg-Loss1.9507)\tAcc 34.2773 (Avg-Acc31.5773)\n",
      "Epoch: [9][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.9616 (Avg-Loss1.9517)\tAcc 23.4375 (Avg-Acc31.5825)\n",
      "EPOCH: 9 train Results: Acc 31.582 Loss: 1.9517\n",
      "Epoch: [9][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.7357 (Avg-Loss1.7357)\tAcc 41.1133 (Avg-Acc41.1133)\n",
      "Epoch: [9][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.7179 (Avg-Loss1.7534)\tAcc 40.8163 (Avg-Acc38.1500)\n",
      "EPOCH: 9 Validation Results: Acc 38.150 Loss: 1.7534\n",
      "Best Accuracy: 38.15.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [10][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.9105 (Avg-Loss1.9105)\tAcc 33.2031 (Avg-Acc33.2031)\n",
      "Epoch: [10][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.9643 (Avg-Loss1.9183)\tAcc 31.6406 (Avg-Acc32.4414)\n",
      "Epoch: [10][18/39]\tTime 0.020 (Avg-Time0.023)\t Loss 1.8747 (Avg-Loss1.9136)\tAcc 36.6211 (Avg-Acc32.8074)\n",
      "Epoch: [10][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.8926 (Avg-Loss1.9067)\tAcc 31.7383 (Avg-Acc32.6765)\n",
      "Epoch: [10][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.9243 (Avg-Loss1.9066)\tAcc 31.2500 (Avg-Acc32.5961)\n",
      "Epoch: [10][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 2.0023 (Avg-Loss1.9060)\tAcc 29.6875 (Avg-Acc32.5775)\n",
      "EPOCH: 10 train Results: Acc 32.578 Loss: 1.9060\n",
      "Epoch: [10][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.7229 (Avg-Loss1.7229)\tAcc 41.6016 (Avg-Acc41.6016)\n",
      "Epoch: [10][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.7048 (Avg-Loss1.7404)\tAcc 41.3265 (Avg-Acc38.7800)\n",
      "EPOCH: 10 Validation Results: Acc 38.780 Loss: 1.7404\n",
      "Best Accuracy: 38.78.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [11][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.8682 (Avg-Loss1.8682)\tAcc 35.2539 (Avg-Acc35.2539)\n",
      "Epoch: [11][9/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.8662 (Avg-Loss1.8779)\tAcc 33.0078 (Avg-Acc33.5938)\n",
      "Epoch: [11][18/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.8596 (Avg-Loss1.8771)\tAcc 31.9336 (Avg-Acc33.4241)\n",
      "Epoch: [11][27/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.8568 (Avg-Loss1.8693)\tAcc 34.3750 (Avg-Acc33.6600)\n",
      "Epoch: [11][36/39]\tTime 0.023 (Avg-Time0.024)\t Loss 1.8359 (Avg-Loss1.8678)\tAcc 33.3008 (Avg-Acc33.5964)\n",
      "Epoch: [11][39/39]\tTime 0.005 (Avg-Time0.023)\t Loss 1.7113 (Avg-Loss1.8674)\tAcc 32.8125 (Avg-Acc33.6250)\n",
      "EPOCH: 11 train Results: Acc 33.625 Loss: 1.8674\n",
      "Epoch: [11][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.7101 (Avg-Loss1.7101)\tAcc 41.5039 (Avg-Acc41.5039)\n",
      "Epoch: [11][9/9]\tTime 0.006 (Avg-Time0.009)\t Loss 1.6933 (Avg-Loss1.7289)\tAcc 41.0714 (Avg-Acc39.0900)\n",
      "EPOCH: 11 Validation Results: Acc 39.090 Loss: 1.7289\n",
      "Best Accuracy: 39.09.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [12][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.9254 (Avg-Loss1.9254)\tAcc 33.2031 (Avg-Acc33.2031)\n",
      "Epoch: [12][9/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.8208 (Avg-Loss1.8569)\tAcc 36.4258 (Avg-Acc33.6719)\n",
      "Epoch: [12][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.8737 (Avg-Loss1.8465)\tAcc 33.6914 (Avg-Acc34.1745)\n",
      "Epoch: [12][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.8615 (Avg-Loss1.8412)\tAcc 35.7422 (Avg-Acc34.3924)\n",
      "Epoch: [12][36/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.7521 (Avg-Loss1.8388)\tAcc 36.5234 (Avg-Acc34.4568)\n",
      "Epoch: [12][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.8986 (Avg-Loss1.8380)\tAcc 35.9375 (Avg-Acc34.4450)\n",
      "EPOCH: 12 train Results: Acc 34.445 Loss: 1.8380\n",
      "Epoch: [12][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.6969 (Avg-Loss1.6969)\tAcc 42.0898 (Avg-Acc42.0898)\n",
      "Epoch: [12][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.6823 (Avg-Loss1.7168)\tAcc 40.8163 (Avg-Acc39.9500)\n",
      "EPOCH: 12 Validation Results: Acc 39.950 Loss: 1.7168\n",
      "Best Accuracy: 39.95.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [13][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.8523 (Avg-Loss1.8523)\tAcc 33.6914 (Avg-Acc33.6914)\n",
      "Epoch: [13][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.8522 (Avg-Loss1.8211)\tAcc 33.2031 (Avg-Acc34.9219)\n",
      "Epoch: [13][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.8055 (Avg-Loss1.8223)\tAcc 35.8398 (Avg-Acc34.9661)\n",
      "Epoch: [13][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.7577 (Avg-Loss1.8193)\tAcc 37.4023 (Avg-Acc35.1528)\n",
      "Epoch: [13][36/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.7707 (Avg-Loss1.8173)\tAcc 36.8164 (Avg-Acc35.0665)\n",
      "Epoch: [13][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.8966 (Avg-Loss1.8166)\tAcc 25.0000 (Avg-Acc35.0775)\n",
      "EPOCH: 13 train Results: Acc 35.078 Loss: 1.8166\n",
      "Epoch: [13][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.6855 (Avg-Loss1.6855)\tAcc 42.9688 (Avg-Acc42.9688)\n",
      "Epoch: [13][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.6712 (Avg-Loss1.7054)\tAcc 41.7092 (Avg-Acc40.4100)\n",
      "EPOCH: 13 Validation Results: Acc 40.410 Loss: 1.7054\n",
      "Best Accuracy: 40.41.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [14][0/39]\tTime 0.032 (Avg-Time0.032)\t Loss 1.8408 (Avg-Loss1.8408)\tAcc 34.1797 (Avg-Acc34.1797)\n",
      "Epoch: [14][9/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.7936 (Avg-Loss1.7854)\tAcc 34.0820 (Avg-Acc36.0352)\n",
      "Epoch: [14][18/39]\tTime 0.030 (Avg-Time0.023)\t Loss 1.7665 (Avg-Loss1.7889)\tAcc 37.1094 (Avg-Acc35.7884)\n",
      "Epoch: [14][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.7733 (Avg-Loss1.7845)\tAcc 36.7188 (Avg-Acc35.8085)\n",
      "Epoch: [14][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.8005 (Avg-Loss1.7871)\tAcc 35.8398 (Avg-Acc35.8451)\n",
      "Epoch: [14][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.8958 (Avg-Loss1.7880)\tAcc 46.8750 (Avg-Acc35.8200)\n",
      "EPOCH: 14 train Results: Acc 35.820 Loss: 1.7880\n",
      "Epoch: [14][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.6751 (Avg-Loss1.6751)\tAcc 43.4570 (Avg-Acc43.4570)\n",
      "Epoch: [14][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.6600 (Avg-Loss1.6962)\tAcc 41.4541 (Avg-Acc40.7400)\n",
      "EPOCH: 14 Validation Results: Acc 40.740 Loss: 1.6962\n",
      "Best Accuracy: 40.74.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [15][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.8044 (Avg-Loss1.8044)\tAcc 35.0586 (Avg-Acc35.0586)\n",
      "Epoch: [15][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.7482 (Avg-Loss1.7780)\tAcc 37.5977 (Avg-Acc36.2793)\n",
      "Epoch: [15][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.8141 (Avg-Loss1.7706)\tAcc 34.6680 (Avg-Acc36.0557)\n",
      "Epoch: [15][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.7343 (Avg-Loss1.7699)\tAcc 38.5742 (Avg-Acc36.3630)\n",
      "Epoch: [15][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.7430 (Avg-Loss1.7678)\tAcc 38.0859 (Avg-Acc36.3281)\n",
      "Epoch: [15][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.6743 (Avg-Loss1.7673)\tAcc 46.8750 (Avg-Acc36.3650)\n",
      "EPOCH: 15 train Results: Acc 36.365 Loss: 1.7673\n",
      "Epoch: [15][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.6653 (Avg-Loss1.6653)\tAcc 43.1641 (Avg-Acc43.1641)\n",
      "Epoch: [15][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.6497 (Avg-Loss1.6858)\tAcc 42.2194 (Avg-Acc41.1800)\n",
      "EPOCH: 15 Validation Results: Acc 41.180 Loss: 1.6858\n",
      "Best Accuracy: 41.18.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [16][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.7154 (Avg-Loss1.7154)\tAcc 39.1602 (Avg-Acc39.1602)\n",
      "Epoch: [16][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.7875 (Avg-Loss1.7533)\tAcc 36.0352 (Avg-Acc37.0312)\n",
      "Epoch: [16][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.7413 (Avg-Loss1.7546)\tAcc 35.1562 (Avg-Acc36.8472)\n",
      "Epoch: [16][27/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.7494 (Avg-Loss1.7484)\tAcc 36.4258 (Avg-Acc37.0292)\n",
      "Epoch: [16][36/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.7560 (Avg-Loss1.7497)\tAcc 37.6953 (Avg-Acc37.0170)\n",
      "Epoch: [16][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.6672 (Avg-Loss1.7502)\tAcc 37.5000 (Avg-Acc37.0800)\n",
      "EPOCH: 16 train Results: Acc 37.080 Loss: 1.7502\n",
      "Epoch: [16][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.6582 (Avg-Loss1.6582)\tAcc 44.3359 (Avg-Acc44.3359)\n",
      "Epoch: [16][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.6423 (Avg-Loss1.6782)\tAcc 41.8367 (Avg-Acc41.7000)\n",
      "EPOCH: 16 Validation Results: Acc 41.700 Loss: 1.6782\n",
      "Best Accuracy: 41.7.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [17][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.7027 (Avg-Loss1.7027)\tAcc 39.0625 (Avg-Acc39.0625)\n",
      "Epoch: [17][9/39]\tTime 0.027 (Avg-Time0.023)\t Loss 1.7404 (Avg-Loss1.7371)\tAcc 37.0117 (Avg-Acc37.3242)\n",
      "Epoch: [17][18/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.8074 (Avg-Loss1.7459)\tAcc 34.0820 (Avg-Acc37.5874)\n",
      "Epoch: [17][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.7261 (Avg-Loss1.7415)\tAcc 36.6211 (Avg-Acc37.5453)\n",
      "Epoch: [17][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.7402 (Avg-Loss1.7403)\tAcc 37.3047 (Avg-Acc37.6056)\n",
      "Epoch: [17][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.5447 (Avg-Loss1.7378)\tAcc 48.4375 (Avg-Acc37.6775)\n",
      "EPOCH: 17 train Results: Acc 37.678 Loss: 1.7378\n",
      "Epoch: [17][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.6484 (Avg-Loss1.6484)\tAcc 44.7266 (Avg-Acc44.7266)\n",
      "Epoch: [17][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.6327 (Avg-Loss1.6688)\tAcc 42.7296 (Avg-Acc42.0800)\n",
      "EPOCH: 17 Validation Results: Acc 42.080 Loss: 1.6688\n",
      "Best Accuracy: 42.08.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [18][0/39]\tTime 0.026 (Avg-Time0.026)\t Loss 1.7657 (Avg-Loss1.7657)\tAcc 37.5000 (Avg-Acc37.5000)\n",
      "Epoch: [18][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.7328 (Avg-Loss1.7343)\tAcc 38.6719 (Avg-Acc38.0176)\n",
      "Epoch: [18][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.7207 (Avg-Loss1.7325)\tAcc 35.6445 (Avg-Acc37.8341)\n",
      "Epoch: [18][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.6688 (Avg-Loss1.7278)\tAcc 39.9414 (Avg-Acc37.9046)\n",
      "Epoch: [18][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.7694 (Avg-Loss1.7267)\tAcc 37.2070 (Avg-Acc37.9619)\n",
      "Epoch: [18][39/39]\tTime 0.002 (Avg-Time0.021)\t Loss 1.9162 (Avg-Loss1.7260)\tAcc 39.0625 (Avg-Acc38.0650)\n",
      "EPOCH: 18 train Results: Acc 38.065 Loss: 1.7260\n",
      "Epoch: [18][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.6390 (Avg-Loss1.6390)\tAcc 44.4336 (Avg-Acc44.4336)\n",
      "Epoch: [18][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.6222 (Avg-Loss1.6605)\tAcc 43.1122 (Avg-Acc42.0100)\n",
      "EPOCH: 18 Validation Results: Acc 42.010 Loss: 1.6605\n",
      "Best Accuracy: 42.01.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [19][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.6664 (Avg-Loss1.6664)\tAcc 38.8672 (Avg-Acc38.8672)\n",
      "Epoch: [19][9/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.6432 (Avg-Loss1.6927)\tAcc 43.4570 (Avg-Acc39.0723)\n",
      "Epoch: [19][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.7337 (Avg-Loss1.7063)\tAcc 37.0117 (Avg-Acc38.8672)\n",
      "Epoch: [19][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6920 (Avg-Loss1.7098)\tAcc 39.7461 (Avg-Acc38.7800)\n",
      "Epoch: [19][36/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.7061 (Avg-Loss1.7058)\tAcc 38.8672 (Avg-Acc38.9912)\n",
      "Epoch: [19][39/39]\tTime 0.004 (Avg-Time0.021)\t Loss 1.6840 (Avg-Loss1.7068)\tAcc 43.7500 (Avg-Acc38.9525)\n",
      "EPOCH: 19 train Results: Acc 38.953 Loss: 1.7068\n",
      "Epoch: [19][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.6328 (Avg-Loss1.6328)\tAcc 44.4336 (Avg-Acc44.4336)\n",
      "Epoch: [19][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.6164 (Avg-Loss1.6544)\tAcc 42.6020 (Avg-Acc42.2400)\n",
      "EPOCH: 19 Validation Results: Acc 42.240 Loss: 1.6544\n",
      "Best Accuracy: 42.24.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [20][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.7027 (Avg-Loss1.7027)\tAcc 41.1133 (Avg-Acc41.1133)\n",
      "Epoch: [20][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.7071 (Avg-Loss1.6925)\tAcc 38.0859 (Avg-Acc39.8926)\n",
      "Epoch: [20][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.6967 (Avg-Loss1.6938)\tAcc 39.3555 (Avg-Acc39.8386)\n",
      "Epoch: [20][27/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.7132 (Avg-Loss1.6958)\tAcc 39.8438 (Avg-Acc39.6170)\n",
      "Epoch: [20][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.6505 (Avg-Loss1.6972)\tAcc 39.7461 (Avg-Acc39.5429)\n",
      "Epoch: [20][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.9457 (Avg-Loss1.6986)\tAcc 23.4375 (Avg-Acc39.4225)\n",
      "EPOCH: 20 train Results: Acc 39.422 Loss: 1.6986\n",
      "Epoch: [20][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.6237 (Avg-Loss1.6237)\tAcc 44.8242 (Avg-Acc44.8242)\n",
      "Epoch: [20][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.6066 (Avg-Loss1.6451)\tAcc 42.8571 (Avg-Acc42.5300)\n",
      "EPOCH: 20 Validation Results: Acc 42.530 Loss: 1.6451\n",
      "Best Accuracy: 42.53.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [21][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.7531 (Avg-Loss1.7531)\tAcc 36.4258 (Avg-Acc36.4258)\n",
      "Epoch: [21][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.6884 (Avg-Loss1.6926)\tAcc 39.0625 (Avg-Acc39.0234)\n",
      "Epoch: [21][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6767 (Avg-Loss1.6855)\tAcc 39.0625 (Avg-Acc39.4685)\n",
      "Epoch: [21][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6097 (Avg-Loss1.6826)\tAcc 43.4570 (Avg-Acc39.6659)\n",
      "Epoch: [21][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.6989 (Avg-Loss1.6835)\tAcc 38.9648 (Avg-Acc39.6352)\n",
      "Epoch: [21][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.5827 (Avg-Loss1.6842)\tAcc 45.3125 (Avg-Acc39.6625)\n",
      "EPOCH: 21 train Results: Acc 39.663 Loss: 1.6842\n",
      "Epoch: [21][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.6178 (Avg-Loss1.6178)\tAcc 45.0195 (Avg-Acc45.0195)\n",
      "Epoch: [21][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.6022 (Avg-Loss1.6392)\tAcc 43.7500 (Avg-Acc43.0700)\n",
      "EPOCH: 21 Validation Results: Acc 43.070 Loss: 1.6392\n",
      "Best Accuracy: 43.07.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [22][0/39]\tTime 0.027 (Avg-Time0.027)\t Loss 1.6969 (Avg-Loss1.6969)\tAcc 38.3789 (Avg-Acc38.3789)\n",
      "Epoch: [22][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.7378 (Avg-Loss1.6829)\tAcc 36.2305 (Avg-Acc39.5508)\n",
      "Epoch: [22][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.6514 (Avg-Loss1.6746)\tAcc 42.4805 (Avg-Acc39.7872)\n",
      "Epoch: [22][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.7027 (Avg-Loss1.6781)\tAcc 38.6719 (Avg-Acc39.7949)\n",
      "Epoch: [22][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6879 (Avg-Loss1.6756)\tAcc 38.2812 (Avg-Acc39.9836)\n",
      "Epoch: [22][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 2.0253 (Avg-Loss1.6764)\tAcc 31.2500 (Avg-Acc39.9475)\n",
      "EPOCH: 22 train Results: Acc 39.947 Loss: 1.6764\n",
      "Epoch: [22][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.6099 (Avg-Loss1.6099)\tAcc 44.6289 (Avg-Acc44.6289)\n",
      "Epoch: [22][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 1.5939 (Avg-Loss1.6320)\tAcc 44.0051 (Avg-Acc43.1300)\n",
      "EPOCH: 22 Validation Results: Acc 43.130 Loss: 1.6320\n",
      "Best Accuracy: 43.13.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [23][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.6466 (Avg-Loss1.6466)\tAcc 41.1133 (Avg-Acc41.1133)\n",
      "Epoch: [23][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6700 (Avg-Loss1.6613)\tAcc 41.0156 (Avg-Acc40.4785)\n",
      "Epoch: [23][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6668 (Avg-Loss1.6584)\tAcc 39.3555 (Avg-Acc40.5222)\n",
      "Epoch: [23][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.6413 (Avg-Loss1.6571)\tAcc 42.1875 (Avg-Acc40.7261)\n",
      "Epoch: [23][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.6846 (Avg-Loss1.6605)\tAcc 40.8203 (Avg-Acc40.7147)\n",
      "Epoch: [23][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.7833 (Avg-Loss1.6614)\tAcc 42.1875 (Avg-Acc40.6775)\n",
      "EPOCH: 23 train Results: Acc 40.678 Loss: 1.6614\n",
      "Epoch: [23][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.6001 (Avg-Loss1.6001)\tAcc 44.9219 (Avg-Acc44.9219)\n",
      "Epoch: [23][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.5857 (Avg-Loss1.6229)\tAcc 43.2398 (Avg-Acc43.3300)\n",
      "EPOCH: 23 Validation Results: Acc 43.330 Loss: 1.6229\n",
      "Best Accuracy: 43.33.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [24][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6510 (Avg-Loss1.6510)\tAcc 39.2578 (Avg-Acc39.2578)\n",
      "Epoch: [24][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.6383 (Avg-Loss1.6355)\tAcc 39.5508 (Avg-Acc41.0742)\n",
      "Epoch: [24][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6591 (Avg-Loss1.6399)\tAcc 39.4531 (Avg-Acc41.0722)\n",
      "Epoch: [24][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.6553 (Avg-Loss1.6465)\tAcc 40.0391 (Avg-Acc40.6948)\n",
      "Epoch: [24][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6646 (Avg-Loss1.6498)\tAcc 40.5273 (Avg-Acc40.7913)\n",
      "Epoch: [24][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.6139 (Avg-Loss1.6522)\tAcc 46.8750 (Avg-Acc40.6450)\n",
      "EPOCH: 24 train Results: Acc 40.645 Loss: 1.6522\n",
      "Epoch: [24][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.5964 (Avg-Loss1.5964)\tAcc 45.2148 (Avg-Acc45.2148)\n",
      "Epoch: [24][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.5799 (Avg-Loss1.6181)\tAcc 44.0051 (Avg-Acc43.5600)\n",
      "EPOCH: 24 Validation Results: Acc 43.560 Loss: 1.6181\n",
      "Best Accuracy: 43.56.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [25][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6875 (Avg-Loss1.6875)\tAcc 38.6719 (Avg-Acc38.6719)\n",
      "Epoch: [25][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.6365 (Avg-Loss1.6446)\tAcc 40.4297 (Avg-Acc41.3672)\n",
      "Epoch: [25][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6135 (Avg-Loss1.6417)\tAcc 42.8711 (Avg-Acc41.1904)\n",
      "Epoch: [25][27/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.6074 (Avg-Loss1.6381)\tAcc 42.3828 (Avg-Acc41.2388)\n",
      "Epoch: [25][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.6043 (Avg-Loss1.6391)\tAcc 41.8945 (Avg-Acc41.1872)\n",
      "Epoch: [25][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.8342 (Avg-Loss1.6398)\tAcc 32.8125 (Avg-Acc41.1600)\n",
      "EPOCH: 25 train Results: Acc 41.160 Loss: 1.6398\n",
      "Epoch: [25][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.5867 (Avg-Loss1.5867)\tAcc 45.3125 (Avg-Acc45.3125)\n",
      "Epoch: [25][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.5697 (Avg-Loss1.6093)\tAcc 43.4949 (Avg-Acc43.5700)\n",
      "EPOCH: 25 Validation Results: Acc 43.570 Loss: 1.6093\n",
      "Best Accuracy: 43.57.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [26][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6281 (Avg-Loss1.6281)\tAcc 43.2617 (Avg-Acc43.2617)\n",
      "Epoch: [26][9/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.6602 (Avg-Loss1.6392)\tAcc 41.3086 (Avg-Acc41.1914)\n",
      "Epoch: [26][18/39]\tTime 0.028 (Avg-Time0.023)\t Loss 1.6454 (Avg-Loss1.6372)\tAcc 40.1367 (Avg-Acc41.5450)\n",
      "Epoch: [26][27/39]\tTime 0.025 (Avg-Time0.024)\t Loss 1.6377 (Avg-Loss1.6341)\tAcc 40.6250 (Avg-Acc41.6399)\n",
      "Epoch: [26][36/39]\tTime 0.026 (Avg-Time0.024)\t Loss 1.6260 (Avg-Loss1.6340)\tAcc 41.6992 (Avg-Acc41.6227)\n",
      "Epoch: [26][39/39]\tTime 0.002 (Avg-Time0.024)\t Loss 1.7354 (Avg-Loss1.6333)\tAcc 35.9375 (Avg-Acc41.6375)\n",
      "EPOCH: 26 train Results: Acc 41.638 Loss: 1.6333\n",
      "Epoch: [26][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.5817 (Avg-Loss1.5817)\tAcc 45.1172 (Avg-Acc45.1172)\n",
      "Epoch: [26][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.5637 (Avg-Loss1.6038)\tAcc 44.8980 (Avg-Acc43.8500)\n",
      "EPOCH: 26 Validation Results: Acc 43.850 Loss: 1.6038\n",
      "Best Accuracy: 43.85.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [27][0/39]\tTime 0.028 (Avg-Time0.028)\t Loss 1.6220 (Avg-Loss1.6220)\tAcc 41.3086 (Avg-Acc41.3086)\n",
      "Epoch: [27][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.5939 (Avg-Loss1.6284)\tAcc 42.1875 (Avg-Acc41.6797)\n",
      "Epoch: [27][18/39]\tTime 0.028 (Avg-Time0.023)\t Loss 1.5958 (Avg-Loss1.6228)\tAcc 43.6523 (Avg-Acc41.9973)\n",
      "Epoch: [27][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.6531 (Avg-Loss1.6249)\tAcc 40.3320 (Avg-Acc41.7934)\n",
      "Epoch: [27][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.5976 (Avg-Loss1.6222)\tAcc 41.9922 (Avg-Acc41.9051)\n",
      "Epoch: [27][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.8341 (Avg-Loss1.6237)\tAcc 40.6250 (Avg-Acc41.8800)\n",
      "EPOCH: 27 train Results: Acc 41.880 Loss: 1.6237\n",
      "Epoch: [27][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.5735 (Avg-Loss1.5735)\tAcc 45.3125 (Avg-Acc45.3125)\n",
      "Epoch: [27][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.5566 (Avg-Loss1.5962)\tAcc 43.7500 (Avg-Acc44.1500)\n",
      "EPOCH: 27 Validation Results: Acc 44.150 Loss: 1.5962\n",
      "Best Accuracy: 44.15.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [28][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.5941 (Avg-Loss1.5941)\tAcc 41.6992 (Avg-Acc41.6992)\n",
      "Epoch: [28][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.5881 (Avg-Loss1.5984)\tAcc 45.1172 (Avg-Acc43.1543)\n",
      "Epoch: [28][18/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.6291 (Avg-Loss1.6170)\tAcc 44.1406 (Avg-Acc42.4085)\n",
      "Epoch: [28][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.6084 (Avg-Loss1.6127)\tAcc 43.8477 (Avg-Acc42.6514)\n",
      "Epoch: [28][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.5944 (Avg-Loss1.6152)\tAcc 41.4062 (Avg-Acc42.4646)\n",
      "Epoch: [28][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.8392 (Avg-Loss1.6152)\tAcc 40.6250 (Avg-Acc42.5025)\n",
      "EPOCH: 28 train Results: Acc 42.502 Loss: 1.6152\n",
      "Epoch: [28][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.5658 (Avg-Loss1.5658)\tAcc 45.8008 (Avg-Acc45.8008)\n",
      "Epoch: [28][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.5486 (Avg-Loss1.5883)\tAcc 43.4949 (Avg-Acc44.3600)\n",
      "EPOCH: 28 Validation Results: Acc 44.360 Loss: 1.5883\n",
      "Best Accuracy: 44.36.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [29][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6020 (Avg-Loss1.6020)\tAcc 43.8477 (Avg-Acc43.8477)\n",
      "Epoch: [29][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.6226 (Avg-Loss1.5996)\tAcc 43.1641 (Avg-Acc42.9590)\n",
      "Epoch: [29][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.5995 (Avg-Loss1.5945)\tAcc 44.5312 (Avg-Acc43.4056)\n",
      "Epoch: [29][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6016 (Avg-Loss1.5990)\tAcc 41.7969 (Avg-Acc43.0664)\n",
      "Epoch: [29][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.6116 (Avg-Loss1.5999)\tAcc 43.3594 (Avg-Acc43.1086)\n",
      "Epoch: [29][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.6330 (Avg-Loss1.6007)\tAcc 48.4375 (Avg-Acc43.0300)\n",
      "EPOCH: 29 train Results: Acc 43.030 Loss: 1.6007\n",
      "Epoch: [29][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.5628 (Avg-Loss1.5628)\tAcc 45.2148 (Avg-Acc45.2148)\n",
      "Epoch: [29][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.5432 (Avg-Loss1.5840)\tAcc 44.8980 (Avg-Acc44.4100)\n",
      "EPOCH: 29 Validation Results: Acc 44.410 Loss: 1.5840\n",
      "Best Accuracy: 44.41.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [30][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6289 (Avg-Loss1.6289)\tAcc 41.2109 (Avg-Acc41.2109)\n",
      "Epoch: [30][9/39]\tTime 0.026 (Avg-Time0.022)\t Loss 1.5658 (Avg-Loss1.5912)\tAcc 46.7773 (Avg-Acc43.4473)\n",
      "Epoch: [30][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.5678 (Avg-Loss1.5915)\tAcc 46.3867 (Avg-Acc43.6061)\n",
      "Epoch: [30][27/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.5702 (Avg-Loss1.5942)\tAcc 44.4336 (Avg-Acc43.2966)\n",
      "Epoch: [30][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.5979 (Avg-Loss1.5965)\tAcc 41.6016 (Avg-Acc43.0822)\n",
      "Epoch: [30][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.4495 (Avg-Loss1.5970)\tAcc 45.3125 (Avg-Acc43.0725)\n",
      "EPOCH: 30 train Results: Acc 43.072 Loss: 1.5970\n",
      "Epoch: [30][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.5546 (Avg-Loss1.5546)\tAcc 46.0938 (Avg-Acc46.0938)\n",
      "Epoch: [30][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.5354 (Avg-Loss1.5768)\tAcc 45.0255 (Avg-Acc44.7100)\n",
      "EPOCH: 30 Validation Results: Acc 44.710 Loss: 1.5768\n",
      "Best Accuracy: 44.71.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [31][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.5818 (Avg-Loss1.5818)\tAcc 45.2148 (Avg-Acc45.2148)\n",
      "Epoch: [31][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.5355 (Avg-Loss1.5880)\tAcc 45.0195 (Avg-Acc43.5156)\n",
      "Epoch: [31][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.6370 (Avg-Loss1.5895)\tAcc 40.5273 (Avg-Acc43.1743)\n",
      "Epoch: [31][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.5680 (Avg-Loss1.5809)\tAcc 42.8711 (Avg-Acc43.3838)\n",
      "Epoch: [31][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6073 (Avg-Loss1.5848)\tAcc 43.3594 (Avg-Acc43.3594)\n",
      "Epoch: [31][39/39]\tTime 0.005 (Avg-Time0.022)\t Loss 1.6127 (Avg-Loss1.5836)\tAcc 39.0625 (Avg-Acc43.3875)\n",
      "EPOCH: 31 train Results: Acc 43.388 Loss: 1.5836\n",
      "Epoch: [31][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.5460 (Avg-Loss1.5460)\tAcc 46.2891 (Avg-Acc46.2891)\n",
      "Epoch: [31][9/9]\tTime 0.009 (Avg-Time0.007)\t Loss 1.5260 (Avg-Loss1.5683)\tAcc 45.4082 (Avg-Acc45.0600)\n",
      "EPOCH: 31 Validation Results: Acc 45.060 Loss: 1.5683\n",
      "Best Accuracy: 45.06.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [32][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.5993 (Avg-Loss1.5993)\tAcc 41.5039 (Avg-Acc41.5039)\n",
      "Epoch: [32][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.5633 (Avg-Loss1.5884)\tAcc 44.0430 (Avg-Acc43.0664)\n",
      "Epoch: [32][18/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.5776 (Avg-Loss1.5796)\tAcc 43.5547 (Avg-Acc43.3388)\n",
      "Epoch: [32][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.5826 (Avg-Loss1.5781)\tAcc 44.1406 (Avg-Acc43.6733)\n",
      "Epoch: [32][36/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.5782 (Avg-Loss1.5774)\tAcc 43.9453 (Avg-Acc43.7632)\n",
      "Epoch: [32][39/39]\tTime 0.004 (Avg-Time0.021)\t Loss 1.5003 (Avg-Loss1.5773)\tAcc 42.1875 (Avg-Acc43.7875)\n",
      "EPOCH: 32 train Results: Acc 43.788 Loss: 1.5773\n",
      "Epoch: [32][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.5395 (Avg-Loss1.5395)\tAcc 46.4844 (Avg-Acc46.4844)\n",
      "Epoch: [32][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.5203 (Avg-Loss1.5622)\tAcc 45.1531 (Avg-Acc45.0500)\n",
      "EPOCH: 32 Validation Results: Acc 45.050 Loss: 1.5622\n",
      "Best Accuracy: 45.05.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [33][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.5478 (Avg-Loss1.5478)\tAcc 44.9219 (Avg-Acc44.9219)\n",
      "Epoch: [33][9/39]\tTime 0.022 (Avg-Time0.021)\t Loss 1.6050 (Avg-Loss1.5683)\tAcc 43.3594 (Avg-Acc43.9062)\n",
      "Epoch: [33][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.5403 (Avg-Loss1.5663)\tAcc 47.1680 (Avg-Acc44.0789)\n",
      "Epoch: [33][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.5589 (Avg-Loss1.5698)\tAcc 46.2891 (Avg-Acc44.1267)\n",
      "Epoch: [33][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.5659 (Avg-Loss1.5686)\tAcc 42.6758 (Avg-Acc44.1485)\n",
      "Epoch: [33][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.5059 (Avg-Loss1.5682)\tAcc 50.0000 (Avg-Acc44.2475)\n",
      "EPOCH: 33 train Results: Acc 44.248 Loss: 1.5682\n",
      "Epoch: [33][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.5304 (Avg-Loss1.5304)\tAcc 46.7773 (Avg-Acc46.7773)\n",
      "Epoch: [33][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.5123 (Avg-Loss1.5550)\tAcc 45.2806 (Avg-Acc45.2000)\n",
      "EPOCH: 33 Validation Results: Acc 45.200 Loss: 1.5550\n",
      "Best Accuracy: 45.2.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [34][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.5244 (Avg-Loss1.5244)\tAcc 43.1641 (Avg-Acc43.1641)\n",
      "Epoch: [34][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.5961 (Avg-Loss1.5526)\tAcc 42.3828 (Avg-Acc44.1699)\n",
      "Epoch: [34][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.6061 (Avg-Loss1.5600)\tAcc 42.7734 (Avg-Acc43.9813)\n",
      "Epoch: [34][27/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.5093 (Avg-Loss1.5597)\tAcc 44.9219 (Avg-Acc44.1929)\n",
      "Epoch: [34][36/39]\tTime 0.029 (Avg-Time0.022)\t Loss 1.5770 (Avg-Loss1.5621)\tAcc 45.5078 (Avg-Acc44.1644)\n",
      "Epoch: [34][39/39]\tTime 0.005 (Avg-Time0.022)\t Loss 1.4360 (Avg-Loss1.5605)\tAcc 46.8750 (Avg-Acc44.2700)\n",
      "EPOCH: 34 train Results: Acc 44.270 Loss: 1.5605\n",
      "Epoch: [34][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.5228 (Avg-Loss1.5228)\tAcc 46.7773 (Avg-Acc46.7773)\n",
      "Epoch: [34][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.5040 (Avg-Loss1.5472)\tAcc 45.9184 (Avg-Acc45.5300)\n",
      "EPOCH: 34 Validation Results: Acc 45.530 Loss: 1.5472\n",
      "Best Accuracy: 45.53.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [35][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.5727 (Avg-Loss1.5727)\tAcc 44.2383 (Avg-Acc44.2383)\n",
      "Epoch: [35][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.5452 (Avg-Loss1.5598)\tAcc 45.5078 (Avg-Acc44.5215)\n",
      "Epoch: [35][18/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.5596 (Avg-Loss1.5546)\tAcc 45.5078 (Avg-Acc44.6700)\n",
      "Epoch: [35][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.5957 (Avg-Loss1.5505)\tAcc 43.5547 (Avg-Acc44.6882)\n",
      "Epoch: [35][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.5958 (Avg-Loss1.5486)\tAcc 42.6758 (Avg-Acc44.5920)\n",
      "Epoch: [35][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.5361 (Avg-Loss1.5479)\tAcc 48.4375 (Avg-Acc44.6225)\n",
      "EPOCH: 35 train Results: Acc 44.623 Loss: 1.5479\n",
      "Epoch: [35][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.5167 (Avg-Loss1.5167)\tAcc 47.2656 (Avg-Acc47.2656)\n",
      "Epoch: [35][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.4988 (Avg-Loss1.5412)\tAcc 46.3010 (Avg-Acc45.7300)\n",
      "EPOCH: 35 Validation Results: Acc 45.730 Loss: 1.5412\n",
      "Best Accuracy: 45.73.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [36][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.5256 (Avg-Loss1.5256)\tAcc 43.7500 (Avg-Acc43.7500)\n",
      "Epoch: [36][9/39]\tTime 0.022 (Avg-Time0.021)\t Loss 1.5928 (Avg-Loss1.5509)\tAcc 44.7266 (Avg-Acc44.5215)\n",
      "Epoch: [36][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.5071 (Avg-Loss1.5429)\tAcc 45.8984 (Avg-Acc44.9887)\n",
      "Epoch: [36][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.5427 (Avg-Loss1.5390)\tAcc 43.3594 (Avg-Acc45.1207)\n",
      "Epoch: [36][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.5286 (Avg-Loss1.5421)\tAcc 44.9219 (Avg-Acc45.0274)\n",
      "Epoch: [36][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.5543 (Avg-Loss1.5423)\tAcc 46.8750 (Avg-Acc44.9925)\n",
      "EPOCH: 36 train Results: Acc 44.992 Loss: 1.5423\n",
      "Epoch: [36][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.5090 (Avg-Loss1.5090)\tAcc 46.9727 (Avg-Acc46.9727)\n",
      "Epoch: [36][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.4905 (Avg-Loss1.5330)\tAcc 47.7041 (Avg-Acc45.8700)\n",
      "EPOCH: 36 Validation Results: Acc 45.870 Loss: 1.5330\n",
      "Best Accuracy: 45.87.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [37][0/39]\tTime 0.020 (Avg-Time0.020)\t Loss 1.5208 (Avg-Loss1.5208)\tAcc 43.2617 (Avg-Acc43.2617)\n",
      "Epoch: [37][9/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.5220 (Avg-Loss1.5159)\tAcc 44.5312 (Avg-Acc45.2734)\n",
      "Epoch: [37][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.5790 (Avg-Loss1.5326)\tAcc 44.4336 (Avg-Acc44.9065)\n",
      "Epoch: [37][27/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.5345 (Avg-Loss1.5318)\tAcc 45.4102 (Avg-Acc45.3753)\n",
      "Epoch: [37][36/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.5467 (Avg-Loss1.5352)\tAcc 44.2383 (Avg-Acc45.1990)\n",
      "Epoch: [37][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.7102 (Avg-Loss1.5354)\tAcc 43.7500 (Avg-Acc45.2825)\n",
      "EPOCH: 37 train Results: Acc 45.282 Loss: 1.5354\n",
      "Epoch: [37][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.5018 (Avg-Loss1.5018)\tAcc 47.3633 (Avg-Acc47.3633)\n",
      "Epoch: [37][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.4838 (Avg-Loss1.5264)\tAcc 47.3214 (Avg-Acc46.2400)\n",
      "EPOCH: 37 Validation Results: Acc 46.240 Loss: 1.5264\n",
      "Best Accuracy: 46.24.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [38][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.4678 (Avg-Loss1.4678)\tAcc 46.3867 (Avg-Acc46.3867)\n",
      "Epoch: [38][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.4930 (Avg-Loss1.5262)\tAcc 48.2422 (Avg-Acc46.2207)\n",
      "Epoch: [38][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.5280 (Avg-Loss1.5203)\tAcc 46.2891 (Avg-Acc45.9910)\n",
      "Epoch: [38][27/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.5243 (Avg-Loss1.5212)\tAcc 42.3828 (Avg-Acc45.5636)\n",
      "Epoch: [38][36/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.5320 (Avg-Loss1.5264)\tAcc 45.7031 (Avg-Acc45.3178)\n",
      "Epoch: [38][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.5765 (Avg-Loss1.5274)\tAcc 45.3125 (Avg-Acc45.3350)\n",
      "EPOCH: 38 train Results: Acc 45.335 Loss: 1.5274\n",
      "Epoch: [38][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.4952 (Avg-Loss1.4952)\tAcc 47.6562 (Avg-Acc47.6562)\n",
      "Epoch: [38][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.4753 (Avg-Loss1.5194)\tAcc 48.2143 (Avg-Acc46.3900)\n",
      "EPOCH: 38 Validation Results: Acc 46.390 Loss: 1.5194\n",
      "Best Accuracy: 46.39.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [39][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.5111 (Avg-Loss1.5111)\tAcc 47.2656 (Avg-Acc47.2656)\n",
      "Epoch: [39][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.5007 (Avg-Loss1.5234)\tAcc 46.1914 (Avg-Acc45.5859)\n",
      "Epoch: [39][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.5365 (Avg-Loss1.5207)\tAcc 46.8750 (Avg-Acc45.7751)\n",
      "Epoch: [39][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.4894 (Avg-Loss1.5213)\tAcc 48.4375 (Avg-Acc45.8496)\n",
      "Epoch: [39][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.5419 (Avg-Loss1.5189)\tAcc 46.4844 (Avg-Acc45.8245)\n",
      "Epoch: [39][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.5107 (Avg-Loss1.5189)\tAcc 54.6875 (Avg-Acc45.9425)\n",
      "EPOCH: 39 train Results: Acc 45.943 Loss: 1.5189\n",
      "Epoch: [39][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.4872 (Avg-Loss1.4872)\tAcc 47.2656 (Avg-Acc47.2656)\n",
      "Epoch: [39][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.4682 (Avg-Loss1.5130)\tAcc 47.5765 (Avg-Acc46.3800)\n",
      "EPOCH: 39 Validation Results: Acc 46.380 Loss: 1.5130\n",
      "Best Accuracy: 46.38.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [40][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.5473 (Avg-Loss1.5473)\tAcc 46.8750 (Avg-Acc46.8750)\n",
      "Epoch: [40][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.5249 (Avg-Loss1.5041)\tAcc 46.4844 (Avg-Acc46.4844)\n",
      "Epoch: [40][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.5204 (Avg-Loss1.5114)\tAcc 44.7266 (Avg-Acc45.8933)\n",
      "Epoch: [40][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.4688 (Avg-Loss1.5099)\tAcc 46.8750 (Avg-Acc45.7659)\n",
      "Epoch: [40][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.5036 (Avg-Loss1.5060)\tAcc 45.9961 (Avg-Acc45.9855)\n",
      "Epoch: [40][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.5267 (Avg-Loss1.5068)\tAcc 51.5625 (Avg-Acc45.9675)\n",
      "EPOCH: 40 train Results: Acc 45.968 Loss: 1.5068\n",
      "Epoch: [40][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.4804 (Avg-Loss1.4804)\tAcc 47.7539 (Avg-Acc47.7539)\n",
      "Epoch: [40][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.4593 (Avg-Loss1.5050)\tAcc 48.5969 (Avg-Acc46.7700)\n",
      "EPOCH: 40 Validation Results: Acc 46.770 Loss: 1.5050\n",
      "Best Accuracy: 46.77.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [41][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.5226 (Avg-Loss1.5226)\tAcc 47.4609 (Avg-Acc47.4609)\n",
      "Epoch: [41][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.5009 (Avg-Loss1.5072)\tAcc 46.4844 (Avg-Acc46.5918)\n",
      "Epoch: [41][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.5055 (Avg-Loss1.4986)\tAcc 44.6289 (Avg-Acc46.7671)\n",
      "Epoch: [41][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.4845 (Avg-Loss1.4986)\tAcc 45.1172 (Avg-Acc46.3937)\n",
      "Epoch: [41][36/39]\tTime 0.026 (Avg-Time0.022)\t Loss 1.5367 (Avg-Loss1.4998)\tAcc 45.7031 (Avg-Acc46.3894)\n",
      "Epoch: [41][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.7011 (Avg-Loss1.5004)\tAcc 31.2500 (Avg-Acc46.3625)\n",
      "EPOCH: 41 train Results: Acc 46.362 Loss: 1.5004\n",
      "Epoch: [41][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.4701 (Avg-Loss1.4701)\tAcc 47.9492 (Avg-Acc47.9492)\n",
      "Epoch: [41][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.4497 (Avg-Loss1.4954)\tAcc 48.9796 (Avg-Acc47.1400)\n",
      "EPOCH: 41 Validation Results: Acc 47.140 Loss: 1.4954\n",
      "Best Accuracy: 47.14.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [42][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.5019 (Avg-Loss1.5019)\tAcc 45.4102 (Avg-Acc45.4102)\n",
      "Epoch: [42][9/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.5079 (Avg-Loss1.4895)\tAcc 45.8008 (Avg-Acc46.8164)\n",
      "Epoch: [42][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.4652 (Avg-Loss1.4864)\tAcc 46.2891 (Avg-Acc46.9521)\n",
      "Epoch: [42][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.5090 (Avg-Loss1.4868)\tAcc 45.8008 (Avg-Acc46.8924)\n",
      "Epoch: [42][36/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.4708 (Avg-Loss1.4887)\tAcc 47.7539 (Avg-Acc46.8856)\n",
      "Epoch: [42][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.4218 (Avg-Loss1.4894)\tAcc 42.1875 (Avg-Acc46.8300)\n",
      "EPOCH: 42 train Results: Acc 46.830 Loss: 1.4894\n",
      "Epoch: [42][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.4663 (Avg-Loss1.4663)\tAcc 48.8281 (Avg-Acc48.8281)\n",
      "Epoch: [42][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.4460 (Avg-Loss1.4909)\tAcc 48.9796 (Avg-Acc47.4200)\n",
      "EPOCH: 42 Validation Results: Acc 47.420 Loss: 1.4909\n",
      "Best Accuracy: 47.42.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [43][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.5073 (Avg-Loss1.5073)\tAcc 45.4102 (Avg-Acc45.4102)\n",
      "Epoch: [43][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.4623 (Avg-Loss1.4962)\tAcc 46.1914 (Avg-Acc46.3672)\n",
      "Epoch: [43][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.4723 (Avg-Loss1.4919)\tAcc 48.9258 (Avg-Acc46.8236)\n",
      "Epoch: [43][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.4851 (Avg-Loss1.4818)\tAcc 45.8008 (Avg-Acc47.3040)\n",
      "Epoch: [43][36/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.4524 (Avg-Loss1.4855)\tAcc 47.5586 (Avg-Acc47.1891)\n",
      "Epoch: [43][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.5581 (Avg-Loss1.4849)\tAcc 45.3125 (Avg-Acc47.2450)\n",
      "EPOCH: 43 train Results: Acc 47.245 Loss: 1.4849\n",
      "Epoch: [43][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.4583 (Avg-Loss1.4583)\tAcc 48.8281 (Avg-Acc48.8281)\n",
      "Epoch: [43][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.4376 (Avg-Loss1.4835)\tAcc 48.9796 (Avg-Acc47.6200)\n",
      "EPOCH: 43 Validation Results: Acc 47.620 Loss: 1.4835\n",
      "Best Accuracy: 47.62.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [44][0/39]\tTime 0.020 (Avg-Time0.020)\t Loss 1.4446 (Avg-Loss1.4446)\tAcc 47.6562 (Avg-Acc47.6562)\n",
      "Epoch: [44][9/39]\tTime 0.020 (Avg-Time0.021)\t Loss 1.4381 (Avg-Loss1.4750)\tAcc 47.4609 (Avg-Acc47.2070)\n",
      "Epoch: [44][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.4185 (Avg-Loss1.4796)\tAcc 49.5117 (Avg-Acc46.9829)\n",
      "Epoch: [44][27/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.4559 (Avg-Loss1.4807)\tAcc 47.3633 (Avg-Acc46.7808)\n",
      "Epoch: [44][36/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.4721 (Avg-Loss1.4765)\tAcc 47.6562 (Avg-Acc47.0096)\n",
      "Epoch: [44][39/39]\tTime 0.005 (Avg-Time0.022)\t Loss 1.3127 (Avg-Loss1.4737)\tAcc 46.8750 (Avg-Acc47.1100)\n",
      "EPOCH: 44 train Results: Acc 47.110 Loss: 1.4737\n",
      "Epoch: [44][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.4509 (Avg-Loss1.4509)\tAcc 49.2188 (Avg-Acc49.2188)\n",
      "Epoch: [44][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.4308 (Avg-Loss1.4761)\tAcc 49.6173 (Avg-Acc47.8800)\n",
      "EPOCH: 44 Validation Results: Acc 47.880 Loss: 1.4761\n",
      "Best Accuracy: 47.88.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [45][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.4466 (Avg-Loss1.4466)\tAcc 49.4141 (Avg-Acc49.4141)\n",
      "Epoch: [45][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.4907 (Avg-Loss1.4586)\tAcc 45.8008 (Avg-Acc47.6953)\n",
      "Epoch: [45][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.4887 (Avg-Loss1.4560)\tAcc 45.8984 (Avg-Acc47.9287)\n",
      "Epoch: [45][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.4643 (Avg-Loss1.4614)\tAcc 47.4609 (Avg-Acc47.7225)\n",
      "Epoch: [45][36/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.4755 (Avg-Loss1.4667)\tAcc 46.6797 (Avg-Acc47.5560)\n",
      "Epoch: [45][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.6061 (Avg-Loss1.4672)\tAcc 48.4375 (Avg-Acc47.5350)\n",
      "EPOCH: 45 train Results: Acc 47.535 Loss: 1.4672\n",
      "Epoch: [45][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.4435 (Avg-Loss1.4435)\tAcc 49.4141 (Avg-Acc49.4141)\n",
      "Epoch: [45][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.4185 (Avg-Loss1.4673)\tAcc 50.2551 (Avg-Acc48.0500)\n",
      "EPOCH: 45 Validation Results: Acc 48.050 Loss: 1.4673\n",
      "Best Accuracy: 48.05.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [46][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.3950 (Avg-Loss1.3950)\tAcc 49.9023 (Avg-Acc49.9023)\n",
      "Epoch: [46][9/39]\tTime 0.022 (Avg-Time0.021)\t Loss 1.4840 (Avg-Loss1.4433)\tAcc 49.7070 (Avg-Acc48.5742)\n",
      "Epoch: [46][18/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.4932 (Avg-Loss1.4577)\tAcc 45.2148 (Avg-Acc48.0623)\n",
      "Epoch: [46][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.4922 (Avg-Loss1.4557)\tAcc 45.8008 (Avg-Acc48.1585)\n",
      "Epoch: [46][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.4568 (Avg-Loss1.4556)\tAcc 46.2891 (Avg-Acc48.1472)\n",
      "Epoch: [46][39/39]\tTime 0.004 (Avg-Time0.021)\t Loss 1.2692 (Avg-Loss1.4559)\tAcc 57.8125 (Avg-Acc48.0975)\n",
      "EPOCH: 46 train Results: Acc 48.097 Loss: 1.4559\n",
      "Epoch: [46][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.4353 (Avg-Loss1.4353)\tAcc 49.8047 (Avg-Acc49.8047)\n",
      "Epoch: [46][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.4124 (Avg-Loss1.4597)\tAcc 50.8929 (Avg-Acc48.5600)\n",
      "EPOCH: 46 Validation Results: Acc 48.560 Loss: 1.4597\n",
      "Best Accuracy: 48.56.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [47][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.4618 (Avg-Loss1.4618)\tAcc 47.4609 (Avg-Acc47.4609)\n",
      "Epoch: [47][9/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.4823 (Avg-Loss1.4437)\tAcc 48.3398 (Avg-Acc48.5742)\n",
      "Epoch: [47][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.4696 (Avg-Loss1.4586)\tAcc 45.9961 (Avg-Acc47.6254)\n",
      "Epoch: [47][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.5017 (Avg-Loss1.4577)\tAcc 46.5820 (Avg-Acc47.7888)\n",
      "Epoch: [47][36/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.4054 (Avg-Loss1.4547)\tAcc 50.8789 (Avg-Acc47.9202)\n",
      "Epoch: [47][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.3539 (Avg-Loss1.4564)\tAcc 56.2500 (Avg-Acc47.9025)\n",
      "EPOCH: 47 train Results: Acc 47.903 Loss: 1.4564\n",
      "Epoch: [47][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.4276 (Avg-Loss1.4276)\tAcc 50.0000 (Avg-Acc50.0000)\n",
      "Epoch: [47][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.4118 (Avg-Loss1.4544)\tAcc 51.0204 (Avg-Acc48.6800)\n",
      "EPOCH: 47 Validation Results: Acc 48.680 Loss: 1.4544\n",
      "Best Accuracy: 48.68.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [48][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.4462 (Avg-Loss1.4462)\tAcc 46.6797 (Avg-Acc46.6797)\n",
      "Epoch: [48][9/39]\tTime 0.022 (Avg-Time0.021)\t Loss 1.4132 (Avg-Loss1.4422)\tAcc 49.5117 (Avg-Acc48.1738)\n",
      "Epoch: [48][18/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.4745 (Avg-Loss1.4439)\tAcc 47.6562 (Avg-Acc48.5043)\n",
      "Epoch: [48][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.4315 (Avg-Loss1.4430)\tAcc 49.0234 (Avg-Acc48.5142)\n",
      "Epoch: [48][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.4695 (Avg-Loss1.4415)\tAcc 48.9258 (Avg-Acc48.6513)\n",
      "Epoch: [48][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.4887 (Avg-Loss1.4418)\tAcc 42.1875 (Avg-Acc48.6550)\n",
      "EPOCH: 48 train Results: Acc 48.655 Loss: 1.4418\n",
      "Epoch: [48][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.4210 (Avg-Loss1.4210)\tAcc 50.1953 (Avg-Acc50.1953)\n",
      "Epoch: [48][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.4032 (Avg-Loss1.4475)\tAcc 50.5102 (Avg-Acc48.7200)\n",
      "EPOCH: 48 Validation Results: Acc 48.720 Loss: 1.4475\n",
      "Best Accuracy: 48.72.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [49][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.4344 (Avg-Loss1.4344)\tAcc 50.0000 (Avg-Acc50.0000)\n",
      "Epoch: [49][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.4176 (Avg-Loss1.4385)\tAcc 49.3164 (Avg-Acc48.8281)\n",
      "Epoch: [49][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.4055 (Avg-Loss1.4409)\tAcc 49.1211 (Avg-Acc48.5249)\n",
      "Epoch: [49][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.4360 (Avg-Loss1.4391)\tAcc 48.4375 (Avg-Acc48.4270)\n",
      "Epoch: [49][36/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.4327 (Avg-Loss1.4352)\tAcc 49.8047 (Avg-Acc48.5747)\n",
      "Epoch: [49][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.3112 (Avg-Loss1.4342)\tAcc 64.0625 (Avg-Acc48.6775)\n",
      "EPOCH: 49 train Results: Acc 48.678 Loss: 1.4342\n",
      "Epoch: [49][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.4158 (Avg-Loss1.4158)\tAcc 50.5859 (Avg-Acc50.5859)\n",
      "Epoch: [49][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.3959 (Avg-Loss1.4418)\tAcc 51.0204 (Avg-Acc49.1400)\n",
      "EPOCH: 49 Validation Results: Acc 49.140 Loss: 1.4418\n",
      "Best Accuracy: 49.14.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [50][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.4430 (Avg-Loss1.4430)\tAcc 49.0234 (Avg-Acc49.0234)\n",
      "Epoch: [50][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.4208 (Avg-Loss1.4272)\tAcc 50.5859 (Avg-Acc49.3164)\n",
      "Epoch: [50][18/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.4408 (Avg-Loss1.4237)\tAcc 48.0469 (Avg-Acc49.2547)\n",
      "Epoch: [50][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.4032 (Avg-Loss1.4292)\tAcc 48.7305 (Avg-Acc48.7863)\n",
      "Epoch: [50][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.4086 (Avg-Loss1.4269)\tAcc 49.9023 (Avg-Acc49.0261)\n",
      "Epoch: [50][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.5130 (Avg-Loss1.4270)\tAcc 40.6250 (Avg-Acc49.0475)\n",
      "EPOCH: 50 train Results: Acc 49.047 Loss: 1.4270\n",
      "Epoch: [50][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.4068 (Avg-Loss1.4068)\tAcc 50.7812 (Avg-Acc50.7812)\n",
      "Epoch: [50][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.3874 (Avg-Loss1.4322)\tAcc 51.4031 (Avg-Acc49.4100)\n",
      "EPOCH: 50 Validation Results: Acc 49.410 Loss: 1.4322\n",
      "Best Accuracy: 49.41.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [51][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.4252 (Avg-Loss1.4252)\tAcc 48.1445 (Avg-Acc48.1445)\n",
      "Epoch: [51][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.4344 (Avg-Loss1.4157)\tAcc 48.1445 (Avg-Acc49.3262)\n",
      "Epoch: [51][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.3951 (Avg-Loss1.4188)\tAcc 49.8047 (Avg-Acc49.3832)\n",
      "Epoch: [51][27/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.4363 (Avg-Loss1.4192)\tAcc 49.6094 (Avg-Acc49.6233)\n",
      "Epoch: [51][36/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.4410 (Avg-Loss1.4169)\tAcc 47.8516 (Avg-Acc49.6886)\n",
      "Epoch: [51][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.6656 (Avg-Loss1.4183)\tAcc 39.0625 (Avg-Acc49.6125)\n",
      "EPOCH: 51 train Results: Acc 49.612 Loss: 1.4183\n",
      "Epoch: [51][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.3966 (Avg-Loss1.3966)\tAcc 50.8789 (Avg-Acc50.8789)\n",
      "Epoch: [51][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.3794 (Avg-Loss1.4237)\tAcc 51.4031 (Avg-Acc49.6200)\n",
      "EPOCH: 51 Validation Results: Acc 49.620 Loss: 1.4237\n",
      "Best Accuracy: 49.62.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [52][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.4028 (Avg-Loss1.4028)\tAcc 51.7578 (Avg-Acc51.7578)\n",
      "Epoch: [52][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.4647 (Avg-Loss1.4031)\tAcc 48.9258 (Avg-Acc50.3516)\n",
      "Epoch: [52][18/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.4592 (Avg-Loss1.4038)\tAcc 46.9727 (Avg-Acc50.0925)\n",
      "Epoch: [52][27/39]\tTime 0.027 (Avg-Time0.023)\t Loss 1.4193 (Avg-Loss1.4056)\tAcc 49.7070 (Avg-Acc49.9198)\n",
      "Epoch: [52][36/39]\tTime 0.021 (Avg-Time0.024)\t Loss 1.4580 (Avg-Loss1.4093)\tAcc 49.8047 (Avg-Acc49.8311)\n",
      "Epoch: [52][39/39]\tTime 0.003 (Avg-Time0.023)\t Loss 1.3058 (Avg-Loss1.4102)\tAcc 56.2500 (Avg-Acc49.7725)\n",
      "EPOCH: 52 train Results: Acc 49.773 Loss: 1.4102\n",
      "Epoch: [52][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.3891 (Avg-Loss1.3891)\tAcc 51.2695 (Avg-Acc51.2695)\n",
      "Epoch: [52][9/9]\tTime 0.008 (Avg-Time0.008)\t Loss 1.3737 (Avg-Loss1.4174)\tAcc 52.1684 (Avg-Acc50.0200)\n",
      "EPOCH: 52 Validation Results: Acc 50.020 Loss: 1.4174\n",
      "Best Accuracy: 50.02.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [53][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.4379 (Avg-Loss1.4379)\tAcc 47.9492 (Avg-Acc47.9492)\n",
      "Epoch: [53][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.3911 (Avg-Loss1.4009)\tAcc 50.3906 (Avg-Acc50.3418)\n",
      "Epoch: [53][18/39]\tTime 0.028 (Avg-Time0.022)\t Loss 1.4145 (Avg-Loss1.4020)\tAcc 49.1211 (Avg-Acc50.2262)\n",
      "Epoch: [53][27/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.3967 (Avg-Loss1.4008)\tAcc 52.3438 (Avg-Acc50.2197)\n",
      "Epoch: [53][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.3694 (Avg-Loss1.3995)\tAcc 50.7812 (Avg-Acc50.0897)\n",
      "Epoch: [53][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.4830 (Avg-Loss1.4004)\tAcc 42.1875 (Avg-Acc50.0125)\n",
      "EPOCH: 53 train Results: Acc 50.013 Loss: 1.4004\n",
      "Epoch: [53][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.3829 (Avg-Loss1.3829)\tAcc 51.1719 (Avg-Acc51.1719)\n",
      "Epoch: [53][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.3652 (Avg-Loss1.4104)\tAcc 52.0408 (Avg-Acc50.0400)\n",
      "EPOCH: 53 Validation Results: Acc 50.040 Loss: 1.4104\n",
      "Best Accuracy: 50.04.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [54][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.3541 (Avg-Loss1.3541)\tAcc 51.8555 (Avg-Acc51.8555)\n",
      "Epoch: [54][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.3986 (Avg-Loss1.3907)\tAcc 49.2188 (Avg-Acc50.4102)\n",
      "Epoch: [54][18/39]\tTime 0.020 (Avg-Time0.023)\t Loss 1.3910 (Avg-Loss1.3956)\tAcc 51.9531 (Avg-Acc50.1491)\n",
      "Epoch: [54][27/39]\tTime 0.020 (Avg-Time0.023)\t Loss 1.4650 (Avg-Loss1.3969)\tAcc 47.5586 (Avg-Acc50.0698)\n",
      "Epoch: [54][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.3472 (Avg-Loss1.3965)\tAcc 51.5625 (Avg-Acc50.0554)\n",
      "Epoch: [54][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.5082 (Avg-Loss1.3961)\tAcc 51.5625 (Avg-Acc50.1300)\n",
      "EPOCH: 54 train Results: Acc 50.130 Loss: 1.3961\n",
      "Epoch: [54][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.3799 (Avg-Loss1.3799)\tAcc 51.3672 (Avg-Acc51.3672)\n",
      "Epoch: [54][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.3657 (Avg-Loss1.4069)\tAcc 52.4235 (Avg-Acc50.3000)\n",
      "EPOCH: 54 Validation Results: Acc 50.300 Loss: 1.4069\n",
      "Best Accuracy: 50.3.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [55][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.4203 (Avg-Loss1.4203)\tAcc 48.4375 (Avg-Acc48.4375)\n",
      "Epoch: [55][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.3782 (Avg-Loss1.3799)\tAcc 50.9766 (Avg-Acc50.6055)\n",
      "Epoch: [55][18/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.3721 (Avg-Loss1.3906)\tAcc 52.4414 (Avg-Acc50.3238)\n",
      "Epoch: [55][27/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.4218 (Avg-Loss1.3940)\tAcc 49.8047 (Avg-Acc50.2476)\n",
      "Epoch: [55][36/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.3682 (Avg-Loss1.3903)\tAcc 50.1953 (Avg-Acc50.4566)\n",
      "Epoch: [55][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.2860 (Avg-Loss1.3903)\tAcc 62.5000 (Avg-Acc50.4300)\n",
      "EPOCH: 55 train Results: Acc 50.430 Loss: 1.3903\n",
      "Epoch: [55][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.3768 (Avg-Loss1.3768)\tAcc 50.9766 (Avg-Acc50.9766)\n",
      "Epoch: [55][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.3593 (Avg-Loss1.4013)\tAcc 52.0408 (Avg-Acc50.4400)\n",
      "EPOCH: 55 Validation Results: Acc 50.440 Loss: 1.4013\n",
      "Best Accuracy: 50.44.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [56][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.3541 (Avg-Loss1.3541)\tAcc 53.6133 (Avg-Acc53.6133)\n",
      "Epoch: [56][9/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.4264 (Avg-Loss1.3895)\tAcc 48.0469 (Avg-Acc50.2148)\n",
      "Epoch: [56][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.4001 (Avg-Loss1.3804)\tAcc 50.1953 (Avg-Acc50.6168)\n",
      "Epoch: [56][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.3981 (Avg-Loss1.3813)\tAcc 50.4883 (Avg-Acc50.7150)\n",
      "Epoch: [56][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.3876 (Avg-Loss1.3856)\tAcc 50.1953 (Avg-Acc50.6255)\n",
      "Epoch: [56][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.5351 (Avg-Loss1.3879)\tAcc 43.7500 (Avg-Acc50.4575)\n",
      "EPOCH: 56 train Results: Acc 50.458 Loss: 1.3879\n",
      "Epoch: [56][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.3678 (Avg-Loss1.3678)\tAcc 52.0508 (Avg-Acc52.0508)\n",
      "Epoch: [56][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.3524 (Avg-Loss1.3938)\tAcc 52.6786 (Avg-Acc51.0100)\n",
      "EPOCH: 56 Validation Results: Acc 51.010 Loss: 1.3938\n",
      "Best Accuracy: 51.01.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [57][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.3489 (Avg-Loss1.3489)\tAcc 50.6836 (Avg-Acc50.6836)\n",
      "Epoch: [57][9/39]\tTime 0.026 (Avg-Time0.022)\t Loss 1.3203 (Avg-Loss1.3650)\tAcc 52.8320 (Avg-Acc51.4746)\n",
      "Epoch: [57][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.3773 (Avg-Loss1.3681)\tAcc 51.3672 (Avg-Acc51.3312)\n",
      "Epoch: [57][27/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.3870 (Avg-Loss1.3729)\tAcc 48.6328 (Avg-Acc50.9487)\n",
      "Epoch: [57][36/39]\tTime 0.026 (Avg-Time0.022)\t Loss 1.4092 (Avg-Loss1.3780)\tAcc 50.0000 (Avg-Acc50.7522)\n",
      "Epoch: [57][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.4230 (Avg-Loss1.3781)\tAcc 48.4375 (Avg-Acc50.7825)\n",
      "EPOCH: 57 train Results: Acc 50.782 Loss: 1.3781\n",
      "Epoch: [57][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.3610 (Avg-Loss1.3610)\tAcc 51.8555 (Avg-Acc51.8555)\n",
      "Epoch: [57][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.3449 (Avg-Loss1.3861)\tAcc 52.9337 (Avg-Acc51.2500)\n",
      "EPOCH: 57 Validation Results: Acc 51.250 Loss: 1.3861\n",
      "Best Accuracy: 51.25.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [58][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.3898 (Avg-Loss1.3898)\tAcc 48.1445 (Avg-Acc48.1445)\n",
      "Epoch: [58][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.3510 (Avg-Loss1.3549)\tAcc 53.1250 (Avg-Acc51.7285)\n",
      "Epoch: [58][18/39]\tTime 0.032 (Avg-Time0.023)\t Loss 1.3626 (Avg-Loss1.3577)\tAcc 51.1719 (Avg-Acc51.4340)\n",
      "Epoch: [58][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.3157 (Avg-Loss1.3600)\tAcc 50.1953 (Avg-Acc51.1265)\n",
      "Epoch: [58][36/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.4387 (Avg-Loss1.3613)\tAcc 49.3164 (Avg-Acc51.1244)\n",
      "Epoch: [58][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.4578 (Avg-Loss1.3624)\tAcc 53.1250 (Avg-Acc51.0900)\n",
      "EPOCH: 58 train Results: Acc 51.090 Loss: 1.3624\n",
      "Epoch: [58][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.3536 (Avg-Loss1.3536)\tAcc 52.0508 (Avg-Acc52.0508)\n",
      "Epoch: [58][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.3415 (Avg-Loss1.3827)\tAcc 53.4439 (Avg-Acc51.5300)\n",
      "EPOCH: 58 Validation Results: Acc 51.530 Loss: 1.3827\n",
      "Best Accuracy: 51.53.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [59][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.3696 (Avg-Loss1.3696)\tAcc 51.6602 (Avg-Acc51.6602)\n",
      "Epoch: [59][9/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.3462 (Avg-Loss1.3610)\tAcc 50.3906 (Avg-Acc51.5332)\n",
      "Epoch: [59][18/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.3879 (Avg-Loss1.3581)\tAcc 49.7070 (Avg-Acc51.4546)\n",
      "Epoch: [59][27/39]\tTime 0.020 (Avg-Time0.023)\t Loss 1.3129 (Avg-Loss1.3565)\tAcc 53.5156 (Avg-Acc51.4544)\n",
      "Epoch: [59][36/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.4073 (Avg-Loss1.3605)\tAcc 49.4141 (Avg-Acc51.3355)\n",
      "Epoch: [59][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.3946 (Avg-Loss1.3612)\tAcc 45.3125 (Avg-Acc51.3225)\n",
      "EPOCH: 59 train Results: Acc 51.322 Loss: 1.3612\n",
      "Epoch: [59][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.3502 (Avg-Loss1.3502)\tAcc 52.8320 (Avg-Acc52.8320)\n",
      "Epoch: [59][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.3306 (Avg-Loss1.3760)\tAcc 54.2092 (Avg-Acc51.6600)\n",
      "EPOCH: 59 Validation Results: Acc 51.660 Loss: 1.3760\n",
      "Best Accuracy: 51.66.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [60][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.3235 (Avg-Loss1.3235)\tAcc 54.1992 (Avg-Acc54.1992)\n",
      "Epoch: [60][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.3999 (Avg-Loss1.3426)\tAcc 50.5859 (Avg-Acc52.4609)\n",
      "Epoch: [60][18/39]\tTime 0.020 (Avg-Time0.021)\t Loss 1.3091 (Avg-Loss1.3433)\tAcc 53.0273 (Avg-Acc52.3951)\n",
      "Epoch: [60][27/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.3590 (Avg-Loss1.3476)\tAcc 52.4414 (Avg-Acc52.0020)\n",
      "Epoch: [60][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.3566 (Avg-Loss1.3493)\tAcc 50.2930 (Avg-Acc51.8344)\n",
      "Epoch: [60][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.5167 (Avg-Loss1.3523)\tAcc 40.6250 (Avg-Acc51.6700)\n",
      "EPOCH: 60 train Results: Acc 51.670 Loss: 1.3523\n",
      "Epoch: [60][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.3434 (Avg-Loss1.3434)\tAcc 53.3203 (Avg-Acc53.3203)\n",
      "Epoch: [60][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.3244 (Avg-Loss1.3689)\tAcc 53.9541 (Avg-Acc51.8600)\n",
      "EPOCH: 60 Validation Results: Acc 51.860 Loss: 1.3689\n",
      "Best Accuracy: 51.86.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [61][0/39]\tTime 0.020 (Avg-Time0.020)\t Loss 1.3542 (Avg-Loss1.3542)\tAcc 51.2695 (Avg-Acc51.2695)\n",
      "Epoch: [61][9/39]\tTime 0.032 (Avg-Time0.023)\t Loss 1.3536 (Avg-Loss1.3416)\tAcc 49.9023 (Avg-Acc51.9824)\n",
      "Epoch: [61][18/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.3189 (Avg-Loss1.3420)\tAcc 53.7109 (Avg-Acc51.7938)\n",
      "Epoch: [61][27/39]\tTime 0.026 (Avg-Time0.026)\t Loss 1.3442 (Avg-Loss1.3462)\tAcc 52.6367 (Avg-Acc51.6218)\n",
      "Epoch: [61][36/39]\tTime 0.023 (Avg-Time0.025)\t Loss 1.3782 (Avg-Loss1.3453)\tAcc 49.2188 (Avg-Acc51.7182)\n",
      "Epoch: [61][39/39]\tTime 0.004 (Avg-Time0.025)\t Loss 1.2121 (Avg-Loss1.3465)\tAcc 60.9375 (Avg-Acc51.7400)\n",
      "EPOCH: 61 train Results: Acc 51.740 Loss: 1.3465\n",
      "Epoch: [61][0/9]\tTime 0.010 (Avg-Time0.010)\t Loss 1.3410 (Avg-Loss1.3410)\tAcc 53.6133 (Avg-Acc53.6133)\n",
      "Epoch: [61][9/9]\tTime 0.004 (Avg-Time0.007)\t Loss 1.3247 (Avg-Loss1.3655)\tAcc 53.3163 (Avg-Acc52.0200)\n",
      "EPOCH: 61 Validation Results: Acc 52.020 Loss: 1.3655\n",
      "Best Accuracy: 52.02.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [62][0/39]\tTime 0.026 (Avg-Time0.026)\t Loss 1.3487 (Avg-Loss1.3487)\tAcc 52.1484 (Avg-Acc52.1484)\n",
      "Epoch: [62][9/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.3584 (Avg-Loss1.3302)\tAcc 49.3164 (Avg-Acc52.4414)\n",
      "Epoch: [62][18/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.3627 (Avg-Loss1.3337)\tAcc 50.6836 (Avg-Acc52.1998)\n",
      "Epoch: [62][27/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.3304 (Avg-Loss1.3344)\tAcc 50.0977 (Avg-Acc52.1101)\n",
      "Epoch: [62][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.3398 (Avg-Loss1.3351)\tAcc 53.6133 (Avg-Acc52.2435)\n",
      "Epoch: [62][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.3244 (Avg-Loss1.3381)\tAcc 51.5625 (Avg-Acc52.2150)\n",
      "EPOCH: 62 train Results: Acc 52.215 Loss: 1.3381\n",
      "Epoch: [62][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.3303 (Avg-Loss1.3303)\tAcc 54.0039 (Avg-Acc54.0039)\n",
      "Epoch: [62][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.3242 (Avg-Loss1.3601)\tAcc 53.4439 (Avg-Acc52.0900)\n",
      "EPOCH: 62 Validation Results: Acc 52.090 Loss: 1.3601\n",
      "Best Accuracy: 52.09.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [63][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.3155 (Avg-Loss1.3155)\tAcc 50.8789 (Avg-Acc50.8789)\n",
      "Epoch: [63][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.3201 (Avg-Loss1.3200)\tAcc 51.4648 (Avg-Acc52.7051)\n",
      "Epoch: [63][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.3684 (Avg-Loss1.3297)\tAcc 49.9023 (Avg-Acc52.4774)\n",
      "Epoch: [63][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.3154 (Avg-Loss1.3254)\tAcc 53.9062 (Avg-Acc52.6890)\n",
      "Epoch: [63][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.3402 (Avg-Loss1.3315)\tAcc 54.0039 (Avg-Acc52.5602)\n",
      "Epoch: [63][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.3556 (Avg-Loss1.3322)\tAcc 53.1250 (Avg-Acc52.5150)\n",
      "EPOCH: 63 train Results: Acc 52.515 Loss: 1.3322\n",
      "Epoch: [63][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.3247 (Avg-Loss1.3247)\tAcc 54.5898 (Avg-Acc54.5898)\n",
      "Epoch: [63][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.3150 (Avg-Loss1.3532)\tAcc 53.9541 (Avg-Acc52.4000)\n",
      "EPOCH: 63 Validation Results: Acc 52.400 Loss: 1.3532\n",
      "Best Accuracy: 52.4.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [64][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.3187 (Avg-Loss1.3187)\tAcc 52.6367 (Avg-Acc52.6367)\n",
      "Epoch: [64][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.3285 (Avg-Loss1.3285)\tAcc 52.8320 (Avg-Acc52.7246)\n",
      "Epoch: [64][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.2946 (Avg-Loss1.3218)\tAcc 55.3711 (Avg-Acc53.0016)\n",
      "Epoch: [64][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.3583 (Avg-Loss1.3239)\tAcc 51.4648 (Avg-Acc52.8809)\n",
      "Epoch: [64][36/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.3351 (Avg-Loss1.3259)\tAcc 51.5625 (Avg-Acc52.7660)\n",
      "Epoch: [64][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.3683 (Avg-Loss1.3267)\tAcc 50.0000 (Avg-Acc52.7375)\n",
      "EPOCH: 64 train Results: Acc 52.737 Loss: 1.3267\n",
      "Epoch: [64][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.3215 (Avg-Loss1.3215)\tAcc 54.4922 (Avg-Acc54.4922)\n",
      "Epoch: [64][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.3161 (Avg-Loss1.3496)\tAcc 53.6990 (Avg-Acc52.3800)\n",
      "EPOCH: 64 Validation Results: Acc 52.380 Loss: 1.3496\n",
      "Best Accuracy: 52.38.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [65][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.2980 (Avg-Loss1.2980)\tAcc 53.3203 (Avg-Acc53.3203)\n",
      "Epoch: [65][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.3073 (Avg-Loss1.3252)\tAcc 53.1250 (Avg-Acc52.5293)\n",
      "Epoch: [65][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.3535 (Avg-Loss1.3202)\tAcc 51.4648 (Avg-Acc52.6778)\n",
      "Epoch: [65][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.2908 (Avg-Loss1.3144)\tAcc 54.3945 (Avg-Acc53.0587)\n",
      "Epoch: [65][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.3491 (Avg-Loss1.3151)\tAcc 48.9258 (Avg-Acc52.9508)\n",
      "Epoch: [65][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.5330 (Avg-Loss1.3158)\tAcc 35.9375 (Avg-Acc52.8875)\n",
      "EPOCH: 65 train Results: Acc 52.888 Loss: 1.3158\n",
      "Epoch: [65][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.3116 (Avg-Loss1.3116)\tAcc 55.6641 (Avg-Acc55.6641)\n",
      "Epoch: [65][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.3092 (Avg-Loss1.3442)\tAcc 54.7194 (Avg-Acc52.6700)\n",
      "EPOCH: 65 Validation Results: Acc 52.670 Loss: 1.3442\n",
      "Best Accuracy: 52.67.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [66][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.3096 (Avg-Loss1.3096)\tAcc 53.6133 (Avg-Acc53.6133)\n",
      "Epoch: [66][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.3229 (Avg-Loss1.3073)\tAcc 51.3672 (Avg-Acc52.8418)\n",
      "Epoch: [66][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.2803 (Avg-Loss1.3029)\tAcc 54.8828 (Avg-Acc53.1250)\n",
      "Epoch: [66][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.2358 (Avg-Loss1.3046)\tAcc 54.1016 (Avg-Acc53.0343)\n",
      "Epoch: [66][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.3271 (Avg-Loss1.3154)\tAcc 53.5156 (Avg-Acc52.5971)\n",
      "Epoch: [66][39/39]\tTime 0.002 (Avg-Time0.021)\t Loss 1.3625 (Avg-Loss1.3146)\tAcc 56.2500 (Avg-Acc52.6800)\n",
      "EPOCH: 66 train Results: Acc 52.680 Loss: 1.3146\n",
      "Epoch: [66][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.3138 (Avg-Loss1.3138)\tAcc 55.2734 (Avg-Acc55.2734)\n",
      "Epoch: [66][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.3053 (Avg-Loss1.3395)\tAcc 55.2296 (Avg-Acc52.8800)\n",
      "EPOCH: 66 Validation Results: Acc 52.880 Loss: 1.3395\n",
      "Best Accuracy: 52.88.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [67][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.3240 (Avg-Loss1.3240)\tAcc 52.4414 (Avg-Acc52.4414)\n",
      "Epoch: [67][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.3222 (Avg-Loss1.2884)\tAcc 52.7344 (Avg-Acc54.2676)\n",
      "Epoch: [67][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.3338 (Avg-Loss1.2968)\tAcc 50.2930 (Avg-Acc53.5670)\n",
      "Epoch: [67][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.2708 (Avg-Loss1.3007)\tAcc 53.0273 (Avg-Acc53.3726)\n",
      "Epoch: [67][36/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.2535 (Avg-Loss1.3005)\tAcc 55.0781 (Avg-Acc53.4127)\n",
      "Epoch: [67][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.2925 (Avg-Loss1.3030)\tAcc 54.6875 (Avg-Acc53.3750)\n",
      "EPOCH: 67 train Results: Acc 53.375 Loss: 1.3030\n",
      "Epoch: [67][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.3076 (Avg-Loss1.3076)\tAcc 54.5898 (Avg-Acc54.5898)\n",
      "Epoch: [67][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.3038 (Avg-Loss1.3349)\tAcc 55.1020 (Avg-Acc53.0300)\n",
      "EPOCH: 67 Validation Results: Acc 53.030 Loss: 1.3349\n",
      "Best Accuracy: 53.03.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [68][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.3022 (Avg-Loss1.3022)\tAcc 53.0273 (Avg-Acc53.0273)\n",
      "Epoch: [68][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.2425 (Avg-Loss1.2967)\tAcc 55.5664 (Avg-Acc53.6426)\n",
      "Epoch: [68][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.3094 (Avg-Loss1.2980)\tAcc 54.1016 (Avg-Acc53.6904)\n",
      "Epoch: [68][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.3134 (Avg-Loss1.3020)\tAcc 52.7344 (Avg-Acc53.4075)\n",
      "Epoch: [68][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.2883 (Avg-Loss1.3022)\tAcc 54.1016 (Avg-Acc53.5579)\n",
      "Epoch: [68][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.4128 (Avg-Loss1.3012)\tAcc 46.8750 (Avg-Acc53.5675)\n",
      "EPOCH: 68 train Results: Acc 53.568 Loss: 1.3012\n",
      "Epoch: [68][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.3056 (Avg-Loss1.3056)\tAcc 55.2734 (Avg-Acc55.2734)\n",
      "Epoch: [68][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2957 (Avg-Loss1.3286)\tAcc 54.4643 (Avg-Acc53.1900)\n",
      "EPOCH: 68 Validation Results: Acc 53.190 Loss: 1.3286\n",
      "Best Accuracy: 53.19.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [69][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.2746 (Avg-Loss1.2746)\tAcc 55.4688 (Avg-Acc55.4688)\n",
      "Epoch: [69][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.2960 (Avg-Loss1.2751)\tAcc 54.4922 (Avg-Acc54.9121)\n",
      "Epoch: [69][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.3260 (Avg-Loss1.2903)\tAcc 53.1250 (Avg-Acc54.4305)\n",
      "Epoch: [69][27/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2769 (Avg-Loss1.2924)\tAcc 54.0039 (Avg-Acc54.1120)\n",
      "Epoch: [69][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2583 (Avg-Loss1.2951)\tAcc 56.5430 (Avg-Acc53.9036)\n",
      "Epoch: [69][39/39]\tTime 0.005 (Avg-Time0.022)\t Loss 1.2926 (Avg-Loss1.2946)\tAcc 54.6875 (Avg-Acc53.8650)\n",
      "EPOCH: 69 train Results: Acc 53.865 Loss: 1.2946\n",
      "Epoch: [69][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.3020 (Avg-Loss1.3020)\tAcc 54.1992 (Avg-Acc54.1992)\n",
      "Epoch: [69][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 1.2929 (Avg-Loss1.3257)\tAcc 54.0816 (Avg-Acc53.1400)\n",
      "EPOCH: 69 Validation Results: Acc 53.140 Loss: 1.3257\n",
      "Best Accuracy: 53.14.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [70][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.2794 (Avg-Loss1.2794)\tAcc 56.1523 (Avg-Acc56.1523)\n",
      "Epoch: [70][9/39]\tTime 0.026 (Avg-Time0.024)\t Loss 1.2753 (Avg-Loss1.2659)\tAcc 55.2734 (Avg-Acc55.0684)\n",
      "Epoch: [70][18/39]\tTime 0.029 (Avg-Time0.025)\t Loss 1.2859 (Avg-Loss1.2741)\tAcc 53.8086 (Avg-Acc54.3329)\n",
      "Epoch: [70][27/39]\tTime 0.021 (Avg-Time0.025)\t Loss 1.3671 (Avg-Loss1.2787)\tAcc 52.8320 (Avg-Acc54.3352)\n",
      "Epoch: [70][36/39]\tTime 0.020 (Avg-Time0.024)\t Loss 1.3032 (Avg-Loss1.2857)\tAcc 53.4180 (Avg-Acc54.1992)\n",
      "Epoch: [70][39/39]\tTime 0.004 (Avg-Time0.024)\t Loss 1.2904 (Avg-Loss1.2843)\tAcc 57.8125 (Avg-Acc54.2375)\n",
      "EPOCH: 70 train Results: Acc 54.237 Loss: 1.2843\n",
      "Epoch: [70][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2938 (Avg-Loss1.2938)\tAcc 55.4688 (Avg-Acc55.4688)\n",
      "Epoch: [70][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2915 (Avg-Loss1.3213)\tAcc 53.9541 (Avg-Acc53.3900)\n",
      "EPOCH: 70 Validation Results: Acc 53.390 Loss: 1.3213\n",
      "Best Accuracy: 53.39.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [71][0/39]\tTime 0.032 (Avg-Time0.032)\t Loss 1.2864 (Avg-Loss1.2864)\tAcc 54.7852 (Avg-Acc54.7852)\n",
      "Epoch: [71][9/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.2556 (Avg-Loss1.2780)\tAcc 54.4922 (Avg-Acc54.1797)\n",
      "Epoch: [71][18/39]\tTime 0.030 (Avg-Time0.023)\t Loss 1.2634 (Avg-Loss1.2755)\tAcc 53.6133 (Avg-Acc54.0707)\n",
      "Epoch: [71][27/39]\tTime 0.020 (Avg-Time0.023)\t Loss 1.3210 (Avg-Loss1.2793)\tAcc 52.5391 (Avg-Acc54.0771)\n",
      "Epoch: [71][36/39]\tTime 0.020 (Avg-Time0.023)\t Loss 1.2665 (Avg-Loss1.2793)\tAcc 53.5156 (Avg-Acc54.0145)\n",
      "Epoch: [71][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2386 (Avg-Loss1.2808)\tAcc 53.1250 (Avg-Acc53.9325)\n",
      "EPOCH: 71 train Results: Acc 53.932 Loss: 1.2808\n",
      "Epoch: [71][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2892 (Avg-Loss1.2892)\tAcc 55.6641 (Avg-Acc55.6641)\n",
      "Epoch: [71][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2890 (Avg-Loss1.3153)\tAcc 54.7194 (Avg-Acc53.4800)\n",
      "EPOCH: 71 Validation Results: Acc 53.480 Loss: 1.3153\n",
      "Best Accuracy: 53.48.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [72][0/39]\tTime 0.028 (Avg-Time0.028)\t Loss 1.2259 (Avg-Loss1.2259)\tAcc 56.0547 (Avg-Acc56.0547)\n",
      "Epoch: [72][9/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.2487 (Avg-Loss1.2689)\tAcc 56.2500 (Avg-Acc54.7852)\n",
      "Epoch: [72][18/39]\tTime 0.026 (Avg-Time0.022)\t Loss 1.2787 (Avg-Loss1.2709)\tAcc 53.4180 (Avg-Acc54.3277)\n",
      "Epoch: [72][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.2730 (Avg-Loss1.2688)\tAcc 53.9062 (Avg-Acc54.4050)\n",
      "Epoch: [72][36/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.3142 (Avg-Loss1.2760)\tAcc 51.7578 (Avg-Acc54.1200)\n",
      "Epoch: [72][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.5204 (Avg-Loss1.2757)\tAcc 39.0625 (Avg-Acc54.1200)\n",
      "EPOCH: 72 train Results: Acc 54.120 Loss: 1.2757\n",
      "Epoch: [72][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2838 (Avg-Loss1.2838)\tAcc 56.3477 (Avg-Acc56.3477)\n",
      "Epoch: [72][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2902 (Avg-Loss1.3137)\tAcc 55.1020 (Avg-Acc53.6000)\n",
      "EPOCH: 72 Validation Results: Acc 53.600 Loss: 1.3137\n",
      "Best Accuracy: 53.6.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [73][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.2601 (Avg-Loss1.2601)\tAcc 54.7852 (Avg-Acc54.7852)\n",
      "Epoch: [73][9/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.2476 (Avg-Loss1.2646)\tAcc 56.9336 (Avg-Acc55.0000)\n",
      "Epoch: [73][18/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2502 (Avg-Loss1.2622)\tAcc 55.8594 (Avg-Acc54.8109)\n",
      "Epoch: [73][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.2743 (Avg-Loss1.2660)\tAcc 52.2461 (Avg-Acc54.5933)\n",
      "Epoch: [73][36/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.2704 (Avg-Loss1.2700)\tAcc 55.4688 (Avg-Acc54.6030)\n",
      "Epoch: [73][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.2746 (Avg-Loss1.2697)\tAcc 57.8125 (Avg-Acc54.6325)\n",
      "EPOCH: 73 train Results: Acc 54.633 Loss: 1.2697\n",
      "Epoch: [73][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2796 (Avg-Loss1.2796)\tAcc 55.9570 (Avg-Acc55.9570)\n",
      "Epoch: [73][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2889 (Avg-Loss1.3085)\tAcc 53.9541 (Avg-Acc53.6900)\n",
      "EPOCH: 73 Validation Results: Acc 53.690 Loss: 1.3085\n",
      "Best Accuracy: 53.69.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [74][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1986 (Avg-Loss1.1986)\tAcc 56.8359 (Avg-Acc56.8359)\n",
      "Epoch: [74][9/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.2271 (Avg-Loss1.2410)\tAcc 56.0547 (Avg-Acc55.9668)\n",
      "Epoch: [74][18/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.2838 (Avg-Loss1.2578)\tAcc 54.6875 (Avg-Acc55.3403)\n",
      "Epoch: [74][27/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.2408 (Avg-Loss1.2609)\tAcc 55.7617 (Avg-Acc55.0432)\n",
      "Epoch: [74][36/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.3179 (Avg-Loss1.2643)\tAcc 51.6602 (Avg-Acc54.8881)\n",
      "Epoch: [74][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2958 (Avg-Loss1.2652)\tAcc 50.0000 (Avg-Acc54.8050)\n",
      "EPOCH: 74 train Results: Acc 54.805 Loss: 1.2652\n",
      "Epoch: [74][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2721 (Avg-Loss1.2721)\tAcc 57.3242 (Avg-Acc57.3242)\n",
      "Epoch: [74][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 1.2867 (Avg-Loss1.3042)\tAcc 54.9745 (Avg-Acc53.9600)\n",
      "EPOCH: 74 Validation Results: Acc 53.960 Loss: 1.3042\n",
      "Best Accuracy: 53.96.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [75][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.2747 (Avg-Loss1.2747)\tAcc 53.9062 (Avg-Acc53.9062)\n",
      "Epoch: [75][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.2259 (Avg-Loss1.2557)\tAcc 54.8828 (Avg-Acc54.9414)\n",
      "Epoch: [75][18/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.2542 (Avg-Loss1.2605)\tAcc 55.4688 (Avg-Acc54.9188)\n",
      "Epoch: [75][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.3115 (Avg-Loss1.2609)\tAcc 51.1719 (Avg-Acc54.8968)\n",
      "Epoch: [75][36/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.2701 (Avg-Loss1.2621)\tAcc 55.3711 (Avg-Acc55.0596)\n",
      "Epoch: [75][39/39]\tTime 0.006 (Avg-Time0.022)\t Loss 1.3890 (Avg-Loss1.2625)\tAcc 46.8750 (Avg-Acc55.0200)\n",
      "EPOCH: 75 train Results: Acc 55.020 Loss: 1.2625\n",
      "Epoch: [75][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2703 (Avg-Loss1.2703)\tAcc 56.8359 (Avg-Acc56.8359)\n",
      "Epoch: [75][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2852 (Avg-Loss1.2981)\tAcc 54.7194 (Avg-Acc54.2800)\n",
      "EPOCH: 75 Validation Results: Acc 54.280 Loss: 1.2981\n",
      "Best Accuracy: 54.28.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [76][0/39]\tTime 0.026 (Avg-Time0.026)\t Loss 1.2396 (Avg-Loss1.2396)\tAcc 56.6406 (Avg-Acc56.6406)\n",
      "Epoch: [76][9/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.2080 (Avg-Loss1.2343)\tAcc 56.5430 (Avg-Acc56.3672)\n",
      "Epoch: [76][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.2650 (Avg-Loss1.2488)\tAcc 55.3711 (Avg-Acc55.7309)\n",
      "Epoch: [76][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.2560 (Avg-Loss1.2483)\tAcc 53.5156 (Avg-Acc55.4792)\n",
      "Epoch: [76][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.2271 (Avg-Loss1.2514)\tAcc 55.8594 (Avg-Acc55.3869)\n",
      "Epoch: [76][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.3372 (Avg-Loss1.2532)\tAcc 50.0000 (Avg-Acc55.3075)\n",
      "EPOCH: 76 train Results: Acc 55.307 Loss: 1.2532\n",
      "Epoch: [76][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2701 (Avg-Loss1.2701)\tAcc 57.1289 (Avg-Acc57.1289)\n",
      "Epoch: [76][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2849 (Avg-Loss1.2972)\tAcc 54.5918 (Avg-Acc54.3800)\n",
      "EPOCH: 76 Validation Results: Acc 54.380 Loss: 1.2972\n",
      "Best Accuracy: 54.38.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [77][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1991 (Avg-Loss1.1991)\tAcc 58.0078 (Avg-Acc58.0078)\n",
      "Epoch: [77][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.2259 (Avg-Loss1.2474)\tAcc 56.9336 (Avg-Acc55.2930)\n",
      "Epoch: [77][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.2151 (Avg-Loss1.2463)\tAcc 57.9102 (Avg-Acc55.6384)\n",
      "Epoch: [77][27/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2549 (Avg-Loss1.2527)\tAcc 55.1758 (Avg-Acc55.2525)\n",
      "Epoch: [77][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2561 (Avg-Loss1.2519)\tAcc 55.9570 (Avg-Acc55.2734)\n",
      "Epoch: [77][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.4426 (Avg-Loss1.2514)\tAcc 45.3125 (Avg-Acc55.3250)\n",
      "EPOCH: 77 train Results: Acc 55.325 Loss: 1.2514\n",
      "Epoch: [77][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2712 (Avg-Loss1.2712)\tAcc 56.1523 (Avg-Acc56.1523)\n",
      "Epoch: [77][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2872 (Avg-Loss1.2958)\tAcc 55.8673 (Avg-Acc54.3100)\n",
      "EPOCH: 77 Validation Results: Acc 54.310 Loss: 1.2958\n",
      "Best Accuracy: 54.31.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [78][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.2249 (Avg-Loss1.2249)\tAcc 56.5430 (Avg-Acc56.5430)\n",
      "Epoch: [78][9/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.2382 (Avg-Loss1.2469)\tAcc 54.6875 (Avg-Acc55.5469)\n",
      "Epoch: [78][18/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.2546 (Avg-Loss1.2448)\tAcc 55.0781 (Avg-Acc55.4739)\n",
      "Epoch: [78][27/39]\tTime 0.028 (Avg-Time0.025)\t Loss 1.2529 (Avg-Loss1.2442)\tAcc 54.6875 (Avg-Acc55.5106)\n",
      "Epoch: [78][36/39]\tTime 0.025 (Avg-Time0.026)\t Loss 1.2269 (Avg-Loss1.2462)\tAcc 57.2266 (Avg-Acc55.3843)\n",
      "Epoch: [78][39/39]\tTime 0.002 (Avg-Time0.025)\t Loss 1.2059 (Avg-Loss1.2480)\tAcc 53.1250 (Avg-Acc55.2550)\n",
      "EPOCH: 78 train Results: Acc 55.255 Loss: 1.2480\n",
      "Epoch: [78][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2650 (Avg-Loss1.2650)\tAcc 57.8125 (Avg-Acc57.8125)\n",
      "Epoch: [78][9/9]\tTime 0.006 (Avg-Time0.008)\t Loss 1.2887 (Avg-Loss1.2941)\tAcc 54.4643 (Avg-Acc54.2300)\n",
      "EPOCH: 78 Validation Results: Acc 54.230 Loss: 1.2941\n",
      "Best Accuracy: 54.23.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [79][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.2084 (Avg-Loss1.2084)\tAcc 58.3008 (Avg-Acc58.3008)\n",
      "Epoch: [79][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.2268 (Avg-Loss1.2314)\tAcc 56.6406 (Avg-Acc55.7227)\n",
      "Epoch: [79][18/39]\tTime 0.027 (Avg-Time0.023)\t Loss 1.2857 (Avg-Loss1.2334)\tAcc 54.6875 (Avg-Acc55.6949)\n",
      "Epoch: [79][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.2247 (Avg-Loss1.2419)\tAcc 56.2500 (Avg-Acc55.2944)\n",
      "Epoch: [79][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.2744 (Avg-Loss1.2436)\tAcc 53.8086 (Avg-Acc55.2734)\n",
      "Epoch: [79][39/39]\tTime 0.003 (Avg-Time0.023)\t Loss 1.2118 (Avg-Loss1.2421)\tAcc 59.3750 (Avg-Acc55.3850)\n",
      "EPOCH: 79 train Results: Acc 55.385 Loss: 1.2421\n",
      "Epoch: [79][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2693 (Avg-Loss1.2693)\tAcc 57.6172 (Avg-Acc57.6172)\n",
      "Epoch: [79][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2753 (Avg-Loss1.2913)\tAcc 55.1020 (Avg-Acc54.5800)\n",
      "EPOCH: 79 Validation Results: Acc 54.580 Loss: 1.2913\n",
      "Best Accuracy: 54.58.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [80][0/39]\tTime 0.026 (Avg-Time0.026)\t Loss 1.2568 (Avg-Loss1.2568)\tAcc 54.7852 (Avg-Acc54.7852)\n",
      "Epoch: [80][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.2679 (Avg-Loss1.2377)\tAcc 54.2969 (Avg-Acc55.2148)\n",
      "Epoch: [80][18/39]\tTime 0.027 (Avg-Time0.022)\t Loss 1.2118 (Avg-Loss1.2358)\tAcc 58.4961 (Avg-Acc55.8337)\n",
      "Epoch: [80][27/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2827 (Avg-Loss1.2444)\tAcc 53.3203 (Avg-Acc55.4478)\n",
      "Epoch: [80][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.2315 (Avg-Loss1.2441)\tAcc 55.7617 (Avg-Acc55.5242)\n",
      "Epoch: [80][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.1340 (Avg-Loss1.2431)\tAcc 62.5000 (Avg-Acc55.5400)\n",
      "EPOCH: 80 train Results: Acc 55.540 Loss: 1.2431\n",
      "Epoch: [80][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2600 (Avg-Loss1.2600)\tAcc 57.4219 (Avg-Acc57.4219)\n",
      "Epoch: [80][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2750 (Avg-Loss1.2874)\tAcc 54.8469 (Avg-Acc54.6300)\n",
      "EPOCH: 80 Validation Results: Acc 54.630 Loss: 1.2874\n",
      "Best Accuracy: 54.63.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [81][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.2109 (Avg-Loss1.2109)\tAcc 57.2266 (Avg-Acc57.2266)\n",
      "Epoch: [81][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2490 (Avg-Loss1.2186)\tAcc 54.3945 (Avg-Acc56.4355)\n",
      "Epoch: [81][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1794 (Avg-Loss1.2202)\tAcc 58.4961 (Avg-Acc56.4453)\n",
      "Epoch: [81][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.2400 (Avg-Loss1.2270)\tAcc 55.1758 (Avg-Acc56.1977)\n",
      "Epoch: [81][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.2246 (Avg-Loss1.2319)\tAcc 55.9570 (Avg-Acc56.0045)\n",
      "Epoch: [81][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.3403 (Avg-Loss1.2318)\tAcc 53.1250 (Avg-Acc56.0375)\n",
      "EPOCH: 81 train Results: Acc 56.038 Loss: 1.2318\n",
      "Epoch: [81][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.2518 (Avg-Loss1.2518)\tAcc 57.1289 (Avg-Acc57.1289)\n",
      "Epoch: [81][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2740 (Avg-Loss1.2839)\tAcc 54.7194 (Avg-Acc54.4200)\n",
      "EPOCH: 81 Validation Results: Acc 54.420 Loss: 1.2839\n",
      "Best Accuracy: 54.42.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [82][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.2439 (Avg-Loss1.2439)\tAcc 55.7617 (Avg-Acc55.7617)\n",
      "Epoch: [82][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2598 (Avg-Loss1.2144)\tAcc 54.4922 (Avg-Acc57.3145)\n",
      "Epoch: [82][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.2444 (Avg-Loss1.2278)\tAcc 56.5430 (Avg-Acc56.3785)\n",
      "Epoch: [82][27/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.2538 (Avg-Loss1.2283)\tAcc 55.6641 (Avg-Acc56.3756)\n",
      "Epoch: [82][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2696 (Avg-Loss1.2284)\tAcc 53.3203 (Avg-Acc56.1154)\n",
      "Epoch: [82][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.3582 (Avg-Loss1.2296)\tAcc 53.1250 (Avg-Acc56.0400)\n",
      "EPOCH: 82 train Results: Acc 56.040 Loss: 1.2296\n",
      "Epoch: [82][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2512 (Avg-Loss1.2512)\tAcc 57.3242 (Avg-Acc57.3242)\n",
      "Epoch: [82][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2794 (Avg-Loss1.2832)\tAcc 54.7194 (Avg-Acc54.4300)\n",
      "EPOCH: 82 Validation Results: Acc 54.430 Loss: 1.2832\n",
      "Best Accuracy: 54.43.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [83][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.2041 (Avg-Loss1.2041)\tAcc 55.6641 (Avg-Acc55.6641)\n",
      "Epoch: [83][9/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1350 (Avg-Loss1.2222)\tAcc 57.8125 (Avg-Acc55.7422)\n",
      "Epoch: [83][18/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.3141 (Avg-Loss1.2221)\tAcc 52.6367 (Avg-Acc56.0958)\n",
      "Epoch: [83][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.3201 (Avg-Loss1.2258)\tAcc 53.4180 (Avg-Acc56.1349)\n",
      "Epoch: [83][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1802 (Avg-Loss1.2299)\tAcc 55.5664 (Avg-Acc55.9755)\n",
      "Epoch: [83][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.3674 (Avg-Loss1.2307)\tAcc 48.4375 (Avg-Acc55.9775)\n",
      "EPOCH: 83 train Results: Acc 55.977 Loss: 1.2307\n",
      "Epoch: [83][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.2581 (Avg-Loss1.2581)\tAcc 57.1289 (Avg-Acc57.1289)\n",
      "Epoch: [83][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2751 (Avg-Loss1.2802)\tAcc 55.1020 (Avg-Acc54.6800)\n",
      "EPOCH: 83 Validation Results: Acc 54.680 Loss: 1.2802\n",
      "Best Accuracy: 54.68.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [84][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.2604 (Avg-Loss1.2604)\tAcc 55.0781 (Avg-Acc55.0781)\n",
      "Epoch: [84][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.2407 (Avg-Loss1.2064)\tAcc 54.4922 (Avg-Acc56.6895)\n",
      "Epoch: [84][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.1834 (Avg-Loss1.2067)\tAcc 58.5938 (Avg-Acc57.0210)\n",
      "Epoch: [84][27/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.2008 (Avg-Loss1.2133)\tAcc 56.5430 (Avg-Acc56.5709)\n",
      "Epoch: [84][36/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.1882 (Avg-Loss1.2148)\tAcc 57.3242 (Avg-Acc56.4348)\n",
      "Epoch: [84][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.2753 (Avg-Loss1.2172)\tAcc 56.2500 (Avg-Acc56.4100)\n",
      "EPOCH: 84 train Results: Acc 56.410 Loss: 1.2172\n",
      "Epoch: [84][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2452 (Avg-Loss1.2452)\tAcc 57.5195 (Avg-Acc57.5195)\n",
      "Epoch: [84][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2762 (Avg-Loss1.2776)\tAcc 53.9541 (Avg-Acc54.8000)\n",
      "EPOCH: 84 Validation Results: Acc 54.800 Loss: 1.2776\n",
      "Best Accuracy: 54.8.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [85][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1735 (Avg-Loss1.1735)\tAcc 58.3984 (Avg-Acc58.3984)\n",
      "Epoch: [85][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.2419 (Avg-Loss1.2042)\tAcc 54.7852 (Avg-Acc57.2559)\n",
      "Epoch: [85][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.2352 (Avg-Loss1.2097)\tAcc 56.0547 (Avg-Acc57.2009)\n",
      "Epoch: [85][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.2142 (Avg-Loss1.2173)\tAcc 57.7148 (Avg-Acc56.8290)\n",
      "Epoch: [85][36/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.2206 (Avg-Loss1.2223)\tAcc 55.9570 (Avg-Acc56.4955)\n",
      "Epoch: [85][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.2384 (Avg-Loss1.2218)\tAcc 54.6875 (Avg-Acc56.5325)\n",
      "EPOCH: 85 train Results: Acc 56.532 Loss: 1.2218\n",
      "Epoch: [85][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2474 (Avg-Loss1.2474)\tAcc 57.6172 (Avg-Acc57.6172)\n",
      "Epoch: [85][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2788 (Avg-Loss1.2778)\tAcc 54.5918 (Avg-Acc54.8100)\n",
      "EPOCH: 85 Validation Results: Acc 54.810 Loss: 1.2778\n",
      "Best Accuracy: 54.81.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [86][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.1758 (Avg-Loss1.1758)\tAcc 58.3008 (Avg-Acc58.3008)\n",
      "Epoch: [86][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.2162 (Avg-Loss1.2175)\tAcc 57.4219 (Avg-Acc56.4941)\n",
      "Epoch: [86][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.2241 (Avg-Loss1.2111)\tAcc 55.9570 (Avg-Acc56.9079)\n",
      "Epoch: [86][27/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2133 (Avg-Loss1.2107)\tAcc 56.7383 (Avg-Acc56.8464)\n",
      "Epoch: [86][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2760 (Avg-Loss1.2156)\tAcc 53.0273 (Avg-Acc56.4242)\n",
      "Epoch: [86][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.3113 (Avg-Loss1.2155)\tAcc 53.1250 (Avg-Acc56.4650)\n",
      "EPOCH: 86 train Results: Acc 56.465 Loss: 1.2155\n",
      "Epoch: [86][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2483 (Avg-Loss1.2483)\tAcc 57.0312 (Avg-Acc57.0312)\n",
      "Epoch: [86][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2738 (Avg-Loss1.2747)\tAcc 54.9745 (Avg-Acc54.6800)\n",
      "EPOCH: 86 Validation Results: Acc 54.680 Loss: 1.2747\n",
      "Best Accuracy: 54.68.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [87][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1736 (Avg-Loss1.1736)\tAcc 58.6914 (Avg-Acc58.6914)\n",
      "Epoch: [87][9/39]\tTime 0.024 (Avg-Time0.025)\t Loss 1.1737 (Avg-Loss1.2001)\tAcc 55.4688 (Avg-Acc57.1094)\n",
      "Epoch: [87][18/39]\tTime 0.027 (Avg-Time0.025)\t Loss 1.1988 (Avg-Loss1.2073)\tAcc 56.1523 (Avg-Acc56.5687)\n",
      "Epoch: [87][27/39]\tTime 0.021 (Avg-Time0.024)\t Loss 1.2149 (Avg-Loss1.2066)\tAcc 56.2500 (Avg-Acc56.7453)\n",
      "Epoch: [87][36/39]\tTime 0.020 (Avg-Time0.024)\t Loss 1.2284 (Avg-Loss1.2129)\tAcc 57.5195 (Avg-Acc56.5905)\n",
      "Epoch: [87][39/39]\tTime 0.004 (Avg-Time0.023)\t Loss 1.1947 (Avg-Loss1.2122)\tAcc 53.1250 (Avg-Acc56.5850)\n",
      "EPOCH: 87 train Results: Acc 56.585 Loss: 1.2122\n",
      "Epoch: [87][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2436 (Avg-Loss1.2436)\tAcc 57.2266 (Avg-Acc57.2266)\n",
      "Epoch: [87][9/9]\tTime 0.004 (Avg-Time0.007)\t Loss 1.2706 (Avg-Loss1.2710)\tAcc 54.7194 (Avg-Acc54.7700)\n",
      "EPOCH: 87 Validation Results: Acc 54.770 Loss: 1.2710\n",
      "Best Accuracy: 54.77.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [88][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.1787 (Avg-Loss1.1787)\tAcc 55.9570 (Avg-Acc55.9570)\n",
      "Epoch: [88][9/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.1716 (Avg-Loss1.1823)\tAcc 58.9844 (Avg-Acc57.6660)\n",
      "Epoch: [88][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.2870 (Avg-Loss1.1969)\tAcc 54.3945 (Avg-Acc57.0569)\n",
      "Epoch: [88][27/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2267 (Avg-Loss1.1960)\tAcc 55.5664 (Avg-Acc57.2824)\n",
      "Epoch: [88][36/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.2293 (Avg-Loss1.2032)\tAcc 57.3242 (Avg-Acc57.0761)\n",
      "Epoch: [88][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2267 (Avg-Loss1.2042)\tAcc 50.0000 (Avg-Acc57.0225)\n",
      "EPOCH: 88 train Results: Acc 57.023 Loss: 1.2042\n",
      "Epoch: [88][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2426 (Avg-Loss1.2426)\tAcc 57.7148 (Avg-Acc57.7148)\n",
      "Epoch: [88][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2756 (Avg-Loss1.2739)\tAcc 54.4643 (Avg-Acc54.6200)\n",
      "EPOCH: 88 Validation Results: Acc 54.620 Loss: 1.2739\n",
      "Best Accuracy: 54.62.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [89][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1993 (Avg-Loss1.1993)\tAcc 58.1055 (Avg-Acc58.1055)\n",
      "Epoch: [89][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2360 (Avg-Loss1.1929)\tAcc 57.6172 (Avg-Acc57.5000)\n",
      "Epoch: [89][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.2065 (Avg-Loss1.1968)\tAcc 58.0078 (Avg-Acc57.6223)\n",
      "Epoch: [89][27/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1899 (Avg-Loss1.1963)\tAcc 58.3984 (Avg-Acc57.5126)\n",
      "Epoch: [89][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1733 (Avg-Loss1.1985)\tAcc 57.7148 (Avg-Acc57.3110)\n",
      "Epoch: [89][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.2921 (Avg-Loss1.1998)\tAcc 46.8750 (Avg-Acc57.2475)\n",
      "EPOCH: 89 train Results: Acc 57.248 Loss: 1.1998\n",
      "Epoch: [89][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2386 (Avg-Loss1.2386)\tAcc 58.5938 (Avg-Acc58.5938)\n",
      "Epoch: [89][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2684 (Avg-Loss1.2677)\tAcc 55.1020 (Avg-Acc54.8400)\n",
      "EPOCH: 89 Validation Results: Acc 54.840 Loss: 1.2677\n",
      "Best Accuracy: 54.84.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [90][0/39]\tTime 0.026 (Avg-Time0.026)\t Loss 1.1463 (Avg-Loss1.1463)\tAcc 59.8633 (Avg-Acc59.8633)\n",
      "Epoch: [90][9/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.1719 (Avg-Loss1.1877)\tAcc 57.0312 (Avg-Acc58.1445)\n",
      "Epoch: [90][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1590 (Avg-Loss1.1867)\tAcc 58.2031 (Avg-Acc58.0078)\n",
      "Epoch: [90][27/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.2137 (Avg-Loss1.1867)\tAcc 57.2266 (Avg-Acc57.9660)\n",
      "Epoch: [90][36/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.2151 (Avg-Loss1.1943)\tAcc 57.6172 (Avg-Acc57.6119)\n",
      "Epoch: [90][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.4478 (Avg-Loss1.1967)\tAcc 50.0000 (Avg-Acc57.4250)\n",
      "EPOCH: 90 train Results: Acc 57.425 Loss: 1.1967\n",
      "Epoch: [90][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2388 (Avg-Loss1.2388)\tAcc 58.4961 (Avg-Acc58.4961)\n",
      "Epoch: [90][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2804 (Avg-Loss1.2700)\tAcc 54.4643 (Avg-Acc54.6200)\n",
      "EPOCH: 90 Validation Results: Acc 54.620 Loss: 1.2700\n",
      "Best Accuracy: 54.62.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [91][0/39]\tTime 0.029 (Avg-Time0.029)\t Loss 1.1609 (Avg-Loss1.1609)\tAcc 58.7891 (Avg-Acc58.7891)\n",
      "Epoch: [91][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.2201 (Avg-Loss1.1963)\tAcc 58.1055 (Avg-Acc57.0410)\n",
      "Epoch: [91][18/39]\tTime 0.028 (Avg-Time0.023)\t Loss 1.1897 (Avg-Loss1.1985)\tAcc 56.7383 (Avg-Acc56.8102)\n",
      "Epoch: [91][27/39]\tTime 0.020 (Avg-Time0.023)\t Loss 1.2184 (Avg-Loss1.2043)\tAcc 56.6406 (Avg-Acc56.6476)\n",
      "Epoch: [91][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1718 (Avg-Loss1.2036)\tAcc 58.9844 (Avg-Acc56.7594)\n",
      "Epoch: [91][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.0607 (Avg-Loss1.2041)\tAcc 57.8125 (Avg-Acc56.7975)\n",
      "EPOCH: 91 train Results: Acc 56.797 Loss: 1.2041\n",
      "Epoch: [91][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.2384 (Avg-Loss1.2384)\tAcc 57.9102 (Avg-Acc57.9102)\n",
      "Epoch: [91][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2728 (Avg-Loss1.2703)\tAcc 55.1020 (Avg-Acc54.6400)\n",
      "EPOCH: 91 Validation Results: Acc 54.640 Loss: 1.2703\n",
      "Best Accuracy: 54.64.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [92][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.1829 (Avg-Loss1.1829)\tAcc 56.8359 (Avg-Acc56.8359)\n",
      "Epoch: [92][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1494 (Avg-Loss1.1789)\tAcc 58.3008 (Avg-Acc57.9785)\n",
      "Epoch: [92][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.2285 (Avg-Loss1.1825)\tAcc 58.0078 (Avg-Acc57.8022)\n",
      "Epoch: [92][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1927 (Avg-Loss1.1858)\tAcc 58.0078 (Avg-Acc57.9067)\n",
      "Epoch: [92][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.2534 (Avg-Loss1.1943)\tAcc 55.0781 (Avg-Acc57.6462)\n",
      "Epoch: [92][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.4184 (Avg-Loss1.1962)\tAcc 48.4375 (Avg-Acc57.5425)\n",
      "EPOCH: 92 train Results: Acc 57.542 Loss: 1.1962\n",
      "Epoch: [92][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2310 (Avg-Loss1.2310)\tAcc 57.8125 (Avg-Acc57.8125)\n",
      "Epoch: [92][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2746 (Avg-Loss1.2650)\tAcc 56.1224 (Avg-Acc55.4000)\n",
      "EPOCH: 92 Validation Results: Acc 55.400 Loss: 1.2650\n",
      "Best Accuracy: 55.4.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [93][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.2253 (Avg-Loss1.2253)\tAcc 55.2734 (Avg-Acc55.2734)\n",
      "Epoch: [93][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1725 (Avg-Loss1.1845)\tAcc 58.0078 (Avg-Acc57.5488)\n",
      "Epoch: [93][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1756 (Avg-Loss1.1966)\tAcc 61.7188 (Avg-Acc57.3242)\n",
      "Epoch: [93][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1621 (Avg-Loss1.1920)\tAcc 57.9102 (Avg-Acc57.5160)\n",
      "Epoch: [93][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2207 (Avg-Loss1.1959)\tAcc 57.2266 (Avg-Acc57.4245)\n",
      "Epoch: [93][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.4231 (Avg-Loss1.1973)\tAcc 53.1250 (Avg-Acc57.4400)\n",
      "EPOCH: 93 train Results: Acc 57.440 Loss: 1.1973\n",
      "Epoch: [93][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2390 (Avg-Loss1.2390)\tAcc 56.4453 (Avg-Acc56.4453)\n",
      "Epoch: [93][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2755 (Avg-Loss1.2651)\tAcc 54.4643 (Avg-Acc54.8000)\n",
      "EPOCH: 93 Validation Results: Acc 54.800 Loss: 1.2651\n",
      "Best Accuracy: 54.8.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [94][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.1835 (Avg-Loss1.1835)\tAcc 58.4961 (Avg-Acc58.4961)\n",
      "Epoch: [94][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.2298 (Avg-Loss1.1881)\tAcc 56.6406 (Avg-Acc57.1191)\n",
      "Epoch: [94][18/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.1958 (Avg-Loss1.1968)\tAcc 57.0312 (Avg-Acc57.2163)\n",
      "Epoch: [94][27/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.1984 (Avg-Loss1.1932)\tAcc 58.1055 (Avg-Acc57.4881)\n",
      "Epoch: [94][36/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.1875 (Avg-Loss1.1921)\tAcc 59.3750 (Avg-Acc57.5327)\n",
      "Epoch: [94][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.3815 (Avg-Loss1.1937)\tAcc 51.5625 (Avg-Acc57.4700)\n",
      "EPOCH: 94 train Results: Acc 57.470 Loss: 1.1937\n",
      "Epoch: [94][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2349 (Avg-Loss1.2349)\tAcc 58.7891 (Avg-Acc58.7891)\n",
      "Epoch: [94][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2788 (Avg-Loss1.2646)\tAcc 55.9949 (Avg-Acc55.3100)\n",
      "EPOCH: 94 Validation Results: Acc 55.310 Loss: 1.2646\n",
      "Best Accuracy: 55.31.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [95][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.1612 (Avg-Loss1.1612)\tAcc 58.3008 (Avg-Acc58.3008)\n",
      "Epoch: [95][9/39]\tTime 0.026 (Avg-Time0.022)\t Loss 1.1973 (Avg-Loss1.1789)\tAcc 57.7148 (Avg-Acc57.8613)\n",
      "Epoch: [95][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.1762 (Avg-Loss1.1855)\tAcc 57.4219 (Avg-Acc57.8279)\n",
      "Epoch: [95][27/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.1736 (Avg-Loss1.1884)\tAcc 58.3008 (Avg-Acc57.8544)\n",
      "Epoch: [95][36/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.1473 (Avg-Loss1.1926)\tAcc 58.9844 (Avg-Acc57.6172)\n",
      "Epoch: [95][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.1623 (Avg-Loss1.1944)\tAcc 54.6875 (Avg-Acc57.4900)\n",
      "EPOCH: 95 train Results: Acc 57.490 Loss: 1.1944\n",
      "Epoch: [95][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2350 (Avg-Loss1.2350)\tAcc 57.8125 (Avg-Acc57.8125)\n",
      "Epoch: [95][9/9]\tTime 0.007 (Avg-Time0.008)\t Loss 1.2712 (Avg-Loss1.2621)\tAcc 56.6327 (Avg-Acc55.1400)\n",
      "EPOCH: 95 Validation Results: Acc 55.140 Loss: 1.2621\n",
      "Best Accuracy: 55.14.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [96][0/39]\tTime 0.028 (Avg-Time0.028)\t Loss 1.1741 (Avg-Loss1.1741)\tAcc 58.3008 (Avg-Acc58.3008)\n",
      "Epoch: [96][9/39]\tTime 0.023 (Avg-Time0.024)\t Loss 1.1540 (Avg-Loss1.1641)\tAcc 59.4727 (Avg-Acc58.6719)\n",
      "Epoch: [96][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1637 (Avg-Loss1.1696)\tAcc 59.4727 (Avg-Acc58.3008)\n",
      "Epoch: [96][27/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.2278 (Avg-Loss1.1773)\tAcc 54.4922 (Avg-Acc58.0776)\n",
      "Epoch: [96][36/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1858 (Avg-Loss1.1822)\tAcc 58.2031 (Avg-Acc57.7940)\n",
      "Epoch: [96][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.1624 (Avg-Loss1.1855)\tAcc 57.8125 (Avg-Acc57.6925)\n",
      "EPOCH: 96 train Results: Acc 57.693 Loss: 1.1855\n",
      "Epoch: [96][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2322 (Avg-Loss1.2322)\tAcc 57.8125 (Avg-Acc57.8125)\n",
      "Epoch: [96][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2747 (Avg-Loss1.2597)\tAcc 55.3571 (Avg-Acc55.4300)\n",
      "EPOCH: 96 Validation Results: Acc 55.430 Loss: 1.2597\n",
      "Best Accuracy: 55.43.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [97][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.1229 (Avg-Loss1.1229)\tAcc 60.2539 (Avg-Acc60.2539)\n",
      "Epoch: [97][9/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.1745 (Avg-Loss1.1714)\tAcc 58.7891 (Avg-Acc58.7500)\n",
      "Epoch: [97][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1592 (Avg-Loss1.1729)\tAcc 58.8867 (Avg-Acc58.1363)\n",
      "Epoch: [97][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1951 (Avg-Loss1.1770)\tAcc 55.8594 (Avg-Acc58.0148)\n",
      "Epoch: [97][36/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.1987 (Avg-Loss1.1823)\tAcc 58.3984 (Avg-Acc57.9392)\n",
      "Epoch: [97][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.2887 (Avg-Loss1.1831)\tAcc 50.0000 (Avg-Acc57.8675)\n",
      "EPOCH: 97 train Results: Acc 57.867 Loss: 1.1831\n",
      "Epoch: [97][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2204 (Avg-Loss1.2204)\tAcc 57.7148 (Avg-Acc57.7148)\n",
      "Epoch: [97][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2675 (Avg-Loss1.2573)\tAcc 54.9745 (Avg-Acc55.5000)\n",
      "EPOCH: 97 Validation Results: Acc 55.500 Loss: 1.2573\n",
      "Best Accuracy: 55.5.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [98][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1497 (Avg-Loss1.1497)\tAcc 58.8867 (Avg-Acc58.8867)\n",
      "Epoch: [98][9/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1169 (Avg-Loss1.1547)\tAcc 61.9141 (Avg-Acc58.8477)\n",
      "Epoch: [98][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.2026 (Avg-Loss1.1736)\tAcc 55.9570 (Avg-Acc58.1260)\n",
      "Epoch: [98][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.2125 (Avg-Loss1.1773)\tAcc 56.9336 (Avg-Acc58.0357)\n",
      "Epoch: [98][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.2497 (Avg-Loss1.1861)\tAcc 54.7852 (Avg-Acc57.7069)\n",
      "Epoch: [98][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.2365 (Avg-Loss1.1862)\tAcc 59.3750 (Avg-Acc57.7450)\n",
      "EPOCH: 98 train Results: Acc 57.745 Loss: 1.1862\n",
      "Epoch: [98][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2213 (Avg-Loss1.2213)\tAcc 57.6172 (Avg-Acc57.6172)\n",
      "Epoch: [98][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 1.2681 (Avg-Loss1.2563)\tAcc 55.3571 (Avg-Acc55.3200)\n",
      "EPOCH: 98 Validation Results: Acc 55.320 Loss: 1.2563\n",
      "Best Accuracy: 55.32.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [99][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.2385 (Avg-Loss1.2385)\tAcc 58.3008 (Avg-Acc58.3008)\n",
      "Epoch: [99][9/39]\tTime 0.028 (Avg-Time0.023)\t Loss 1.1354 (Avg-Loss1.1741)\tAcc 58.8867 (Avg-Acc57.9688)\n",
      "Epoch: [99][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1563 (Avg-Loss1.1789)\tAcc 57.3242 (Avg-Acc57.9873)\n",
      "Epoch: [99][27/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1688 (Avg-Loss1.1766)\tAcc 59.5703 (Avg-Acc58.0008)\n",
      "Epoch: [99][36/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.1473 (Avg-Loss1.1746)\tAcc 58.7891 (Avg-Acc58.1530)\n",
      "Epoch: [99][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.0470 (Avg-Loss1.1762)\tAcc 67.1875 (Avg-Acc58.1050)\n",
      "EPOCH: 99 train Results: Acc 58.105 Loss: 1.1762\n",
      "Epoch: [99][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.2155 (Avg-Loss1.2155)\tAcc 58.2031 (Avg-Acc58.2031)\n",
      "Epoch: [99][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2696 (Avg-Loss1.2581)\tAcc 54.5918 (Avg-Acc55.3400)\n",
      "EPOCH: 99 Validation Results: Acc 55.340 Loss: 1.2581\n",
      "Best Accuracy: 55.34.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [100][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.1835 (Avg-Loss1.1835)\tAcc 57.5195 (Avg-Acc57.5195)\n",
      "Epoch: [100][9/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1659 (Avg-Loss1.1633)\tAcc 57.7148 (Avg-Acc58.0859)\n",
      "Epoch: [100][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.2115 (Avg-Loss1.1608)\tAcc 56.7383 (Avg-Acc58.1980)\n",
      "Epoch: [100][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.2466 (Avg-Loss1.1680)\tAcc 54.9805 (Avg-Acc57.9729)\n",
      "Epoch: [100][36/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.1501 (Avg-Loss1.1712)\tAcc 58.5938 (Avg-Acc57.9339)\n",
      "Epoch: [100][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.1802 (Avg-Loss1.1721)\tAcc 54.6875 (Avg-Acc57.8650)\n",
      "EPOCH: 100 train Results: Acc 57.865 Loss: 1.1721\n",
      "Epoch: [100][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2262 (Avg-Loss1.2262)\tAcc 58.3008 (Avg-Acc58.3008)\n",
      "Epoch: [100][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2708 (Avg-Loss1.2591)\tAcc 53.8265 (Avg-Acc55.4700)\n",
      "EPOCH: 100 Validation Results: Acc 55.470 Loss: 1.2591\n",
      "Best Accuracy: 55.47.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [101][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1068 (Avg-Loss1.1068)\tAcc 58.8867 (Avg-Acc58.8867)\n",
      "Epoch: [101][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1702 (Avg-Loss1.1407)\tAcc 58.4961 (Avg-Acc59.4043)\n",
      "Epoch: [101][18/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1417 (Avg-Loss1.1492)\tAcc 59.1797 (Avg-Acc58.9124)\n",
      "Epoch: [101][27/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.2495 (Avg-Loss1.1634)\tAcc 54.0039 (Avg-Acc58.2729)\n",
      "Epoch: [101][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1994 (Avg-Loss1.1683)\tAcc 59.0820 (Avg-Acc58.3351)\n",
      "Epoch: [101][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.1248 (Avg-Loss1.1715)\tAcc 56.2500 (Avg-Acc58.2050)\n",
      "EPOCH: 101 train Results: Acc 58.205 Loss: 1.1715\n",
      "Epoch: [101][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2100 (Avg-Loss1.2100)\tAcc 58.1055 (Avg-Acc58.1055)\n",
      "Epoch: [101][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2634 (Avg-Loss1.2493)\tAcc 56.5051 (Avg-Acc55.8100)\n",
      "EPOCH: 101 Validation Results: Acc 55.810 Loss: 1.2493\n",
      "Best Accuracy: 55.81.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [102][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0963 (Avg-Loss1.0963)\tAcc 62.0117 (Avg-Acc62.0117)\n",
      "Epoch: [102][9/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.1901 (Avg-Loss1.1588)\tAcc 57.5195 (Avg-Acc59.1797)\n",
      "Epoch: [102][18/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1940 (Avg-Loss1.1743)\tAcc 57.4219 (Avg-Acc58.3265)\n",
      "Epoch: [102][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1703 (Avg-Loss1.1703)\tAcc 60.0586 (Avg-Acc58.5205)\n",
      "Epoch: [102][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1493 (Avg-Loss1.1674)\tAcc 56.6406 (Avg-Acc58.5067)\n",
      "Epoch: [102][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.2562 (Avg-Loss1.1703)\tAcc 51.5625 (Avg-Acc58.4050)\n",
      "EPOCH: 102 train Results: Acc 58.405 Loss: 1.1703\n",
      "Epoch: [102][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.2210 (Avg-Loss1.2210)\tAcc 57.7148 (Avg-Acc57.7148)\n",
      "Epoch: [102][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2693 (Avg-Loss1.2539)\tAcc 54.8469 (Avg-Acc55.5600)\n",
      "EPOCH: 102 Validation Results: Acc 55.560 Loss: 1.2539\n",
      "Best Accuracy: 55.56.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [103][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1582 (Avg-Loss1.1582)\tAcc 58.6914 (Avg-Acc58.6914)\n",
      "Epoch: [103][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.1102 (Avg-Loss1.1475)\tAcc 60.6445 (Avg-Acc58.8965)\n",
      "Epoch: [103][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1927 (Avg-Loss1.1584)\tAcc 56.7383 (Avg-Acc58.5475)\n",
      "Epoch: [103][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.2131 (Avg-Loss1.1643)\tAcc 55.3711 (Avg-Acc58.4298)\n",
      "Epoch: [103][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1737 (Avg-Loss1.1668)\tAcc 59.2773 (Avg-Acc58.5357)\n",
      "Epoch: [103][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.3394 (Avg-Loss1.1694)\tAcc 54.6875 (Avg-Acc58.4425)\n",
      "EPOCH: 103 train Results: Acc 58.443 Loss: 1.1694\n",
      "Epoch: [103][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2178 (Avg-Loss1.2178)\tAcc 57.5195 (Avg-Acc57.5195)\n",
      "Epoch: [103][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2649 (Avg-Loss1.2527)\tAcc 55.6122 (Avg-Acc55.8100)\n",
      "EPOCH: 103 Validation Results: Acc 55.810 Loss: 1.2527\n",
      "Best Accuracy: 55.81.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [104][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1167 (Avg-Loss1.1167)\tAcc 60.8398 (Avg-Acc60.8398)\n",
      "Epoch: [104][9/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1733 (Avg-Loss1.1546)\tAcc 56.8359 (Avg-Acc58.6816)\n",
      "Epoch: [104][18/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.2444 (Avg-Loss1.1705)\tAcc 54.8828 (Avg-Acc58.1723)\n",
      "Epoch: [104][27/39]\tTime 0.021 (Avg-Time0.024)\t Loss 1.1356 (Avg-Loss1.1773)\tAcc 60.2539 (Avg-Acc57.8439)\n",
      "Epoch: [104][36/39]\tTime 0.022 (Avg-Time0.024)\t Loss 1.1622 (Avg-Loss1.1760)\tAcc 58.0078 (Avg-Acc57.9154)\n",
      "Epoch: [104][39/39]\tTime 0.003 (Avg-Time0.023)\t Loss 1.1720 (Avg-Loss1.1746)\tAcc 59.3750 (Avg-Acc57.9800)\n",
      "EPOCH: 104 train Results: Acc 57.980 Loss: 1.1746\n",
      "Epoch: [104][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2175 (Avg-Loss1.2175)\tAcc 57.4219 (Avg-Acc57.4219)\n",
      "Epoch: [104][9/9]\tTime 0.010 (Avg-Time0.007)\t Loss 1.2668 (Avg-Loss1.2497)\tAcc 55.2296 (Avg-Acc55.7900)\n",
      "EPOCH: 104 Validation Results: Acc 55.790 Loss: 1.2497\n",
      "Best Accuracy: 55.79.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [105][0/39]\tTime 0.029 (Avg-Time0.029)\t Loss 1.0742 (Avg-Loss1.0742)\tAcc 62.9883 (Avg-Acc62.9883)\n",
      "Epoch: [105][9/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1957 (Avg-Loss1.1393)\tAcc 57.2266 (Avg-Acc59.5508)\n",
      "Epoch: [105][18/39]\tTime 0.027 (Avg-Time0.023)\t Loss 1.1704 (Avg-Loss1.1516)\tAcc 56.6406 (Avg-Acc58.7839)\n",
      "Epoch: [105][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1536 (Avg-Loss1.1590)\tAcc 59.1797 (Avg-Acc58.7926)\n",
      "Epoch: [105][36/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.1559 (Avg-Loss1.1627)\tAcc 58.9844 (Avg-Acc58.6069)\n",
      "Epoch: [105][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.0122 (Avg-Loss1.1631)\tAcc 60.9375 (Avg-Acc58.5550)\n",
      "EPOCH: 105 train Results: Acc 58.555 Loss: 1.1631\n",
      "Epoch: [105][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2181 (Avg-Loss1.2181)\tAcc 58.0078 (Avg-Acc58.0078)\n",
      "Epoch: [105][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2572 (Avg-Loss1.2467)\tAcc 56.6327 (Avg-Acc55.9200)\n",
      "EPOCH: 105 Validation Results: Acc 55.920 Loss: 1.2467\n",
      "Best Accuracy: 55.92.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [106][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1725 (Avg-Loss1.1725)\tAcc 58.3008 (Avg-Acc58.3008)\n",
      "Epoch: [106][9/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.1778 (Avg-Loss1.1450)\tAcc 58.2031 (Avg-Acc59.7852)\n",
      "Epoch: [106][18/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1455 (Avg-Loss1.1577)\tAcc 58.3984 (Avg-Acc59.0923)\n",
      "Epoch: [106][27/39]\tTime 0.020 (Avg-Time0.023)\t Loss 1.1405 (Avg-Loss1.1640)\tAcc 60.2539 (Avg-Acc58.6112)\n",
      "Epoch: [106][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0965 (Avg-Loss1.1680)\tAcc 61.6211 (Avg-Acc58.4855)\n",
      "Epoch: [106][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.1825 (Avg-Loss1.1681)\tAcc 60.9375 (Avg-Acc58.5075)\n",
      "EPOCH: 106 train Results: Acc 58.508 Loss: 1.1681\n",
      "Epoch: [106][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2102 (Avg-Loss1.2102)\tAcc 58.0078 (Avg-Acc58.0078)\n",
      "Epoch: [106][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2575 (Avg-Loss1.2491)\tAcc 55.4847 (Avg-Acc55.6600)\n",
      "EPOCH: 106 Validation Results: Acc 55.660 Loss: 1.2491\n",
      "Best Accuracy: 55.66.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [107][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.1397 (Avg-Loss1.1397)\tAcc 59.5703 (Avg-Acc59.5703)\n",
      "Epoch: [107][9/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.1775 (Avg-Loss1.1429)\tAcc 57.8125 (Avg-Acc59.0918)\n",
      "Epoch: [107][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.1687 (Avg-Loss1.1543)\tAcc 58.3984 (Avg-Acc58.8148)\n",
      "Epoch: [107][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1639 (Avg-Loss1.1595)\tAcc 56.9336 (Avg-Acc58.5345)\n",
      "Epoch: [107][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1328 (Avg-Loss1.1621)\tAcc 60.8398 (Avg-Acc58.5225)\n",
      "Epoch: [107][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2800 (Avg-Loss1.1628)\tAcc 51.5625 (Avg-Acc58.5750)\n",
      "EPOCH: 107 train Results: Acc 58.575 Loss: 1.1628\n",
      "Epoch: [107][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.2106 (Avg-Loss1.2106)\tAcc 57.3242 (Avg-Acc57.3242)\n",
      "Epoch: [107][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2604 (Avg-Loss1.2488)\tAcc 55.6122 (Avg-Acc55.5800)\n",
      "EPOCH: 107 Validation Results: Acc 55.580 Loss: 1.2488\n",
      "Best Accuracy: 55.58.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [108][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1298 (Avg-Loss1.1298)\tAcc 60.8398 (Avg-Acc60.8398)\n",
      "Epoch: [108][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1824 (Avg-Loss1.1284)\tAcc 56.4453 (Avg-Acc59.4238)\n",
      "Epoch: [108][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1507 (Avg-Loss1.1434)\tAcc 60.0586 (Avg-Acc59.0666)\n",
      "Epoch: [108][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1450 (Avg-Loss1.1523)\tAcc 58.3984 (Avg-Acc58.7542)\n",
      "Epoch: [108][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1665 (Avg-Loss1.1586)\tAcc 58.9844 (Avg-Acc58.5700)\n",
      "Epoch: [108][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.4272 (Avg-Loss1.1590)\tAcc 43.7500 (Avg-Acc58.4750)\n",
      "EPOCH: 108 train Results: Acc 58.475 Loss: 1.1590\n",
      "Epoch: [108][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2105 (Avg-Loss1.2105)\tAcc 57.5195 (Avg-Acc57.5195)\n",
      "Epoch: [108][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2558 (Avg-Loss1.2453)\tAcc 55.1020 (Avg-Acc55.5600)\n",
      "EPOCH: 108 Validation Results: Acc 55.560 Loss: 1.2453\n",
      "Best Accuracy: 55.56.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [109][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1050 (Avg-Loss1.1050)\tAcc 59.3750 (Avg-Acc59.3750)\n",
      "Epoch: [109][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1361 (Avg-Loss1.1363)\tAcc 61.1328 (Avg-Acc59.7168)\n",
      "Epoch: [109][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1806 (Avg-Loss1.1558)\tAcc 57.5195 (Avg-Acc58.6863)\n",
      "Epoch: [109][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1229 (Avg-Loss1.1606)\tAcc 60.7422 (Avg-Acc58.4821)\n",
      "Epoch: [109][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1625 (Avg-Loss1.1649)\tAcc 58.0078 (Avg-Acc58.3509)\n",
      "Epoch: [109][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.3063 (Avg-Loss1.1662)\tAcc 57.8125 (Avg-Acc58.4025)\n",
      "EPOCH: 109 train Results: Acc 58.403 Loss: 1.1662\n",
      "Epoch: [109][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2062 (Avg-Loss1.2062)\tAcc 58.3008 (Avg-Acc58.3008)\n",
      "Epoch: [109][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 1.2473 (Avg-Loss1.2476)\tAcc 55.7398 (Avg-Acc55.7800)\n",
      "EPOCH: 109 Validation Results: Acc 55.780 Loss: 1.2476\n",
      "Best Accuracy: 55.78.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [110][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.1271 (Avg-Loss1.1271)\tAcc 57.9102 (Avg-Acc57.9102)\n",
      "Epoch: [110][9/39]\tTime 0.026 (Avg-Time0.022)\t Loss 1.1494 (Avg-Loss1.1388)\tAcc 58.6914 (Avg-Acc59.0918)\n",
      "Epoch: [110][18/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1677 (Avg-Loss1.1446)\tAcc 58.6914 (Avg-Acc59.0769)\n",
      "Epoch: [110][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.2012 (Avg-Loss1.1518)\tAcc 57.0312 (Avg-Acc58.7716)\n",
      "Epoch: [110][36/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.1539 (Avg-Loss1.1524)\tAcc 58.7891 (Avg-Acc58.7759)\n",
      "Epoch: [110][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.2823 (Avg-Loss1.1540)\tAcc 57.8125 (Avg-Acc58.6950)\n",
      "EPOCH: 110 train Results: Acc 58.695 Loss: 1.1540\n",
      "Epoch: [110][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2042 (Avg-Loss1.2042)\tAcc 58.6914 (Avg-Acc58.6914)\n",
      "Epoch: [110][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2466 (Avg-Loss1.2418)\tAcc 56.5051 (Avg-Acc56.4200)\n",
      "EPOCH: 110 Validation Results: Acc 56.420 Loss: 1.2418\n",
      "Best Accuracy: 56.42.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [111][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1336 (Avg-Loss1.1336)\tAcc 59.7656 (Avg-Acc59.7656)\n",
      "Epoch: [111][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1260 (Avg-Loss1.1337)\tAcc 60.8398 (Avg-Acc60.1660)\n",
      "Epoch: [111][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0728 (Avg-Loss1.1399)\tAcc 64.2578 (Avg-Acc59.8633)\n",
      "Epoch: [111][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1619 (Avg-Loss1.1413)\tAcc 59.5703 (Avg-Acc59.8145)\n",
      "Epoch: [111][36/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.1411 (Avg-Loss1.1496)\tAcc 62.0117 (Avg-Acc59.3697)\n",
      "Epoch: [111][39/39]\tTime 0.004 (Avg-Time0.023)\t Loss 1.1776 (Avg-Loss1.1537)\tAcc 64.0625 (Avg-Acc59.2150)\n",
      "EPOCH: 111 train Results: Acc 59.215 Loss: 1.1537\n",
      "Epoch: [111][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2157 (Avg-Loss1.2157)\tAcc 58.9844 (Avg-Acc58.9844)\n",
      "Epoch: [111][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2548 (Avg-Loss1.2447)\tAcc 56.5051 (Avg-Acc56.0800)\n",
      "EPOCH: 111 Validation Results: Acc 56.080 Loss: 1.2447\n",
      "Best Accuracy: 56.08.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [112][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.1599 (Avg-Loss1.1599)\tAcc 58.8867 (Avg-Acc58.8867)\n",
      "Epoch: [112][9/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.1589 (Avg-Loss1.1495)\tAcc 59.1797 (Avg-Acc59.1797)\n",
      "Epoch: [112][18/39]\tTime 0.030 (Avg-Time0.023)\t Loss 1.1540 (Avg-Loss1.1499)\tAcc 57.5195 (Avg-Acc59.1129)\n",
      "Epoch: [112][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1473 (Avg-Loss1.1503)\tAcc 58.9844 (Avg-Acc59.1518)\n",
      "Epoch: [112][36/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1674 (Avg-Loss1.1527)\tAcc 58.1055 (Avg-Acc59.0477)\n",
      "Epoch: [112][39/39]\tTime 0.002 (Avg-Time0.023)\t Loss 0.9650 (Avg-Loss1.1527)\tAcc 73.4375 (Avg-Acc59.0525)\n",
      "EPOCH: 112 train Results: Acc 59.053 Loss: 1.1527\n",
      "Epoch: [112][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.2131 (Avg-Loss1.2131)\tAcc 58.6914 (Avg-Acc58.6914)\n",
      "Epoch: [112][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 1.2388 (Avg-Loss1.2392)\tAcc 56.1224 (Avg-Acc56.3100)\n",
      "EPOCH: 112 Validation Results: Acc 56.310 Loss: 1.2392\n",
      "Best Accuracy: 56.31.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [113][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.1348 (Avg-Loss1.1348)\tAcc 58.7891 (Avg-Acc58.7891)\n",
      "Epoch: [113][9/39]\tTime 0.024 (Avg-Time0.025)\t Loss 1.1437 (Avg-Loss1.1214)\tAcc 61.1328 (Avg-Acc60.4883)\n",
      "Epoch: [113][18/39]\tTime 0.022 (Avg-Time0.024)\t Loss 1.1765 (Avg-Loss1.1415)\tAcc 57.5195 (Avg-Acc59.4007)\n",
      "Epoch: [113][27/39]\tTime 0.020 (Avg-Time0.023)\t Loss 1.1449 (Avg-Loss1.1451)\tAcc 60.1562 (Avg-Acc59.3052)\n",
      "Epoch: [113][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1594 (Avg-Loss1.1469)\tAcc 59.5703 (Avg-Acc59.1902)\n",
      "Epoch: [113][39/39]\tTime 0.004 (Avg-Time0.023)\t Loss 1.2179 (Avg-Loss1.1488)\tAcc 54.6875 (Avg-Acc59.0900)\n",
      "EPOCH: 113 train Results: Acc 59.090 Loss: 1.1488\n",
      "Epoch: [113][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2092 (Avg-Loss1.2092)\tAcc 57.8125 (Avg-Acc57.8125)\n",
      "Epoch: [113][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2402 (Avg-Loss1.2380)\tAcc 57.0153 (Avg-Acc56.3100)\n",
      "EPOCH: 113 Validation Results: Acc 56.310 Loss: 1.2380\n",
      "Best Accuracy: 56.31.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [114][0/39]\tTime 0.035 (Avg-Time0.035)\t Loss 1.1160 (Avg-Loss1.1160)\tAcc 60.8398 (Avg-Acc60.8398)\n",
      "Epoch: [114][9/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.1426 (Avg-Loss1.1342)\tAcc 56.9336 (Avg-Acc59.7266)\n",
      "Epoch: [114][18/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.1864 (Avg-Loss1.1413)\tAcc 56.5430 (Avg-Acc59.0820)\n",
      "Epoch: [114][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1431 (Avg-Loss1.1478)\tAcc 59.2773 (Avg-Acc59.0193)\n",
      "Epoch: [114][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1240 (Avg-Loss1.1488)\tAcc 58.7891 (Avg-Acc58.8524)\n",
      "Epoch: [114][39/39]\tTime 0.003 (Avg-Time0.023)\t Loss 1.1415 (Avg-Loss1.1507)\tAcc 57.8125 (Avg-Acc58.8200)\n",
      "EPOCH: 114 train Results: Acc 58.820 Loss: 1.1507\n",
      "Epoch: [114][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1903 (Avg-Loss1.1903)\tAcc 59.1797 (Avg-Acc59.1797)\n",
      "Epoch: [114][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 1.2286 (Avg-Loss1.2343)\tAcc 57.2704 (Avg-Acc56.7200)\n",
      "EPOCH: 114 Validation Results: Acc 56.720 Loss: 1.2343\n",
      "Best Accuracy: 56.72.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [115][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1582 (Avg-Loss1.1582)\tAcc 60.3516 (Avg-Acc60.3516)\n",
      "Epoch: [115][9/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.1397 (Avg-Loss1.1269)\tAcc 60.3516 (Avg-Acc60.3516)\n",
      "Epoch: [115][18/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.1360 (Avg-Loss1.1274)\tAcc 57.1289 (Avg-Acc59.8890)\n",
      "Epoch: [115][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1227 (Avg-Loss1.1349)\tAcc 61.3281 (Avg-Acc59.6226)\n",
      "Epoch: [115][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1621 (Avg-Loss1.1405)\tAcc 59.1797 (Avg-Acc59.3882)\n",
      "Epoch: [115][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.2426 (Avg-Loss1.1433)\tAcc 59.3750 (Avg-Acc59.3025)\n",
      "EPOCH: 115 train Results: Acc 59.303 Loss: 1.1433\n",
      "Epoch: [115][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2008 (Avg-Loss1.2008)\tAcc 58.6914 (Avg-Acc58.6914)\n",
      "Epoch: [115][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2306 (Avg-Loss1.2372)\tAcc 56.1224 (Avg-Acc56.6000)\n",
      "EPOCH: 115 Validation Results: Acc 56.600 Loss: 1.2372\n",
      "Best Accuracy: 56.6.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [116][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1322 (Avg-Loss1.1322)\tAcc 58.9844 (Avg-Acc58.9844)\n",
      "Epoch: [116][9/39]\tTime 0.027 (Avg-Time0.023)\t Loss 1.1225 (Avg-Loss1.1279)\tAcc 59.3750 (Avg-Acc59.7363)\n",
      "Epoch: [116][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.2253 (Avg-Loss1.1403)\tAcc 57.2266 (Avg-Acc59.4675)\n",
      "Epoch: [116][27/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1424 (Avg-Loss1.1408)\tAcc 60.3516 (Avg-Acc59.4761)\n",
      "Epoch: [116][36/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.1247 (Avg-Loss1.1441)\tAcc 60.1562 (Avg-Acc59.2747)\n",
      "Epoch: [116][39/39]\tTime 0.004 (Avg-Time0.023)\t Loss 1.3630 (Avg-Loss1.1449)\tAcc 51.5625 (Avg-Acc59.2875)\n",
      "EPOCH: 116 train Results: Acc 59.288 Loss: 1.1449\n",
      "Epoch: [116][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2048 (Avg-Loss1.2048)\tAcc 58.1055 (Avg-Acc58.1055)\n",
      "Epoch: [116][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2355 (Avg-Loss1.2356)\tAcc 56.7602 (Avg-Acc56.5400)\n",
      "EPOCH: 116 Validation Results: Acc 56.540 Loss: 1.2356\n",
      "Best Accuracy: 56.54.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [117][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0798 (Avg-Loss1.0798)\tAcc 62.7930 (Avg-Acc62.7930)\n",
      "Epoch: [117][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.0453 (Avg-Loss1.1090)\tAcc 62.3047 (Avg-Acc60.7520)\n",
      "Epoch: [117][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1256 (Avg-Loss1.1309)\tAcc 58.9844 (Avg-Acc59.6885)\n",
      "Epoch: [117][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1912 (Avg-Loss1.1374)\tAcc 57.6172 (Avg-Acc59.6191)\n",
      "Epoch: [117][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1772 (Avg-Loss1.1425)\tAcc 58.2031 (Avg-Acc59.4278)\n",
      "Epoch: [117][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.0414 (Avg-Loss1.1442)\tAcc 62.5000 (Avg-Acc59.4125)\n",
      "EPOCH: 117 train Results: Acc 59.413 Loss: 1.1442\n",
      "Epoch: [117][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2050 (Avg-Loss1.2050)\tAcc 58.7891 (Avg-Acc58.7891)\n",
      "Epoch: [117][9/9]\tTime 0.004 (Avg-Time0.007)\t Loss 1.2462 (Avg-Loss1.2384)\tAcc 57.6531 (Avg-Acc56.3400)\n",
      "EPOCH: 117 Validation Results: Acc 56.340 Loss: 1.2384\n",
      "Best Accuracy: 56.34.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [118][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.1206 (Avg-Loss1.1206)\tAcc 60.1562 (Avg-Acc60.1562)\n",
      "Epoch: [118][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1665 (Avg-Loss1.1222)\tAcc 58.2031 (Avg-Acc60.0000)\n",
      "Epoch: [118][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1263 (Avg-Loss1.1244)\tAcc 59.5703 (Avg-Acc59.9250)\n",
      "Epoch: [118][27/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.1719 (Avg-Loss1.1391)\tAcc 58.1055 (Avg-Acc59.4727)\n",
      "Epoch: [118][36/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1625 (Avg-Loss1.1424)\tAcc 57.8125 (Avg-Acc59.4383)\n",
      "Epoch: [118][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2932 (Avg-Loss1.1418)\tAcc 50.0000 (Avg-Acc59.4125)\n",
      "EPOCH: 118 train Results: Acc 59.413 Loss: 1.1418\n",
      "Epoch: [118][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2042 (Avg-Loss1.2042)\tAcc 59.4727 (Avg-Acc59.4727)\n",
      "Epoch: [118][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 1.2371 (Avg-Loss1.2366)\tAcc 56.6327 (Avg-Acc56.3000)\n",
      "EPOCH: 118 Validation Results: Acc 56.300 Loss: 1.2366\n",
      "Best Accuracy: 56.3.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [119][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1253 (Avg-Loss1.1253)\tAcc 60.0586 (Avg-Acc60.0586)\n",
      "Epoch: [119][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1069 (Avg-Loss1.1196)\tAcc 59.6680 (Avg-Acc59.8047)\n",
      "Epoch: [119][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.1348 (Avg-Loss1.1284)\tAcc 60.8398 (Avg-Acc59.7708)\n",
      "Epoch: [119][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1091 (Avg-Loss1.1371)\tAcc 60.0586 (Avg-Acc59.5633)\n",
      "Epoch: [119][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1817 (Avg-Loss1.1425)\tAcc 57.1289 (Avg-Acc59.3169)\n",
      "Epoch: [119][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.3322 (Avg-Loss1.1447)\tAcc 59.3750 (Avg-Acc59.2500)\n",
      "EPOCH: 119 train Results: Acc 59.250 Loss: 1.1447\n",
      "Epoch: [119][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2102 (Avg-Loss1.2102)\tAcc 57.6172 (Avg-Acc57.6172)\n",
      "Epoch: [119][9/9]\tTime 0.004 (Avg-Time0.007)\t Loss 1.2492 (Avg-Loss1.2416)\tAcc 57.1429 (Avg-Acc56.1800)\n",
      "EPOCH: 119 Validation Results: Acc 56.180 Loss: 1.2416\n",
      "Best Accuracy: 56.18.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [120][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1167 (Avg-Loss1.1167)\tAcc 60.6445 (Avg-Acc60.6445)\n",
      "Epoch: [120][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0745 (Avg-Loss1.1128)\tAcc 61.8164 (Avg-Acc61.0156)\n",
      "Epoch: [120][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1489 (Avg-Loss1.1330)\tAcc 56.6406 (Avg-Acc59.8170)\n",
      "Epoch: [120][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1486 (Avg-Loss1.1364)\tAcc 59.2773 (Avg-Acc59.6191)\n",
      "Epoch: [120][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1641 (Avg-Loss1.1412)\tAcc 57.9102 (Avg-Acc59.4172)\n",
      "Epoch: [120][39/39]\tTime 0.004 (Avg-Time0.021)\t Loss 1.1322 (Avg-Loss1.1418)\tAcc 57.8125 (Avg-Acc59.3875)\n",
      "EPOCH: 120 train Results: Acc 59.388 Loss: 1.1418\n",
      "Epoch: [120][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2053 (Avg-Loss1.2053)\tAcc 58.7891 (Avg-Acc58.7891)\n",
      "Epoch: [120][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2420 (Avg-Loss1.2391)\tAcc 57.1429 (Avg-Acc56.4400)\n",
      "EPOCH: 120 Validation Results: Acc 56.440 Loss: 1.2391\n",
      "Best Accuracy: 56.44.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [121][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.0791 (Avg-Loss1.0791)\tAcc 61.7188 (Avg-Acc61.7188)\n",
      "Epoch: [121][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1214 (Avg-Loss1.1192)\tAcc 61.1328 (Avg-Acc60.4980)\n",
      "Epoch: [121][18/39]\tTime 0.026 (Avg-Time0.024)\t Loss 1.1567 (Avg-Loss1.1286)\tAcc 57.1289 (Avg-Acc59.9815)\n",
      "Epoch: [121][27/39]\tTime 0.026 (Avg-Time0.025)\t Loss 1.1609 (Avg-Loss1.1355)\tAcc 59.2773 (Avg-Acc59.7517)\n",
      "Epoch: [121][36/39]\tTime 0.021 (Avg-Time0.024)\t Loss 1.1488 (Avg-Loss1.1383)\tAcc 60.2539 (Avg-Acc59.7155)\n",
      "Epoch: [121][39/39]\tTime 0.005 (Avg-Time0.024)\t Loss 1.1570 (Avg-Loss1.1391)\tAcc 57.8125 (Avg-Acc59.6850)\n",
      "EPOCH: 121 train Results: Acc 59.685 Loss: 1.1391\n",
      "Epoch: [121][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2033 (Avg-Loss1.2033)\tAcc 58.5938 (Avg-Acc58.5938)\n",
      "Epoch: [121][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2478 (Avg-Loss1.2370)\tAcc 56.5051 (Avg-Acc56.3500)\n",
      "EPOCH: 121 Validation Results: Acc 56.350 Loss: 1.2370\n",
      "Best Accuracy: 56.35.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [122][0/39]\tTime 0.027 (Avg-Time0.027)\t Loss 1.0862 (Avg-Loss1.0862)\tAcc 59.6680 (Avg-Acc59.6680)\n",
      "Epoch: [122][9/39]\tTime 0.023 (Avg-Time0.024)\t Loss 1.1398 (Avg-Loss1.1270)\tAcc 59.4727 (Avg-Acc60.1465)\n",
      "Epoch: [122][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0783 (Avg-Loss1.1301)\tAcc 62.9883 (Avg-Acc60.2488)\n",
      "Epoch: [122][27/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.0857 (Avg-Loss1.1347)\tAcc 61.8164 (Avg-Acc59.8982)\n",
      "Epoch: [122][36/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.1845 (Avg-Loss1.1387)\tAcc 58.2031 (Avg-Acc59.5703)\n",
      "Epoch: [122][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 0.8991 (Avg-Loss1.1374)\tAcc 70.3125 (Avg-Acc59.6025)\n",
      "EPOCH: 122 train Results: Acc 59.602 Loss: 1.1374\n",
      "Epoch: [122][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2037 (Avg-Loss1.2037)\tAcc 58.2031 (Avg-Acc58.2031)\n",
      "Epoch: [122][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2370 (Avg-Loss1.2348)\tAcc 56.6327 (Avg-Acc56.6500)\n",
      "EPOCH: 122 Validation Results: Acc 56.650 Loss: 1.2348\n",
      "Best Accuracy: 56.65.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [123][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0715 (Avg-Loss1.0715)\tAcc 62.0117 (Avg-Acc62.0117)\n",
      "Epoch: [123][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.1262 (Avg-Loss1.1061)\tAcc 59.5703 (Avg-Acc60.9473)\n",
      "Epoch: [123][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1211 (Avg-Loss1.1128)\tAcc 61.3281 (Avg-Acc60.6651)\n",
      "Epoch: [123][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.0928 (Avg-Loss1.1228)\tAcc 60.8398 (Avg-Acc60.0795)\n",
      "Epoch: [123][36/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.0939 (Avg-Loss1.1291)\tAcc 61.0352 (Avg-Acc59.7234)\n",
      "Epoch: [123][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2704 (Avg-Loss1.1314)\tAcc 51.5625 (Avg-Acc59.6250)\n",
      "EPOCH: 123 train Results: Acc 59.625 Loss: 1.1314\n",
      "Epoch: [123][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2059 (Avg-Loss1.2059)\tAcc 58.1055 (Avg-Acc58.1055)\n",
      "Epoch: [123][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2355 (Avg-Loss1.2338)\tAcc 57.6531 (Avg-Acc56.3800)\n",
      "EPOCH: 123 Validation Results: Acc 56.380 Loss: 1.2338\n",
      "Best Accuracy: 56.38.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [124][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.1229 (Avg-Loss1.1229)\tAcc 59.4727 (Avg-Acc59.4727)\n",
      "Epoch: [124][9/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.1500 (Avg-Loss1.1171)\tAcc 58.8867 (Avg-Acc60.2832)\n",
      "Epoch: [124][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1838 (Avg-Loss1.1317)\tAcc 57.6172 (Avg-Acc59.6063)\n",
      "Epoch: [124][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1601 (Avg-Loss1.1330)\tAcc 58.6914 (Avg-Acc59.6296)\n",
      "Epoch: [124][36/39]\tTime 0.026 (Avg-Time0.022)\t Loss 1.1287 (Avg-Loss1.1369)\tAcc 59.3750 (Avg-Acc59.4832)\n",
      "Epoch: [124][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.1276 (Avg-Loss1.1382)\tAcc 54.6875 (Avg-Acc59.4550)\n",
      "EPOCH: 124 train Results: Acc 59.455 Loss: 1.1382\n",
      "Epoch: [124][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2051 (Avg-Loss1.2051)\tAcc 57.9102 (Avg-Acc57.9102)\n",
      "Epoch: [124][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2361 (Avg-Loss1.2334)\tAcc 56.8878 (Avg-Acc56.7700)\n",
      "EPOCH: 124 Validation Results: Acc 56.770 Loss: 1.2334\n",
      "Best Accuracy: 56.77.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [125][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.0944 (Avg-Loss1.0944)\tAcc 61.6211 (Avg-Acc61.6211)\n",
      "Epoch: [125][9/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.1215 (Avg-Loss1.1152)\tAcc 60.3516 (Avg-Acc60.2832)\n",
      "Epoch: [125][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0606 (Avg-Loss1.1223)\tAcc 62.7930 (Avg-Acc60.2436)\n",
      "Epoch: [125][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1713 (Avg-Loss1.1207)\tAcc 57.4219 (Avg-Acc60.2295)\n",
      "Epoch: [125][36/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.2001 (Avg-Loss1.1252)\tAcc 57.8125 (Avg-Acc59.9715)\n",
      "Epoch: [125][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.1350 (Avg-Loss1.1252)\tAcc 59.3750 (Avg-Acc59.9675)\n",
      "EPOCH: 125 train Results: Acc 59.968 Loss: 1.1252\n",
      "Epoch: [125][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2035 (Avg-Loss1.2035)\tAcc 58.1055 (Avg-Acc58.1055)\n",
      "Epoch: [125][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2380 (Avg-Loss1.2337)\tAcc 57.1429 (Avg-Acc56.1900)\n",
      "EPOCH: 125 Validation Results: Acc 56.190 Loss: 1.2337\n",
      "Best Accuracy: 56.19.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [126][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.1235 (Avg-Loss1.1235)\tAcc 61.2305 (Avg-Acc61.2305)\n",
      "Epoch: [126][9/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1297 (Avg-Loss1.1169)\tAcc 58.7891 (Avg-Acc59.9219)\n",
      "Epoch: [126][18/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1295 (Avg-Loss1.1203)\tAcc 58.4961 (Avg-Acc59.7810)\n",
      "Epoch: [126][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1677 (Avg-Loss1.1265)\tAcc 58.8867 (Avg-Acc59.7028)\n",
      "Epoch: [126][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1610 (Avg-Loss1.1339)\tAcc 58.4961 (Avg-Acc59.4806)\n",
      "Epoch: [126][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.1526 (Avg-Loss1.1355)\tAcc 67.1875 (Avg-Acc59.4225)\n",
      "EPOCH: 126 train Results: Acc 59.422 Loss: 1.1355\n",
      "Epoch: [126][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.2014 (Avg-Loss1.2014)\tAcc 57.9102 (Avg-Acc57.9102)\n",
      "Epoch: [126][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2401 (Avg-Loss1.2361)\tAcc 55.8673 (Avg-Acc56.2400)\n",
      "EPOCH: 126 Validation Results: Acc 56.240 Loss: 1.2361\n",
      "Best Accuracy: 56.24.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [127][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1246 (Avg-Loss1.1246)\tAcc 60.7422 (Avg-Acc60.7422)\n",
      "Epoch: [127][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1058 (Avg-Loss1.1059)\tAcc 61.8164 (Avg-Acc60.6641)\n",
      "Epoch: [127][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1315 (Avg-Loss1.1207)\tAcc 59.9609 (Avg-Acc60.3413)\n",
      "Epoch: [127][27/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1248 (Avg-Loss1.1253)\tAcc 60.5469 (Avg-Acc60.3167)\n",
      "Epoch: [127][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1342 (Avg-Loss1.1293)\tAcc 60.4492 (Avg-Acc60.0401)\n",
      "Epoch: [127][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.0316 (Avg-Loss1.1291)\tAcc 65.6250 (Avg-Acc60.0625)\n",
      "EPOCH: 127 train Results: Acc 60.062 Loss: 1.1291\n",
      "Epoch: [127][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2011 (Avg-Loss1.2011)\tAcc 57.8125 (Avg-Acc57.8125)\n",
      "Epoch: [127][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2337 (Avg-Loss1.2315)\tAcc 57.0153 (Avg-Acc56.4700)\n",
      "EPOCH: 127 Validation Results: Acc 56.470 Loss: 1.2315\n",
      "Best Accuracy: 56.47.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [128][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0945 (Avg-Loss1.0945)\tAcc 62.7930 (Avg-Acc62.7930)\n",
      "Epoch: [128][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1836 (Avg-Loss1.1085)\tAcc 58.2031 (Avg-Acc60.5273)\n",
      "Epoch: [128][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1431 (Avg-Loss1.1227)\tAcc 60.2539 (Avg-Acc60.0894)\n",
      "Epoch: [128][27/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1497 (Avg-Loss1.1360)\tAcc 60.7422 (Avg-Acc59.7831)\n",
      "Epoch: [128][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1772 (Avg-Loss1.1387)\tAcc 57.9102 (Avg-Acc59.6363)\n",
      "Epoch: [128][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.0919 (Avg-Loss1.1368)\tAcc 59.3750 (Avg-Acc59.7000)\n",
      "EPOCH: 128 train Results: Acc 59.700 Loss: 1.1368\n",
      "Epoch: [128][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2089 (Avg-Loss1.2089)\tAcc 58.2031 (Avg-Acc58.2031)\n",
      "Epoch: [128][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2273 (Avg-Loss1.2298)\tAcc 57.6531 (Avg-Acc56.6200)\n",
      "EPOCH: 128 Validation Results: Acc 56.620 Loss: 1.2298\n",
      "Best Accuracy: 56.62.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [129][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0982 (Avg-Loss1.0982)\tAcc 60.8398 (Avg-Acc60.8398)\n",
      "Epoch: [129][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1840 (Avg-Loss1.1118)\tAcc 55.8594 (Avg-Acc60.1660)\n",
      "Epoch: [129][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.1195 (Avg-Loss1.1075)\tAcc 60.7422 (Avg-Acc60.4081)\n",
      "Epoch: [129][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1920 (Avg-Loss1.1189)\tAcc 56.7383 (Avg-Acc60.0551)\n",
      "Epoch: [129][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0917 (Avg-Loss1.1251)\tAcc 62.0117 (Avg-Acc59.8395)\n",
      "Epoch: [129][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.2593 (Avg-Loss1.1251)\tAcc 53.1250 (Avg-Acc59.8450)\n",
      "EPOCH: 129 train Results: Acc 59.845 Loss: 1.1251\n",
      "Epoch: [129][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2059 (Avg-Loss1.2059)\tAcc 58.2031 (Avg-Acc58.2031)\n",
      "Epoch: [129][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 1.2341 (Avg-Loss1.2256)\tAcc 58.0357 (Avg-Acc56.7900)\n",
      "EPOCH: 129 Validation Results: Acc 56.790 Loss: 1.2256\n",
      "Best Accuracy: 56.79.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [130][0/39]\tTime 0.028 (Avg-Time0.028)\t Loss 1.1039 (Avg-Loss1.1039)\tAcc 61.8164 (Avg-Acc61.8164)\n",
      "Epoch: [130][9/39]\tTime 0.026 (Avg-Time0.024)\t Loss 1.1112 (Avg-Loss1.0958)\tAcc 61.2305 (Avg-Acc61.3086)\n",
      "Epoch: [130][18/39]\tTime 0.029 (Avg-Time0.025)\t Loss 1.0942 (Avg-Loss1.1114)\tAcc 60.3516 (Avg-Acc60.5006)\n",
      "Epoch: [130][27/39]\tTime 0.023 (Avg-Time0.026)\t Loss 1.1599 (Avg-Loss1.1234)\tAcc 58.4961 (Avg-Acc60.0377)\n",
      "Epoch: [130][36/39]\tTime 0.021 (Avg-Time0.025)\t Loss 1.0572 (Avg-Loss1.1229)\tAcc 62.4023 (Avg-Acc60.1035)\n",
      "Epoch: [130][39/39]\tTime 0.006 (Avg-Time0.024)\t Loss 1.1369 (Avg-Loss1.1244)\tAcc 57.8125 (Avg-Acc60.0450)\n",
      "EPOCH: 130 train Results: Acc 60.045 Loss: 1.1244\n",
      "Epoch: [130][0/9]\tTime 0.008 (Avg-Time0.008)\t Loss 1.2036 (Avg-Loss1.2036)\tAcc 58.0078 (Avg-Acc58.0078)\n",
      "Epoch: [130][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2279 (Avg-Loss1.2299)\tAcc 56.7602 (Avg-Acc56.6700)\n",
      "EPOCH: 130 Validation Results: Acc 56.670 Loss: 1.2299\n",
      "Best Accuracy: 56.67.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [131][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.0768 (Avg-Loss1.0768)\tAcc 61.5234 (Avg-Acc61.5234)\n",
      "Epoch: [131][9/39]\tTime 0.022 (Avg-Time0.024)\t Loss 1.1413 (Avg-Loss1.0972)\tAcc 59.7656 (Avg-Acc61.5234)\n",
      "Epoch: [131][18/39]\tTime 0.023 (Avg-Time0.024)\t Loss 1.1431 (Avg-Loss1.1172)\tAcc 59.8633 (Avg-Acc60.3978)\n",
      "Epoch: [131][27/39]\tTime 0.023 (Avg-Time0.024)\t Loss 1.1639 (Avg-Loss1.1255)\tAcc 58.0078 (Avg-Acc60.0481)\n",
      "Epoch: [131][36/39]\tTime 0.023 (Avg-Time0.024)\t Loss 1.1665 (Avg-Loss1.1272)\tAcc 58.6914 (Avg-Acc59.9108)\n",
      "Epoch: [131][39/39]\tTime 0.007 (Avg-Time0.023)\t Loss 1.0442 (Avg-Loss1.1278)\tAcc 67.1875 (Avg-Acc59.9025)\n",
      "EPOCH: 131 train Results: Acc 59.903 Loss: 1.1278\n",
      "Epoch: [131][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2051 (Avg-Loss1.2051)\tAcc 58.9844 (Avg-Acc58.9844)\n",
      "Epoch: [131][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2217 (Avg-Loss1.2297)\tAcc 57.9082 (Avg-Acc56.6800)\n",
      "EPOCH: 131 Validation Results: Acc 56.680 Loss: 1.2297\n",
      "Best Accuracy: 56.68.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [132][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0796 (Avg-Loss1.0796)\tAcc 60.5469 (Avg-Acc60.5469)\n",
      "Epoch: [132][9/39]\tTime 0.027 (Avg-Time0.023)\t Loss 1.1180 (Avg-Loss1.0962)\tAcc 61.3281 (Avg-Acc61.3477)\n",
      "Epoch: [132][18/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.1468 (Avg-Loss1.1131)\tAcc 60.2539 (Avg-Acc60.7422)\n",
      "Epoch: [132][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1133 (Avg-Loss1.1188)\tAcc 60.0586 (Avg-Acc60.6027)\n",
      "Epoch: [132][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1786 (Avg-Loss1.1239)\tAcc 57.9102 (Avg-Acc60.3173)\n",
      "Epoch: [132][39/39]\tTime 0.005 (Avg-Time0.023)\t Loss 1.1615 (Avg-Loss1.1244)\tAcc 57.8125 (Avg-Acc60.2775)\n",
      "EPOCH: 132 train Results: Acc 60.278 Loss: 1.1244\n",
      "Epoch: [132][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.2077 (Avg-Loss1.2077)\tAcc 57.9102 (Avg-Acc57.9102)\n",
      "Epoch: [132][9/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2297 (Avg-Loss1.2315)\tAcc 56.8878 (Avg-Acc56.6500)\n",
      "EPOCH: 132 Validation Results: Acc 56.650 Loss: 1.2315\n",
      "Best Accuracy: 56.65.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [133][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1498 (Avg-Loss1.1498)\tAcc 59.2773 (Avg-Acc59.2773)\n",
      "Epoch: [133][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1206 (Avg-Loss1.1080)\tAcc 61.7188 (Avg-Acc60.5273)\n",
      "Epoch: [133][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1370 (Avg-Loss1.1181)\tAcc 58.4961 (Avg-Acc60.2179)\n",
      "Epoch: [133][27/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.1111 (Avg-Loss1.1206)\tAcc 60.3516 (Avg-Acc60.3062)\n",
      "Epoch: [133][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0983 (Avg-Loss1.1231)\tAcc 60.1562 (Avg-Acc60.2011)\n",
      "Epoch: [133][39/39]\tTime 0.004 (Avg-Time0.021)\t Loss 1.3933 (Avg-Loss1.1249)\tAcc 51.5625 (Avg-Acc60.0900)\n",
      "EPOCH: 133 train Results: Acc 60.090 Loss: 1.1249\n",
      "Epoch: [133][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2086 (Avg-Loss1.2086)\tAcc 58.0078 (Avg-Acc58.0078)\n",
      "Epoch: [133][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2342 (Avg-Loss1.2298)\tAcc 57.7806 (Avg-Acc56.7200)\n",
      "EPOCH: 133 Validation Results: Acc 56.720 Loss: 1.2298\n",
      "Best Accuracy: 56.72.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [134][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.0465 (Avg-Loss1.0465)\tAcc 64.0625 (Avg-Acc64.0625)\n",
      "Epoch: [134][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1516 (Avg-Loss1.1123)\tAcc 60.8398 (Avg-Acc61.3770)\n",
      "Epoch: [134][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0863 (Avg-Loss1.1191)\tAcc 61.1328 (Avg-Acc60.7370)\n",
      "Epoch: [134][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1635 (Avg-Loss1.1263)\tAcc 57.9102 (Avg-Acc60.4527)\n",
      "Epoch: [134][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1361 (Avg-Loss1.1292)\tAcc 60.7422 (Avg-Acc60.3595)\n",
      "Epoch: [134][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2251 (Avg-Loss1.1301)\tAcc 57.8125 (Avg-Acc60.3300)\n",
      "EPOCH: 134 train Results: Acc 60.330 Loss: 1.1301\n",
      "Epoch: [134][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.1997 (Avg-Loss1.1997)\tAcc 58.4961 (Avg-Acc58.4961)\n",
      "Epoch: [134][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2391 (Avg-Loss1.2277)\tAcc 56.6327 (Avg-Acc56.6100)\n",
      "EPOCH: 134 Validation Results: Acc 56.610 Loss: 1.2277\n",
      "Best Accuracy: 56.61.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [135][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.0988 (Avg-Loss1.0988)\tAcc 61.1328 (Avg-Acc61.1328)\n",
      "Epoch: [135][9/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.1239 (Avg-Loss1.1085)\tAcc 60.2539 (Avg-Acc60.8301)\n",
      "Epoch: [135][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0944 (Avg-Loss1.1110)\tAcc 62.2070 (Avg-Acc60.8398)\n",
      "Epoch: [135][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1884 (Avg-Loss1.1164)\tAcc 58.6914 (Avg-Acc60.6445)\n",
      "Epoch: [135][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1491 (Avg-Loss1.1221)\tAcc 59.7656 (Avg-Acc60.2011)\n",
      "Epoch: [135][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.1781 (Avg-Loss1.1227)\tAcc 54.6875 (Avg-Acc60.1975)\n",
      "EPOCH: 135 train Results: Acc 60.197 Loss: 1.1227\n",
      "Epoch: [135][0/9]\tTime 0.008 (Avg-Time0.008)\t Loss 1.1997 (Avg-Loss1.1997)\tAcc 58.7891 (Avg-Acc58.7891)\n",
      "Epoch: [135][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2408 (Avg-Loss1.2287)\tAcc 57.3980 (Avg-Acc56.8800)\n",
      "EPOCH: 135 Validation Results: Acc 56.880 Loss: 1.2287\n",
      "Best Accuracy: 56.88.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [136][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.1021 (Avg-Loss1.1021)\tAcc 60.7422 (Avg-Acc60.7422)\n",
      "Epoch: [136][9/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.1103 (Avg-Loss1.0949)\tAcc 61.0352 (Avg-Acc61.2598)\n",
      "Epoch: [136][18/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1443 (Avg-Loss1.1013)\tAcc 59.3750 (Avg-Acc61.1225)\n",
      "Epoch: [136][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1193 (Avg-Loss1.1101)\tAcc 59.0820 (Avg-Acc60.6201)\n",
      "Epoch: [136][36/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.1809 (Avg-Loss1.1179)\tAcc 57.7148 (Avg-Acc60.2565)\n",
      "Epoch: [136][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2982 (Avg-Loss1.1192)\tAcc 51.5625 (Avg-Acc60.2300)\n",
      "EPOCH: 136 train Results: Acc 60.230 Loss: 1.1192\n",
      "Epoch: [136][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.1998 (Avg-Loss1.1998)\tAcc 59.4727 (Avg-Acc59.4727)\n",
      "Epoch: [136][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2325 (Avg-Loss1.2331)\tAcc 56.1224 (Avg-Acc56.5700)\n",
      "EPOCH: 136 Validation Results: Acc 56.570 Loss: 1.2331\n",
      "Best Accuracy: 56.57.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [137][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.1118 (Avg-Loss1.1118)\tAcc 61.1328 (Avg-Acc61.1328)\n",
      "Epoch: [137][9/39]\tTime 0.022 (Avg-Time0.021)\t Loss 1.2036 (Avg-Loss1.1019)\tAcc 57.1289 (Avg-Acc61.0156)\n",
      "Epoch: [137][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1615 (Avg-Loss1.1086)\tAcc 57.9102 (Avg-Acc60.8090)\n",
      "Epoch: [137][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1594 (Avg-Loss1.1144)\tAcc 58.3984 (Avg-Acc60.6585)\n",
      "Epoch: [137][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1391 (Avg-Loss1.1186)\tAcc 59.5703 (Avg-Acc60.4334)\n",
      "Epoch: [137][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.2173 (Avg-Loss1.1209)\tAcc 53.1250 (Avg-Acc60.3175)\n",
      "EPOCH: 137 train Results: Acc 60.318 Loss: 1.1209\n",
      "Epoch: [137][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1946 (Avg-Loss1.1946)\tAcc 58.8867 (Avg-Acc58.8867)\n",
      "Epoch: [137][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2313 (Avg-Loss1.2281)\tAcc 57.1429 (Avg-Acc56.3400)\n",
      "EPOCH: 137 Validation Results: Acc 56.340 Loss: 1.2281\n",
      "Best Accuracy: 56.34.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [138][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0435 (Avg-Loss1.0435)\tAcc 63.1836 (Avg-Acc63.1836)\n",
      "Epoch: [138][9/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.1124 (Avg-Loss1.0938)\tAcc 60.2539 (Avg-Acc61.3281)\n",
      "Epoch: [138][18/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1770 (Avg-Loss1.1059)\tAcc 59.4727 (Avg-Acc60.6651)\n",
      "Epoch: [138][27/39]\tTime 0.025 (Avg-Time0.024)\t Loss 1.1862 (Avg-Loss1.1083)\tAcc 58.8867 (Avg-Acc60.6899)\n",
      "Epoch: [138][36/39]\tTime 0.035 (Avg-Time0.025)\t Loss 1.1538 (Avg-Loss1.1167)\tAcc 59.2773 (Avg-Acc60.4043)\n",
      "Epoch: [138][39/39]\tTime 0.003 (Avg-Time0.024)\t Loss 1.3443 (Avg-Loss1.1181)\tAcc 43.7500 (Avg-Acc60.2975)\n",
      "EPOCH: 138 train Results: Acc 60.297 Loss: 1.1181\n",
      "Epoch: [138][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1982 (Avg-Loss1.1982)\tAcc 57.9102 (Avg-Acc57.9102)\n",
      "Epoch: [138][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2296 (Avg-Loss1.2268)\tAcc 56.8878 (Avg-Acc56.5700)\n",
      "EPOCH: 138 Validation Results: Acc 56.570 Loss: 1.2268\n",
      "Best Accuracy: 56.57.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [139][0/39]\tTime 0.028 (Avg-Time0.028)\t Loss 1.0977 (Avg-Loss1.0977)\tAcc 60.5469 (Avg-Acc60.5469)\n",
      "Epoch: [139][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0764 (Avg-Loss1.1023)\tAcc 62.8906 (Avg-Acc61.1816)\n",
      "Epoch: [139][18/39]\tTime 0.032 (Avg-Time0.022)\t Loss 1.1052 (Avg-Loss1.1055)\tAcc 62.0117 (Avg-Acc61.0557)\n",
      "Epoch: [139][27/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.0947 (Avg-Loss1.1101)\tAcc 60.4492 (Avg-Acc60.6376)\n",
      "Epoch: [139][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1438 (Avg-Loss1.1113)\tAcc 58.7891 (Avg-Acc60.4651)\n",
      "Epoch: [139][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.1021 (Avg-Loss1.1114)\tAcc 56.2500 (Avg-Acc60.4875)\n",
      "EPOCH: 139 train Results: Acc 60.487 Loss: 1.1114\n",
      "Epoch: [139][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1935 (Avg-Loss1.1935)\tAcc 58.3008 (Avg-Acc58.3008)\n",
      "Epoch: [139][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2425 (Avg-Loss1.2269)\tAcc 56.8878 (Avg-Acc56.6500)\n",
      "EPOCH: 139 Validation Results: Acc 56.650 Loss: 1.2269\n",
      "Best Accuracy: 56.65.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [140][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.0575 (Avg-Loss1.0575)\tAcc 62.5000 (Avg-Acc62.5000)\n",
      "Epoch: [140][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.0773 (Avg-Loss1.0955)\tAcc 62.1094 (Avg-Acc60.8203)\n",
      "Epoch: [140][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1439 (Avg-Loss1.1110)\tAcc 59.4727 (Avg-Acc60.2128)\n",
      "Epoch: [140][27/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.1338 (Avg-Loss1.1188)\tAcc 58.3984 (Avg-Acc60.0412)\n",
      "Epoch: [140][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1291 (Avg-Loss1.1201)\tAcc 59.0820 (Avg-Acc60.0243)\n",
      "Epoch: [140][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.1005 (Avg-Loss1.1205)\tAcc 60.9375 (Avg-Acc60.0100)\n",
      "EPOCH: 140 train Results: Acc 60.010 Loss: 1.1205\n",
      "Epoch: [140][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2086 (Avg-Loss1.2086)\tAcc 57.9102 (Avg-Acc57.9102)\n",
      "Epoch: [140][9/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2488 (Avg-Loss1.2327)\tAcc 57.7806 (Avg-Acc56.5100)\n",
      "EPOCH: 140 Validation Results: Acc 56.510 Loss: 1.2327\n",
      "Best Accuracy: 56.51.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [141][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.0383 (Avg-Loss1.0383)\tAcc 64.3555 (Avg-Acc64.3555)\n",
      "Epoch: [141][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1092 (Avg-Loss1.0824)\tAcc 60.9375 (Avg-Acc61.7285)\n",
      "Epoch: [141][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1789 (Avg-Loss1.0966)\tAcc 58.7891 (Avg-Acc61.1945)\n",
      "Epoch: [141][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1340 (Avg-Loss1.1084)\tAcc 60.2539 (Avg-Acc60.7980)\n",
      "Epoch: [141][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1445 (Avg-Loss1.1116)\tAcc 58.5938 (Avg-Acc60.5178)\n",
      "Epoch: [141][39/39]\tTime 0.005 (Avg-Time0.021)\t Loss 1.2544 (Avg-Loss1.1143)\tAcc 54.6875 (Avg-Acc60.3725)\n",
      "EPOCH: 141 train Results: Acc 60.373 Loss: 1.1143\n",
      "Epoch: [141][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1905 (Avg-Loss1.1905)\tAcc 59.8633 (Avg-Acc59.8633)\n",
      "Epoch: [141][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 1.2410 (Avg-Loss1.2263)\tAcc 58.0357 (Avg-Acc57.1900)\n",
      "EPOCH: 141 Validation Results: Acc 57.190 Loss: 1.2263\n",
      "Best Accuracy: 57.19.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [142][0/39]\tTime 0.028 (Avg-Time0.028)\t Loss 1.0889 (Avg-Loss1.0889)\tAcc 59.6680 (Avg-Acc59.6680)\n",
      "Epoch: [142][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1071 (Avg-Loss1.0835)\tAcc 60.5469 (Avg-Acc61.4941)\n",
      "Epoch: [142][18/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.2273 (Avg-Loss1.1056)\tAcc 56.7383 (Avg-Acc60.8398)\n",
      "Epoch: [142][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1216 (Avg-Loss1.1081)\tAcc 59.0820 (Avg-Acc60.7840)\n",
      "Epoch: [142][36/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.0962 (Avg-Loss1.1109)\tAcc 61.3281 (Avg-Acc60.6947)\n",
      "Epoch: [142][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.1200 (Avg-Loss1.1139)\tAcc 60.9375 (Avg-Acc60.6350)\n",
      "EPOCH: 142 train Results: Acc 60.635 Loss: 1.1139\n",
      "Epoch: [142][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1979 (Avg-Loss1.1979)\tAcc 58.3008 (Avg-Acc58.3008)\n",
      "Epoch: [142][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2428 (Avg-Loss1.2263)\tAcc 57.3980 (Avg-Acc56.9800)\n",
      "EPOCH: 142 Validation Results: Acc 56.980 Loss: 1.2263\n",
      "Best Accuracy: 56.98.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [143][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0517 (Avg-Loss1.0517)\tAcc 62.7930 (Avg-Acc62.7930)\n",
      "Epoch: [143][9/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.1376 (Avg-Loss1.0826)\tAcc 61.1328 (Avg-Acc62.0020)\n",
      "Epoch: [143][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1302 (Avg-Loss1.0978)\tAcc 59.3750 (Avg-Acc61.1585)\n",
      "Epoch: [143][27/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1367 (Avg-Loss1.1018)\tAcc 58.3984 (Avg-Acc60.9828)\n",
      "Epoch: [143][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1193 (Avg-Loss1.1075)\tAcc 60.6445 (Avg-Acc60.8003)\n",
      "Epoch: [143][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2205 (Avg-Loss1.1097)\tAcc 50.0000 (Avg-Acc60.7350)\n",
      "EPOCH: 143 train Results: Acc 60.735 Loss: 1.1097\n",
      "Epoch: [143][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.1983 (Avg-Loss1.1983)\tAcc 58.2031 (Avg-Acc58.2031)\n",
      "Epoch: [143][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2418 (Avg-Loss1.2266)\tAcc 56.8878 (Avg-Acc56.9000)\n",
      "EPOCH: 143 Validation Results: Acc 56.900 Loss: 1.2266\n",
      "Best Accuracy: 56.9.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [144][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.0869 (Avg-Loss1.0869)\tAcc 62.2070 (Avg-Acc62.2070)\n",
      "Epoch: [144][9/39]\tTime 0.020 (Avg-Time0.023)\t Loss 1.0489 (Avg-Loss1.0794)\tAcc 62.3047 (Avg-Acc62.0117)\n",
      "Epoch: [144][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1178 (Avg-Loss1.0899)\tAcc 59.0820 (Avg-Acc61.5646)\n",
      "Epoch: [144][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1083 (Avg-Loss1.0976)\tAcc 60.5469 (Avg-Acc61.0038)\n",
      "Epoch: [144][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1202 (Avg-Loss1.1051)\tAcc 60.7422 (Avg-Acc60.7395)\n",
      "Epoch: [144][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.0345 (Avg-Loss1.1058)\tAcc 68.7500 (Avg-Acc60.7650)\n",
      "EPOCH: 144 train Results: Acc 60.765 Loss: 1.1058\n",
      "Epoch: [144][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2034 (Avg-Loss1.2034)\tAcc 58.3984 (Avg-Acc58.3984)\n",
      "Epoch: [144][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2430 (Avg-Loss1.2303)\tAcc 55.6122 (Avg-Acc56.6500)\n",
      "EPOCH: 144 Validation Results: Acc 56.650 Loss: 1.2303\n",
      "Best Accuracy: 56.65.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [145][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.0657 (Avg-Loss1.0657)\tAcc 61.0352 (Avg-Acc61.0352)\n",
      "Epoch: [145][9/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.1029 (Avg-Loss1.0841)\tAcc 60.5469 (Avg-Acc61.5820)\n",
      "Epoch: [145][18/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1151 (Avg-Loss1.0948)\tAcc 59.7656 (Avg-Acc61.0609)\n",
      "Epoch: [145][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0960 (Avg-Loss1.1045)\tAcc 61.0352 (Avg-Acc60.6550)\n",
      "Epoch: [145][36/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.1408 (Avg-Loss1.1113)\tAcc 59.2773 (Avg-Acc60.4914)\n",
      "Epoch: [145][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.1262 (Avg-Loss1.1113)\tAcc 57.8125 (Avg-Acc60.4750)\n",
      "EPOCH: 145 train Results: Acc 60.475 Loss: 1.1113\n",
      "Epoch: [145][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1893 (Avg-Loss1.1893)\tAcc 58.1055 (Avg-Acc58.1055)\n",
      "Epoch: [145][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 1.2459 (Avg-Loss1.2266)\tAcc 55.4847 (Avg-Acc56.3500)\n",
      "EPOCH: 145 Validation Results: Acc 56.350 Loss: 1.2266\n",
      "Best Accuracy: 56.35.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [146][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0899 (Avg-Loss1.0899)\tAcc 60.6445 (Avg-Acc60.6445)\n",
      "Epoch: [146][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1182 (Avg-Loss1.0944)\tAcc 60.0586 (Avg-Acc61.0938)\n",
      "Epoch: [146][18/39]\tTime 0.031 (Avg-Time0.023)\t Loss 1.0812 (Avg-Loss1.1096)\tAcc 61.2305 (Avg-Acc60.5520)\n",
      "Epoch: [146][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1200 (Avg-Loss1.1097)\tAcc 60.5469 (Avg-Acc60.5818)\n",
      "Epoch: [146][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0990 (Avg-Loss1.1134)\tAcc 59.5703 (Avg-Acc60.3621)\n",
      "Epoch: [146][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.2716 (Avg-Loss1.1138)\tAcc 46.8750 (Avg-Acc60.3800)\n",
      "EPOCH: 146 train Results: Acc 60.380 Loss: 1.1138\n",
      "Epoch: [146][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2059 (Avg-Loss1.2059)\tAcc 58.0078 (Avg-Acc58.0078)\n",
      "Epoch: [146][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2463 (Avg-Loss1.2266)\tAcc 56.1224 (Avg-Acc56.2600)\n",
      "EPOCH: 146 Validation Results: Acc 56.260 Loss: 1.2266\n",
      "Best Accuracy: 56.26.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [147][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0473 (Avg-Loss1.0473)\tAcc 61.3281 (Avg-Acc61.3281)\n",
      "Epoch: [147][9/39]\tTime 0.026 (Avg-Time0.025)\t Loss 1.0783 (Avg-Loss1.0805)\tAcc 60.8398 (Avg-Acc61.4844)\n",
      "Epoch: [147][18/39]\tTime 0.026 (Avg-Time0.027)\t Loss 1.1039 (Avg-Loss1.1011)\tAcc 60.2539 (Avg-Acc60.7730)\n",
      "Epoch: [147][27/39]\tTime 0.022 (Avg-Time0.026)\t Loss 1.1673 (Avg-Loss1.1107)\tAcc 59.1797 (Avg-Acc60.4841)\n",
      "Epoch: [147][36/39]\tTime 0.021 (Avg-Time0.025)\t Loss 1.1862 (Avg-Loss1.1140)\tAcc 57.5195 (Avg-Acc60.5046)\n",
      "Epoch: [147][39/39]\tTime 0.004 (Avg-Time0.024)\t Loss 0.8012 (Avg-Loss1.1156)\tAcc 71.8750 (Avg-Acc60.3875)\n",
      "EPOCH: 147 train Results: Acc 60.388 Loss: 1.1156\n",
      "Epoch: [147][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1951 (Avg-Loss1.1951)\tAcc 59.7656 (Avg-Acc59.7656)\n",
      "Epoch: [147][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2478 (Avg-Loss1.2273)\tAcc 56.1224 (Avg-Acc56.5800)\n",
      "EPOCH: 147 Validation Results: Acc 56.580 Loss: 1.2273\n",
      "Best Accuracy: 56.58.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [148][0/39]\tTime 0.030 (Avg-Time0.030)\t Loss 1.0739 (Avg-Loss1.0739)\tAcc 61.1328 (Avg-Acc61.1328)\n",
      "Epoch: [148][9/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.0593 (Avg-Loss1.0894)\tAcc 62.5977 (Avg-Acc60.9668)\n",
      "Epoch: [148][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1560 (Avg-Loss1.0897)\tAcc 58.7891 (Avg-Acc61.2819)\n",
      "Epoch: [148][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1071 (Avg-Loss1.0872)\tAcc 61.0352 (Avg-Acc61.4851)\n",
      "Epoch: [148][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1299 (Avg-Loss1.0969)\tAcc 59.2773 (Avg-Acc61.0193)\n",
      "Epoch: [148][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.1035 (Avg-Loss1.0993)\tAcc 60.9375 (Avg-Acc60.9250)\n",
      "EPOCH: 148 train Results: Acc 60.925 Loss: 1.0993\n",
      "Epoch: [148][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2024 (Avg-Loss1.2024)\tAcc 59.1797 (Avg-Acc59.1797)\n",
      "Epoch: [148][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2454 (Avg-Loss1.2258)\tAcc 56.6327 (Avg-Acc56.7500)\n",
      "EPOCH: 148 Validation Results: Acc 56.750 Loss: 1.2258\n",
      "Best Accuracy: 56.75.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [149][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0888 (Avg-Loss1.0888)\tAcc 62.6953 (Avg-Acc62.6953)\n",
      "Epoch: [149][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.0330 (Avg-Loss1.0756)\tAcc 64.2578 (Avg-Acc62.0996)\n",
      "Epoch: [149][18/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1289 (Avg-Loss1.0894)\tAcc 60.8398 (Avg-Acc61.5646)\n",
      "Epoch: [149][27/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1434 (Avg-Loss1.0950)\tAcc 60.0586 (Avg-Acc61.3246)\n",
      "Epoch: [149][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0976 (Avg-Loss1.0995)\tAcc 62.0117 (Avg-Acc61.2833)\n",
      "Epoch: [149][39/39]\tTime 0.005 (Avg-Time0.022)\t Loss 1.1673 (Avg-Loss1.1005)\tAcc 51.5625 (Avg-Acc61.2375)\n",
      "EPOCH: 149 train Results: Acc 61.237 Loss: 1.1005\n",
      "Epoch: [149][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2037 (Avg-Loss1.2037)\tAcc 57.9102 (Avg-Acc57.9102)\n",
      "Epoch: [149][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2336 (Avg-Loss1.2237)\tAcc 55.7398 (Avg-Acc56.8100)\n",
      "EPOCH: 149 Validation Results: Acc 56.810 Loss: 1.2237\n",
      "Best Accuracy: 56.81.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [150][0/39]\tTime 0.029 (Avg-Time0.029)\t Loss 1.0123 (Avg-Loss1.0123)\tAcc 64.5508 (Avg-Acc64.5508)\n",
      "Epoch: [150][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1209 (Avg-Loss1.0773)\tAcc 60.3516 (Avg-Acc62.2754)\n",
      "Epoch: [150][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0836 (Avg-Loss1.0852)\tAcc 61.5234 (Avg-Acc61.6931)\n",
      "Epoch: [150][27/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.0776 (Avg-Loss1.0947)\tAcc 61.9141 (Avg-Acc61.3630)\n",
      "Epoch: [150][36/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.1423 (Avg-Loss1.1020)\tAcc 58.9844 (Avg-Acc61.1275)\n",
      "Epoch: [150][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 0.9990 (Avg-Loss1.1038)\tAcc 70.3125 (Avg-Acc61.0175)\n",
      "EPOCH: 150 train Results: Acc 61.017 Loss: 1.1038\n",
      "Epoch: [150][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.1958 (Avg-Loss1.1958)\tAcc 57.2266 (Avg-Acc57.2266)\n",
      "Epoch: [150][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2310 (Avg-Loss1.2206)\tAcc 56.3776 (Avg-Acc56.7200)\n",
      "EPOCH: 150 Validation Results: Acc 56.720 Loss: 1.2206\n",
      "Best Accuracy: 56.72.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [151][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.0855 (Avg-Loss1.0855)\tAcc 61.5234 (Avg-Acc61.5234)\n",
      "Epoch: [151][9/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.0415 (Avg-Loss1.0692)\tAcc 62.7930 (Avg-Acc62.1289)\n",
      "Epoch: [151][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0870 (Avg-Loss1.0863)\tAcc 62.7930 (Avg-Acc61.5543)\n",
      "Epoch: [151][27/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0806 (Avg-Loss1.0952)\tAcc 63.2812 (Avg-Acc61.2095)\n",
      "Epoch: [151][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1231 (Avg-Loss1.0945)\tAcc 59.8633 (Avg-Acc61.0457)\n",
      "Epoch: [151][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.2294 (Avg-Loss1.0975)\tAcc 53.1250 (Avg-Acc60.9250)\n",
      "EPOCH: 151 train Results: Acc 60.925 Loss: 1.0975\n",
      "Epoch: [151][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1997 (Avg-Loss1.1997)\tAcc 60.1562 (Avg-Acc60.1562)\n",
      "Epoch: [151][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2357 (Avg-Loss1.2284)\tAcc 56.3776 (Avg-Acc56.7900)\n",
      "EPOCH: 151 Validation Results: Acc 56.790 Loss: 1.2284\n",
      "Best Accuracy: 56.79.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [152][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0908 (Avg-Loss1.0908)\tAcc 62.1094 (Avg-Acc62.1094)\n",
      "Epoch: [152][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1064 (Avg-Loss1.0916)\tAcc 60.6445 (Avg-Acc61.1914)\n",
      "Epoch: [152][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1013 (Avg-Loss1.0900)\tAcc 60.1562 (Avg-Acc61.4258)\n",
      "Epoch: [152][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0800 (Avg-Loss1.0938)\tAcc 61.0352 (Avg-Acc61.1363)\n",
      "Epoch: [152][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0941 (Avg-Loss1.0957)\tAcc 61.3281 (Avg-Acc61.1249)\n",
      "Epoch: [152][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.0479 (Avg-Loss1.0965)\tAcc 64.0625 (Avg-Acc61.1000)\n",
      "EPOCH: 152 train Results: Acc 61.100 Loss: 1.0965\n",
      "Epoch: [152][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2060 (Avg-Loss1.2060)\tAcc 58.3984 (Avg-Acc58.3984)\n",
      "Epoch: [152][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2550 (Avg-Loss1.2298)\tAcc 55.7398 (Avg-Acc56.7800)\n",
      "EPOCH: 152 Validation Results: Acc 56.780 Loss: 1.2298\n",
      "Best Accuracy: 56.78.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [153][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.1097 (Avg-Loss1.1097)\tAcc 59.7656 (Avg-Acc59.7656)\n",
      "Epoch: [153][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0993 (Avg-Loss1.0752)\tAcc 62.0117 (Avg-Acc62.1875)\n",
      "Epoch: [153][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.1139 (Avg-Loss1.0852)\tAcc 59.4727 (Avg-Acc61.6879)\n",
      "Epoch: [153][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0833 (Avg-Loss1.0860)\tAcc 59.5703 (Avg-Acc61.3491)\n",
      "Epoch: [153][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1155 (Avg-Loss1.0966)\tAcc 60.6445 (Avg-Acc61.1460)\n",
      "Epoch: [153][39/39]\tTime 0.004 (Avg-Time0.021)\t Loss 1.1549 (Avg-Loss1.0975)\tAcc 59.3750 (Avg-Acc61.1100)\n",
      "EPOCH: 153 train Results: Acc 61.110 Loss: 1.0975\n",
      "Epoch: [153][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.1928 (Avg-Loss1.1928)\tAcc 59.1797 (Avg-Acc59.1797)\n",
      "Epoch: [153][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2263 (Avg-Loss1.2209)\tAcc 56.8878 (Avg-Acc56.9500)\n",
      "EPOCH: 153 Validation Results: Acc 56.950 Loss: 1.2209\n",
      "Best Accuracy: 56.95.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [154][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.0538 (Avg-Loss1.0538)\tAcc 64.0625 (Avg-Acc64.0625)\n",
      "Epoch: [154][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0713 (Avg-Loss1.0580)\tAcc 62.3047 (Avg-Acc63.0371)\n",
      "Epoch: [154][18/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.0969 (Avg-Loss1.0810)\tAcc 62.6953 (Avg-Acc62.1042)\n",
      "Epoch: [154][27/39]\tTime 0.019 (Avg-Time0.022)\t Loss 1.0957 (Avg-Loss1.0913)\tAcc 62.7930 (Avg-Acc61.5827)\n",
      "Epoch: [154][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1403 (Avg-Loss1.0966)\tAcc 59.5703 (Avg-Acc61.1882)\n",
      "Epoch: [154][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 0.9413 (Avg-Loss1.0955)\tAcc 70.3125 (Avg-Acc61.2350)\n",
      "EPOCH: 154 train Results: Acc 61.235 Loss: 1.0955\n",
      "Epoch: [154][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.1951 (Avg-Loss1.1951)\tAcc 59.0820 (Avg-Acc59.0820)\n",
      "Epoch: [154][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2492 (Avg-Loss1.2262)\tAcc 56.3776 (Avg-Acc56.9000)\n",
      "EPOCH: 154 Validation Results: Acc 56.900 Loss: 1.2262\n",
      "Best Accuracy: 56.9.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [155][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.0244 (Avg-Loss1.0244)\tAcc 62.4023 (Avg-Acc62.4023)\n",
      "Epoch: [155][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0966 (Avg-Loss1.0724)\tAcc 62.5000 (Avg-Acc62.0801)\n",
      "Epoch: [155][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0997 (Avg-Loss1.0846)\tAcc 62.2070 (Avg-Acc61.6776)\n",
      "Epoch: [155][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0766 (Avg-Loss1.0855)\tAcc 60.6445 (Avg-Acc61.7013)\n",
      "Epoch: [155][36/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.1042 (Avg-Loss1.0924)\tAcc 62.3047 (Avg-Acc61.5894)\n",
      "Epoch: [155][39/39]\tTime 0.003 (Avg-Time0.023)\t Loss 1.4121 (Avg-Loss1.0947)\tAcc 53.1250 (Avg-Acc61.4550)\n",
      "EPOCH: 155 train Results: Acc 61.455 Loss: 1.0947\n",
      "Epoch: [155][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.1996 (Avg-Loss1.1996)\tAcc 57.5195 (Avg-Acc57.5195)\n",
      "Epoch: [155][9/9]\tTime 0.007 (Avg-Time0.008)\t Loss 1.2462 (Avg-Loss1.2239)\tAcc 57.6531 (Avg-Acc56.6800)\n",
      "EPOCH: 155 Validation Results: Acc 56.680 Loss: 1.2239\n",
      "Best Accuracy: 56.68.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [156][0/39]\tTime 0.029 (Avg-Time0.029)\t Loss 1.0656 (Avg-Loss1.0656)\tAcc 60.3516 (Avg-Acc60.3516)\n",
      "Epoch: [156][9/39]\tTime 0.024 (Avg-Time0.030)\t Loss 1.0929 (Avg-Loss1.0663)\tAcc 60.2539 (Avg-Acc62.0215)\n",
      "Epoch: [156][18/39]\tTime 0.025 (Avg-Time0.027)\t Loss 1.0757 (Avg-Loss1.0712)\tAcc 61.3281 (Avg-Acc61.9089)\n",
      "Epoch: [156][27/39]\tTime 0.022 (Avg-Time0.026)\t Loss 1.0973 (Avg-Loss1.0887)\tAcc 61.9141 (Avg-Acc61.3525)\n",
      "Epoch: [156][36/39]\tTime 0.024 (Avg-Time0.025)\t Loss 1.1661 (Avg-Loss1.0985)\tAcc 58.3008 (Avg-Acc60.9797)\n",
      "Epoch: [156][39/39]\tTime 0.003 (Avg-Time0.024)\t Loss 1.1821 (Avg-Loss1.1014)\tAcc 53.1250 (Avg-Acc60.9125)\n",
      "EPOCH: 156 train Results: Acc 60.913 Loss: 1.1014\n",
      "Epoch: [156][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.2014 (Avg-Loss1.2014)\tAcc 58.3008 (Avg-Acc58.3008)\n",
      "Epoch: [156][9/9]\tTime 0.004 (Avg-Time0.007)\t Loss 1.2395 (Avg-Loss1.2312)\tAcc 57.3980 (Avg-Acc56.4900)\n",
      "EPOCH: 156 Validation Results: Acc 56.490 Loss: 1.2312\n",
      "Best Accuracy: 56.49.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [157][0/39]\tTime 0.030 (Avg-Time0.030)\t Loss 1.0860 (Avg-Loss1.0860)\tAcc 60.5469 (Avg-Acc60.5469)\n",
      "Epoch: [157][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0834 (Avg-Loss1.0853)\tAcc 61.8164 (Avg-Acc61.3867)\n",
      "Epoch: [157][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1303 (Avg-Loss1.0933)\tAcc 61.5234 (Avg-Acc61.0146)\n",
      "Epoch: [157][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0907 (Avg-Loss1.0969)\tAcc 62.0117 (Avg-Acc61.0700)\n",
      "Epoch: [157][36/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1329 (Avg-Loss1.1014)\tAcc 59.4727 (Avg-Acc60.9349)\n",
      "Epoch: [157][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 0.9852 (Avg-Loss1.1012)\tAcc 65.6250 (Avg-Acc60.9325)\n",
      "EPOCH: 157 train Results: Acc 60.932 Loss: 1.1012\n",
      "Epoch: [157][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1918 (Avg-Loss1.1918)\tAcc 58.0078 (Avg-Acc58.0078)\n",
      "Epoch: [157][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2449 (Avg-Loss1.2257)\tAcc 56.3776 (Avg-Acc56.6000)\n",
      "EPOCH: 157 Validation Results: Acc 56.600 Loss: 1.2257\n",
      "Best Accuracy: 56.6.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [158][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.0650 (Avg-Loss1.0650)\tAcc 62.0117 (Avg-Acc62.0117)\n",
      "Epoch: [158][9/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.0742 (Avg-Loss1.0938)\tAcc 62.3047 (Avg-Acc61.3477)\n",
      "Epoch: [158][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0875 (Avg-Loss1.0928)\tAcc 61.3281 (Avg-Acc61.3127)\n",
      "Epoch: [158][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1518 (Avg-Loss1.0927)\tAcc 58.7891 (Avg-Acc61.2444)\n",
      "Epoch: [158][36/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.0949 (Avg-Loss1.0949)\tAcc 61.8164 (Avg-Acc61.2516)\n",
      "Epoch: [158][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.1691 (Avg-Loss1.0976)\tAcc 54.6875 (Avg-Acc61.1775)\n",
      "EPOCH: 158 train Results: Acc 61.178 Loss: 1.0976\n",
      "Epoch: [158][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2040 (Avg-Loss1.2040)\tAcc 57.3242 (Avg-Acc57.3242)\n",
      "Epoch: [158][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2549 (Avg-Loss1.2302)\tAcc 56.2500 (Avg-Acc56.3000)\n",
      "EPOCH: 158 Validation Results: Acc 56.300 Loss: 1.2302\n",
      "Best Accuracy: 56.3.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [159][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.0173 (Avg-Loss1.0173)\tAcc 64.4531 (Avg-Acc64.4531)\n",
      "Epoch: [159][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0703 (Avg-Loss1.0685)\tAcc 60.6445 (Avg-Acc62.4414)\n",
      "Epoch: [159][18/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1087 (Avg-Loss1.0831)\tAcc 62.9883 (Avg-Acc61.5543)\n",
      "Epoch: [159][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0994 (Avg-Loss1.0925)\tAcc 61.2305 (Avg-Acc61.2898)\n",
      "Epoch: [159][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0674 (Avg-Loss1.0959)\tAcc 61.8164 (Avg-Acc61.2146)\n",
      "Epoch: [159][39/39]\tTime 0.003 (Avg-Time0.023)\t Loss 1.1165 (Avg-Loss1.0981)\tAcc 53.1250 (Avg-Acc61.0675)\n",
      "EPOCH: 159 train Results: Acc 61.068 Loss: 1.0981\n",
      "Epoch: [159][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1995 (Avg-Loss1.1995)\tAcc 59.3750 (Avg-Acc59.3750)\n",
      "Epoch: [159][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2529 (Avg-Loss1.2302)\tAcc 56.8878 (Avg-Acc56.6200)\n",
      "EPOCH: 159 Validation Results: Acc 56.620 Loss: 1.2302\n",
      "Best Accuracy: 56.62.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [160][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0286 (Avg-Loss1.0286)\tAcc 64.3555 (Avg-Acc64.3555)\n",
      "Epoch: [160][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.1222 (Avg-Loss1.0788)\tAcc 58.4961 (Avg-Acc61.7773)\n",
      "Epoch: [160][18/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.0746 (Avg-Loss1.0957)\tAcc 63.2812 (Avg-Acc61.0506)\n",
      "Epoch: [160][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0643 (Avg-Loss1.0980)\tAcc 62.6953 (Avg-Acc60.9061)\n",
      "Epoch: [160][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0825 (Avg-Loss1.0956)\tAcc 61.3281 (Avg-Acc60.9533)\n",
      "Epoch: [160][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2934 (Avg-Loss1.0968)\tAcc 57.8125 (Avg-Acc60.9100)\n",
      "EPOCH: 160 train Results: Acc 60.910 Loss: 1.0968\n",
      "Epoch: [160][0/9]\tTime 0.008 (Avg-Time0.008)\t Loss 1.1951 (Avg-Loss1.1951)\tAcc 58.5938 (Avg-Acc58.5938)\n",
      "Epoch: [160][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 1.2509 (Avg-Loss1.2294)\tAcc 56.1224 (Avg-Acc56.5800)\n",
      "EPOCH: 160 Validation Results: Acc 56.580 Loss: 1.2294\n",
      "Best Accuracy: 56.58.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [161][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0733 (Avg-Loss1.0733)\tAcc 62.1094 (Avg-Acc62.1094)\n",
      "Epoch: [161][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.0227 (Avg-Loss1.0661)\tAcc 63.5742 (Avg-Acc63.0176)\n",
      "Epoch: [161][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0870 (Avg-Loss1.0761)\tAcc 60.9375 (Avg-Acc62.2276)\n",
      "Epoch: [161][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1548 (Avg-Loss1.0858)\tAcc 60.0586 (Avg-Acc61.7327)\n",
      "Epoch: [161][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1453 (Avg-Loss1.0976)\tAcc 59.0820 (Avg-Acc61.4337)\n",
      "Epoch: [161][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.0574 (Avg-Loss1.0979)\tAcc 59.3750 (Avg-Acc61.4125)\n",
      "EPOCH: 161 train Results: Acc 61.413 Loss: 1.0979\n",
      "Epoch: [161][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2006 (Avg-Loss1.2006)\tAcc 58.8867 (Avg-Acc58.8867)\n",
      "Epoch: [161][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2429 (Avg-Loss1.2308)\tAcc 55.9949 (Avg-Acc56.3900)\n",
      "EPOCH: 161 Validation Results: Acc 56.390 Loss: 1.2308\n",
      "Best Accuracy: 56.39.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [162][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.0260 (Avg-Loss1.0260)\tAcc 63.5742 (Avg-Acc63.5742)\n",
      "Epoch: [162][9/39]\tTime 0.026 (Avg-Time0.022)\t Loss 1.1402 (Avg-Loss1.0821)\tAcc 59.3750 (Avg-Acc61.7676)\n",
      "Epoch: [162][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0515 (Avg-Loss1.0832)\tAcc 62.3047 (Avg-Acc61.7085)\n",
      "Epoch: [162][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0984 (Avg-Loss1.0879)\tAcc 61.3281 (Avg-Acc61.6176)\n",
      "Epoch: [162][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0982 (Avg-Loss1.0970)\tAcc 59.9609 (Avg-Acc61.3756)\n",
      "Epoch: [162][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.1085 (Avg-Loss1.0989)\tAcc 57.8125 (Avg-Acc61.2950)\n",
      "EPOCH: 162 train Results: Acc 61.295 Loss: 1.0989\n",
      "Epoch: [162][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.2014 (Avg-Loss1.2014)\tAcc 58.4961 (Avg-Acc58.4961)\n",
      "Epoch: [162][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2428 (Avg-Loss1.2259)\tAcc 56.1224 (Avg-Acc56.5900)\n",
      "EPOCH: 162 Validation Results: Acc 56.590 Loss: 1.2259\n",
      "Best Accuracy: 56.59.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [163][0/39]\tTime 0.027 (Avg-Time0.027)\t Loss 1.0380 (Avg-Loss1.0380)\tAcc 61.9141 (Avg-Acc61.9141)\n",
      "Epoch: [163][9/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.0448 (Avg-Loss1.0568)\tAcc 62.2070 (Avg-Acc62.3145)\n",
      "Epoch: [163][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1612 (Avg-Loss1.0783)\tAcc 59.0820 (Avg-Acc61.9449)\n",
      "Epoch: [163][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1355 (Avg-Loss1.0905)\tAcc 60.9375 (Avg-Acc61.5095)\n",
      "Epoch: [163][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1481 (Avg-Loss1.0952)\tAcc 60.2539 (Avg-Acc61.3994)\n",
      "Epoch: [163][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 0.9984 (Avg-Loss1.0952)\tAcc 59.3750 (Avg-Acc61.4100)\n",
      "EPOCH: 163 train Results: Acc 61.410 Loss: 1.0952\n",
      "Epoch: [163][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.1890 (Avg-Loss1.1890)\tAcc 58.7891 (Avg-Acc58.7891)\n",
      "Epoch: [163][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2510 (Avg-Loss1.2268)\tAcc 56.8878 (Avg-Acc56.4400)\n",
      "EPOCH: 163 Validation Results: Acc 56.440 Loss: 1.2268\n",
      "Best Accuracy: 56.44.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [164][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0423 (Avg-Loss1.0423)\tAcc 62.8906 (Avg-Acc62.8906)\n",
      "Epoch: [164][9/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.0981 (Avg-Loss1.0460)\tAcc 60.5469 (Avg-Acc63.1445)\n",
      "Epoch: [164][18/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.1303 (Avg-Loss1.0710)\tAcc 60.7422 (Avg-Acc62.1659)\n",
      "Epoch: [164][27/39]\tTime 0.024 (Avg-Time0.026)\t Loss 1.1543 (Avg-Loss1.0864)\tAcc 60.0586 (Avg-Acc61.5409)\n",
      "Epoch: [164][36/39]\tTime 0.021 (Avg-Time0.025)\t Loss 1.1623 (Avg-Loss1.0898)\tAcc 59.1797 (Avg-Acc61.4838)\n",
      "Epoch: [164][39/39]\tTime 0.005 (Avg-Time0.024)\t Loss 1.0140 (Avg-Loss1.0903)\tAcc 62.5000 (Avg-Acc61.4050)\n",
      "EPOCH: 164 train Results: Acc 61.405 Loss: 1.0903\n",
      "Epoch: [164][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1997 (Avg-Loss1.1997)\tAcc 57.9102 (Avg-Acc57.9102)\n",
      "Epoch: [164][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2507 (Avg-Loss1.2255)\tAcc 56.3776 (Avg-Acc56.6300)\n",
      "EPOCH: 164 Validation Results: Acc 56.630 Loss: 1.2255\n",
      "Best Accuracy: 56.63.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [165][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0213 (Avg-Loss1.0213)\tAcc 64.8438 (Avg-Acc64.8438)\n",
      "Epoch: [165][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0872 (Avg-Loss1.0711)\tAcc 60.8398 (Avg-Acc62.5977)\n",
      "Epoch: [165][18/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1267 (Avg-Loss1.0827)\tAcc 60.8398 (Avg-Acc61.9963)\n",
      "Epoch: [165][27/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.1238 (Avg-Loss1.0811)\tAcc 60.1562 (Avg-Acc61.9385)\n",
      "Epoch: [165][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1245 (Avg-Loss1.0904)\tAcc 60.6445 (Avg-Acc61.6105)\n",
      "Epoch: [165][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.0483 (Avg-Loss1.0924)\tAcc 60.9375 (Avg-Acc61.5325)\n",
      "EPOCH: 165 train Results: Acc 61.532 Loss: 1.0924\n",
      "Epoch: [165][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1903 (Avg-Loss1.1903)\tAcc 59.7656 (Avg-Acc59.7656)\n",
      "Epoch: [165][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2432 (Avg-Loss1.2264)\tAcc 56.5051 (Avg-Acc56.6100)\n",
      "EPOCH: 165 Validation Results: Acc 56.610 Loss: 1.2264\n",
      "Best Accuracy: 56.61.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [166][0/39]\tTime 0.029 (Avg-Time0.029)\t Loss 1.0260 (Avg-Loss1.0260)\tAcc 62.1094 (Avg-Acc62.1094)\n",
      "Epoch: [166][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0775 (Avg-Loss1.0609)\tAcc 60.8398 (Avg-Acc62.2070)\n",
      "Epoch: [166][18/39]\tTime 0.026 (Avg-Time0.022)\t Loss 1.0203 (Avg-Loss1.0787)\tAcc 63.3789 (Avg-Acc61.4823)\n",
      "Epoch: [166][27/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1353 (Avg-Loss1.0856)\tAcc 59.3750 (Avg-Acc61.3316)\n",
      "Epoch: [166][36/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.1219 (Avg-Loss1.0908)\tAcc 59.4727 (Avg-Acc61.3281)\n",
      "Epoch: [166][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 0.9462 (Avg-Loss1.0909)\tAcc 62.5000 (Avg-Acc61.2850)\n",
      "EPOCH: 166 train Results: Acc 61.285 Loss: 1.0909\n",
      "Epoch: [166][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1901 (Avg-Loss1.1901)\tAcc 58.8867 (Avg-Acc58.8867)\n",
      "Epoch: [166][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2291 (Avg-Loss1.2237)\tAcc 56.8878 (Avg-Acc56.7000)\n",
      "EPOCH: 166 Validation Results: Acc 56.700 Loss: 1.2237\n",
      "Best Accuracy: 56.7.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [167][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0622 (Avg-Loss1.0622)\tAcc 60.4492 (Avg-Acc60.4492)\n",
      "Epoch: [167][9/39]\tTime 0.025 (Avg-Time0.022)\t Loss 0.9829 (Avg-Loss1.0509)\tAcc 65.0391 (Avg-Acc62.8613)\n",
      "Epoch: [167][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0989 (Avg-Loss1.0674)\tAcc 62.3047 (Avg-Acc62.4332)\n",
      "Epoch: [167][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0849 (Avg-Loss1.0759)\tAcc 63.5742 (Avg-Acc62.1582)\n",
      "Epoch: [167][36/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.1362 (Avg-Loss1.0835)\tAcc 59.1797 (Avg-Acc61.8560)\n",
      "Epoch: [167][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2755 (Avg-Loss1.0848)\tAcc 59.3750 (Avg-Acc61.7600)\n",
      "EPOCH: 167 train Results: Acc 61.760 Loss: 1.0848\n",
      "Epoch: [167][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1911 (Avg-Loss1.1911)\tAcc 58.4961 (Avg-Acc58.4961)\n",
      "Epoch: [167][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2436 (Avg-Loss1.2260)\tAcc 55.9949 (Avg-Acc56.7300)\n",
      "EPOCH: 167 Validation Results: Acc 56.730 Loss: 1.2260\n",
      "Best Accuracy: 56.73.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [168][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.0463 (Avg-Loss1.0463)\tAcc 64.1602 (Avg-Acc64.1602)\n",
      "Epoch: [168][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0612 (Avg-Loss1.0738)\tAcc 61.4258 (Avg-Acc61.7090)\n",
      "Epoch: [168][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0843 (Avg-Loss1.0807)\tAcc 62.4023 (Avg-Acc61.4669)\n",
      "Epoch: [168][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0664 (Avg-Loss1.0839)\tAcc 63.1836 (Avg-Acc61.4920)\n",
      "Epoch: [168][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0752 (Avg-Loss1.0841)\tAcc 62.3047 (Avg-Acc61.5525)\n",
      "Epoch: [168][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.3861 (Avg-Loss1.0861)\tAcc 56.2500 (Avg-Acc61.5000)\n",
      "EPOCH: 168 train Results: Acc 61.500 Loss: 1.0861\n",
      "Epoch: [168][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.1911 (Avg-Loss1.1911)\tAcc 59.4727 (Avg-Acc59.4727)\n",
      "Epoch: [168][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2422 (Avg-Loss1.2275)\tAcc 56.8878 (Avg-Acc56.7100)\n",
      "EPOCH: 168 Validation Results: Acc 56.710 Loss: 1.2275\n",
      "Best Accuracy: 56.71.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [169][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.1163 (Avg-Loss1.1163)\tAcc 61.3281 (Avg-Acc61.3281)\n",
      "Epoch: [169][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.0259 (Avg-Loss1.0682)\tAcc 63.8672 (Avg-Acc62.0898)\n",
      "Epoch: [169][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.0915 (Avg-Loss1.0879)\tAcc 61.5234 (Avg-Acc61.6211)\n",
      "Epoch: [169][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1116 (Avg-Loss1.0947)\tAcc 61.7188 (Avg-Acc61.4432)\n",
      "Epoch: [169][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1445 (Avg-Loss1.0943)\tAcc 58.4961 (Avg-Acc61.2014)\n",
      "Epoch: [169][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.0946 (Avg-Loss1.0954)\tAcc 59.3750 (Avg-Acc61.1025)\n",
      "EPOCH: 169 train Results: Acc 61.102 Loss: 1.0954\n",
      "Epoch: [169][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1911 (Avg-Loss1.1911)\tAcc 58.4961 (Avg-Acc58.4961)\n",
      "Epoch: [169][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2422 (Avg-Loss1.2242)\tAcc 56.5051 (Avg-Acc56.4300)\n",
      "EPOCH: 169 Validation Results: Acc 56.430 Loss: 1.2242\n",
      "Best Accuracy: 56.43.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [170][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 0.9690 (Avg-Loss0.9690)\tAcc 65.6250 (Avg-Acc65.6250)\n",
      "Epoch: [170][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1110 (Avg-Loss1.0626)\tAcc 61.3281 (Avg-Acc62.5195)\n",
      "Epoch: [170][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1480 (Avg-Loss1.0768)\tAcc 57.8125 (Avg-Acc61.3692)\n",
      "Epoch: [170][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0587 (Avg-Loss1.0832)\tAcc 62.2070 (Avg-Acc61.2619)\n",
      "Epoch: [170][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1116 (Avg-Loss1.0882)\tAcc 61.3281 (Avg-Acc61.3149)\n",
      "Epoch: [170][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.2167 (Avg-Loss1.0903)\tAcc 53.1250 (Avg-Acc61.2650)\n",
      "EPOCH: 170 train Results: Acc 61.265 Loss: 1.0903\n",
      "Epoch: [170][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2054 (Avg-Loss1.2054)\tAcc 58.1055 (Avg-Acc58.1055)\n",
      "Epoch: [170][9/9]\tTime 0.004 (Avg-Time0.007)\t Loss 1.2374 (Avg-Loss1.2300)\tAcc 57.2704 (Avg-Acc56.6200)\n",
      "EPOCH: 170 Validation Results: Acc 56.620 Loss: 1.2300\n",
      "Best Accuracy: 56.62.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [171][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.0726 (Avg-Loss1.0726)\tAcc 61.3281 (Avg-Acc61.3281)\n",
      "Epoch: [171][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0651 (Avg-Loss1.0715)\tAcc 63.6719 (Avg-Acc62.0801)\n",
      "Epoch: [171][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1117 (Avg-Loss1.0771)\tAcc 60.2539 (Avg-Acc61.8472)\n",
      "Epoch: [171][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0732 (Avg-Loss1.0784)\tAcc 61.6211 (Avg-Acc61.6176)\n",
      "Epoch: [171][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0831 (Avg-Loss1.0842)\tAcc 62.4023 (Avg-Acc61.6105)\n",
      "Epoch: [171][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.1836 (Avg-Loss1.0866)\tAcc 54.6875 (Avg-Acc61.5200)\n",
      "EPOCH: 171 train Results: Acc 61.520 Loss: 1.0866\n",
      "Epoch: [171][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1926 (Avg-Loss1.1926)\tAcc 58.5938 (Avg-Acc58.5938)\n",
      "Epoch: [171][9/9]\tTime 0.006 (Avg-Time0.007)\t Loss 1.2364 (Avg-Loss1.2286)\tAcc 55.6122 (Avg-Acc56.4400)\n",
      "EPOCH: 171 Validation Results: Acc 56.440 Loss: 1.2286\n",
      "Best Accuracy: 56.44.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [172][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0570 (Avg-Loss1.0570)\tAcc 61.0352 (Avg-Acc61.0352)\n",
      "Epoch: [172][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1102 (Avg-Loss1.0773)\tAcc 60.1562 (Avg-Acc62.3535)\n",
      "Epoch: [172][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0739 (Avg-Loss1.0770)\tAcc 62.3047 (Avg-Acc62.1197)\n",
      "Epoch: [172][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0763 (Avg-Loss1.0819)\tAcc 62.2070 (Avg-Acc61.9455)\n",
      "Epoch: [172][36/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.1085 (Avg-Loss1.0850)\tAcc 60.8398 (Avg-Acc61.7029)\n",
      "Epoch: [172][39/39]\tTime 0.003 (Avg-Time0.023)\t Loss 0.9785 (Avg-Loss1.0863)\tAcc 67.1875 (Avg-Acc61.6175)\n",
      "EPOCH: 172 train Results: Acc 61.617 Loss: 1.0863\n",
      "Epoch: [172][0/9]\tTime 0.009 (Avg-Time0.009)\t Loss 1.1873 (Avg-Loss1.1873)\tAcc 59.0820 (Avg-Acc59.0820)\n",
      "Epoch: [172][9/9]\tTime 0.006 (Avg-Time0.008)\t Loss 1.2323 (Avg-Loss1.2283)\tAcc 57.3980 (Avg-Acc56.8400)\n",
      "EPOCH: 172 Validation Results: Acc 56.840 Loss: 1.2283\n",
      "Best Accuracy: 56.84.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [173][0/39]\tTime 0.030 (Avg-Time0.030)\t Loss 1.0562 (Avg-Loss1.0562)\tAcc 62.4023 (Avg-Acc62.4023)\n",
      "Epoch: [173][9/39]\tTime 0.023 (Avg-Time0.027)\t Loss 1.0888 (Avg-Loss1.0673)\tAcc 61.2305 (Avg-Acc62.0312)\n",
      "Epoch: [173][18/39]\tTime 0.024 (Avg-Time0.025)\t Loss 1.1260 (Avg-Loss1.0763)\tAcc 60.5469 (Avg-Acc61.8010)\n",
      "Epoch: [173][27/39]\tTime 0.021 (Avg-Time0.024)\t Loss 1.0949 (Avg-Loss1.0784)\tAcc 62.5977 (Avg-Acc61.6978)\n",
      "Epoch: [173][36/39]\tTime 0.022 (Avg-Time0.024)\t Loss 1.1305 (Avg-Loss1.0860)\tAcc 61.0352 (Avg-Acc61.6026)\n",
      "Epoch: [173][39/39]\tTime 0.004 (Avg-Time0.023)\t Loss 1.1823 (Avg-Loss1.0873)\tAcc 54.6875 (Avg-Acc61.5775)\n",
      "EPOCH: 173 train Results: Acc 61.578 Loss: 1.0873\n",
      "Epoch: [173][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.1866 (Avg-Loss1.1866)\tAcc 59.1797 (Avg-Acc59.1797)\n",
      "Epoch: [173][9/9]\tTime 0.004 (Avg-Time0.007)\t Loss 1.2192 (Avg-Loss1.2216)\tAcc 58.1633 (Avg-Acc56.8400)\n",
      "EPOCH: 173 Validation Results: Acc 56.840 Loss: 1.2216\n",
      "Best Accuracy: 56.84.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [174][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0078 (Avg-Loss1.0078)\tAcc 64.1602 (Avg-Acc64.1602)\n",
      "Epoch: [174][9/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.0858 (Avg-Loss1.0646)\tAcc 61.3281 (Avg-Acc62.1484)\n",
      "Epoch: [174][18/39]\tTime 0.020 (Avg-Time0.023)\t Loss 1.1096 (Avg-Loss1.0775)\tAcc 60.0586 (Avg-Acc61.4258)\n",
      "Epoch: [174][27/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.1001 (Avg-Loss1.0847)\tAcc 60.9375 (Avg-Acc61.3874)\n",
      "Epoch: [174][36/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.0454 (Avg-Loss1.0805)\tAcc 61.7188 (Avg-Acc61.5234)\n",
      "Epoch: [174][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 0.9904 (Avg-Loss1.0833)\tAcc 65.6250 (Avg-Acc61.3675)\n",
      "EPOCH: 174 train Results: Acc 61.367 Loss: 1.0833\n",
      "Epoch: [174][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1900 (Avg-Loss1.1900)\tAcc 56.9336 (Avg-Acc56.9336)\n",
      "Epoch: [174][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2260 (Avg-Loss1.2239)\tAcc 56.8878 (Avg-Acc56.6300)\n",
      "EPOCH: 174 Validation Results: Acc 56.630 Loss: 1.2239\n",
      "Best Accuracy: 56.63.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [175][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.0598 (Avg-Loss1.0598)\tAcc 61.9141 (Avg-Acc61.9141)\n",
      "Epoch: [175][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.1032 (Avg-Loss1.0614)\tAcc 60.5469 (Avg-Acc62.3145)\n",
      "Epoch: [175][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1811 (Avg-Loss1.0766)\tAcc 56.0547 (Avg-Acc61.7136)\n",
      "Epoch: [175][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.0748 (Avg-Loss1.0770)\tAcc 61.2305 (Avg-Acc61.7362)\n",
      "Epoch: [175][36/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.0587 (Avg-Loss1.0817)\tAcc 61.3281 (Avg-Acc61.5287)\n",
      "Epoch: [175][39/39]\tTime 0.002 (Avg-Time0.023)\t Loss 0.8408 (Avg-Loss1.0840)\tAcc 70.3125 (Avg-Acc61.4325)\n",
      "EPOCH: 175 train Results: Acc 61.432 Loss: 1.0840\n",
      "Epoch: [175][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.1825 (Avg-Loss1.1825)\tAcc 60.4492 (Avg-Acc60.4492)\n",
      "Epoch: [175][9/9]\tTime 0.004 (Avg-Time0.007)\t Loss 1.2324 (Avg-Loss1.2216)\tAcc 57.5255 (Avg-Acc57.1400)\n",
      "EPOCH: 175 Validation Results: Acc 57.140 Loss: 1.2216\n",
      "Best Accuracy: 57.14.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [176][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.0578 (Avg-Loss1.0578)\tAcc 62.8906 (Avg-Acc62.8906)\n",
      "Epoch: [176][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.0996 (Avg-Loss1.0612)\tAcc 60.3516 (Avg-Acc62.6465)\n",
      "Epoch: [176][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1048 (Avg-Loss1.0639)\tAcc 61.0352 (Avg-Acc62.4178)\n",
      "Epoch: [176][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0495 (Avg-Loss1.0714)\tAcc 62.2070 (Avg-Acc62.1966)\n",
      "Epoch: [176][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0520 (Avg-Loss1.0770)\tAcc 62.6953 (Avg-Acc61.8903)\n",
      "Epoch: [176][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2270 (Avg-Loss1.0796)\tAcc 53.1250 (Avg-Acc61.7400)\n",
      "EPOCH: 176 train Results: Acc 61.740 Loss: 1.0796\n",
      "Epoch: [176][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1880 (Avg-Loss1.1880)\tAcc 59.2773 (Avg-Acc59.2773)\n",
      "Epoch: [176][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2288 (Avg-Loss1.2265)\tAcc 56.7602 (Avg-Acc57.0500)\n",
      "EPOCH: 176 Validation Results: Acc 57.050 Loss: 1.2265\n",
      "Best Accuracy: 57.05.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [177][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0757 (Avg-Loss1.0757)\tAcc 62.8906 (Avg-Acc62.8906)\n",
      "Epoch: [177][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0449 (Avg-Loss1.0664)\tAcc 64.1602 (Avg-Acc62.5195)\n",
      "Epoch: [177][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0527 (Avg-Loss1.0670)\tAcc 62.3047 (Avg-Acc62.2430)\n",
      "Epoch: [177][27/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.0977 (Avg-Loss1.0755)\tAcc 59.7656 (Avg-Acc61.9036)\n",
      "Epoch: [177][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0638 (Avg-Loss1.0786)\tAcc 62.6953 (Avg-Acc61.7663)\n",
      "Epoch: [177][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.0609 (Avg-Loss1.0809)\tAcc 65.6250 (Avg-Acc61.6850)\n",
      "EPOCH: 177 train Results: Acc 61.685 Loss: 1.0809\n",
      "Epoch: [177][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1944 (Avg-Loss1.1944)\tAcc 58.7891 (Avg-Acc58.7891)\n",
      "Epoch: [177][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2358 (Avg-Loss1.2259)\tAcc 56.6327 (Avg-Acc57.1300)\n",
      "EPOCH: 177 Validation Results: Acc 57.130 Loss: 1.2259\n",
      "Best Accuracy: 57.13.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [178][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 0.9563 (Avg-Loss0.9563)\tAcc 67.8711 (Avg-Acc67.8711)\n",
      "Epoch: [178][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.1315 (Avg-Loss1.0593)\tAcc 59.8633 (Avg-Acc62.5195)\n",
      "Epoch: [178][18/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.0998 (Avg-Loss1.0674)\tAcc 60.7422 (Avg-Acc62.1968)\n",
      "Epoch: [178][27/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1160 (Avg-Loss1.0739)\tAcc 60.0586 (Avg-Acc61.8862)\n",
      "Epoch: [178][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.1430 (Avg-Loss1.0782)\tAcc 58.4961 (Avg-Acc61.6290)\n",
      "Epoch: [178][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2412 (Avg-Loss1.0805)\tAcc 56.2500 (Avg-Acc61.5825)\n",
      "EPOCH: 178 train Results: Acc 61.583 Loss: 1.0805\n",
      "Epoch: [178][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2075 (Avg-Loss1.2075)\tAcc 58.7891 (Avg-Acc58.7891)\n",
      "Epoch: [178][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2456 (Avg-Loss1.2266)\tAcc 55.9949 (Avg-Acc57.0800)\n",
      "EPOCH: 178 Validation Results: Acc 57.080 Loss: 1.2266\n",
      "Best Accuracy: 57.08.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [179][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0847 (Avg-Loss1.0847)\tAcc 60.4492 (Avg-Acc60.4492)\n",
      "Epoch: [179][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0800 (Avg-Loss1.0537)\tAcc 61.3281 (Avg-Acc62.3438)\n",
      "Epoch: [179][18/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.0994 (Avg-Loss1.0705)\tAcc 60.2539 (Avg-Acc61.9912)\n",
      "Epoch: [179][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1004 (Avg-Loss1.0771)\tAcc 61.3281 (Avg-Acc61.6908)\n",
      "Epoch: [179][36/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.1194 (Avg-Loss1.0778)\tAcc 59.3750 (Avg-Acc61.7319)\n",
      "Epoch: [179][39/39]\tTime 0.002 (Avg-Time0.022)\t Loss 1.2683 (Avg-Loss1.0800)\tAcc 54.6875 (Avg-Acc61.6600)\n",
      "EPOCH: 179 train Results: Acc 61.660 Loss: 1.0800\n",
      "Epoch: [179][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.1930 (Avg-Loss1.1930)\tAcc 60.0586 (Avg-Acc60.0586)\n",
      "Epoch: [179][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2480 (Avg-Loss1.2306)\tAcc 57.2704 (Avg-Acc56.8300)\n",
      "EPOCH: 179 Validation Results: Acc 56.830 Loss: 1.2306\n",
      "Best Accuracy: 56.83.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [180][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0679 (Avg-Loss1.0679)\tAcc 63.4766 (Avg-Acc63.4766)\n",
      "Epoch: [180][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0585 (Avg-Loss1.0703)\tAcc 62.1094 (Avg-Acc61.9531)\n",
      "Epoch: [180][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0769 (Avg-Loss1.0784)\tAcc 60.4492 (Avg-Acc61.9552)\n",
      "Epoch: [180][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.0641 (Avg-Loss1.0810)\tAcc 63.3789 (Avg-Acc61.8652)\n",
      "Epoch: [180][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0665 (Avg-Loss1.0856)\tAcc 60.8398 (Avg-Acc61.5973)\n",
      "Epoch: [180][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.3067 (Avg-Loss1.0871)\tAcc 59.3750 (Avg-Acc61.5900)\n",
      "EPOCH: 180 train Results: Acc 61.590 Loss: 1.0871\n",
      "Epoch: [180][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.1950 (Avg-Loss1.1950)\tAcc 58.8867 (Avg-Acc58.8867)\n",
      "Epoch: [180][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2432 (Avg-Loss1.2287)\tAcc 56.2500 (Avg-Acc56.6800)\n",
      "EPOCH: 180 Validation Results: Acc 56.680 Loss: 1.2287\n",
      "Best Accuracy: 56.68.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [181][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 1.0727 (Avg-Loss1.0727)\tAcc 61.8164 (Avg-Acc61.8164)\n",
      "Epoch: [181][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.1043 (Avg-Loss1.0671)\tAcc 61.0352 (Avg-Acc61.7090)\n",
      "Epoch: [181][18/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0880 (Avg-Loss1.0782)\tAcc 62.5000 (Avg-Acc61.5337)\n",
      "Epoch: [181][27/39]\tTime 0.032 (Avg-Time0.025)\t Loss 1.0881 (Avg-Loss1.0846)\tAcc 60.0586 (Avg-Acc61.2898)\n",
      "Epoch: [181][36/39]\tTime 0.022 (Avg-Time0.025)\t Loss 1.0966 (Avg-Loss1.0854)\tAcc 59.0820 (Avg-Acc61.3967)\n",
      "Epoch: [181][39/39]\tTime 0.006 (Avg-Time0.025)\t Loss 1.3165 (Avg-Loss1.0866)\tAcc 45.3125 (Avg-Acc61.3200)\n",
      "EPOCH: 181 train Results: Acc 61.320 Loss: 1.0866\n",
      "Epoch: [181][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1837 (Avg-Loss1.1837)\tAcc 59.8633 (Avg-Acc59.8633)\n",
      "Epoch: [181][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2377 (Avg-Loss1.2273)\tAcc 57.0153 (Avg-Acc56.7000)\n",
      "EPOCH: 181 Validation Results: Acc 56.700 Loss: 1.2273\n",
      "Best Accuracy: 56.7.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [182][0/39]\tTime 0.031 (Avg-Time0.031)\t Loss 1.0581 (Avg-Loss1.0581)\tAcc 62.8906 (Avg-Acc62.8906)\n",
      "Epoch: [182][9/39]\tTime 0.022 (Avg-Time0.024)\t Loss 1.0702 (Avg-Loss1.0664)\tAcc 60.5469 (Avg-Acc62.5195)\n",
      "Epoch: [182][18/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.0899 (Avg-Loss1.0762)\tAcc 59.7656 (Avg-Acc62.0066)\n",
      "Epoch: [182][27/39]\tTime 0.025 (Avg-Time0.023)\t Loss 1.0993 (Avg-Loss1.0806)\tAcc 60.9375 (Avg-Acc61.8862)\n",
      "Epoch: [182][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0708 (Avg-Loss1.0832)\tAcc 62.1094 (Avg-Acc61.7425)\n",
      "Epoch: [182][39/39]\tTime 0.006 (Avg-Time0.023)\t Loss 0.9172 (Avg-Loss1.0820)\tAcc 67.1875 (Avg-Acc61.7650)\n",
      "EPOCH: 182 train Results: Acc 61.765 Loss: 1.0820\n",
      "Epoch: [182][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.1932 (Avg-Loss1.1932)\tAcc 59.4727 (Avg-Acc59.4727)\n",
      "Epoch: [182][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2370 (Avg-Loss1.2232)\tAcc 56.2500 (Avg-Acc56.5800)\n",
      "EPOCH: 182 Validation Results: Acc 56.580 Loss: 1.2232\n",
      "Best Accuracy: 56.58.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [183][0/39]\tTime 0.027 (Avg-Time0.027)\t Loss 0.9882 (Avg-Loss0.9882)\tAcc 65.6250 (Avg-Acc65.6250)\n",
      "Epoch: [183][9/39]\tTime 0.021 (Avg-Time0.024)\t Loss 1.0822 (Avg-Loss1.0450)\tAcc 59.8633 (Avg-Acc63.1445)\n",
      "Epoch: [183][18/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0876 (Avg-Loss1.0597)\tAcc 62.3047 (Avg-Acc62.5668)\n",
      "Epoch: [183][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0668 (Avg-Loss1.0667)\tAcc 62.4023 (Avg-Acc62.2105)\n",
      "Epoch: [183][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.1045 (Avg-Loss1.0734)\tAcc 61.4258 (Avg-Acc61.9695)\n",
      "Epoch: [183][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.0777 (Avg-Loss1.0730)\tAcc 60.9375 (Avg-Acc61.9900)\n",
      "EPOCH: 183 train Results: Acc 61.990 Loss: 1.0730\n",
      "Epoch: [183][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.1931 (Avg-Loss1.1931)\tAcc 58.9844 (Avg-Acc58.9844)\n",
      "Epoch: [183][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2480 (Avg-Loss1.2227)\tAcc 56.5051 (Avg-Acc56.8700)\n",
      "EPOCH: 183 Validation Results: Acc 56.870 Loss: 1.2227\n",
      "Best Accuracy: 56.87.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [184][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.0163 (Avg-Loss1.0163)\tAcc 62.8906 (Avg-Acc62.8906)\n",
      "Epoch: [184][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.0665 (Avg-Loss1.0521)\tAcc 62.5977 (Avg-Acc62.7344)\n",
      "Epoch: [184][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0700 (Avg-Loss1.0597)\tAcc 63.3789 (Avg-Acc62.6285)\n",
      "Epoch: [184][27/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0369 (Avg-Loss1.0668)\tAcc 62.1094 (Avg-Acc62.0954)\n",
      "Epoch: [184][36/39]\tTime 0.026 (Avg-Time0.023)\t Loss 1.0954 (Avg-Loss1.0740)\tAcc 60.4492 (Avg-Acc61.7557)\n",
      "Epoch: [184][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2153 (Avg-Loss1.0760)\tAcc 56.2500 (Avg-Acc61.7025)\n",
      "EPOCH: 184 train Results: Acc 61.703 Loss: 1.0760\n",
      "Epoch: [184][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1966 (Avg-Loss1.1966)\tAcc 58.4961 (Avg-Acc58.4961)\n",
      "Epoch: [184][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2296 (Avg-Loss1.2216)\tAcc 58.0357 (Avg-Acc57.0700)\n",
      "EPOCH: 184 Validation Results: Acc 57.070 Loss: 1.2216\n",
      "Best Accuracy: 57.07.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [185][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0659 (Avg-Loss1.0659)\tAcc 61.2305 (Avg-Acc61.2305)\n",
      "Epoch: [185][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0423 (Avg-Loss1.0564)\tAcc 63.4766 (Avg-Acc62.5098)\n",
      "Epoch: [185][18/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.0389 (Avg-Loss1.0726)\tAcc 62.8906 (Avg-Acc62.0477)\n",
      "Epoch: [185][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0757 (Avg-Loss1.0785)\tAcc 63.5742 (Avg-Acc61.9106)\n",
      "Epoch: [185][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0858 (Avg-Loss1.0811)\tAcc 60.8398 (Avg-Acc61.6290)\n",
      "Epoch: [185][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.1781 (Avg-Loss1.0815)\tAcc 54.6875 (Avg-Acc61.6725)\n",
      "EPOCH: 185 train Results: Acc 61.672 Loss: 1.0815\n",
      "Epoch: [185][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.1938 (Avg-Loss1.1938)\tAcc 59.1797 (Avg-Acc59.1797)\n",
      "Epoch: [185][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2256 (Avg-Loss1.2253)\tAcc 55.7398 (Avg-Acc56.7300)\n",
      "EPOCH: 185 Validation Results: Acc 56.730 Loss: 1.2253\n",
      "Best Accuracy: 56.73.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [186][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0676 (Avg-Loss1.0676)\tAcc 63.4766 (Avg-Acc63.4766)\n",
      "Epoch: [186][9/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.0726 (Avg-Loss1.0612)\tAcc 61.5234 (Avg-Acc62.7734)\n",
      "Epoch: [186][18/39]\tTime 0.020 (Avg-Time0.023)\t Loss 1.0701 (Avg-Loss1.0700)\tAcc 60.6445 (Avg-Acc62.0888)\n",
      "Epoch: [186][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0868 (Avg-Loss1.0762)\tAcc 61.1328 (Avg-Acc61.9210)\n",
      "Epoch: [186][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0785 (Avg-Loss1.0793)\tAcc 61.1328 (Avg-Acc61.8666)\n",
      "Epoch: [186][39/39]\tTime 0.001 (Avg-Time0.022)\t Loss 1.1332 (Avg-Loss1.0809)\tAcc 56.2500 (Avg-Acc61.7625)\n",
      "EPOCH: 186 train Results: Acc 61.763 Loss: 1.0809\n",
      "Epoch: [186][0/9]\tTime 0.004 (Avg-Time0.004)\t Loss 1.1914 (Avg-Loss1.1914)\tAcc 59.6680 (Avg-Acc59.6680)\n",
      "Epoch: [186][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2359 (Avg-Loss1.2267)\tAcc 57.6531 (Avg-Acc56.8600)\n",
      "EPOCH: 186 Validation Results: Acc 56.860 Loss: 1.2267\n",
      "Best Accuracy: 56.86.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [187][0/39]\tTime 0.021 (Avg-Time0.021)\t Loss 1.0331 (Avg-Loss1.0331)\tAcc 63.8672 (Avg-Acc63.8672)\n",
      "Epoch: [187][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.0528 (Avg-Loss1.0428)\tAcc 63.8672 (Avg-Acc63.2129)\n",
      "Epoch: [187][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 0.9962 (Avg-Loss1.0513)\tAcc 64.0625 (Avg-Acc62.6953)\n",
      "Epoch: [187][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0511 (Avg-Loss1.0646)\tAcc 63.8672 (Avg-Acc62.1896)\n",
      "Epoch: [187][36/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.1248 (Avg-Loss1.0706)\tAcc 60.1562 (Avg-Acc62.0276)\n",
      "Epoch: [187][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 0.9398 (Avg-Loss1.0711)\tAcc 62.5000 (Avg-Acc62.0475)\n",
      "EPOCH: 187 train Results: Acc 62.047 Loss: 1.0711\n",
      "Epoch: [187][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1910 (Avg-Loss1.1910)\tAcc 59.7656 (Avg-Acc59.7656)\n",
      "Epoch: [187][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2406 (Avg-Loss1.2245)\tAcc 57.1429 (Avg-Acc56.8900)\n",
      "EPOCH: 187 Validation Results: Acc 56.890 Loss: 1.2245\n",
      "Best Accuracy: 56.89.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [188][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0082 (Avg-Loss1.0082)\tAcc 64.0625 (Avg-Acc64.0625)\n",
      "Epoch: [188][9/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.0695 (Avg-Loss1.0440)\tAcc 61.5234 (Avg-Acc62.6562)\n",
      "Epoch: [188][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0901 (Avg-Loss1.0549)\tAcc 62.8906 (Avg-Acc62.6234)\n",
      "Epoch: [188][27/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.0585 (Avg-Loss1.0644)\tAcc 62.3047 (Avg-Acc62.2803)\n",
      "Epoch: [188][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0420 (Avg-Loss1.0614)\tAcc 64.2578 (Avg-Acc62.3839)\n",
      "Epoch: [188][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.0305 (Avg-Loss1.0635)\tAcc 64.0625 (Avg-Acc62.3175)\n",
      "EPOCH: 188 train Results: Acc 62.318 Loss: 1.0635\n",
      "Epoch: [188][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1833 (Avg-Loss1.1833)\tAcc 59.1797 (Avg-Acc59.1797)\n",
      "Epoch: [188][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2398 (Avg-Loss1.2216)\tAcc 58.0357 (Avg-Acc56.8500)\n",
      "EPOCH: 188 Validation Results: Acc 56.850 Loss: 1.2216\n",
      "Best Accuracy: 56.85.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [189][0/39]\tTime 0.025 (Avg-Time0.025)\t Loss 1.0120 (Avg-Loss1.0120)\tAcc 63.6719 (Avg-Acc63.6719)\n",
      "Epoch: [189][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0782 (Avg-Loss1.0427)\tAcc 61.5234 (Avg-Acc63.1543)\n",
      "Epoch: [189][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1671 (Avg-Loss1.0638)\tAcc 56.3477 (Avg-Acc62.3972)\n",
      "Epoch: [189][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0841 (Avg-Loss1.0738)\tAcc 59.9609 (Avg-Acc61.9699)\n",
      "Epoch: [189][36/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.0817 (Avg-Loss1.0776)\tAcc 61.0352 (Avg-Acc61.8006)\n",
      "Epoch: [189][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.4416 (Avg-Loss1.0775)\tAcc 50.0000 (Avg-Acc61.8300)\n",
      "EPOCH: 189 train Results: Acc 61.830 Loss: 1.0775\n",
      "Epoch: [189][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1755 (Avg-Loss1.1755)\tAcc 58.9844 (Avg-Acc58.9844)\n",
      "Epoch: [189][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2409 (Avg-Loss1.2174)\tAcc 57.9082 (Avg-Acc56.5700)\n",
      "EPOCH: 189 Validation Results: Acc 56.570 Loss: 1.2174\n",
      "Best Accuracy: 56.57.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [190][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0336 (Avg-Loss1.0336)\tAcc 63.3789 (Avg-Acc63.3789)\n",
      "Epoch: [190][9/39]\tTime 0.030 (Avg-Time0.025)\t Loss 1.1422 (Avg-Loss1.0647)\tAcc 58.4961 (Avg-Acc62.2168)\n",
      "Epoch: [190][18/39]\tTime 0.025 (Avg-Time0.027)\t Loss 1.0759 (Avg-Loss1.0760)\tAcc 59.9609 (Avg-Acc61.6571)\n",
      "Epoch: [190][27/39]\tTime 0.027 (Avg-Time0.026)\t Loss 1.0859 (Avg-Loss1.0815)\tAcc 60.9375 (Avg-Acc61.4816)\n",
      "Epoch: [190][36/39]\tTime 0.021 (Avg-Time0.025)\t Loss 1.1369 (Avg-Loss1.0797)\tAcc 60.0586 (Avg-Acc61.7003)\n",
      "Epoch: [190][39/39]\tTime 0.004 (Avg-Time0.024)\t Loss 1.0954 (Avg-Loss1.0811)\tAcc 62.5000 (Avg-Acc61.6900)\n",
      "EPOCH: 190 train Results: Acc 61.690 Loss: 1.0811\n",
      "Epoch: [190][0/9]\tTime 0.007 (Avg-Time0.007)\t Loss 1.1835 (Avg-Loss1.1835)\tAcc 59.7656 (Avg-Acc59.7656)\n",
      "Epoch: [190][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2405 (Avg-Loss1.2255)\tAcc 57.6531 (Avg-Acc56.7400)\n",
      "EPOCH: 190 Validation Results: Acc 56.740 Loss: 1.2255\n",
      "Best Accuracy: 56.74.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [191][0/39]\tTime 0.028 (Avg-Time0.028)\t Loss 1.0014 (Avg-Loss1.0014)\tAcc 65.6250 (Avg-Acc65.6250)\n",
      "Epoch: [191][9/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0844 (Avg-Loss1.0413)\tAcc 59.9609 (Avg-Acc62.9102)\n",
      "Epoch: [191][18/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0755 (Avg-Loss1.0595)\tAcc 60.7422 (Avg-Acc62.1505)\n",
      "Epoch: [191][27/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0976 (Avg-Loss1.0703)\tAcc 64.2578 (Avg-Acc62.0815)\n",
      "Epoch: [191][36/39]\tTime 0.024 (Avg-Time0.023)\t Loss 1.1310 (Avg-Loss1.0741)\tAcc 61.3281 (Avg-Acc61.9378)\n",
      "Epoch: [191][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.1571 (Avg-Loss1.0763)\tAcc 62.5000 (Avg-Acc61.8575)\n",
      "EPOCH: 191 train Results: Acc 61.858 Loss: 1.0763\n",
      "Epoch: [191][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1846 (Avg-Loss1.1846)\tAcc 59.6680 (Avg-Acc59.6680)\n",
      "Epoch: [191][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2316 (Avg-Loss1.2257)\tAcc 58.0357 (Avg-Acc56.8000)\n",
      "EPOCH: 191 Validation Results: Acc 56.800 Loss: 1.2257\n",
      "Best Accuracy: 56.8.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [192][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0682 (Avg-Loss1.0682)\tAcc 63.6719 (Avg-Acc63.6719)\n",
      "Epoch: [192][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0522 (Avg-Loss1.0567)\tAcc 63.4766 (Avg-Acc62.7637)\n",
      "Epoch: [192][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0252 (Avg-Loss1.0663)\tAcc 61.9141 (Avg-Acc62.1608)\n",
      "Epoch: [192][27/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.1067 (Avg-Loss1.0693)\tAcc 61.5234 (Avg-Acc62.1756)\n",
      "Epoch: [192][36/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1163 (Avg-Loss1.0764)\tAcc 60.1562 (Avg-Acc61.9220)\n",
      "Epoch: [192][39/39]\tTime 0.002 (Avg-Time0.021)\t Loss 1.1018 (Avg-Loss1.0766)\tAcc 62.5000 (Avg-Acc61.9625)\n",
      "EPOCH: 192 train Results: Acc 61.962 Loss: 1.0766\n",
      "Epoch: [192][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.1903 (Avg-Loss1.1903)\tAcc 58.6914 (Avg-Acc58.6914)\n",
      "Epoch: [192][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2372 (Avg-Loss1.2256)\tAcc 57.7806 (Avg-Acc56.9600)\n",
      "EPOCH: 192 Validation Results: Acc 56.960 Loss: 1.2256\n",
      "Best Accuracy: 56.96.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [193][0/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0210 (Avg-Loss1.0210)\tAcc 65.0391 (Avg-Acc65.0391)\n",
      "Epoch: [193][9/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.0402 (Avg-Loss1.0451)\tAcc 63.4766 (Avg-Acc63.1152)\n",
      "Epoch: [193][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.1124 (Avg-Loss1.0622)\tAcc 60.8398 (Avg-Acc62.3458)\n",
      "Epoch: [193][27/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.0896 (Avg-Loss1.0680)\tAcc 61.1328 (Avg-Acc62.1687)\n",
      "Epoch: [193][36/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.0352 (Avg-Loss1.0676)\tAcc 64.7461 (Avg-Acc62.2308)\n",
      "Epoch: [193][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.0755 (Avg-Loss1.0679)\tAcc 65.6250 (Avg-Acc62.2100)\n",
      "EPOCH: 193 train Results: Acc 62.210 Loss: 1.0679\n",
      "Epoch: [193][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1818 (Avg-Loss1.1818)\tAcc 59.5703 (Avg-Acc59.5703)\n",
      "Epoch: [193][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2396 (Avg-Loss1.2235)\tAcc 56.1224 (Avg-Acc56.7500)\n",
      "EPOCH: 193 Validation Results: Acc 56.750 Loss: 1.2235\n",
      "Best Accuracy: 56.75.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [194][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0850 (Avg-Loss1.0850)\tAcc 60.8398 (Avg-Acc60.8398)\n",
      "Epoch: [194][9/39]\tTime 0.024 (Avg-Time0.022)\t Loss 1.0686 (Avg-Loss1.0585)\tAcc 62.0117 (Avg-Acc62.2461)\n",
      "Epoch: [194][18/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0802 (Avg-Loss1.0600)\tAcc 61.3281 (Avg-Acc62.3612)\n",
      "Epoch: [194][27/39]\tTime 0.021 (Avg-Time0.023)\t Loss 1.1062 (Avg-Loss1.0678)\tAcc 59.3750 (Avg-Acc62.0082)\n",
      "Epoch: [194][36/39]\tTime 0.022 (Avg-Time0.023)\t Loss 1.0948 (Avg-Loss1.0708)\tAcc 59.8633 (Avg-Acc61.9352)\n",
      "Epoch: [194][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.0471 (Avg-Loss1.0704)\tAcc 65.6250 (Avg-Acc61.9850)\n",
      "EPOCH: 194 train Results: Acc 61.985 Loss: 1.0704\n",
      "Epoch: [194][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1893 (Avg-Loss1.1893)\tAcc 59.5703 (Avg-Acc59.5703)\n",
      "Epoch: [194][9/9]\tTime 0.004 (Avg-Time0.006)\t Loss 1.2346 (Avg-Loss1.2202)\tAcc 57.2704 (Avg-Acc56.8900)\n",
      "EPOCH: 194 Validation Results: Acc 56.890 Loss: 1.2202\n",
      "Best Accuracy: 56.89.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [195][0/39]\tTime 0.027 (Avg-Time0.027)\t Loss 0.9999 (Avg-Loss0.9999)\tAcc 65.0391 (Avg-Acc65.0391)\n",
      "Epoch: [195][9/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0413 (Avg-Loss1.0219)\tAcc 62.3047 (Avg-Acc63.9844)\n",
      "Epoch: [195][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0341 (Avg-Loss1.0491)\tAcc 62.3047 (Avg-Acc63.1579)\n",
      "Epoch: [195][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0320 (Avg-Loss1.0557)\tAcc 62.5977 (Avg-Acc62.7895)\n",
      "Epoch: [195][36/39]\tTime 0.023 (Avg-Time0.022)\t Loss 1.0866 (Avg-Loss1.0638)\tAcc 62.8906 (Avg-Acc62.4762)\n",
      "Epoch: [195][39/39]\tTime 0.003 (Avg-Time0.021)\t Loss 1.0407 (Avg-Loss1.0655)\tAcc 64.0625 (Avg-Acc62.4250)\n",
      "EPOCH: 195 train Results: Acc 62.425 Loss: 1.0655\n",
      "Epoch: [195][0/9]\tTime 0.005 (Avg-Time0.005)\t Loss 1.1805 (Avg-Loss1.1805)\tAcc 59.3750 (Avg-Acc59.3750)\n",
      "Epoch: [195][9/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.2477 (Avg-Loss1.2236)\tAcc 57.2704 (Avg-Acc56.7200)\n",
      "EPOCH: 195 Validation Results: Acc 56.720 Loss: 1.2236\n",
      "Best Accuracy: 56.72.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [196][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0188 (Avg-Loss1.0188)\tAcc 65.8203 (Avg-Acc65.8203)\n",
      "Epoch: [196][9/39]\tTime 0.025 (Avg-Time0.022)\t Loss 1.1320 (Avg-Loss1.0569)\tAcc 60.2539 (Avg-Acc63.0371)\n",
      "Epoch: [196][18/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0930 (Avg-Loss1.0516)\tAcc 62.3047 (Avg-Acc62.8444)\n",
      "Epoch: [196][27/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.0573 (Avg-Loss1.0591)\tAcc 61.6211 (Avg-Acc62.5942)\n",
      "Epoch: [196][36/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0496 (Avg-Loss1.0646)\tAcc 63.3789 (Avg-Acc62.5950)\n",
      "Epoch: [196][39/39]\tTime 0.005 (Avg-Time0.022)\t Loss 1.3283 (Avg-Loss1.0657)\tAcc 46.8750 (Avg-Acc62.5950)\n",
      "EPOCH: 196 train Results: Acc 62.595 Loss: 1.0657\n",
      "Epoch: [196][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1904 (Avg-Loss1.1904)\tAcc 59.6680 (Avg-Acc59.6680)\n",
      "Epoch: [196][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2506 (Avg-Loss1.2273)\tAcc 55.2296 (Avg-Acc56.6800)\n",
      "EPOCH: 196 Validation Results: Acc 56.680 Loss: 1.2273\n",
      "Best Accuracy: 56.68.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [197][0/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0516 (Avg-Loss1.0516)\tAcc 62.6953 (Avg-Acc62.6953)\n",
      "Epoch: [197][9/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0614 (Avg-Loss1.0553)\tAcc 61.5234 (Avg-Acc62.6465)\n",
      "Epoch: [197][18/39]\tTime 0.022 (Avg-Time0.022)\t Loss 1.0973 (Avg-Loss1.0636)\tAcc 59.3750 (Avg-Acc62.3098)\n",
      "Epoch: [197][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0612 (Avg-Loss1.0707)\tAcc 64.1602 (Avg-Acc62.0675)\n",
      "Epoch: [197][36/39]\tTime 0.023 (Avg-Time0.023)\t Loss 1.0884 (Avg-Loss1.0748)\tAcc 61.1328 (Avg-Acc61.8692)\n",
      "Epoch: [197][39/39]\tTime 0.004 (Avg-Time0.022)\t Loss 1.2769 (Avg-Loss1.0771)\tAcc 53.1250 (Avg-Acc61.8175)\n",
      "EPOCH: 197 train Results: Acc 61.818 Loss: 1.0771\n",
      "Epoch: [197][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1899 (Avg-Loss1.1899)\tAcc 59.7656 (Avg-Acc59.7656)\n",
      "Epoch: [197][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2426 (Avg-Loss1.2244)\tAcc 58.2908 (Avg-Acc57.0900)\n",
      "EPOCH: 197 Validation Results: Acc 57.090 Loss: 1.2244\n",
      "Best Accuracy: 57.09.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [198][0/39]\tTime 0.024 (Avg-Time0.024)\t Loss 0.9856 (Avg-Loss0.9856)\tAcc 65.5273 (Avg-Acc65.5273)\n",
      "Epoch: [198][9/39]\tTime 0.023 (Avg-Time0.024)\t Loss 1.0643 (Avg-Loss1.0525)\tAcc 61.6211 (Avg-Acc62.8125)\n",
      "Epoch: [198][18/39]\tTime 0.028 (Avg-Time0.023)\t Loss 1.0823 (Avg-Loss1.0641)\tAcc 62.1094 (Avg-Acc62.4897)\n",
      "Epoch: [198][27/39]\tTime 0.020 (Avg-Time0.023)\t Loss 1.1072 (Avg-Loss1.0668)\tAcc 60.9375 (Avg-Acc62.4372)\n",
      "Epoch: [198][36/39]\tTime 0.028 (Avg-Time0.023)\t Loss 1.0662 (Avg-Loss1.0716)\tAcc 62.1094 (Avg-Acc62.1727)\n",
      "Epoch: [198][39/39]\tTime 0.002 (Avg-Time0.023)\t Loss 1.1194 (Avg-Loss1.0730)\tAcc 53.1250 (Avg-Acc62.1200)\n",
      "EPOCH: 198 train Results: Acc 62.120 Loss: 1.0730\n",
      "Epoch: [198][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1761 (Avg-Loss1.1761)\tAcc 60.2539 (Avg-Acc60.2539)\n",
      "Epoch: [198][9/9]\tTime 0.007 (Avg-Time0.008)\t Loss 1.2455 (Avg-Loss1.2276)\tAcc 56.8878 (Avg-Acc56.4700)\n",
      "EPOCH: 198 Validation Results: Acc 56.470 Loss: 1.2276\n",
      "Best Accuracy: 56.47.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [199][0/39]\tTime 0.033 (Avg-Time0.033)\t Loss 1.0110 (Avg-Loss1.0110)\tAcc 64.4531 (Avg-Acc64.4531)\n",
      "Epoch: [199][9/39]\tTime 0.022 (Avg-Time0.025)\t Loss 1.0611 (Avg-Loss1.0208)\tAcc 62.7930 (Avg-Acc63.9453)\n",
      "Epoch: [199][18/39]\tTime 0.027 (Avg-Time0.025)\t Loss 1.1397 (Avg-Loss1.0504)\tAcc 56.9336 (Avg-Acc62.6079)\n",
      "Epoch: [199][27/39]\tTime 0.021 (Avg-Time0.024)\t Loss 1.0275 (Avg-Loss1.0605)\tAcc 64.9414 (Avg-Acc62.4721)\n",
      "Epoch: [199][36/39]\tTime 0.022 (Avg-Time0.024)\t Loss 1.1020 (Avg-Loss1.0710)\tAcc 60.9375 (Avg-Acc62.0962)\n",
      "Epoch: [199][39/39]\tTime 0.005 (Avg-Time0.023)\t Loss 1.2060 (Avg-Loss1.0738)\tAcc 64.0625 (Avg-Acc61.9675)\n",
      "EPOCH: 199 train Results: Acc 61.968 Loss: 1.0738\n",
      "Epoch: [199][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1828 (Avg-Loss1.1828)\tAcc 59.8633 (Avg-Acc59.8633)\n",
      "Epoch: [199][9/9]\tTime 0.005 (Avg-Time0.006)\t Loss 1.2335 (Avg-Loss1.2220)\tAcc 57.6531 (Avg-Acc57.0600)\n",
      "EPOCH: 199 Validation Results: Acc 57.060 Loss: 1.2220\n",
      "Best Accuracy: 57.06.4f\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [200][0/39]\tTime 0.032 (Avg-Time0.032)\t Loss 1.0118 (Avg-Loss1.0118)\tAcc 63.7695 (Avg-Acc63.7695)\n",
      "Epoch: [200][9/39]\tTime 0.022 (Avg-Time0.024)\t Loss 1.0426 (Avg-Loss1.0432)\tAcc 64.3555 (Avg-Acc63.0859)\n",
      "Epoch: [200][18/39]\tTime 0.027 (Avg-Time0.023)\t Loss 1.0675 (Avg-Loss1.0594)\tAcc 61.8164 (Avg-Acc62.4692)\n",
      "Epoch: [200][27/39]\tTime 0.021 (Avg-Time0.022)\t Loss 1.0789 (Avg-Loss1.0602)\tAcc 62.3047 (Avg-Acc62.6081)\n",
      "Epoch: [200][36/39]\tTime 0.020 (Avg-Time0.022)\t Loss 1.0710 (Avg-Loss1.0663)\tAcc 62.3047 (Avg-Acc62.4736)\n",
      "Epoch: [200][39/39]\tTime 0.003 (Avg-Time0.022)\t Loss 1.2580 (Avg-Loss1.0682)\tAcc 53.1250 (Avg-Acc62.3450)\n",
      "EPOCH: 200 train Results: Acc 62.345 Loss: 1.0682\n",
      "Epoch: [200][0/9]\tTime 0.006 (Avg-Time0.006)\t Loss 1.1834 (Avg-Loss1.1834)\tAcc 58.8867 (Avg-Acc58.8867)\n",
      "Epoch: [200][9/9]\tTime 0.005 (Avg-Time0.007)\t Loss 1.2453 (Avg-Loss1.2199)\tAcc 56.5051 (Avg-Acc56.6000)\n",
      "EPOCH: 200 Validation Results: Acc 56.600 Loss: 1.2199\n",
      "Best Accuracy: 56.6.4f\n",
      "\n",
      "End time:  Fri Mar 28 16:06:22 2025\n",
      "train executed in 190.0223 seconds\n"
     ]
    }
   ],
   "execution_count": 183
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:06:46.624861Z",
     "start_time": "2025-03-28T05:06:46.556877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_hat=model._predict(test_dataloader.X)\n",
    "acc=accuracy(y_hat, test_dataloader.y)\n",
    "print(f\"Accuracy in Test Data is : {acc:.4f}%\")"
   ],
   "id": "fa5126bd65ef1423",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in Test Data is : 57.2000%\n"
     ]
    }
   ],
   "execution_count": 186
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
